<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/tags/2025-09/"><meta property="og:site_name" content=" "><meta property="og:title" content="2025-09"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>2025-09 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/tags/2025-09/><link rel=stylesheet href=/book.min.30a7836b6a89342da3b88e7afd1036166aeced16c8de12df060ded2031837886.css integrity="sha256-MKeDa2qJNC2juI56/RA2Fmrs7RbI3hLfBg3tIDGDeIY=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.8575bdc0ca02b9d35ac83cf3086aa234089a0f48cdce980be8704064bf26f8c3.js integrity="sha256-hXW9wMoCudNayDzzCGqiNAiaD0jNzpgL6HBAZL8m+MM=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=https://yshghid.github.io/tags/2025-09/index.xml title=" "></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/book/>글</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/be/>BE</a><ul></ul></li><li><a href=/docs/study/fe/>FE</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>2025-09</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><p><em>2025-09-23</em> ⋯ Langchain #2 RAG 기반 LLM API 서버 구축</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/be/be29/>1. 실습1 - LLM 질문-응답 Agent 구현 작업 위치 설정 백엔드 띄우기 프론트 띄우기 여기서 App.vue를 practice_LLM_App_front.vue의 내용으로 수정하기. 2. 실습2 - RAG 기반 LLM API 서버 구축 위치 설정, swagger ui 띄우기 cf) 패키지 버전 확인 실습 수행 1. 서버 실행 및 초기화 - 포트 8003에서 FastAPI 서버 실행 - 시작 시점에 SQLite 인메모리 DB(customers, products)가 자동 생성됨 - 벡터스토어 상태를 확인하면? - 아직 어떤 벡터DB도 초기화되지 않은 상태. 2. PDF 업로드 및 벡터화 - 엔드포인트: POST /upload-paper - Swagger UI - 로그 - PDF(paper-attention.pdf)를 업로드하면 RecursiveCharacterTextSplitter로 쪼개고, OpenAI 임베딩을 적용하여 FAISS에 저장해서 VECTORSTORE_PDF 초기화. 3. 내부 DB 테이블 확인 - 엔드포인트: GET /db-tables - Swagger UI - 로그 - { "tables": ["customers", "products"] } - 인메모리 SQLite에 두 개의 샘플 테이블(customers, products)이 준비되어 있음을 확인 4. 내부 테이블 벡터화 - 엔드포인트: POST /upload-dbtable?table=customers - Swagger UI - 로그 - customers 테이블 데이터가 벡터화되어 VECTORSTORE_INTERNAL 생성 5. 외부 웹 검색 데이터 벡터화 - 엔드포인트: POST /upload-topic?topic=생성형 AI - Swagger UI - 로그 - Naver 뉴스 API를 통해 생성형 AI 관련 기사 20개를 크롤링 -> OpenAI 임베딩 적용 -> VECTORSTORE_EXTERNAL 생성 완료. 6. RAG 질의 - 엔드포인트: POST /rag-query - Swagger UI - 로그 - 설명: 외부 벡터스토어에서 문맥을 검색 -> LLM(ChatGPT API, gpt-4o)을 통해 요약 답변을 생성. - 출처 표기도 괄호 형태로 포함시켜 “환각 최소화 + 근거 제시” 방식으로 동작. 7. 내부 파이프라인 셀프 테스트 - 엔드포인트: GET /selftest-internal - Swagger UI - 설명 - 실제 데이터 존재: "Enterprise" 반환 - 존재하지 않는 속성: "지식베이스에서 답을 찾을 수 없습니다." - 스키마 무관 질문(Attention?): "지식베이스에서 답을 찾을 수 없습니다." - 환각 억제 규칙이 잘 작동함을 보여줌. - internal로 해보기 - pdf로 해보기</a></p><hr><p><em>2025-09-22</em> ⋯ AI #2 HPO, XAI 실습</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/ai/ai38/>1. 실습 개요 - 목적
- UCI Breast Cancer 데이터를 로드하고 전처리 후 XGBoost 모델을 구축 및 평가
- 교차검증(StratifiedKFold, KFold)과 하이퍼파라미터 탐색 기법(RandomizedSearchCV, Optuna)을 비교하여 최적 성능을 도출
- SHAP을 활용하여 전역적·집단적·개별적 수준에서 해석력을 확보하고 도메인 지식과 연결
- 구현
- 데이터 로드: UCI Breast Cancer 데이터셋
- 데이터 전처리: 타겟(Diagnosis)을 이진화(M=1, B=0), StandardScaler로 범위 스케일링, 상관계수 0.9 이상인 중복 변수 제거
- 모델 구축: xgboost.XGBClassifier
- 모델 평가: 정확도, AUC, 분류리포트, 혼동행렬, feature importance
- 교차검증: KFold, StratifiedKFold
- 하이퍼파라미터 최적화: Random Search, Optuna TPE
- 모델 해석 (SHAP)
- Bar Summary Plot: 전역적 중요도(평균 |SHAP|)를 통해 주요 변수 확인
- Beeswarm Plot: 변수 값 크기(빨강/파랑)와 방향성(+/−)에 따른 분포 해석
- Force / Waterfall Plot: 3가지 개별 환자 샘플(예측 확률 극단/불확실, SHAP 영향력 최대, 도메인 특이 케이스)을 선택하여 모델이 어떤 요인 때문에 해당 예측을 내렸는지 설명 2. 실습 보고서 링크 - https://open-trust-407.notion.site/AI-2-XAI-274766ec530e80c8977cd13e3c27af84?source=copy_link 3. 프롬프팅1 - 하이퍼파라미터별 의미 - 값이 클수록?
- n_estimators (트리 개수): 크면 학습을 오래 시킴, 성능을 높일 수 있음, overfitting
- max_depth (트리 깊이): 깊으면 데이터의 세부 패턴까지 학습해 overfitting
- learning_rate (학습률): 크면 빠르게 학습하지만 최적점을 지나쳐 버릴 수 있어 overfitting. 작으면 한 스텝씩 조심스럽게 학습해 일반화 성능은 좋아지지만 많은 트리 n_estimators가 필요할수있음.
- subsample (샘플 비율): 크면 variance가 크고 overfitting.
- colsample_bytree (특성 샘플링 비율): 1.0이면 모든 피처를 쓰고 낮추면 랜덤성을 주므로 모델 다양성을 높여 overfitting 완화. 4. 프롬프팅2 - SHAP 플롯 해석법 1. Bar Summary Plot
- 봐야하는것: 막대 길이
- 막대 길이가 길수록 전체 모델 예측에서 해당 변수가 차지하는 중요도가 크다.
- 해석 포인트
- 막대 길이가 길다: 전역적으로 중요한 변수
- 막대 길이가 짧다: 영향력이 거의 없는 변수 2. Beeswarm Plot
- 봐야하는것: 막대 길이, 방향성, 색깔
- 막대 길이의 분포가 넓을수록 샘플별로 변수 효과가 다양하다.
- 방향성이 +일수록 예측값을 올린다, -일수록 예측값을 낮춘다.
- 색깔이 빨강 = 변수 값이 크다, 파랑 = 변수 값이 작다.
- 해석 포인트
- 막대길이와 방향성 조합
- 막대길이가 길고 방향성이 양수: 변수 값이 커질수록 예측 확률을 크게 올린다.
- 막대길이가 길고 방향성이 음수: 변수 값이 작아질수록 예측 확률을 크게 낮춘다.
- 막대길이가 짧거나 방향성이 양/음 혼재: 전체에 미치는 영향은 작음.
- 색깔과 방향성 조합
- 빨강이면서 방향성이 양수: 값이 클 때 예측값↑
- 파랑이면서 방향성이 음수: 값이 작을 때 예측값↓
- 원칙적으로는 위와 같은데
- 빨강이면서 음수에 몰림: 값이 클수록 오히려 예측값↓ (역효과)
- 파랑이면서 양수에 몰림: 값이 작을수록 오히려 예측값↑ (역효과)
- 이런 경우는 보통
- 비선형 관계: 변수 값이 커질수록 예측이 올라가다가, 일정 임계점 넘으면 오히려 내려가는 경우
- 상호작용 효과: 다른 변수와 조합됐을 때만 특정 방향으로 작용하는 경우. 3. Force / Waterfall Plot (개별 샘플 해석)
- 봐야하는것: 막대 길이, 방향성
- 막대 길이 길수록 해당 샘플의 예측에 기여한 정도가 크다.
- 방향성이 +일수록 예측값을 기준선(Base Value)에서 올리는 요인, -일수록 낮추는 요인.
- 해석 포인트
- 막대길이가 길고 방향성이 양수: 이 변수 때문에 해당 샘플의 예측 확률이 많이 올라감.
- 막대길이가 길고 방향성이 음수: 이 변수 때문에 해당 샘플의 예측 확률이 많이 내려감.
- 막대길이가 짧음: 해당 샘플에서는 영향이 거의 없음.</a></p><hr><p><em>2025-09-19</em> ⋯ AI #1 ML 방법론 기초</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/ai/ai36/>ML 방법론 - 통계기반 방법론은? - linear regression이나 logistic regression 같은걸 말함 - 가설과 근거가 명확히 세워져 있고 - 데이터가 알고리즘에 맞게 정제돼있고 - 통계적 유의성으로 결과가 나오는 깔끔한 방식 - ML 방법론은? - 작은 경연을 열듯 시행착오를 거치며 가장 적합한 모델을 찾는다는 컨셉이다. 지도 비지도 준지도 - 모두 입력 데이터에 존재하는 구조를 추론함 - 준지도 - 이상 탐지: 처럼 라벨링 비용이 클때 - 딥러닝: 은 파라미터 수가 많아 안정적인 학습을 위해 충분한 데이터가 필요한데 - 우선 라벨이 있는 데이터로 기본 학습을 진행하고 -> 라벨이 없는 데이터의 구조나 의사결정 경계를 활용해 모델을 보완함 regression, instance based algorithm - 알고리즘을 일하는 방식의 유사성에 따라 묶으면 regression, instance based algorithm. - regression - 선형 회귀 - 모델이 예측한 값과 실제 값의 오차를 측정하는 전형적인 선형 기반 방법이고 - 로지스틱 회귀 - 작은 선형 회귀들을 이어붙여 분류 문제를 푸는 방식. - instance based algorithm - 유사 사례를 구축해놓고 내가풀려는 케이스랑 유사한케이스를 찾아서 그걸기반으로 의사결정. - 정석적인정의는? - 데이터 그 자체를 중요한 정보로 삼아 의사결정을 내리고 - 유사 사례를 저장해 두었다가 새로운 입력이 들어오면 가장 비슷한 사례를 찾아 예측에 활용. - k-최근접 이웃(kNN) - 데이터 불균형 조정에도 활용 - Lazy Learning - 데이터만 잘 저장해 두면 학습이 끝난 것으로 볼 수 있지만 예측 시에는 거리 계산을 반복해야 하므로 데이터가 많을수록 연산이 무거워질 수 있다. - instance based algorithm의 대표는 kNN이고 느슨하게 해석하면 서포트 벡터만을 활용해 예측하는 SVM도 포함될 수 있다. - 다만 SVM은 정확히는 거리 기반이 아니라 커널 기반 모델이지만... - 전체데이터를 다 바라보는게 아니라 소수의 인스턴스(서포트벡터)에 집중한다는 점에서 유사하게 분류되기도 한다. 불편 추정량 (p.11) - 안정적인 추정량을 얻으려면? - 주어진 데이터로 기울기와 절편을 예측할건데 1. 기울기와 절편 추정치들의 분산이 작게 나와야한다. - 이를 위해서는 분자는 작고 분모는 커야 하고 - 표본 수 n이 많을수록 1/(n-1)이 작아져 (기울기와 절편의) 분산이 줄어들어 더 안정적인 추정이 가능하다. 2. 입력 변수 x의 분산은 충분히 커야 예측력이 높아진다. - 다만 x의 분산이 지나치게 넓으면 분류가 어렵고 반대로 값들이 한곳에 몰려 있으면 y를 구분하기 힘들다. linear 모델의 강건성 - 강건성 - 선형 모델을 선택하는이유? - 복잡한 모델에 비해 가정들을 잘 충족하며 다양한 상황에서 안정적으로 작동하기 때문에. - 복잡도를 높이면? - 학습 데이터에서는 성능이 향상되지만 실제 테스트 데이터에서는 어느 시점 이후 오히려 성능이 떨어지면서 오버피팅이 발생한다. - 데이터 분포가 조금만 바뀌어도 성능이 무너질수있는데 단순한 모델은 이런 변화에도 비교적 강건하게 대응한다 - 즉 조건이 바뀌거나 노이즈가 생기더라도 입력의 작은 변화가 출력에 크게 영향을 주지 않기 때문에 모델의 성과가 오래 유지된다. knn (p.14) - knn은 사람의 의사결정 방식에서 착안한 사례 기반 추론 알고리즘이다. - 새로운 사례가 등장했을 때 과거의 유사한 문제와 그 답을 참고해 판단을 내린다는 아이디어에 기반하는데 - 예를 들어 분류 문제에서. - 어떤 점이 별 모양인지 삼각형인지 결정하고 싶다고할때 그림에서 기준을 K=3으로 두면 가장 가까운 세 개 중 다수가 삼각형이므로 삼각형으로 분류되고 K=7로 두면 별이 더 많아져 별로 분류된다. - 회귀 문제에선 - 입력 변수가 하나일 때 테스트 포인트가 주어지면 가장 가까운 K개의 값을 찾아 그 평균이 예측값이된다 - K=3이라면 가까운 세 개의 y값을 평균내어 예측하고, K=1이라면 가장 가까운 하나의 값이 그대로 예측 결과가 된다 - 즉 K가 클수록 추정은 부드럽지만 세밀함이 줄고 K가 작을수록 개별 사례의 영향을 크게 받아 예측이 민감해진다 knn에서 좋은 이웃? - 좋은 이웃을 어떻게 정할까? 1. 어떤 유사도를 측정할까 - 보통 데이터를 벡터로 변환한 뒤 거리 기반으로 유사도를 평가함 - 이때 어떤 거리 메트릭을 쓸지? - 기본적으로 많이 쓰이는 것은 유클리디언 거리. 맨해튼 거리도 있고 이를 일반화*한 것이 민코프스키 거리. - 일반화? - 맨해튼은 두점사이 x축 y축 평행 거리. 유클리디언은 직선 거리. - 민코프스키는 맨해튼, 유클리디언 둘다에 해당하는 공식. 차수 p를 어떻게 주느냐에 따라 다른 거리가 나오고 p=1이면 맨해튼 p=2이면 유클리디언 p=∞이면 체비셰프 거리. - cf) - 어떤 속성을 거리 계산에 포함할지 얼마나 반영할지는 전처리 단계에서 결정되고 중요하지 않은 변수를 제거하거나 가중치를 달리 부여해 조정할 수 있다. - 어떤 속성을 거리 계산에 포함할지(중요하지 않은 변수를 제거) - KNN은 “학습으로 규칙을 만들어내는” 모델이 아니라 “그대로 두고 거리만 재서 판단하는” 모델이라서 예측의 성패가 모델 내부 파라미터가 아니라 우리가 미리 만들어 놓은 좌표계—즉 어떤 축들(특성)을 쓸지, 각 축을 얼마나 길게 혹은 짧게 잡을지—에 달려 있다. - 얼마나 반영할지 - 특정 축을 스케일링해서 더 길거나 짧게 만드는 일. - 중요한 특성에는 자를 늘려 그 방향 차이가 크게 반영되게 하고, 덜 중요한 특성에는 자를 줄여 그 차이가 작게 반영되게 만든다. - 수식으로 보면 특성마다 계수(스케일)를 곱해 좌표를 변환한 뒤 민코프스키 같은 거리 공식을 적용하는 것과 같다. 좌표계를 바꾸면 같은 두 점이라도 거리가 달라지고, 거리가 달라지면 “가까운 이웃”의 순위가 바뀌고, 결국 예측이 달라진다. 2. 가중치를 적용할것인가? - 단순 다수결(voting)을 쓰면 모든 이웃을 똑같이 취급하지만 실제로는 가까운 이웃이 더 중요하다고 보고 거리 기반 가중치를 적용할 수 있다 - (가장 가까운 두세 개 이웃은 크게 반영하고, 나머지는 약하게 반영하는 식) - 사이킷런 같은 라이브러리에서는 기본값이 uniform(모두 동일)이고 distance 옵션을 선택하면 거리에 반비례해 가중치를줄수있다. - cf) - 거리 기반 말고 다른것도있나? - “거리 기반”도 형태가 매우 다양하고, 랭크/커널/밀도/시간/클래스 비용 등 목적에 맞게 이웃의 표를 설계할 수 있다 데이터가 불균형·노이즈·개정 주기가 크다면 단순 distance기반보다 다른 전략들이 더 견고하게 먹히는 경우가 많다. - 순위(랭크) 가중치: 거리값 대신 “가까운 순서”로만 가중치 부여. 예) 1등=1, 2등=1/2, 3등=1/3 …처럼 내림 가중. - 커널 가중치: 가우시안, Epanechnikov, 삼각형 등 커널을 써서 부드럽게 감쇠. 수학적으론 거리 함수지만, 1/d 타입보다 훨씬 유연한 모양을 가짐. - 클래스/코스트 가중치: 불균형 완화를 위해 희소 클래스 표에 더 큰 가중. 실전에서는 리샘플링(SMOTE/ENN/CNN 등)이나 사후 의사결정 임계값 조정과 함께 씀. - 밀도/신뢰도 가중치: 이웃 점의 로컬 밀도(또는 LOF 같은 이상치 점수), 지역 정확도(leave-one-out 성능)로 신뢰 높은 이웃 표를 키우고, 의심스러운 이웃 표를 줄임. - 시간 감쇠 가중치: 시계열·온라인 데이터에서 최신 사례에 더 큰 표를 주는 방식. - 공유 최근접 이웃(SNN) 기반: 두 점이 “공유하는 이웃 수”로 유사도를 정의해 그 값으로 가중. 순수 거리 대신 그래프적 근접성을 씀 svm - 서포트벡터? - svm의 핵심은 두 집단을 가장 크게 벌려 나누는 선형 경계를 찾는 것인데 - 이때 경계에 가장 가까이 붙어 있는 점들이 ‘서포트 벡터’이고 - 마진을 최대로 하는 최적화 문제를 풀면 자연스럽게 어떤 점들이 서포트 벡터로 선택된다. - 마진을 최대로 한다? - “두 집단을 가르는 결정경계(직선/평면)를 중심으로, 양쪽 클래스가 비어 있는 완충지대(버퍼)를 가장 두껍게 만들자”. - 완충지대의 두께가 마진인데 마진이 두꺼울수록 경계가 흔들려도(노이즈·분포 미세 변화) 오분류로 넘어가기 어렵기 때문에 일반화가 좋아진다. - "데이터 전체를 안 쓴다"의 의미? - 해의 형태가 서포트 벡터에만 의존하므로 모든 표본이 아닌 경계 부근의 소수 표본만이 결정에 실질적으로 기여한다는 의미. - 덕분에 경계에서 멀리 떨어진 외곽 이상치의 영향은 상대적으로 작아 강건성이 생긴다. - 소프트 마진? - 현실 데이터의 노이즈를 허용하기 위해서 소프트 마진을 씀 - 위반 정도를 나타내는 슬랙 변수의 총합에 패널티를 주는 C를 함께 최소화한다. C를 크게 잡으면 위반에 대한 벌점이 커져 오류를 덜 허용하는 경계가, 작게 잡으면 더 너그러운 경계가 나온다. - svm에서 하이퍼파라미터? - 하이퍼파라미터는 모델이 학습을 통해 스스로 조정하는 값(예: 회귀계수, 신경망의 가중치)과 달리 학습 전에 사람이 직접 정해줘야 하는 설정값. - svm에서 C는 "오류를 얼마나 용인할것인지" - 결정 방법은? - 보통 validation set으로 성능을 비교하거나 교차검증을 돌리면서 가장좋은성능을주는값을 선택하거나 - 도메인 지식을 쓴다: 데이터가 매우 노이즈가 많다 하면 C를 크게 두는건 불리하니 오히려 작은 C가 적합할수있다 - 단점? - 기본적으로 이진 분류를 위한 알고리즘이어서 다중 클래스 문제를 다루기 위해서는 one-vs-one이나 one-vs-rest 같은 확장 방식을 사용해야하는데 - 피쳐 수가 많거나 라벨 종류가 늘어나면 학습과 예측 속도가 급격히 떨어질 수 있다. decision tree 기반 앙상블모델 - 결정트리 기반 앙상블(random forest, gradient boosting)은 각 특성의 실제 값에 따라 분기를 만들어 규칙을 쌓아 가고 - 분류에서는 불순도(지니·엔트로피)를 줄이고 회귀에서는 리프의 예측 오차(MSE·분산)를 줄이도록 학습한다. - 분류와 회귀의 이해 - 회귀를 “선을 긋는 것”, 분류를 “가까운 것끼리 묶는 것”으로 단순화하기보다는 - 회귀는 수치 오차를 최소화하는 함수 추정, 분류는 손실(또는 불순도)을 최소화하는 경계 학습으로 이해하는 것이 정확하다. - 과적합을 막기 - svm - svr(회귀) 과적합 제어는 ‘ε-무감도 손실(ε-insensitive)’과 C(위반 페널티)로 수행 - svc(분류) 에서는 ε를 쓰지 않고 소프트 마진 + C로 마진 위반을 얼마나 허용할지 제어 - C 커지면 복잡도 커져서 과적합 위험. - 트리 계열은 최대 깊이·리프 최소 표본 수·가지치기·학습률(부스팅)·트리 수(앙상블) 같은 복잡도 제어로 과적합을 막는다. - 결론 - SVR은 ε와 C(그리고 커널 파라미터)로, SVC는 C(와 커널 파라미터)로 과적합을 조절하고 트리 계열은 깊이·노드 최소 표본·가지치기·샘플링·학습률·트리 수(및 조기 종료)로 모델 복잡도를 관리한다. - svm과 트리앙상블 비교 결론 - 트리·앙상블은 값 기반 분기와 모델 복잡도 패널티로, SVM은 마진 최대화와 슬랙, C 조절로 강건성을 확보한다 - 둘 다 분류와 회귀에 쓸 수 있지만 과적합 제어의 수단과 최적화 목표가 다르다. 불순도 - 불순도가 낮다는 건 한 그룹 안에 같은 클래스가 많이 모여 있어 훨씬 명확하다는 뜻. 불순도가 높다는 건 한 그룹 안에 서로 다른 클래스가 많이 섞여 있어서 결과를 이해하기 어렵다는 뜻. - 어떤 방식으로 데이터를 나눠야 불순도가 더 많이 줄어들까? 해보기. 1. 처음 데이터의 Gini 지수가 0.42라면? - 꽤 섞여 있어서 완전히 깨끗하지 않은 상태. 2. 특성 A로 분할을 시도해 본다. - 왼쪽 그룹과 오른쪽 그룹으로 나누고 나서 다시 각 그룹의 불순도를 계산했는데 -> 두 그룹이 완전히 한쪽 클래스만 포함하게 되어 Gini 지수가 0이됨 - 원래 0.42였던 불순도가 0으로 줄었으니까 0.42만큼의 불순도가 줄어들었고 0.42만큼 정보를 얻었다. 3. 특성 B로 나눠본다. - 나누고 나니 -> 그룹 내부에 여전히 섞임이 남아 있고 Gini 지수가 0.342. - 원래 0.42에서 0.342로 줄었으니 0.078만큼의 불순도가 줄어들었다 4. 결론 - 불순도를 줄인 양을 봤을때 즉 정보 이득을 봤을때 특성 A로 나누는것이 정보 이득이 훨씬 크다고 결론내려서 해당 노드에서 분할A를 선택한다 - 결론 - 부모는 섞여 있었는데 자식으로 갈수록 덜 섞여 있으면, 그만큼 정보를 더 알아낸 것. - 분할을 통해 트리 성장 = 섞인 것을 덜 섞이게 만드는 방향으로 선을 긋고, 그 과정을 반복해서 더 순수한 그룹을 만드는 것.</a></p><hr><p><em>2025-09-17</em> ⋯ FastAPI #3 비동기 데이터베이스</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/be/be25/>main.py - main.py - fastapi app 서버를 구성. - fastapi 프레임워크 - 웹 요청이 들어오면 특정 함수로 연결해준다. - 연결 지점 = 엔드포인트. - ex) 누군가 브라우저에서 http://127.0.0.1:8001/hello를 호출하면, FastAPI는 이 요청을 보고 “아 이건 /hello 경로의 GET 요청이구나” 하고, 미리 등록해둔 hello() 함수를 실행한 뒤 그 반환값을 JSON으로 돌려준다. - fastapi 기본 구조 - 먼저 app = FastAPI()로 애플리케이션 객체를 만들고 - 그 뒤에 @app.get("/"), @app.get("/hello") 같은 데코레이터로 함수를 등록하기. - 보통 FastAPI는 /docs 주소로 들어가면 자동으로 Swagger UI라는 API 설명서가 나오지만 여기서는 app = FastAPI(docs_url=None)라고 작성해 기본 /docs 경로를 막음 - 나중에 직접 커스터마이징한 /docs 엔드포인트를 등록하려고. - 대신 / 경로에서는 단순히 "Welcome to the FastAPI server!"라는 메시지를 주고, /hello 경로에서는 "hello world!"라는 메시지를 줌. - 라우터? - 엔드포인트들을 별도 파일로 나누어 관리할 수 있는 기능. - 할 일(Task)을 관리하는 API, 완료(Done)를 관리하는 API처럼 종류별로 나누면 프로젝트가 훨씬 깔끔해진다. - app.include_router(task_a.router), app.include_router(done_a.router) - task_a.py 안에 정의된 라우터들을 불러와서 fastapi 앱에 등록한다. - /tasks 같은 엔드포인트들이 main.py에 직접 쓰여 있지 않아도 라우터 파일이 include되면서 실제 서버에서 동작한다. - 정적 파일(static files)은 fastapi에 연결해서 /static으로 접근. - favicon.ico - 웹 브라우저가 기본적으로 요청하는 아이콘 파일이기 때문에 @app.get("/favicon.ico") 엔드포인트를 만들어서 직접 반환한다. - swagger ui 커스터마이징 - swagger_favicon_url="/static/favicon.ico" - 지정한 아이콘을 Swagger UI 화면에 반영. - FastAPI와 SQLAlchemy를 이용해서 비동기 방식으로 데이터베이스와 연결하기 - 웹 애플리케이션이 데이터베이스와 소통하려면 - “어디에 있는 DB에, 어떤 계정으로 접속할 것인지”를 정하고 - 그 DB에 요청을 보냈다가 결과를 받는 과정을 반복한다. - 근데 단순히 한두 번 요청하는 게 아니라 수많은 요청을 동시에 처리해야 하므로 연결을 효율적으로 관리하는 체계가 필요함 - ASYNC_DB_URL - 데이터베이스 접속 주소. - async_engine - SQL 명령을 실행하는 데이터베이스 엔진 - echo=True - 실행되는 SQL 쿼리가 콘솔에 그대로 찍힌다 - future=True - SQLAlchemy의 최신 API 스타일을 쓰겠다. - 이 엔진을 통해 DB에 연결할 수 있다. - 세션(session) - DB에 연결해서 여러 쿼리를 실행하고 최종적으로 결과를 반영하거나 취소하는 과정 전체를 관리. - AsyncSessionLocal - 세션 팩토리 (세션을 필요할 때마다 새로 찍어내는 공장) - Base - SQLAlchemy에서 테이블 구조를 코드로 표현할 때 요 클래스를 쓴다고함. - declarative_base(cls=AsyncAttrs) - 비동기 처리를 지원하는 기능을 포함한 Base 클래스를 만들겠다 - Task, Done 같은 모델들은 모두 이 Base를 상속받아 정의된다이제. - get_db() - router에서 db: AsyncSession = Depends(get_db)라고 쓰면 - fastapi는 함수를 실행해서 세션을 꺼내고 작업이 끝나면 자동으로 커밋, 문제가 생기면 자동롤백, 끝나면 연결을 닫는다.</a></p><hr><p><em>2025-09-17</em> ⋯ FastAPI #2 논문 업로드 및 벡터화 API</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/be/be26/>1. 실행 제대로 떴으니깐 pdf 처리 해보기 질의응답: attention is all you need가 무엇인가? 답변 잘 나온다. RAG가 뭐냐고 물어보면? 논문에 없는건 답변하지말라고 햇기때문에 답안해줌 2. 코드</a></p><hr><p><em>2025-09-17</em> ⋯ FastAPI #1 MariaDB, DB Migration, Swagger UI</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/be/be24/>1. 실습 내용 maria db container 띄우기 - docker desktop에서도 확인 db connection - vscode에서 database client extension 열고 create connection cf) 컨테이너 기반 실습 환경을 구성하는 이유와 장점 - 컨테이너 기반 실습 환경을 구성하는 이유 - 목적1: 모두가 똑같은 환경에서 실습을 하기 위해 - 내부에서 설치된 라이브러리 버전이나 운영체제 차이 때문에 동일한 python-app.py를 실행해도 실행이안대거나 오류가날수있다. - Docker 컨테이너라는 상자 안에 Python 실행 환경을 일정하게 담아두고 Mac이든 Windows든 그 상자를 똑같이 실행시키면 누구든 동일한 환경에서 같은 결과를 낼수있으니까 환경 차이로 인한 오류가 없어진다 - 목적2: 작업 환경을 제한 - 단순히 Python 코드만 실행하는 것이 아니라 데이터베이스(DBMS)까지 연결해야 할 때가 많은데 오픈소스 데이터베이스인 MySQL이나 MariaDB 같은 프로그램을 직접 로컬에 설치할 수도 있지만 얘네는 운영체제에 따라 설치 과정이 복잡하고 하드웨어 자원에 의존적이라 충돌이나 오류가 발생하기 쉬운프로그램들이다. - Docker 컨테이너를 사용하면 데이터베이스를 별도의 격리된 공간에서 실행할 수 있다. FastAPI를 실행하는 컨테이너 하나, MySQL을 실행하는 컨테이너 하나를 띄워두고, 이 둘을 내부 네트워크로 연결해주는 식으로 작업하면 데이터베이스나 Python 실행 환경이 호스트 컴퓨터 전체를 더럽히지 않고, 필요 없을 때 컨테이너만 지우면 깨끗하게 정리된다. - uvicorn - uvicorn을 실행해 fastapi 앱을 실행한다. - api 서버가 실행되면 브라우저에서 127.0.0.1:8001로 접속하면 서버가 응답을 돌려줄수있다. - swagger ui - api를만들면 기능이 코드안에 가려져있어서 어떤 요청을 보내야 하고 어떤 응답이 돌아오는지 알기 어려운데 - swagger ui가 있으면 내가만든 fast api 서버가 swagger ui를 통해 “이런 엔드포인트들이 있습니다, 이런 식으로 요청을 보내면 되고, 이런 응답이 옵니다”를 자동으로 보여준다. - ui 화면에서 실제 api 요청도 보낼수있다. - routers/task_a.py → done_crud 임포트 및 done 여부 확인 부분 수정 - routers/done_a.py → DoneResponse 반환 시 done 필드 제거 - cruds/task_a.py → get_tasks_with_done, update_task에서 Done 여부 올바르게 체크 - schemas/task_a.py → done 필드 반드시 포함 - schemas/done_a.py → DoneResponse 정의 필요 (예: class DoneResponse(BaseModel): id: int) cf2 SQLAlchemy 로그 해석? - tasks와 dones 테이블을 조인해서 각 할 일이 완료되었는지 여부(done)를 계산 - FastAPI가 JSON으로 가공해 클라이언트에 반환. cf3 swagger ui에서 확인 실습정리 가상환경 만들기? - source ./demo-app/bin/activate - 나는 어케하는지몰라서 그냥 conda환경만들엇는데 갠찮겠지.. 필요 패키지 설치 - pip install fastapi “uvicorn[standard]” - pip install sqlalchemy aiomysql pymysql greenlet PathOperation 함수는 경로동작 함수 - 모듈 임포트하는 모든 경로에, `__init__.py` 만든다 2. 개념 - 목적 - Docker 컨테이너(MariaDB), SQLAlchemy, FastAPI, Uvicorn을 활용해 동일한 데이터베이스 환경에서 FastAPI 서버를 구축하고 Swagger UI를 통해 API 동작을 확인하는 것 - 구현 - 컨테이너 실행(Docker + MariaDB): Docker Compose를 이용해 MariaDB 컨테이너를 띄우고, 로컬 환경과 독립된 동일한 DB 환경을 구성함 - DB 연결(Database Client + SQLAlchemy): VS Code Database Client Extension과 SQLAlchemy를 통해 MariaDB에 연결해 테이블을 조회하고 쿼리를 실행함 - FastAPI 서버 실행(Uvicorn + FastAPI): FastAPI 앱을 uvicorn으로 구동하여 API 서버를 실행하고, 로컬 브라우저에서 엔드포인트에 접근 가능하게 함 - Swagger UI 확인(Swagger UI): 자동 문서화된 API 명세서를 통해 엔드포인트 구조와 요청/응답을 직관적으로 확인하고 직접 API 요청을 테스트함 - DB 마이그레이션(SQLAlchemy ORM): migrate_db_a 모듈을 실행해 tasks/dones 테이블을 자동 생성하고, 조인 쿼리를 통해 완료 여부를 조회하도록 구현함 - 라우터 및 스키마 수정(FastAPI routers/schemas): routers, cruds, schemas 모듈을 수정하여 done 여부를 올바르게 반영하고 DoneResponse를 정의하여 API 응답 형식을 보장함 - 의문점1 - (MariaDB를 로컬 운영체제에 직접 깔지 않고) Docker Compose를 이용해 MariaDB 컨테이너를 띄운 이유? - 답1 - mariadb같은 db를 로컬 환경에 깔려고 하면 운영체제마다 설치 방법도 다르고 버전 호환 문제도 많아서 똑같은 코드를 실행해도 어떤 컴퓨터에서는 잘 되고 다른 컴퓨터에서는 에러가 날수있다. - 이때 docker에 mariadb를 세팅된 상태로 담아두고 돌리면 2가지 이점이 있는데 - 맥이든 서버가 리눅스든 상관없이 항상 동일한 MariaDB 환경이 보장되고 - 삭제할때 컨테이너만 지우면 깨끗하게 정리돼서 추후 호환문제가 발생하는것도 방지할수있다. - 의문점2 - db를 왜 로컬환경에 설치하는가? db가 무엇인가? - 답2 - db는 많은 양의 데이터를 체계적으로 관리하고 동시에 여러 사용자가 빠르게 조회할 수 있도록 도와주는 시스템. - 엑셀 파일처럼 몇 줄짜리 데이터만 다룰 거라면 굳이 DB가 필요 없지만 - 웹 서비스나 API 서버를 만든다고 하면? - 예를 들어 할 일 관리 앱을 만든다고 하면 - 사용자가 추가한 작업들을 어딘가에 저장해 두었다가 나중에 다시 보여줘야하는데 - 만약 메모리에만 저장하면 서버가 꺼지는 순간 다 사라지고 파일로 저장하면 여러 사람이 동시에 접속해서 데이터를 읽고 쓰기 시작하면 꼬일수있다 그래서 신뢰성 있게 "데이터를 관리"할 수 있는 db가 필요하다! - 의문점3 - "데이터를 관리"한다란? - 답3 - 서비스를 구동하면 데이터가 들어오니까 데이터를 저장하고 조회하고 해야한다. - 의문점4 - VS Code Database Client Extension과 SQLAlchemy가 각각 하는일이 무엇인가? - 답4 - vscode db client extension을 쓰면 - vs code를 통해서 db에 들어있는 테이블이랑 적재된 데이터를 볼수있고 - db에 쿼리문을 입력해서 결과를 볼수있고 - 구조를 시각적으로 확인할수도있다. - 즉 db 상태를 빠르게확인하고 단순한수준의 조작을 할수있다. - sql alchemy를 쓰면 - 파이썬 객체와 데이터베이스 테이블을 연결할수있다. - 의문점5 - SQLAlchemy 의 기본 뼈대 Engine + Base + Session? - 답5 -</a></p><hr><p><em>2025-09-15</em> ⋯ Ray #1 Batch Prediction with Ray Core</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/be/be6/>스터디때 준비해갔던 Ray Core를 사용해서 batch prediction 수행하는 예제!! - batch prediction이 batch를 예측하는건줄알았는데(..) batch로 prediction하는것이었다. - 순서는 1. Task 기반 batch prediction 2. Actor 기반 batch prediction 3. GPU 기반 수행 코드 - 출처는 Ray Document의 Batch Prediction with Ray Core이다. 0. 개요 - 목적 - Parquet 형식의 대규모 데이터셋을 Ray를 이용해 분산 처리하며, 더미 모델을 로딩하여 배치 예측(batch prediction) 을 수행한다. - Task와 Actor 두 가지 실행 방식을 비교하고, CPU/GPU 자원 활용 차이를 이해한다. - 설계 - 데이터셋 분할: S3에 저장된 Parquet 파일(12 shards)을 불러와 분산 태스크 단위로 처리 - 모델 로딩: 더미 모델(load_model)을 정의하고 ray.put()을 통해 오브젝트 스토어에 1회 저장 - 배치 예측(Task 기반): @ray.remote 태스크로 각 shard를 병렬 예측, 결과 크기 반환 - 배치 예측(Actor 기반): BatchPredictor 클래스를 Ray Actor로 등록하고, ActorPool을 이용해 shard 분산 예측 - 자원 활용(CPU/GPU): CPU 환경에서는 기본 Task 실행, GPU 환경에서는 @ray.remote(num_gpus=1)를 사용해 GPU에서 모델을 실행하도록 구성 - 결과 확인: 각 shard에 대해 예측된 결과 크기를 출력하여 병렬 처리 동작을 검증 1. 코드 - 실습에서는 분산 처리 흐름을 보는 것이 핵심이기 때문에 실제 모델이 갖는 특성을 갖는 더미 모델을 생성해준다. - 실제 모델이 갖는 특성 = 정확히는 실제 모델이 갖는 특성 중 분산 처리에 관여하는 특성. - 실제 모델이 갖는 특성 2가지? 1. 큰 메모리 용량. 실제 머신러닝 모델, 특히 딥러닝 모델은 수백 MB에서 수 GB에 달하는 가중치 파라미터를 담고 있다 예를 들어 BERT나 GPT 같은 모델은 엄청난 수의 파라미터를 갖기 때문에, 한 노드에서 다른 노드로 옮길 때 그 자체로 데이터 전송 비용이 크므로 이를 구현해준다. 2. 입력 데이터를 받아서 변환된 출력을 만듭니다. 실제 모델은 어떤 입력(이미지, 텍스트, 테이블 데이터 등)을 받아서 예측값을 내놓으므로, 이를 구현해줍니다. - 구현 방법? 1. model.payload = np.zeros(100_000_000) - 큰 메모리의 가중치 파라미터를 담고 있음을 모방하는 코드. 모델이 내부적으로 “큰 덩어리” 데이터를 가진 객체처럼 보이며 이를 통해 Ray가 이 모델을 여러 노드에 배포할 때 진짜처럼 부담을 준다. 2. {"score": batch["passenger_count"] % 2 == 0} - 입력값을 받아서 예측값을 내놓음을 모방하는 코드. 모델은 dataframe을 input으로 받아 승객 수가 짝수냐 홀수냐를 판별한다 즉 “입력 데이터를 보고 뭔가 계산해서 새로운 결과를 만든다”라는 모델의 핵심 행위만 구현한다. 1. Task 기반 batch prediction - Ray에서 Task 기반 분산처리란? - 데이터 파일을 통째로 처리하지 않고 여러 조각(Task)으로 잘라 각 조각을 서로 다른 Worker에게 맡기기. - 코드 설명 - input_files - 2009년 뉴욕시 택시 데이터. parquet 포맷이며 12개 데이터로 구성 - function make_prediction(model, shard_path) - shard 파일 경로를 받아서 pyarrow.parquet.read_table(shard_path)로 데이터를 불러고 df로 변환해서 더미 모델 model에 입력 - 앞서 더미 모델인 model은 passenger_count 값이 짝수인지 여부를 판단해서 불리언 값으로 반환하는 모델이었다! - ray.put(model) - 모델이 큰 메모리 객체를 내부적으로 가지고 있고(payload=1억) 따라서 매번 모델을 직접 태스크로 전달하면 드라이버의 오브젝트 스토어가 과부하될 수 있다. - 그래서 ray.put(model)을 사용해서 모델을 오브젝트 스토어에 단 한 번만 저장하고 이후 태스크에는 그 참조값 model_ref 만 넘긴다. - 이렇게 해야 각 태스크가 동일한 모델을 공유하되 불필요한 데이터 복제가 발생하지 않는다. cf1 - 의문점1 - ray.put(model)을 해야 각 태스크가 동일한 모델을 공유하되 불필요한 데이터 복제가 발생하지 않는다고 했는데 - 모델을 ray.put()으로 한 번만 넣었을 때와, 매번 remote 호출마다 모델을 넘겼을 때 오브젝트 스토어 메모리 사용량 차이는 얼마일까? - 확인1 - Ray에서 메모리 현황을 ray memory 명령어를 통해 확인할 수 있음 - 위의 두 Case 에서 ray memory를 호출하여 메모리 사용량과 참조 개수를 확인해보면 Ray 오브젝트 스토어에 몇 개의 모델 사본이 올라갔는지, 그리고 참조 개수가 어떻게 달라졌는지를 확인해서 메모리 사용량 차이 확인이 가능! - 결과 - Mem Used by Objects 비교 - Good Case - 1178.0 B - Bad Case - 1235.0 B - 비슷한이유는뭘까? 더미 데이터에서 파라미터 부하를 모방한다고 작성한 np.zeros(100_000_000)은 실제로는 800MB짜리 배열이어야 하지만 Ray와 NumPy 내부에서 메모리 최적화 (zero-copy, lazy allocation) 때문에 실제 크기가 반영되지 않았고 ray memory 출력에서도 몇 백 byte 수준으로 나왔다. - Mem Used by Objects 비교 - 실제 숫자를 넣어줫다면? - model.payload = np.random.rand(100_000_000)처럼 랜덤 값을 채우면 실제 메모리가 할당되었을것이고(float64 → 약 800MB) - 이경우 Good Case (ray.put(model) 한 번)는 Mem Used by Objects ≈ 800MB, Bad Case (태스크 3개에 직접 모델 전달) Mem Used by Objects ≈ 2400MB (800MB × 3) 가 출력되었을것이다. 즉, 모델 크기 × 태스크 수 만큼 차이가 벌어지는 게 일반적인 결과! - Local References 비교 - Good Case - 16 - Bad Case - 19 - 결과설명? ray.put(model)을 호출 후 생성된 ObjectRef는 오브젝트 스토어에 저장된 모델을 가리키는 “포인터” 같은 역할을 한다. - Good Case에서는 드라이버 프로세스(파이썬에서 코드를 실행하는 쪽)와 태스크 실행 시 필요한 내부 참조들이 모두 합쳐져서 16개 참조가 생겼다 즉 모델 사본은 1개지만 그 사본을 가리키는 참조가 16개 있다. - Bad Case와 같이 모델을 직접 태스크 인자로 넘기면 태스크가 실행될 때마다 Ray 내부적으로 새로운 ray.put(model) 이 실행되고 따라서 태스크 3개를 실행하면 모델 사본이 3개 만들어지고, 각각의 사본에 대해 참조가 따로 생기고 Good Case에서 16이었던 값이 3 증가해서 19가된다 즉 여기서 +3은 곧 태스크 개수만큼 늘어난 중복 ref 숫자. - 메모리 사본이 중복 생성되면(중복 참조되면) 왜 안되는가? - 모델이 태스크 개수만큼 복제돼서 올라가서, 만약 모델이 800MB라면 태스크가 3개면 2.4GB, 10개면 8GB까지 차지하게 되니까 메모리 낭비가 발생하고 큰 모델을 쓰면 금방 object store OOM(Out Of Memory) 에러가 난다 - 작은 더미 모델일 땐 차이가 안 드러나지만, 실제 대형 모델(PyTorch, TensorFlow 등)을 쓰면 시스템이 바로 느려지고 OOM으로 죽을 수 있다. - make_prediction - 각 parquet 파일을 읽어 데이터프레임으로 만든 뒤, 더미 모델을 적용했다. 더미 모델은 `passenger_count`가 짝수인지 여부를 판별해서 불리언(`True`/`False`) 값을 반환하구 - 12개 파일에 대해 잘 수행되었다!! 2. Actor 기반 batch prediction - Ray의 Actor 기반 분산처리? - 모델을 Actor 안에 올려 상태를 유지하고, 여러 Actor를 풀로 관리해 병렬성을 확보. - @ray.remote class BatchPredictor - 함수 대신 클래스가 원격 실행 단위로 선언되어 있음. - 참고로 Task에서는 다음과 같이 선언돼있었는데 - 보면 self.model 같은 멤버 변수가 없고, 그냥 model이라는 인자를 받는다. - result = model(df)처럼 함수의 인자로 모델을 받아 쓰고 함수가 끝나면 모델은 사라지고, 다음 작업에서는 또 다시 같은 model_ref를 넘긴다. - 원래 코드로 돌아와서 보면,, - `__init__` 안에서 self.model = model을 저장하면 모델은 Actor의 상태로 남는다. 따라서 한 번 생성된 Actor는 이후 여러 shard 데이터를 받아도 같은 모델을 반복해서 활용한다. - 이게 Actor의 가장 중요한 특징인데 단순 태스크에서는 매번 model_ref를 전달하고 실행이 끝나면 상태가 사라지지만, Actor에서는 이 모델이 메모리에 계속 붙어있다. - actors = [BatchPredictor.remote(model_ref) for _ in range(4)] - 네 개의 Actor 인스턴스를 생성. 각각은 독립된 워커 프로세스로 Ray 클러스터 안에 배치된다 즉, 네 개의 예측기가 동시에 shard 파일을 읽고 결과를 계산할 수 있다. - ActorPool - Actor를 관리하는 유틸리티. 여러 Actor를 모아두고, 사용할 수 있는 Actor가 생기면 작업을 하나씩 할당한다. - for file in input_files: pool.submit(lambda a, v: a.predict.remote(v), file) - a는 Actor 하나, v는 shard 파일 경로. - 제출된 작업은 내부적으로 큐에 쌓이고 Actor가 놀고 있으면 즉시 할당되기 때문에, 사용자가 Actor 스케줄링을 직접 신경 쓰지 않고도 여러 데이터를 효율적으로 분배할 수 있다. - 결과 수집 루프 (while pool.has_next()) - 결과 수집 루프 돌렸고 12개 파일에 대해 정상적으로 수행!! cf2 - 의문점2 - Actor 기반 방법은 모델을 Actor 안에 올려 상태를 유지하고, 여러 Actor를 풀로 관리해 병렬성을 확보한다구했다. - 궁극적으로 Task 기반과의 성능 차이? - 확인2 - Task 기반과 Actor 기반 실행에서 시작 시간과 종료 시간을 time으로 측정하면 실행 시간을 확인해볼수 있다. - 결과 - 실행 시간 비교 - Task 기반: 대부분 2.5초대, 몇몇 shard는 2.8~3.3초 소요 / 총합 31.63초 - Actor 기반: 대부분 2.4~2.7초에 안정적으로 분포 / 총합 30.97초 - 실행 시간에 영향을 주는 요소 중 Task와 Actor의 방식 차이와 직접적으로 연관된 요소는? - 모델 로딩 비용: 로딩 비용을 매번 치르느냐, 한 번만 치르느냐. - 모델 로딩 비용은 load_model() 안에서 np.zeros(100_000_000)을 만들면서 메모리 초기화할때 발생하는데, 한 번 할 때마다 0.5~1초 가까운 오버헤드가 발생할 수 있고 이게 Task 기반에서는 shard마다 반복되고, Actor 기반에서는 딱 한 번만 발생한다. - 일반적인 결과 차이 - 모델이 커지거나 연산량이 많아지면, Task 기반 방식은 shard 수가 많아질수록 모델을 계속 새로 불러야 하니 실행 시간이 선형적으로 증가하고 Actor 기반 방식은 초기 한 번만 로딩, 이후에는 오로지 데이터 I/O + 추론만 걸리므로 평균 실행 시간이 안정적이고 훨씬 짧다. 즉, 일반적으로는 Actor 기반이 훨씬 빠르고 안정적이다. - 이번 결과에서 두 방식의 총합이 31.6초 vs 31.0초로 거의 비슷했던 이유? - 데이터 I/O가 지배적이었기 때문 즉 12개의 parquet 파일을 병렬로 읽는 데 걸리는 시간이 모델 로딩 비용보다 더 크게 작용했기 때문에 비슷하게 나왔다. - 모델 로딩이 실제로는 몇백 MB 정도라 현대 CPU/메모리 환경에서는 빠르게 끝났고 따라서 “모델 로딩 절약 효과”가 “I/O 지연 변동”에 묻힌듯하다 데이터가 단순해서 모델 로딩 오버헤드가 확인이잘안됐다. 3. GPU에서 실행 - 을 설명하기 앞서 현재까지 진행된 내용을 정리하면? - 기본 Task 기반 배치 예측 - @ray.remote 태스크로 파일 단위(shard) 배치를 실행 - Ray에서 여러 파일을 나눠 태스크로 돌리면 이렇게 분산 병렬 예측을 할 수 있다. - Actor 기반 배치 예측 - BatchPredictor라는 클래스를 @ray.remote로 선언해서, 한 번 생성된 Actor 내부에 모델을 올려두었고 모델을 계속 재사용하는 장기 실행 프로세스를 사용 - 계속 모델을 다시 올리지 않고, 같은 Actor 안에서 여러 shard를 처리할 수 있다. - GPU Task 기반 배치 예측 - 다음 코드에서는 GPU 자원을 요구하는 태스크를 실행 - 앞선 2개 코드에서는 CPU 배치 예측을 수행했는데, Ray Core로 GPU 자원 스케줄링도 가능하며 @ray.remote(num_gpus=1)로 GPU 할당, model.to("cuda")로 GPU 메모리를 이동하여 수행할거고 - GPU 리소스도 Ray가 알아서 분산 배치할 수 있고, 모델은 GPU 메모리에 옮겨야 함을 확인할예정. - ray.cluster_resources - 현재 Ray 클러스터에 등록된 전체 자원(capacity)을 확인해본결과 다음과 같다. - CPU: 2.0 - Ray가 인식한 논리 CPU 코어 수는 2개 - 현재 클러스터 전체에서 2개의 CPU 코어를 태스크 실행에 사용할 수 있으며 Ray 태스크를 실행할 때 @ray.remote(num_cpus=1) 같은 식으로 요청하면 여기서 소모됨. - GPU: 1.0 - Ray가 인식한 논리 GPU 코어 수는 1개 - 현재 클러스터 전체에서 1개의 GPU 코어를 태스크 실행에 사용할 수 있으며 Ray 태스크를 실행할 때 @ray.remote(num_gpus=1)로 요청할 수있다, - @ray.remote(num_gpus=1) - 이 부분이 없었을때는 Ray는 태스크를 CPU 자원만 필요로 하는 일반 작업으로 인식해서 아무 노드에나 배치했었음. - 참고로 Task에선 다음과 같이 적어줬엇다 - 원래 코드로 돌아와서보면 - Task때와 반대로 이 속성을 지정하면 스케줄러는 반드시 GPU가 하나 이상 있는 노드에서만 해당 태스크를 실행시킨다. - model.to(torch.device("cuda") - 일반적으로 PyTorch 모델은 처음 생성하면 CPU 메모리에 적재되므로 GPU에서 연산을 시도하려고 하는 GPU 태스크에서는 모델을 반드시 CUDA 디바이스로 옮겨주어야 한다. - torch_model = torch.nn.Linear(10, 1), torch_model_ref = ray.put(torch_model) - 여기서는 여기서는 예시로 간단한 torch.nn.Linear(10, 1) 모델을 만들고 모델을 ray.put으로 객체 저장소에 올린 뒤 make_torch_prediction.remote 호출 시 참조(torch_model_ref)를 전달하여 최종 학습을 수행. cf3 - 의문점3 - Ray에서 CPU와 GPU를 활용했을 때 시스템 메모리 사용량 변화를 가시화해보면?? - 확인3 - 간단한 torch.nn.Linear(10, 1) 모델에서 “실행전 → CPU 태스크 후 → GPU 태스크 후” 동안 RAM 사용량을 확인해보기. - 실행 전 - CPU RAM - 3.49 GB 사용 중 - GPU VRAM - 0.00 GB 사용 중 - CPU 태스크 실행 후 - CPU RAM - 3.50 GB 사용 중: CPU에서 모델+데이터를 생성해서 RAM이 0.01 GB 증가 - GPU VRAM - 변화 없음 - GPU 태스크 실행 후 - CPU RAM - 3.89 GB 사용 중: GPU를 쓸 때도 CPU에서 메타데이터, 버퍼, 연산 준비용 객체를 유지하기 때문에 0.39 GB가 증가 - GPU VRAM - 1296 MiB (약 1.3 GB) 사용 중 - [GPU 태스크 실행 후] 출력에는 torch.cuda.memory_allocated() 값을 사용했는데, Ray 워커 프로세스에서 GPU를 사용했기 때문에 VRAM 점유량을 잡아내지 못해서 0.0 GB 사용중으로 나온다. - nvidia-smi 확인 결과 GPU 태스크가 모델과 입력 데이터를 GPU에 올려서 약 1.3 GB를 사용한 것이 확인된다. - GPU Utilization (GPU-Util) 100% - 태스크 실행 시 GPU 연산이 꽉 차서 돌았음을 확인 가능. - 결론 - “실행전 → CPU 태스크 후 → GPU 태스크 후” 동안 RAM 사용량이 CPU: 3.49 GB(27%) → 3.50 GB (28%) → 3.89 GB (31%)으로 변화하였고 GPU: 0GB → 0GB → ≈1.3 GB으로 변화했다. 출처 Ray Document - Batch Prediction with Ray Core https://docs.ray.io/en/latest/ray-core/examples/batch_prediction.html 전체 코드 - google colab https://colab.research.google.com/drive/1Kp1zMDVJB2ZgIb0JwPqHD2Wpbumm0XUi?usp=sharing</a></p><hr><p><em>2025-09-13</em> ⋯ AI #1 ML 방법론 기초</p><p style=height:4.5em;overflow:hidden><a href=/docs/study/ai/ai33/>ML type (p.31-33) - ML의 학습방법 3가지 - 지도학습(Supervised) - 입력 데이터와 출력 데이터가 모두 제공되고 모델은 입력을 보면 어떤 출력이 나와야 하는지를 배움. 학습한 모델은 새로운 데이터가 들어오면 예측을 하고 -> 결과를 실제 정답과 비교해 정확도 계산. - 비지도학습(Unsupervised) - 문제는 있지만 정답 라벨이 없음. 비슷한 특징을 가진 학생들을 묶어서 그룹을 만들고 어떤 그룹이 우수한지 알 수 없지만 데이터 안에서 자연스럽게 나타나는 구조를 파악한다(클러스터링) - 준지도학습(Semi-Supervised) - 라벨이 붙은 소량의 데이터와, 라벨이 없는 대량의 데이터를 동시에 사용하면 더 나은 모델을 만들 수 있다 왜냐하면 100% 라벨링된 데이터가 있을 때만큼 정확하지는 않지만, 현실에서는 라벨링이 부족한 경우가 많고 라벨 없는 데이터가 양은 많아서 데이터 분포를 더 잘 보여주기 때문이다. 머신러닝 알고리즘 (p.34) - 트리 기반 방법(CART) - 결정트리(Decision Tree) - 데이터를 여러 조건으로 나누어가면서 최종 답을 찾는다 예를 들어 "이 과일은 달콤한가?" -> "색깔은 빨간가?" -> "크기는 작은가?" 같은 질문을 따라가면서 사과, 딸기, 체리처럼 답을 얻는다. - 랜덤포레스트(Random Forest) - 결정트리를 하나만 쓰지 않고 여러 개를 무작위로 만들어서 숲을 형성하고 각각의 나무가 약간씩 다른 조건을 사용하기 때문에 전체적으로는 더 튼튼하고 안정적인 예측을 한다. 여러 명이 각자 판단한 결과를 모아 집단지성을활용함. - 커널 기반 방법(SVM) - SVM은 통계학자가 아니라 항공우주 연구자들이 만든 알고리즘. - 우주에서 달과 그 주변의 별들을 구분하려 한다면 하늘의 모든 별을 고려할 필요는 없고 달의 경계 근처에 있는 몇몇 별만 봐도 구분 선을 그을 수 있다. - SVM은 바로 이 “경계에 가까운 데이터”만 집중해서 보는데 달과 별을 가르는 선을 그을 때 이 선과 가장 가까운 점들과의 거리를 최대화한다 그래서 SVM은 전체 데이터를 다 보지 않고도 효과적으로 두 집단을 구분할 수 있다. - 또한 경계를 그을 때 약간의 오차는 허용하는데 현실 세계 데이터가 완벽하게 나눠지지 않는 경우가 많다는 사실을 고려한 것이다. - 부스팅 - 약한 모델들을 모아서 강한 모델을 만드는 전략. 시험을 본다고 하면 한 학생이 문제를 틀린 부분만 복습하고 또 시험을 본다. 또 틀리면 다시 그 부분만 공부한다. 이런 식으로 반복해서 학습하면 점점 더 성적이 오른다. - XGBoost는 틀린 데이터에 더 높은 가중치를 주면서 여러 약한 트리를 합쳐 성능을 끌어올린다. LightGBM은 XGBoost의 연산 방식을 최적화해서 더 빠르게, 그리고 더 효율적으로 학습할 수 있도록 만든 버전. - 정규화 - 회귀 문제용. - 회귀 모델은 데이터의 입력 변수와 출력 값을 수학적으로 연결하는데 변수가 너무 많으면 모델이 복잡해지고 오히려 예측력이 떨어진다. 정규화는 규칙을 추가해 모델이 과도하게 커지는 것을 막는다. - LASSO는 회귀 계수 중 일부를 아예 0으로 만들어 변수를 줄이는 방법. Ridge는 모든 변수를 유지하되 크기를 작게 줄이는 방법. 둘 다 모델이 단순해지도록 돕고, 과적합을 막아 예측력을 높인다. - “너무 많은 변수에 휘둘리지 말고, 꼭 필요한 신호만 잡아내라”라는 규칙을 주입하는 과정. 지니계수 & 엔트로피 - 불순도 측정방법. - 불순도는 여러 클래스가 얼마나 섞여 있는지. - 지니 계수 - 확률을 가지고 계산. 예를 들어 어떤 마디에 빨간색이 30%, 파란색이 70% 있다고 하면 임의로 두 개를 뽑았을 때 색이 서로 다를 확률을 계산하는데 값이 클수록 섞여 있다는 뜻이고 값이 0에 가까우면 거의 한 가지 색만 있다는 뜻. 즉 무작위로 두 개를 뽑았을 때 다를 확률. - 엔트로피 - 숫자를 맞추는 스무고개 게임을 할때. 상대가 생각한 숫자가 1부터 1000까지 중 하나면 그냥 무작정 맞추는 건 비효율적이고 보통은 반으로 나누는 질문을 한다 “500보다 크냐?”, “750보다 크냐?” 이런 식으로 세 번 질문하면 대략 1000개 중 하나를 알아낼 수 있다. 이때 필요한 질문의 횟수가 정뵤량. - 엔트로피는 질문의 평균 횟수를 수학적으로 표현한 값이다 클래스가 균등하게 섞여 있을수록 질문을 많이 해야 하고 한 클래스가 압도적으로 많으면 질문을 거의 안 해도 알 수 있으니까 엔트로피가 낮다. - 결론 - 지니 계수는 두 개 뽑았을 때 다를 확률을 계산하는 방식이고 엔트로피는 그 집합을 완전히 구분하려면 평균적으로 몇 번 질문해야 하는가를 계산하는 방식. 부스팅 - 한두 번은 맞지만 전체적으로는 성능이 낮은 약한 모델을 여러 개 모아 강한 모델을 만들기 - 모델이 틀린 부분에 가중치를 더 주고, 그다음 모델이 그 틀린 부분을 집중적으로 학습하게 만들고를 여러 번 반복한다. - 손실 함수의 기울기를 계산해서 “어느 방향으로, 얼마나 고쳐야 성능이 나아질지”를 봐서 단순히 틀린 데이터를 다시 보는 게 아니라 오차를 줄이는 방향으로 학습 (그래디언트)</a></p><hr><div class=pagination style=margin-top:2rem;display:flex;justify-content:center;align-items:center;gap:1rem><span style=color:#666>1 / 2
</span><a href=/tags/2025-09/page/2/ style="padding:.5rem 1rem;text-decoration:none">→</a></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>