<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2024-12 on Lifelog 2025</title><link>https://yshghid.github.io/tags/2024-12/</link><description>Recent content in 2024-12 on Lifelog 2025</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 31 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://yshghid.github.io/tags/2024-12/index.xml" rel="self" type="application/rss+xml"/><item><title>블로그 시작 (부제: 제발열심히살자..)</title><link>https://yshghid.github.io/docs/hobby/daily/daily1/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/daily/daily1/</guid><description>블로그 시작 (부제: 제발열심히살자..) # #2024-12-31
최근에 무기력한 기분이 너무 오래가서&amp;hellip; 느슨해지다못해 일시정지해버린 일상에 긴장감을 주기 위해 블로그를 시작한다. 공부도 하기싫고 취준도 하기싫고 구냥 아무것도 하고싶지않다 ㅠㅠ
오늘도 사실 랩미팅 피피티만들어야되는데 하기싫어서, 전에 오류나서 엎었던 블로그 다시 만들었다. 정말이지 일하는것빼고 다 재밌는듯.
그리고 독감걸린동안 아무것도 안했는데 내일이면 휴가 끝나니까 그것도 너무 두렵다. 이제 몸은 안아픈데 정신이 아픈거같음.. ㅋㅋ
일단 지금 해야되는일은
SQL 공부 (시험일: 3.8) 빅분기 필기 공부 (시험일: 4.</description></item><item><title>DESeq2 워크플로우</title><link>https://yshghid.github.io/docs/study/bioinformatics/bi1/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/bi1/</guid><description>[코드] DESeq2 워크플로우 # Load package # suppressMessages({ library(&amp;#34;DESeq2&amp;#34;) library(pheatmap) library(withr) #library(tidyverse) library(RColorBrewer) library(gplots) library(dplyr) }) Set path # setwd(&amp;#34;/data-blog/bi1&amp;#34;) getwd() &amp;#39;/data-blog/bi1&amp;#39; Run DESeq2 # S1 &amp;lt;- &amp;#39;33&amp;#39; S2 &amp;lt;- &amp;#39;150&amp;#39; countdata &amp;lt;- read.csv(&amp;#34;results.csv&amp;#34;, header=TRUE, sep=&amp;#39;,&amp;#39;) colnames(countdata) &amp;lt;- c(&amp;#39;GeneID&amp;#39;,&amp;#39;150-1&amp;#39;,&amp;#39;150-2&amp;#39;,&amp;#39;150-3&amp;#39;,&amp;#39;33-1&amp;#39;,&amp;#39;33-2&amp;#39;,&amp;#39;33-3&amp;#39;,&amp;#39;con-1&amp;#39;,&amp;#39;con-2&amp;#39;,&amp;#39;con-3&amp;#39;) countdata &amp;lt;- countdata[, c(&amp;#39;GeneID&amp;#39;,&amp;#39;150-1&amp;#39;,&amp;#39;150-2&amp;#39;,&amp;#39;150-3&amp;#39;,&amp;#39;33-1&amp;#39;,&amp;#39;33-2&amp;#39;,&amp;#39;33-3&amp;#39;,&amp;#39;con-1&amp;#39;,&amp;#39;con-2&amp;#39;,&amp;#39;con-3&amp;#39;)] selected_columns &amp;lt;- paste(c(&amp;#39;GeneID&amp;#39;,paste0(S2,&amp;#34;-1&amp;#34;), paste0(S2,&amp;#34;-2&amp;#34;), paste0(S2,&amp;#34;-3&amp;#34;),paste0(S1,&amp;#34;-1&amp;#34;), paste0(S1,&amp;#34;-2&amp;#34;), paste0(S1,&amp;#34;-3&amp;#34;)), sep=&amp;#34;&amp;#34;) countdata &amp;lt;- countdata[, selected_columns] countdata &amp;lt;- countdata[rowSums(countdata[, -1]) != 0, ] sample.names &amp;lt;- paste(c(paste0(S2,&amp;#34;-1&amp;#34;), paste0(S2,&amp;#34;-2&amp;#34;), paste0(S2,&amp;#34;-3&amp;#34;),paste0(S1,&amp;#34;-1&amp;#34;), paste0(S1,&amp;#34;-2&amp;#34;), paste0(S1,&amp;#34;-3&amp;#34;)), sep=&amp;#34;&amp;#34;) conditions &amp;lt;- factor(c(S2,S2,S2,S1,S1,S1)) metadata &amp;lt;- data.</description></item><item><title>DESeq2 워크플로우</title><link>https://yshghid.github.io/docs/study/bioinformatics/bi10/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/bi10/</guid><description>[코드] DESeq2 워크플로우 # Load package # # Input: genome_positions = list of genomic loci with H-scores # H_scores = dict {position: H_score} # Parameters: # MinPts = 5 # eps_scale = 10 # diminish_factor = 3 initialize hotspots = [] # STEP 1. Search Candidate Core Mutation (CCM) for position in genome_positions: H = H_scores[position] Deps = eps_scale * H neighborhood = get_neighbors_within_deps(position, Deps) avg_H = mean([H_scores[n] for n in neighborhood]) sum_H = sum([H_scores[n] for n in neighborhood]) num_mutants = len([n for n in neighborhood if H_scores[n] &amp;gt; 0]) if H &amp;gt; 0.</description></item><item><title>EndNote 사용법</title><link>https://yshghid.github.io/docs/study/bioinformatics/bi16/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/bi16/</guid><description>EndNote 사용법 # 1. EndNote 설치 및 계정 설정 # 계정 설정: 공식 웹사이트에서 End note 계정을 생성한다.
설치: 나의 경우 여기에서 다운로드해줬다.
2. 레퍼런스 추가 방법 # Google Scholar에 논문 제목을 검색해서 인용&amp;gt;EndNote를 클릭하면 .enw 파일이 다운로드된다. 3. 레퍼런스 관리 # Endnote에 접속한다. Collect&amp;gt;Import References로 들어간다 파일 선택&amp;gt;아까 저장한 .enw 파일을 선택해준다 Import Option&amp;gt;EndNote Import를 선택해준다 To&amp;gt;New Group을 하면 논문 주제별로 그룹을 생성하여 정리 가능. 생성한 그룹이 이미 있으면 원하는 그룹 선택해준다.</description></item><item><title>Hugo로 깃허브 블로그 만들기</title><link>https://yshghid.github.io/docs/study/tech/cs1/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/tech/cs1/</guid><description>[Github] Hugo로 깃허브 블로그 만들기 # 목록 # 2024-12-31 ⋯ 사이트 생성, 깃허브 배포
2024-12-31 ⋯ Favicon 변경, Giscus 댓글창 추가
사이트 생성, 깃허브 배포 # Hugo 설치 # $ brew install hugo $ hugo version hugo v0.131.0+extended darwin/arm64 BuildDate=2024-08-02T09:03:48Z VendorInfo=brew Hugo v0.112.0 이상인지 확인하면 된다.
Hugo 사이트 생성 # 작업하고 싶은 위치에 Hugo 디렉토리를 만들어준다.
$ mkdir Hugo $ cd Hugo Hugo로 들어가서 hugo 사이트 틀을 생성해준다. 나는 blog라는 이름으로 생성하였다.</description></item><item><title>RNA-seq Preprocessing: EBV genome</title><link>https://yshghid.github.io/docs/study/bioinformatics/bi11/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/bi11/</guid><description>RNA-seq Preprocessing: EBV genome # 분석 목적
제공받은 fastq를 human genome에 매핑해서 전처리, 분석 후 DE 결과 보냄 DE 분석시에 EBV 유전자도 포함해달라는 요청 해야하는것
fastq를 EBV genome에 매핑해서 전처리, EBV count 생성 human count에 EBV count를 붙이기 통합 count로 DE 분석 재수행 1. Alignment # Load package, Set Path # library(edgeR) library(Rsubread) library(org.Hs.eg.db) setwd(&amp;#34;/data/home/ysh980101/2311/RNA-seq_ebv/Rsubread&amp;#34;) getwd() &amp;#39;/data1/home/ysh980101/2311/RNA-seq_ebv/Rsubread&amp;#39; Build Index # # build index ref &amp;lt;- &amp;#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/NC_007605.1.fa&amp;#34; output_basename &amp;lt;- &amp;#34;NC_007605.1_idx&amp;#34; buildindex(basename = output_basename, reference = ref) Feature Count # # feature.</description></item><item><title>구글 BERT의 정석 | BERT 입문</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs16/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs16/</guid><description>[딥러닝] 구글 BERT의 정석 | BERT 입문 # 목록 # 2024-12-31 ⋯ 2.3 BERT의 구조
2024-12-31 ⋯ 2.4 BERT 사전 학습
2.3 BERT의 구조 # BERT의 전체 구조 # 트랜스포머의 인코더(Encoder) 블록을 여러 개 쌓은 형태. 입력: 문장 (토큰화된 형태) 내부 구조: N개의 Transformer Encoder Blocks (기본 모델은 12개, 큰 모델은 24개) 출력: 각 토큰의 벡터 표현 (Contextual Embedding) cf) BERT의 대표적인 모델 크기
모델 # 인코더 층 숨겨진 차원 (dmodel) 어텐션 헤드 수 파라미터 수 BERT-Base 12 768 12 110M BERT-Large 24 1024 16 340M BERT의 입력 처리 # 입력 토큰 (Token Embedding) WordPiece Tokenization을 사용하며, 단어를 서브워드(subword) 단위로 분할하고 각 토큰은 고유한 임베딩 벡터로 변환된다.</description></item><item><title>구글 BERT의 정석 | BERT의 파생 모델</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs17/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs17/</guid><description>[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 2024-12-31 ⋯ 4.1 ALBERT
2024-12-31 ⋯ 4.3 RoBERTa
2024-12-31 ⋯ 4.4 ELECTRA
4.1 ALBERT # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.
크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.</description></item><item><title>구글 BERT의 정석 | 트랜스포머 입문</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs15/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs15/</guid><description>[딥러닝] 구글 BERT의 정석 | 트랜스포머 입문 # 목록 # 2024-12-31 ⋯ 1.2 트랜스포머의 인코더 이해하기
2024-12-31 ⋯ 1.3 트랜스포머의 디코더 이해하기
1.2 트랜스포머의 인코더 이해하기 # 셀프 어텐션 # 셀프 어텐션은 문장 내 단어들이 서로 얼마나 중요한지를 계산하는 과정. 트랜스포머는 이를 위해 입력 단어를 쿼리(Query), 키(Key), 밸류(Value) 세 가지 벡터로 변환하여 연관성을 구한다. 어텐션 점수 계산 예제 # &amp;ldquo;The cat sat on the mat.&amp;rdquo;
각 단어 벡터(예: 512차원)를 가중치 행렬과 곱하여 쿼리(Q), 키(K), 밸류(V)벡터를 생성한다.</description></item><item><title>깃허브 오류 There was an error committing your changes: File could not be edited</title><link>https://yshghid.github.io/docs/study/tech/cs5/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/tech/cs5/</guid><description>[깃허브] 깃허브 오류 There was an error committing your changes: File could not be edited # 갑자기 모든 파일의 수정이 안되고 page deployment도 오류가 났다. 브라우저 캐시 문제인가 해서 방문기록이랑 캐시를 모두 삭제해보았다. 그래도 오류가 났다. 구글링하니까 내 경우랑 맞아떨어지는 한국인 블로그글이 있어서 시키는대로 https://www.githubstatus.com/에 들어가봤다. 블로그 글이랑 같은 창이 떴는데 그냥 기다려야된다길래 그냥 기다림. 2시간 뒤에 들어가니까 이 창으로 바뀌었다. 그리고 된다. 또 블로그 부셔진줄&amp;hellip; 다행이다&amp;hellip;.</description></item><item><title>당신의 특별한 우울 | 린다 개스크</title><link>https://yshghid.github.io/docs/hobby/book/book2/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book2/</guid><description>당신의 특별한 우울 | 린다 개스크 # 목록 # 2024-12-31 ⋯ [북마크] 애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다.
2024-12-31 ⋯ [북마크] 불행한 것과 우울한 것.
2024-12-31 ⋯ [관련 영상] 결핍과 그에 대한 애도의 기간(라디오스타 김영철)
애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다. # 1 # 지금까지 의사로 일하면서, 인생 계획을 완벽하게 할 수 있다고 생각하는 사람들을 많이 보았다.</description></item><item><title>딥러닝을 이용한 자연어 처리 입문 | BERT</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs14/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs14/</guid><description>[딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT # 목록 # 2024-12-31 ⋯ 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
2024-12-31 ⋯ 17-03 구글 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-04 한국어 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-05 구글 BERT의 다음 문장 예측
2024-12-31 ⋯ 17-06 한국어 BERT의 다음 문장 예측
17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) # BERT?
BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.</description></item><item><title>루틴의 힘 | 댄 애리얼리, 그레첸 루빈, 세스 고딘 외</title><link>https://yshghid.github.io/docs/hobby/book/book13/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book13/</guid><description>루틴의 힘 | 댄 애리얼리, 그레첸 루빈, 세스 고딘 외 # 흔들리지 않고 끝까지 계속하게 만드는 루틴의 힘
목록 # 2024-12-31 ⋯ [북마크] 전문가의 세상으로 나가는것에 대한 두려움
2024-12-31 ⋯ [북마크] 진전의 가시화
전문가의 세상으로 나가는것에 대한 두려움 # 1 # 자주 하면, 부담이 줄어든다. 일주일 동안의 결과물이 겨우 한 페이지, 블로그 포스팅 한 건, 스케치 하나라면 당연히 &amp;lsquo;특출나게 잘해야 한다&amp;rsquo;는 생각이 들고 작업물의 질에 대해 조바심을 내게 된다. 반면 매일 쓰면 하루치 정도는 그다지 중요하지 않다.</description></item><item><title>물고기는 존재하지 않는다 | 룰루 밀러</title><link>https://yshghid.github.io/docs/hobby/book/book6/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book6/</guid><description>물고기는 존재하지 않는다 | 룰루 밀러 # https://blog.naver.com/afx1979/222154049972?trackingCode=blog_bloghome_searchlist
이 블로그 글에는 이런 말이 나온다.
미(학)적으로는 우울이나 자살이 아름다워 보일지 몰라도 진선미가 다 우울의 편을 든다고 해도 나는 분노가 더 낫다고 본다. 분노는 삶에 도움이 되고 삶을 더 좋게 변화시키는 원동력이 되기 때문이다.
미학적으로는 진실만 받아들이는 것이 온전해보인다. 그런데 진실은 조금 밀어놓고 일단 달리기 시작하는 사람도 있다. 하이젠베르크의 불확정성 원리처럼 둘 다를 챙기는 것은 불가능하고 둘 중 하나는 어쩔 수 없이 포기해야 한다.</description></item><item><title>불변의 법칙 | 모던 하우절</title><link>https://yshghid.github.io/docs/hobby/book/book12/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book12/</guid><description>불변의 법칙 | 모던 하우절 # 세상 모든 일은 예측 불가능한 방식으로 서로 영향을 주고받고, 혼합되고, 그 결과가 증폭된다. 세상은 운과 우연에 이토록 취약하다.
목록 # 2024-12-31 ⋯ [북마크] 사건의 복리효과
2024-12-31 ⋯ 플레이리스트
2024-12-31 ⋯ [관련 영상] 김지민 생일 Birthday - 쿠쿠크루(Cuckoo Crew)
2025-04-10 ⋯ [북마크] 결국 상황은 나아질것이다
2025-04-10 ⋯ [유튜브] 미얀마 강진 생존자 인터뷰
사건의 복리효과 # 💛1 # 오늘의 세상 모습이 어떻든, 무엇이 당연해 보이든, 내일이 되면 그 누구도 생각하지 못한 작은 우연 때문에 모든 게 달라질 수 있다.</description></item><item><title>세이노의 가르침</title><link>https://yshghid.github.io/docs/hobby/book/book5/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book5/</guid><description>세이노의 가르침 # 원래 같으면 조금 읽고 덮었을 것 같은데 취준시즌에 읽어서 꽤 많이 읽음. &amp;lsquo;나&amp;rsquo;에게 도움이 되는 책인지는 모르겠는데 &amp;lsquo;취준하는 나&amp;rsquo;에게는 매우 유용한 책이었다! ㅋㅋ
목록 # 2024-12-31 ⋯ [북마크] 공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다.
2024-12-31 ⋯ [북마크] 잘할 수 있는 일을 찾기 vs 일을 잘하기
2024-12-31 ⋯ [북마크] 인테그리티
공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다. # 1 # 카드 다섯 장을 쥐고 하는 포커판에서 나올 수 있는 카드패에는 2,598,960개 종류가 있다고 한다.</description></item><item><title>우리가 빛의 속도로 갈 수 없다면 | 김초엽</title><link>https://yshghid.github.io/docs/hobby/book/book11/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book11/</guid><description>우리가 빛의 속도로 갈 수 없다면 | 김초엽 # 가볍게 읽기 좋은 SF 소설.
필요 이상으로 개연성을 설명하려하거나 등장인물이 많은 작품 별로 안좋아하는데 둘다 해당하지 않아서 읽기 편했다!
목록 # 2024-12-31 ⋯ [북마크] 순례자들은 왜 돌아오지 않는가
2024-12-31 ⋯ [북마크] 스펙트럼
2024-12-31 ⋯ [북마크] 공생 가설
2024-12-31 ⋯ [북마크] 감정의 물성
순례자들은 왜 돌아오지 않는가 # 소피. 마지막으로 한 가지 말할 것이 남았어. 내가 처음으로 마을에 대해 의문을 품게 되었던 계기, 그 오두막 뒤에 있던 귀환자 말야.</description></item><item><title>일론 머스크 | 월터 아이작슨</title><link>https://yshghid.github.io/docs/hobby/book/book7/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book7/</guid><description>일론 머스크 | 월터 아이작슨 # 똑똑하면서 적당히 착한 마음이 있는 사람은 다 좋다.
머스크는 태풍이 몰려올 때 가장 강력한 생기를 느끼는 그런 사람 중 한 명이다. “나는 폭풍을 위해 태어났어요. 그러니 고요함은 나에게 적합하지 않지요.” 미국의 7대 대통령 앤드류 잭슨이 한 말이다. 일론 머스크도 마찬가지다. 그는 위기나 데드라인, 할 일의 폭증과 같은 상황에서 번성했다. 복잡하고 난해한 도전에 직면하면, 그로 인한 긴장으로 종종 잠을 이루지 못하거나 심지어 토하기도 했다. 그러나 그런 상황은 그에게 활력도 불어넣었다.</description></item><item><title>자신의 존재에 대해 사과하지 말 것 | 카밀라 팡</title><link>https://yshghid.github.io/docs/hobby/book/book4/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book4/</guid><description>자신의 존재에 대해 사과하지 말 것 | 카밀라 팡 # 최정문 북클럽 2023년 07월 도서여서 읽어봤다! 저자가 생물정보학 과학자이다.
목록 # 2024-12-31 ⋯ [북마크] 깔끔한 상자 모서리는 든든하지만 환상일 뿐이다.
2024-12-31 ⋯ [북마크] 모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까?
깔끔한 상자 모서리는 든든하지만 환상일 뿐이다. # 1 # 더 나은 의사 결정을 하기 위해, 정보에 접근하고 해석하는 방식을 더 체계화할 필요는 없다. 머신러닝이 우리를 그런 방향으로 이끌 것이라고 예상하게 되지만 사실 그 반대다.</description></item><item><title>지적 생활의 즐거움 | P.G.해머튼</title><link>https://yshghid.github.io/docs/hobby/book/book3/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book3/</guid><description>지적 생활의 즐거움 | P.G.해머튼 # 목록 # 2024-12-31 ⋯ [북마크] 결혼과 행복
결혼과 행복 # 1 # 결혼은 안해도 되는데 한 사람들이 더 행복함.
사랑이 있든 없든 간에 정신적, 육체적으로 한 명의 남편 혹은 부인에게 초점을 맞추고 가족, 친구, 이웃, 나아가 잠깐 만나는 캐주얼한 섹스 파트너와 전남편 혹은 전 부인까지 양파 껍질처럼 차곡차곡 쌓인 울타리를 만듦으로써 우리 삶은 안정되고 행복해질 수 있다.
2 # https://youtu.be/vFN_DoqWAL4 우리는 종종 너무나 단순한 걸 놓치고 허우적대곤 하지만요.</description></item><item><title>콜 미 바이 유어 네임 (2017)</title><link>https://yshghid.github.io/docs/hobby/movie/movie2/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/movie/movie2/</guid><description>콜 미 바이 유어 네임 (2017) # 여름 감성 최고봉 영화! 특히 ost가 너무 좋다.
목록 # 2024-12-31 ⋯ [플레이리스트] Sufjan Stevens - Mystery of Love
2024-12-31 ⋯ 원작 소설
플레이리스트 # https://www.youtube.com/watch?v=n50Z3HGj4QE
Sufjan Stevens - Mystery of Love https://www.youtube.com/watch?v=XPPp0Gn45_8
| 𝐩𝐥𝐚𝐲𝐥𝐢𝐬𝐭 | 𝐬𝐨𝐦𝐞𝐰𝐡𝐞𝐫𝐞 𝐢𝐧 𝐧𝐨r𝐭𝐡𝐞𝐫𝐧 𝐢𝐭𝐚𝐥𝐲 🌳🍃 이건 비슷한 감성을 느끼고 싶을때 듣기 좋은 플리.
⏶ top
원작 소설 # 콜 미 바이 유어 네임 - 안드레 애치먼 &amp;raquo;</description></item><item><title>콜 미 바이 유어 네임 | 안드레 애치먼</title><link>https://yshghid.github.io/docs/hobby/book/book8/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/hobby/book/book8/</guid><description>콜 미 바이 유어 네임 | 안드레 애치먼 # 목록 # 2024-12-31 ⋯ [북마크] bookmark1
2024-12-31 ⋯ [북마크] bookmark2
bookmark1 # 1 # 그는 뭘 하면서 지내느냐고 물었다. 테니스. 수영. 밤에 시내로 놀러 나가기. 조깅. 편곡. 독서.
2 # 모든 것이 올리버가 우리 집에 온 그 여름에 시작되었다. 그것들은 그해 여름에 유행한 곡과 그가 머무는 동안 그리고 떠난 후에 읽은 책들, 뜨거운 날의 로즈메리 냄새부터 오후의 요란한 매미 소리까지 모든 것에 새겨졌다.</description></item><item><title>혼자 공부하는 딥러닝 | ANN</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs12/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs12/</guid><description>[딥러닝] 혼자 공부하는 딥러닝 | ANN # 목록 # 2024-12-31 ⋯ 17. 간단한 인공 신경망 모델 만들기
2024-12-31 ⋯ 18. 인공 신경망에 층을 추가하여 심층 신경망 만들어 보기
2024-12-31 ⋯ 19. 인경 신경망 모델 훈련의 모범 사례 학습하기
17. 간단한 인공 신경망 모델 만들기 # 데이터 준비 fashion_mnist 데이터셋에서 학습과 테스트용 이미지 데이터를 가져온다. 학습 데이터는 60,000개의 28x28 픽셀 이미지, 테스트 데이터는 10,000개의 28x28 픽셀 이미지. train_target과 test_target은 각 이미지에 해당하는 레이블(0~9)을 갖고있다.</description></item></channel></rss>