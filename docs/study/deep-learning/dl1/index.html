<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  #4 신경망
  #

#2025-07-07


  1. 딥러닝?
  #

딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/deep-learning/dl1/"><meta property="og:site_name" content=" "><meta property="og:title" content="#4 신경망"><meta property="og:description" content="#4 신경망 # #2025-07-07
1. 딥러닝? # 딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-11T00:00:00+00:00"><meta property="article:tag" content="2025-07"><title>#4 신경망 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/deep-learning/dl1/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a41db5e1379d97b987f21752435131c120b8c6da83e8a100f696ac336b915c62.js integrity="sha256-pB214Tedl7mH8hdSQ1ExwSC4xtqD6KEA9pasM2uRXGI=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/baking/>베이킹</a><ul></ul></li><li><a href=/docs/hobby/favorite/>🌸</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/bioinformatics/>생물정보학</a><ul></ul></li><li><a href=/docs/study/algorithm/>코테</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li><li><a href=/docs/study/etc/>기타</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>#4 신경망</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#1-딥러닝>1. 딥러닝?</a></li><li><a href=#2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조</a></li><li><a href=#3-가중치와-편향의-역할>3. 가중치와 편향의 역할</a></li><li><a href=#4-활성화-함수의-필요성>4. 활성화 함수의 필요성</a></li><li><a href=#5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들</a></li><li><a href=#6-은닉층의-개념과-역할>6. 은닉층의 개념과 역할</a></li><li><a href=#7-출력층-설계>7. 출력층 설계</a></li><li><a href=#8-모델-구조의-표현과-설계>8. 모델 구조의 표현과 설계</a></li><li><a href=#9-정방향-연산>9. 정방향 연산</a></li><li><a href=#10-손실-함수의-정의와-선택>10. 손실 함수의 정의와 선택</a></li><li><a href=#11-미분-가능성differentiability과-학습을-위한-전제-조건>11. 미분 가능성(Differentiability)과 학습을 위한 전제 조건</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=4-신경망>#4 신경망
<a class=anchor href=#4-%ec%8b%a0%ea%b2%bd%eb%a7%9d>#</a></h1><p>#2025-07-07</p><hr><h3 id=1-딥러닝>1. 딥러닝?
<a class=anchor href=#1-%eb%94%a5%eb%9f%ac%eb%8b%9d>#</a></h3><p>딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다.</p><p>예를 들어, 딥러닝에게 고양이 사진을 수만 장 보여주면서 “이게 고양이야”라고 알려준다. 그러면 딥러닝은 그 수만 장의 공통점, 즉 고양이에게만 있는 특징을 찾아낸다. 귀의 모양, 눈의 위치, 털의 질감, 얼굴 비율 등을 조금씩 파악하면서 학습한다. 이렇게 학습이 끝나면, 처음 보는 고양이 사진을 줘도 “이건 고양이야”라고 스스로 말할 수 있게 된다. 중요한 점은, 사람이 일일이 고양이의 특징을 알려주지 않아도, 딥러닝이 스스로 그 특징을 찾아낸다는 것이다. 이것이 바로 딥러닝의 힘이다.</p><p>그럼 이 딥러닝이 실제로 어떻게 작동하는 걸까? 딥러닝은 ‘인공신경망’이라는 구조를 바탕으로 한다. 이것은 여러 층(layer)으로 구성되는데, 가장 앞에는 입력층(input layer), 가장 뒤에는 출력층(output layer), 그리고 그 사이에는 수많은 은닉층(hidden layer)이 존재한다. 각각의 층에는 ‘뉴런’이라는 작은 계산 단위가 가득 들어 있다. 하나의 뉴런은 아주 단순한 계산만 할 수 있지만, 이 뉴런이 수백 개, 수천 개 모여 층을 이루고, 층이 다시 여러 겹 쌓이면 매우 복잡한 문제도 풀 수 있게 된다. 말하자면, 뉴런 하나는 벽돌이고, 벽돌을 쌓아 집을 짓는다고 보면 된다.</p><p>입력층은 말 그대로 정보가 들어오는 곳이다. 예를 들어 이미지가 입력되면, 그 이미지는 픽셀이라는 아주 작은 점들의 집합이고, 각 점에는 숫자가 있다. 밝으면 큰 숫자, 어두우면 작은 숫자다. 이 숫자들이 입력층으로 들어간다. 그리고 각 입력은 첫 번째 은닉층의 뉴런들과 연결되어 있다. 연결되어 있다는 건, 숫자가 전달된다는 뜻이고, 이 전달에는 ‘가중치(weight)’라는 것이 함께 붙는다. 이 가중치는 각각의 입력이 얼마나 중요한지를 나타내는 숫자다. 예를 들어 어떤 입력은 0.9의 가중치를 받고, 어떤 입력은 0.1의 가중치를 받을 수 있다. 이 가중치가 딥러닝의 핵심이다.</p><p>입력과 가중치를 곱한 뒤, 그 값을 더하고, 마지막으로 ‘편향(bias)’이라는 값을 추가한다. 그리고 이 결과값을 활성화 함수(activation function)라는 계산에 넣는다. 이 함수는 일종의 필터다. 이 값을 어느 정도 이상이면 다음 뉴런에 신호를 전달하고, 그렇지 않으면 무시한다. 이런 식으로 정보가 첫 번째 은닉층을 지나고, 두 번째 은닉층을 지나고, 계속해서 다음 층으로 전달되면서 점점 복잡한 특징을 추출하게 된다. 처음에는 단순한 선이나 색깔을 파악하다가, 나중에는 귀의 윤곽, 눈의 대칭성, 얼굴의 구조 같은 추상적인 개념도 파악하게 된다.</p><p>이런 모든 과정을 거쳐 출력층에 도달하면, 딥러닝은 최종적으로 “이건 고양이다”, “이건 강아지다” 같은 판단을 내린다. 이 판단이 맞았는지 틀렸는지는 사람이 알려준다. 틀렸다면, 딥러닝은 “왜 틀렸지?” 하고 스스로의 계산을 되짚어보고, 잘못된 가중치를 조금씩 조정한다. 이 과정을 ‘역전파(backpropagation)’라고 하는데, 이는 마치 시험을 보고 틀린 문제를 복습해서 다시 공부하는 것과 비슷하다. 이런 공부 과정을 반복할수록 딥러닝은 점점 더 정답에 가까워지고, 결국 사람처럼 판단하게 된다.</p><p>중요한 건, 딥러닝은 우리가 직접 가르치는 게 아니라 ‘보여주고 실수하게 하면서’ 배우게 한다는 점이다. 그리고 그 학습은 수천 번, 수만 번 반복된다. 사람도 어릴 때 수없이 넘어지면서 걷는 법을 배운다. 딥러닝도 마찬가지다. 실수를 하고, 그 실수를 수정하면서 조금씩 정답에 가까워지는 것이다. 이런 특성 때문에, 딥러닝은 수많은 데이터가 있어야 하고, 그 데이터를 바탕으로 학습하면서 비로소 쓸 만한 모델이 된다.</p><p>그렇다면 딥러닝은 어떤 일을 할 수 있을까? 앞서 말한 이미지 분류 외에도, 딥러닝은 자율주행차의 눈 역할을 하기도 하고, 음성 인식으로 우리 말을 컴퓨터가 이해하게 만들기도 하며, 글을 읽고 이해하거나 번역하는 데에도 쓰인다. 심지어 예술작품을 그리거나 작곡을 할 수도 있다. 바둑을 두고 사람을 이길 수도 있고, 의료 영상에서 암세포를 찾아내는 데에도 활용된다. 이런 일을 하려면 기존의 컴퓨터 알고리즘으로는 한계가 있지만, 딥러닝은 데이터를 직접 보며 스스로 규칙을 찾아내기 때문에 훨씬 유연하게 문제를 해결할 수 있다.</p><p>그럼에도 불구하고 딥러닝이 만능은 아니다. 딥러닝은 설명력이 약하다. 즉, 왜 그런 판단을 내렸는지를 설명하기 어렵다. 예를 들어 고양이와 강아지를 구분해냈다고 하자. 딥러닝이 왜 고양이라고 판단했는지를 우리는 정확히 알 수 없다. 인간은 눈이 크니까 고양이라고 판단했는지, 귀가 뾰족해서 그런지, 우리가 생각하는 방식으로 설명하지 않는다. 그래서 이를 ‘블랙박스’ 모델이라고 부르기도 한다. 최근에는 이런 문제를 해결하기 위해 ‘설명 가능한 AI(Explainable AI)’에 대한 연구도 활발히 진행되고 있다.</p><p>또한 딥러닝은 많은 데이터를 요구한다. 적은 데이터로는 제대로 배우지 못한다. 어린아이는 고양이 한두 번 보면 고양이를 알아보지만, 딥러닝은 수천 장, 수만 장을 봐야 같은 수준에 도달한다. 이처럼 데이터가 풍부한 환경에서는 매우 강력하지만, 데이터가 부족하면 힘을 발휘하지 못하는 것이 단점이다.</p><p>마지막으로, 딥러닝은 계산 비용이 크다. 뉴런이 많고, 층이 깊을수록, 계산할 것도 많아지고 시간이 오래 걸린다. 그래서 딥러닝을 잘 작동시키려면 GPU 같은 강력한 컴퓨터 장비가 필요하다. 마치 수많은 문제를 빠르게 푸는 수학 천재가 필요하듯이 말이다.</p><p>결국 딥러닝은, 아주 많은 계산을 통해 패턴을 학습하고, 그 패턴을 바탕으로 새로운 것을 판단하는 시스템이다. 사람의 두뇌를 흉내 내지만, 여전히 우리는 그 원리와 한계에 대해 계속해서 탐구하고 있다. 딥러닝은 &ldquo;입력을 넣으면 출력을 내놓는 똑똑한 계산기"지만, 그 계산기의 뇌 속에서 무슨 일이 벌어지는지는 우리가 명확히 이해하려면 아직 시간이 필요하다. 그렇기 때문에 지금도 우리는 딥러닝을 가르치고, 실수하게 하고, 더 나은 판단을 할 수 있도록 조금씩 성장시키는 중이다. 그리고 언젠가는 그 계산기가 스스로 학습하고, 설명하고, 새로운 창조까지 하는 세상도 올지 모른다. 바로 그것이 딥러닝의 현재이며, 앞으로 우리가 함께 만들어갈 미래다.</p><h3 id=2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조
<a class=anchor href=#2-%eb%89%b4%eb%9f%b0-%eb%aa%a8%eb%8d%b8%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ea%b5%ac%ec%a1%b0>#</a></h3><p>딥러닝에서 뉴런(Neuron) 모델이란, 아주 작고 단순한 계산 단위라고 할 수 있다. 마치 사람 뇌의 신경세포 하나처럼 생각하면 되는데, 실제 뇌의 뉴런이 전기 신호를 받고 다음 뉴런에게 신호를 전달하듯, 인공 신경망 속 뉴런도 어떤 입력 값을 받아서 처리한 뒤, 그 결과를 다음 단계로 넘긴다. 하지만 인공 뉴런은 실제 뉴런과 달리 아주 단순하다. 본질적으로는 숫자를 받아서 계산한 뒤, 또 다른 숫자를 내보내는 계산기 하나라고 보면 된다. 그럼 이 계산기 안에서는 어떤 일이 벌어질까?</p><p>가장 먼저 뉴런은 입력을 받는다. 예를 들어 &ldquo;고양이 사진이 들어왔다"고 하면, 실제로는 사진 속 픽셀들의 숫자가 뉴런에게 입력되는 것이다. 한 장의 흑백 이미지가 있다면, 각 픽셀은 0에서 255 사이의 숫자일 수 있다. 이 숫자들이 여러 개 모여 뉴런에게 전달된다. 예를 들어 총 5개의 입력 값이 들어온다고 하자. 각각 x₁, x₂, x₃, x₄, x₅라고 부르자. 이 숫자들은 우리가 처리해야 할 데이터다. 그런데 이 입력들이 모두 똑같이 중요한 건 아니다. 예를 들어 눈의 위치는 중요할 수 있지만, 배경의 회색 그림자는 덜 중요할 수 있다. 그래서 각각의 입력에는 ‘가중치’라는 숫자를 곱해준다. 이를 각각 w₁, w₂, w₃, w₄, w₅라고 하자. 이 가중치는 입력이 얼마나 중요한지를 알려주는 역할을 한다.</p><p>이제 뉴런은 각 입력과 가중치를 곱하고, 그 결과를 모두 더한다. 이 과정은 x₁·w₁ + x₂·w₂ + x₃·w₃ + x₄·w₄ + x₅·w₅ 처럼 표현된다. 이렇게 곱하고 더하는 이유는 간단하다. 입력이 클수록, 그리고 그 입력의 가중치가 클수록 결과에 더 많은 영향을 주기 때문이다. 마치 시험에서 국어보다 수학이 2배 중요하다고 하면, 수학 점수에 더 큰 가중치를 주는 것과 비슷하다. 이렇게 입력과 가중치를 곱해서 모두 더한 값을 우리는 흔히 z라고 부른다. 수학적으로는 z = Σ(xᵢ·wᵢ)라고 쓸 수 있다. 그런데 여기서 하나 더, 뉴런은 항상 일정한 방향으로만 판단하지 않는다. 어느 정도 기준점이 필요하다. 그래서 우리는 이 계산 결과에 ‘편향(bias)’이라는 숫자를 하나 더해준다. 편향은 말 그대로 기준선을 조금 위로 올리거나 아래로 내리는 역할을 한다. 즉, z = Σ(xᵢ·wᵢ) + b가 되는 것이다.</p><p>이제 이 값을 그대로 출력하면 될까? 아니다. 딥러닝은 복잡하고 다양한 패턴을 찾아야 하기 때문에, 이 z 값을 그대로 쓰지 않고, 특별한 수학적 함수를 한 번 더 거친다. 이 함수를 ‘활성화 함수(activation function)’라고 부른다. 왜냐하면 이 함수가 뉴런이 ‘활성화’될지를 결정하기 때문이다. 가장 기본적인 활성화 함수는 ‘계단 함수’다. z가 어떤 기준보다 크면 1을 출력하고, 아니면 0을 출력한다. 마치 &ldquo;이 정도 점수 이상이면 합격, 아니면 불합격&rdquo; 같은 느낌이다. 하지만 이 함수는 너무 뚝 끊긴 느낌이라, 요즘은 부드럽게 동작하는 함수들을 많이 쓴다. 예를 들어 ‘시그모이드(sigmoid)’ 함수는 z의 값을 0과 1 사이의 값으로 바꿔준다. z가 아주 크면 1에 가까워지고, 아주 작으면 0에 가까워진다. 즉, 흑백처럼 판단하는 것이 아니라 회색 지대를 인정하면서, &ldquo;이건 고양이일 확률이 85%쯤 돼&rdquo; 같은 식으로 표현하게 된다. 또 다른 활성화 함수로는 ReLU(Rectified Linear Unit)가 있다. 이건 z가 0보다 작으면 0, 크면 그대로 통과시키는 아주 간단한 함수인데, 복잡한 계산 없이도 성능이 좋아서 널리 쓰인다.</p><p>그럼 다시 한 번 정리해보자. 하나의 인공 뉴런이 하는 일은, (1) 여러 입력 값을 받고, (2) 각 입력에 가중치를 곱해서 더한 뒤, (3) 편향을 더하고, (4) 그 결과를 활성화 함수에 넣어서 (5) 출력 값을 만드는 것이다. 이 출력은 다음 뉴런으로 전달된다. 그리고 이 과정은 한 층(layer)에서 또 다음 층으로, 그리고 또 그다음 층으로 계속 이어지면서 전체 신경망이 작동하게 된다.</p><p>그런데 중요한 질문이 하나 있다. “이 가중치 w와 편향 b는 누가 정하나?” 처음엔 아무도 모른다. 그래서 딥러닝은 이 값을 무작위로 시작하고, 점점 더 좋은 값으로 바꿔나간다. 이 과정이 바로 학습이다. 딥러닝은 정답을 알고 있는 데이터를 바탕으로 예측을 해보고, 그 예측이 얼마나 틀렸는지를 계산한 뒤, 그 오차를 줄이기 위해 w와 b를 조금씩 수정한다. 이 과정을 반복하면서 w와 b는 점점 더 정답에 가까운 값을 갖게 된다. 결국 하나의 뉴런이 똑똑해진다는 건, 이 w와 b가 똑똑해졌다는 뜻이다.</p><p>이렇게 보면, 인공 뉴런은 생각보다 단순하다. 단지 입력을 받고 곱하고 더하고, 함수를 한 번 통과시킬 뿐이다. 그런데 이런 뉴런이 수천 개, 수만 개 모여서 층을 이루고, 그 층이 다시 깊이 쌓이면, 고양이 사진을 알아보거나, 사람의 말을 인식하거나, 심지어 장면을 보고 다음 장면을 예측하는 일까지 할 수 있다. 이는 마치 하나의 퍼즐 조각은 단순하지만, 그것들이 모이면 아름다운 그림이 되는 것과 비슷하다.</p><p>한 가지 더 재미있는 점은, 사람의 뇌도 비슷한 방식으로 작동한다는 것이다. 물론 훨씬 복잡하지만, 기본 원리는 비슷하다. 눈으로 들어온 이미지가 시각 뉴런들을 따라 전달되면서, 점점 더 고차원적인 특징을 추출하게 된다. 초기에 단순한 선이나 모양을 인식하다가, 점점 얼굴, 표정, 감정까지 인식하게 된다. 인공신경망에서도 초기 뉴런은 단순한 패턴을 배우고, 깊은 층의 뉴런일수록 복잡한 의미를 배운다. 그래서 “깊은 신경망(deep neural network)”이라는 말이 나온 것이다.</p><p>요약하자면, 뉴런은 단순한 계산기다. 입력을 받고, 가중치를 곱하고, 모두 더한 다음 편향을 더하고, 활성화 함수를 거쳐 출력한다. 이 모든 과정은 수학적으로는 아주 간단한 식 하나로 표현할 수 있다. y = f(Σ(xᵢ·wᵢ) + b), 여기서 f는 활성화 함수다. 이 간단한 식이 모이고 모여, 우리 눈에는 마법처럼 보이는 AI 시스템이 만들어지는 것이다. 마치 셀 수 없이 많은 작은 기계 부품들이 모여 자동차를 움직이듯, 딥러닝도 수많은 뉴런이 정해진 방식으로 작동하면서 전체 시스템이 학습하고, 예측하고, 판단하게 된다.</p><p>인공 뉴런은 수를 받아 계산하는 작은 친구일 뿐이다. 그런데 이 친구들이 손을 맞잡고 함께 일하면, 놀라운 일을 할 수 있다. 한 명은 선을 보고, 한 명은 곡선을 보고, 또 한 명은 얼굴을 보고, 마지막엔 “이건 고양이야”라고 말하는 팀이 되는 것이다. 그래서 우리는 이 단순한 뉴런 하나하나를 정성껏 설계하고, 그 연결을 조정하면서 더 똑똑한 AI를 만들어가고 있는 중이다. 딥러닝은 그런 수많은 조그마한 뉴런들의 팀워크로 이루어진 거대한 생각 기계라 할 수 있다.</p><h3 id=3-가중치와-편향의-역할>3. 가중치와 편향의 역할
<a class=anchor href=#3-%ea%b0%80%ec%a4%91%ec%b9%98%ec%99%80-%ed%8e%b8%ed%96%a5%ec%9d%98-%ec%97%ad%ed%95%a0>#</a></h3><p>인공신경망에서 가중치(Weights)와 편향(Bias)의 역할은, 우리가 친구에게 “이게 중요하고, 저건 별로 중요하지 않아”라고 말하는 것과 비슷하다고 볼 수 있다. 인공신경망이라는 건 사실 수많은 입력 값을 받아서 “이건 고양이야” 혹은 “이건 숫자 3이야”라고 판단하는 계산기인데, 이 계산기의 핵심은 무엇을 얼마나 중요하게 여길지 스스로 판단할 수 있다는 점이다. 바로 그 판단을 가능하게 해주는 것이 가중치와 편향이다.</p><p>먼저 가중치부터 이야기해보자. 어떤 뉴런이 입력을 여러 개 받는다고 상상해보자. 예를 들어 이미지라면 각각의 픽셀이 입력이 될 수 있다. 그 입력값 하나하나가 그냥 있는 그대로 다 중요하냐고 묻는다면, 그렇지 않다. 어떤 픽셀은 진짜 핵심일 수 있고, 어떤 픽셀은 배경이거나 노이즈일 수 있다. 그래서 우리는 각 입력에 ‘중요도’를 부여하는데, 그게 바로 가중치다. 예를 들어 어떤 입력이 0.9의 가중치를 가진다면, 그건 매우 중요한 입력이라는 뜻이고, 0.01의 가중치를 가진다면 별로 신경 쓸 필요 없는 입력이라는 뜻이다. 즉, 가중치는 입력의 영향력을 조절하는 손잡이다.</p><p>그럼 이걸 좀 더 일상적인 예로 풀어보자. 당신이 친구와 햄버거를 먹으러 갈지, 피자를 먹으러 갈지 고민한다고 해보자. 선택은 여러 요인에 따라 달라질 수 있다. 예를 들어 당신은 배가 고프고, 날씨는 덥고, 돈은 얼마 없고, 어제는 피자를 먹었다고 하자. 이때 각각의 요인은 입력이다. 그리고 각 요인이 당신의 선택에 얼마나 영향을 미치느냐는 다를 수 있다. 배고픔은 매우 큰 요인일 수 있으니 가중치가 크고, 날씨는 별로 상관없다면 가중치는 작을 것이다. 이런 식으로 당신은 마음속에서 입력 × 중요도를 계산해서 결정을 내린다. 인공신경망도 마찬가지다. 각 입력에 가중치를 곱해서 얼마나 중요한지를 판단한다.</p><p>계산 과정은 간단하다. 입력 값 x가 들어오면, 이에 가중치 w를 곱해서 wx를 만든다. 이 과정을 모든 입력에 대해 수행하고, 그 결과를 다 더한다. 이걸 z라고 부르자. 그러니까 z = x₁·w₁ + x₂·w₂ + &mldr; + xₙ·wₙ이다. 이 z는 아직 판단 결과가 아니다. 말하자면 지금까지 계산된 총합일 뿐이다. 그런데 여기서 “이 정도 점수가 넘어가면 고양이다” 같은 기준선을 정해주어야 하는데, 이때 등장하는 게 편향이다.</p><p>편향(bias)은 말 그대로, 계산된 총합에 더하거나 빼는 하나의 값이다. 이것이 왜 중요할까? 편향이 없다면, 입력이 전혀 없을 땐 항상 0을 출력해야 할지도 모른다. 그런데 현실은 다르다. 예를 들어 어떤 학생이 시험을 보면 항상 최소한 30점은 맞는다. 이 30점은 공부와 상관없이 기본으로 갖고 있는 능력이라고 할 수 있다. 마찬가지로, 편향은 입력 값이 아무리 작거나 없다 해도, 어느 정도 기본값을 유지하거나 기준선을 조정할 수 있게 해준다.</p><p>편향의 역할은 직관적으로 말해 ‘문턱을 조절하는 스위치’다. 예를 들어 시그모이드 같은 함수에서는 z가 0보다 크면 0.5 이상, 작으면 0.5 미만으로 결과가 나온다. 이때 편향을 더하면 이 문턱을 옮길 수 있다. 즉, 더 엄격하게 판단할 수도 있고, 더 관대하게 판단할 수도 있는 것이다. 쉽게 말해서 어떤 뉴런이 “조금만 자극이 와도 반응할지” 아니면 “강한 자극이 올 때만 반응할지”를 조정하는 건 편향이 하는 일이다. 이렇게 생각하면, 편향은 뉴런의 성격을 바꾼다고도 할 수 있다. 어떤 뉴런은 예민하고, 어떤 뉴런은 둔감하게 만드는 조절기다.</p><p>조금 더 수학적으로 보면, 인공신경망의 뉴런은 y = f(wx + b)라는 구조로 작동한다. 여기서 x는 입력, w는 가중치, b는 편향, f는 활성화 함수다. 이 식에서 w는 입력의 크기를 조절하고, b는 기준점을 조절한다. 결과적으로 이 두 요소는 입력이 어떻게 처리될지를 완전히 결정짓는다.</p><p>이제, 가중치와 편향은 처음에는 아무렇게나 설정된다. 우리가 무작위로 시작한다. 그러고 나서 딥러닝은 데이터를 보면서 정답과 예측이 얼마나 다른지 비교하고, 그 차이를 줄이기 위해 w와 b를 조금씩 바꾼다. 예를 들어, 어떤 입력이 결과에 큰 영향을 줘야 했는데 실제로는 반영이 잘 안 됐다면, 그 입력의 가중치를 키운다. 혹은 기준점이 너무 높아서 뉴런이 자꾸 반응을 못 했다면, 편향을 낮춘다. 이런 식으로 w와 b를 조정하면서 딥러닝은 더 똑똑해진다. 마치 사람이 문제를 틀리면 “아, 여긴 더 신경 써야겠다” 하고 다음에 잘 풀 수 있도록 마음을 고쳐먹는 것처럼, 신경망도 w와 b를 고쳐가면서 학습한다.</p><p>가중치는 마치 각 입력에 대한 스피커의 볼륨이라고 보면 된다. 어떤 소리는 크게 들려야 하고, 어떤 소리는 작게 들려야 한다. 편향은 스피커의 기본 출력 세기라고 할 수 있다. 아무 소리도 없는데도 약간의 잡음이 나올 수 있고, 그것이 필요할 때가 있다. 예를 들어 의사가 환자를 진료할 때, 병력이나 외부 증상 외에도 항상 어느 정도는 “혹시 모를 경우”라는 기본 경계심을 갖는다. 이게 일종의 편향이다. 아무런 입력이 없더라도 어느 정도의 경계심을 갖고 판단을 내리는 것. 딥러닝도 마찬가지다. 완전한 입력이 없더라도, 편향을 통해 기준을 설정하고 판단을 조절한다.</p><p>좀 더 직관적인 비유를 하나 더 해보자. 인공신경망은 마치 요리사다. 다양한 재료들(입력)을 받고, 그 재료들이 얼마나 중요한지를 판단해서 맛을 조절한다. 이때 가중치는 각 재료의 비율을 나타낸다. 예를 들어 짜장면을 만들려면 춘장은 많이 넣고, 물은 적게 넣는다. 재료가 많다고 다 중요한 건 아니다. 가중치를 잘못 주면 맛이 이상해진다. 편향은 마치 요리의 기본 양념이다. 모든 요리에 약간의 소금이 들어가듯, 어떤 결과를 내기 위한 기본값이 존재하는 것이다. 요리를 잘하는 요리사는 재료의 양도 조절하고, 기본 양념도 정확히 넣는다. 딥러닝도 마찬가지다. 가중치와 편향을 잘 조절해야 훌륭한 결과를 낼 수 있다.</p><p>결국, 가중치와 편향은 인공신경망이 세상을 이해하는 방식의 핵심 열쇠다. 가중치는 입력을 어떻게 다룰지 결정하고, 편향은 언제 반응할지를 조절한다. 이 두 가지가 정확히 맞아떨어질 때, 신경망은 복잡한 문제를 정확하게 해결할 수 있다. 그래서 딥러닝에서 학습이란 건 사실 이 두 값을 찾아가는 여정이다. 이 입력은 얼마나 중요하고, 어느 선에서 판단을 바꿔야 할지를 끊임없이 조율해가는 과정이다. 가중치와 편향을 통해 신경망은 단순한 숫자 계산을 넘어, 고양이와 강아지를 구분하고, 언어의 의미를 이해하며, 심지어 예술적인 감각까지 모방할 수 있는 존재로 진화한다.</p><p>이 모든 것이 단지 입력에 숫자를 곱하고 더하는 간단한 수식에서 시작된다는 건 정말 놀라운 일이다. 하지만 이 단순한 수식이 반복되고 조정되고 조합되면서, 우리는 인간처럼 사고하는 기계를 만들어가고 있다. “그냥 숫자 몇 개 곱하고 더하는 건데, 그게 세상을 이해하는 방법이라니. 참 멋진 일이잖아!”</p><h3 id=4-활성화-함수의-필요성>4. 활성화 함수의 필요성
<a class=anchor href=#4-%ed%99%9c%ec%84%b1%ed%99%94-%ed%95%a8%ec%88%98%ec%9d%98-%ed%95%84%ec%9a%94%ec%84%b1>#</a></h3><p>딥러닝에서 활성화 함수(activation function)가 왜 필요한지를 이해하려면, 먼저 아주 단순한 신경망을 떠올려보는 것이 좋다. 입력이 있고, 그 입력에 어떤 가중치를 곱해서 더한 다음, 결과를 출력하는 구조를 생각해보자. 수학적으로는 이런 구조를 &ldquo;선형(linear) 모델"이라고 부른다. 즉, 출력은 입력의 선형 조합이라는 뜻이다. 예를 들어 x₁이라는 입력에 w₁이라는 가중치를 곱하고, x₂에 w₂를 곱한 뒤, 이 둘을 더하고, 마지막으로 편향 b를 더한 것이 출력이 되는 구조다. 아무리 이 과정을 반복해서 층을 쌓고 쌓아도, 그 결과는 여전히 선형(linear)의 범주를 벗어나지 않는다. 왜냐하면 선형끼리 아무리 곱하고 더해도 결국 또 하나의 선형 함수일 뿐이기 때문이다.</p><p>그럼 이게 왜 문제가 될까? 실제 세상은 선형이 아니다. 아주 단순한 예를 들어 보자. 어떤 사람이 배가 고플 때는 음식을 보고 기뻐하지만, 너무 배부를 때는 오히려 그 음식을 보면 짜증이 난다. 즉, 입력과 출력의 관계가 단순한 직선이 아니라, 어떤 구부러진 곡선이 된다. 또는 고양이와 개를 구분할 때, 귀의 크기만으로는 판단이 안 되고, 귀의 크기와 눈의 간격이 특정 조합일 때만 “아, 고양이다!” 하고 판단할 수 있을지도 모른다. 이런 건 선형 함수로는 도저히 설명할 수 없다. 이럴 때 필요한 것이 바로 &ldquo;비선형성(non-linearity)&ldquo;이다. 그리고 이 비선형성을 도입하는 도구가 바로 활성화 함수다.</p><p>한마디로 말해, 활성화 함수는 뉴런에게 생각의 자유를 주는 문이다. 이 문이 없다면 뉴런은 무조건 정해진 선형식대로만 계산을 하게 된다. 이 경우 아무리 층을 많이 쌓아도, 결국 전체 모델은 하나의 선형 모델과 다를 바 없다. 층이 많아봤자 아무 소용이 없는 것이다. 하지만 활성화 함수가 있으면, 그 층마다 새로운 비선형 특성이 들어가고, 그렇게 복잡하고 다양한 패턴을 인식할 수 있는 능력이 생긴다. 다시 말해, 활성화 함수는 단순한 계산기인 뉴런에게 “생각의 전환점”을 제공한다.</p><p>예를 들어, 아주 간단한 활성화 함수 중 하나인 계단 함수(step function)를 보자. 이 함수는 입력이 어떤 기준값보다 크면 출력은 1, 작으면 0을 출력한다. 이 함수는 뉴런에게 “이 정도 자극이 오면 반응해, 아니면 조용히 있어”라는 규칙을 주는 것이다. 그런데 이 함수는 갑자기 뚝 끊어지듯 동작하기 때문에, 현재는 잘 쓰이지 않는다. 대신 좀 더 부드러운 함수들이 널리 쓰인다. 대표적인 예가 시그모이드 함수(sigmoid function)다. 이 함수는 입력이 작을 때는 0에 가까운 값을, 클 때는 1에 가까운 값을 출력하고, 중간에서는 완만하게 상승하는 S자 곡선을 그린다. 이것은 뉴런이 단순히 &ldquo;반응한다, 안 한다"로 나뉘는 것이 아니라, &ldquo;얼마나 강하게 반응할까?&ldquo;를 연속적인 값으로 표현하게 해준다.</p><p>또 다른 유명한 함수는 ReLU(Rectified Linear Unit) 함수다. 이 함수는 입력이 0보다 작으면 0, 크면 그대로 출력한다. 이 단순한 구조는 계산이 빠르고, 깊은 신경망에서도 효과적으로 작동하기 때문에 딥러닝의 표준이 되었다. ReLU는 특히 음수에 대해서는 반응하지 않고, 양수에 대해서만 반응하므로, 뉴런이 “이건 별로 중요하지 않아”라고 판단할 수 있는 기회를 주기도 한다. 이는 마치 우리 뇌가 어떤 소리를 들었을 때 “그냥 백색소음이야”라고 무시하고, 중요한 말소리는 반응하는 것과 비슷하다.</p><p>이렇게 다양한 활성화 함수들은 딥러닝 모델에게 유연함을 선사한다. 이미지 인식, 음성 분석, 언어 이해 같은 문제들은 모두 단순한 직선으로 설명할 수 없는 복잡한 구조를 가지고 있다. 입력 값 사이의 관계도 비선형적이고, 다양한 조건과 조합에 따라 결과가 달라진다. 이러한 현실 세계의 복잡성을 담기 위해선, 신경망이 반드시 비선형성을 갖춰야 하고, 그 핵심이 바로 활성화 함수다.</p><p>활성화 함수는 또 다른 중요한 역할을 한다. 바로 출력 값을 일정한 범위로 제한하는 것이다. 만약 활성화 함수가 없다면, 뉴런의 출력은 입력의 크기에 따라 무한히 커질 수 있다. 예를 들어, 입력이 10이고 가중치도 10이라면 출력은 100이 될 수 있다. 이렇게 점점 커지면 나중에 숫자가 너무 커져서 계산이 어려워진다. 하지만 시그모이드 함수처럼 출력이 항상 0과 1 사이에 머무는 함수는 숫자가 폭주하는 것을 막아준다. 이는 학습의 안정성에도 매우 중요하다. 물론 ReLU처럼 제한 없이 커질 수 있는 함수도 있지만, 이는 각 층마다 정규화를 추가하거나 적절한 초기화 기법으로 조절이 가능하다.</p><p>또한, 활성화 함수는 네트워크가 “결정 경계(decision boundary)”를 학습하게 도와준다. 예를 들어 어떤 데이터가 두 개의 클래스(예: 고양이 vs 강아지)로 나뉜다고 할 때, 그 둘을 나누는 선이나 면을 잘 학습하는 것이 목표다. 선형 함수만으로는 직선이나 평면밖에 만들 수 없다. 그러나 실제 데이터는 복잡하게 뒤얽혀 있기 때문에, 곡선이나 비선형 경계가 필요하다. 이때 활성화 함수가 뉴런의 출력을 비틀어 줌으로써, 모델은 더욱 복잡하고 세밀한 결정 경계를 만들 수 있다. 덕분에 데이터가 뒤얽혀 있어도 올바르게 분류할 수 있게 된다.</p><p>활성화 함수의 존재는 딥러닝의 핵심이자, 신경망을 단순한 선형 회귀와 구분 짓는 근본적인 이유다. 만약 활성화 함수가 없다면, 아무리 층을 쌓아도 선형 모델의 틀을 벗어나지 못한다. 딥러닝이라는 말이 의미 있으려면, &lsquo;딥&rsquo; 즉 층이 깊어질수록 더 풍부한 표현력을 가져야 하고, 그걸 가능하게 해주는 것이 바로 이 작은 비선형 함수들이다.</p><p>이제 파인만 식으로 정말 쉽게 말해보자. 뉴런은 전기를 받는 전구와 같다. 입력이라는 전압이 들어오면, 일정 수준 이상일 때 불이 켜진다. 너무 약하면 불이 안 켜진다. 그런데 이 전구가 단순히 켜지거나 꺼지는 것만 할 수 있다면, 우리는 아주 복잡한 전기 회로를 만들 수 없을 것이다. 그래서 전구의 밝기가 입력 전압에 따라 달라지도록 만든다고 상상해보자. 아주 약하면 어둡고, 점점 밝아지고, 아주 강하면 최대 밝기까지 올라간다. 이것이 바로 시그모이드다. 또는 입력이 없으면 불이 꺼져 있고, 일정 전압 이상이 되면 그 전압만큼 밝아진다고 하자. 이건 ReLU다. 이런 방식으로 전구가 단순히 ‘켜짐/꺼짐’을 넘어서, 입력의 차이에 따라 다양한 반응을 보이게 된다면, 우리는 더 섬세하고 정교한 회로를 만들 수 있게 된다. 딥러닝도 똑같다. 뉴런들이 입력에 따라 다양한 방식으로 반응해야, 복잡한 세상의 패턴을 잘 따라갈 수 있다. 바로 그 역할을 활성화 함수가 해주는 것이다.</p><p>딥러닝의 발전 역사에서도 활성화 함수의 변화는 매우 중요한 순간들이었다. 초기에는 시그모이드나 tanh 함수가 널리 쓰였지만, 이 함수들은 입력이 크거나 작을 때 출력이 포화(saturation)되어, 그 구간에서는 거의 반응을 하지 않게 된다는 단점이 있었다. 이 때문에 학습이 매우 느려지거나 멈추는 문제가 있었다. 이를 해결하기 위해 ReLU 같은 새로운 함수가 제안되었고, 이는 깊은 네트워크를 효율적으로 학습시키는 데 큰 도움이 되었다. 이후에도 Leaky ReLU, ELU, Swish 등 다양한 함수들이 제안되며, 딥러닝 모델의 성능을 끌어올리는 데 기여하고 있다.</p><p>결론적으로, 활성화 함수는 뉴런에게 생각할 수 있는 힘을 준다. 입력을 단순히 더하고 곱하는 것만으로는 세상의 복잡함을 따라갈 수 없다. 하지만 그 계산 결과에 비선형 함수를 적용하면, 뉴런은 다양한 방식으로 반응하게 되고, 그 결과 우리는 사진을 인식하고, 말을 이해하고, 글을 생성하고, 음악을 작곡하는 AI를 만들 수 있다. 이 작은 함수 하나하나가 딥러닝의 진짜 마법을 일으키는 열쇠인 셈이다. 뉴런은 입력을 받아 반응하지만, 그 반응의 방식이 얼마나 다양하고 부드럽냐에 따라 전체 모델이 얼마나 똑똑해질 수 있는지가 결정된다. 그래서 활성화 함수는 단순한 수학 함수 그 이상으로, 딥러닝의 두뇌를 살아 있게 만드는 숨결과도 같다.</p><h3 id=5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들
<a class=anchor href=#5-%eb%8c%80%ed%91%9c%ec%a0%81%ec%9d%b8-%ed%99%9c%ec%84%b1%ed%99%94-%ed%95%a8%ec%88%98%eb%93%a4>#</a></h3><p>딥러닝에서 ‘활성화 함수’란 말은 처음 들으면 어렵게 느껴질 수 있다. 하지만 사실 그 개념은 아주 간단하다. 한마디로 말해, 인공 뉴런이 받은 입력을 ‘어떻게 반응할지’를 결정하는 스위치 역할을 하는 것이 바로 활성화 함수다. 그러니까 입력값이 들어오면, 이걸 그대로 출력하지 않고, 먼저 특정한 함수를 통과시킨 다음 결과를 내보내는 것이다. 왜 그럴까? 그 이유는 이 함수 덕분에 딥러닝이 단순한 계산기 수준을 넘어서서, 복잡하고 비선형적인 세상의 패턴을 배울 수 있기 때문이다. 쉽게 말해, 입력값을 그냥 선형적으로만 계산하면, 곡선도, 문양도, 고양이 얼굴도 절대 배울 수 없다. 비선형성, 즉 곡선 같은 복잡한 패턴을 배워야 진짜 AI처럼 작동한다. 그래서 우리는 계산한 값을 함수에 한 번 통과시켜, 뉴런이 더 똑똑한 판단을 할 수 있게 해주는 것이다.</p><p>가장 먼저 소개할 함수는 시그모이드(Sigmoid) 함수다. 이 함수는 아주 오래전부터 인공 신경망에서 많이 사용되었고, 지금도 몇몇 상황에서는 여전히 의미가 있다. 시그모이드는 어떤 입력값이 들어오든지 간에, 항상 0과 1 사이의 값으로 결과를 압축해준다. 마치 “이건 확률적으로 얼마나 고양이일까?”를 말해주는 것처럼, 결과가 0.9면 90% 확률로 고양이라고 판단한다는 의미다. 시그모이드 함수의 수식은 이렇게 생겼다: f(x) = 1 / (1 + e^(-x)). 이 수식은 음수든 양수든 모든 x를 받아들여서 0과 1 사이로 매끄럽게 바꿔준다. 예를 들어 x가 -10이면 결과는 거의 0에 가깝고, x가 10이면 거의 1에 가깝다. 이 함수의 장점은 부드럽다는 점이다. 결과가 연속적이고, 그 변화가 매끄러워서 최적화할 때 유리하다. 그러나 단점도 있다. 입력이 너무 크거나 작으면 결과가 거의 0이나 1로 고정되어버려서, 뉴런이 더 이상 학습하지 않게 되는 문제가 생긴다. 이를 &lsquo;기울기 소실(vanishing gradient)&lsquo;이라고 부른다. 마치 너무 많은 경험을 한 사람이 더 이상 변화하지 않는 것처럼, 시그모이드 뉴런도 포화되면 움직이지 않는다.</p><p>그다음은 쌍둥이처럼 생긴 함수인 tanh, 즉 하이퍼볼릭 탄젠트 함수가 있다. 이 함수는 시그모이드와 비슷하지만, 출력 범위가 -1에서 1 사이로 바뀐 버전이다. 식은 f(x) = (e^x - e^(-x)) / (e^x + e^(-x))로 나타내는데, 복잡해 보여도 핵심은 이렇다. tanh는 입력값이 작을 땐 음수로, 클 땐 양수로 나오며, 중앙값이 0이라는 점에서 시그모이드보다 유리한 경우가 많다. 왜냐하면 입력이 0 근처일 때는 더 민감하게 반응하기 때문이다. 하지만 tanh도 시그모이드와 같은 문제를 가지고 있다. 입력이 너무 커지거나 작아지면, 역시 출력이 거의 고정되어버리고, 그 결과 뉴런이 잘 학습하지 못하게 된다. 그래서 이 두 함수는 단순하거나 얕은 신경망에서는 괜찮지만, 층이 깊어질수록 점점 문제가 커지게 된다.</p><p>이 문제를 해결하기 위해 등장한 것이 바로 ReLU(Rectified Linear Unit)다. ReLU는 이름은 어려워 보여도 사실은 아주 간단한 함수다. 수식으로는 f(x) = max(0, x)이다. 즉, 입력값이 0보다 크면 그대로 출력하고, 0보다 작으면 그냥 0을 출력한다. 예를 들어 x가 3이면 출력도 3, x가 -2면 출력은 0이 된다. 이 단순한 규칙 덕분에 ReLU는 아주 빠르게 계산할 수 있고, 포화 영역이 없기 때문에 학습이 잘 진행된다. 기울기 소실 문제도 거의 없어서 딥러닝 모델이 아주 깊어져도 잘 작동한다. 이 함수는 2010년대 이후 거의 모든 딥러닝 모델의 표준처럼 자리 잡았다. 하지만 ReLU도 완벽하진 않다. 입력이 음수이면 무조건 0이 되어버리기 때문에, 뉴런이 죽는 현상, 즉 &lsquo;죽은 뉴런(dead neuron)&rsquo; 문제가 생긴다. 어떤 뉴런은 입력값이 계속 음수여서 계속 0만 출력하게 되고, 결국 학습에서 빠져버리는 것이다.</p><p>이 문제를 해결하기 위해 고안된 것이 Leaky ReLU다. 말 그대로 약간 새는(ReLU에 누수 기능이 있는) 버전이다. 수식은 f(x) = x (if x > 0), f(x) = a·x (if x &lt; 0)이다. 여기서 a는 아주 작은 숫자, 예를 들어 0.01 정도다. 그러니까 음수 입력에도 완전히 0이 아니라, 아주 작게나마 값을 넘긴다. 이렇게 하면 뉴런이 완전히 죽지 않고, 살아 있는 상태로 학습을 계속할 수 있게 된다. 이 작은 변화가 실제로는 큰 차이를 만든다. 그래서 최근에는 ReLU 대신 Leaky ReLU나 그 변형들이 자주 사용된다.</p><p>비슷한 맥락에서 나온 함수로는 Parametric ReLU(PReLU)와 ELU(Exponential Linear Unit) 같은 것도 있다. PReLU는 Leaky ReLU와 비슷하지만, 그 작은 계수 a를 학습하도록 만든 것이다. 즉, 학습 도중에 딥러닝 모델이 스스로 “나는 음수 영역을 얼마나 반영할지”를 결정하게 만든다. 이는 더 유연하고 정교한 학습을 가능하게 해준다. ELU는 조금 더 공격적인 접근인데, 입력이 음수일 때 단순히 직선이 아니라 지수 함수로 부드럽게 연결되도록 만들어준다. 수식으로는 f(x) = x (if x > 0), f(x) = α·(exp(x)-1) (if x &lt; 0)이다. 이 함수는 음수 영역에서도 0 근처로 수렴하게 되므로, 평균이 0에 가까워지고, 학습이 더 안정적으로 진행되는 장점이 있다.</p><p>한편, 출력층에서 자주 사용되는 특별한 함수도 있다. 그 대표적인 것이 Softmax 함수다. 이 함수는 다중 클래스 분류 문제에서 매우 자주 등장한다. 예를 들어 &ldquo;이 사진이 고양이일까, 강아지일까, 토끼일까?&ldquo;처럼 여러 가지 정답 중 하나를 선택해야 할 때 쓰인다. Softmax는 각 클래스가 정답일 확률을 계산해준다. 수식은 f(xᵢ) = e^(xᵢ) / Σ(e^(xⱼ))이다. 즉, 각 입력값에 지수함수를 취하고, 그 값을 모두 더한 뒤 각각을 나눠서, 전체가 1이 되도록 정규화한다. 이 함수의 좋은 점은, 입력값이 클수록 더 높은 확률을 주고, 작은 값은 작은 확률로 나타나게 된다는 것이다. 결국 딥러닝 모델이 가장 가능성 있는 정답에 높은 확률을 주고, 나머지는 낮은 확률로 분배하면서 “나는 이게 정답이라고 80% 확신해” 같은 식으로 판단할 수 있게 되는 것이다.</p><p>반면 회귀 문제에서는 출력값이 연속적인 실수여야 하기 때문에, 아무런 제한을 두지 않는 선형 함수, 즉 f(x) = x를 그대로 쓰기도 한다. 이 경우는 ‘활성화 함수 없음’이라고 보기도 한다. 왜냐하면 우리가 출력값을 제한하거나 변형할 필요가 없기 때문이다. 예를 들어 집값을 예측하는 모델이라면, 정답이 0.8이나 1.3 같은 확률이 아니라, 8억 5천만 원처럼 정확한 숫자이기 때문이다.</p><p>이처럼 활성화 함수는 뉴런이 &ldquo;나는 이 입력을 어떻게 처리할까?&ldquo;를 결정하게 해주는 중요한 장치다. 어떤 함수를 쓰느냐에 따라 학습의 속도도, 성능도, 결과의 해석 가능성도 달라진다. 단순히 수학적으로 멋진 함수라고 해서 되는 게 아니라, 실제 문제에 맞게 선택해야 한다. 그래서 딥러닝을 설계할 때 가장 중요한 판단 중 하나가 바로 “어떤 활성화 함수를 사용할 것인가?”라는 질문이다.</p><p>파인만 식으로 말하자면, 활성화 함수는 딥러닝의 ‘기분 조절 장치’라고 할 수 있다. 입력값이 들어왔을 때, “좋아, 그대로 출력할게” 할 수도 있고, “지금은 좀 눌러서 작게 보여줄래” 하거나, “아예 무시할게” 할 수도 있다. 이 모든 반응은 바로 함수에 달려 있다. 사람도 같은 자극을 받아도 반응이 다르듯, 뉴런도 어떤 함수가 연결되어 있는지에 따라 전혀 다른 출력을 낸다. 그래서 뉴런 하나하나는 단순해 보여도, 그 속에 달린 함수가 다르면 전혀 다른 행동을 한다. 마치 같은 사람이라도 기분이 좋을 땐 활발하고, 우울할 땐 조용하듯이 말이다.</p><p>결국 활성화 함수는 딥러닝이 단순한 계산기 그 이상이 되도록 만들어주는 열쇠다. 그 덕분에 딥러닝은 곡선도, 모양도, 얼굴도, 감정도 이해하게 되고, 인간처럼 복잡한 판단을 내릴 수 있는 모델이 된다. 이 작은 함수 하나가, 그 전체 신경망의 성격을 결정하는 셈이다. 그리고 바로 이 점이 딥러닝이 단순한 선형 회귀와 구분되는 가장 중요한 특징 중 하나다. 함수 하나 바꿨을 뿐인데, 세상을 보는 눈이 달라지는 것이다. 그러니 딥러닝에서 활성화 함수는 단순한 수학적 도구가 아니라, 그 자체로 하나의 성격, 하나의 사고방식이라고 말할 수 있다.</p><h3 id=6-은닉층의-개념과-역할>6. 은닉층의 개념과 역할
<a class=anchor href=#6-%ec%9d%80%eb%8b%89%ec%b8%b5%ec%9d%98-%ea%b0%9c%eb%85%90%ea%b3%bc-%ec%97%ad%ed%95%a0>#</a></h3><p>딥러닝에서 은닉층(Hidden Layer)이란 무엇일까? 겉으로 보면 입력층과 출력층 사이에 끼어 있는 정체불명의 덩어리처럼 느껴질 수 있다. 겉으로 드러나지도 않고, 처음엔 뭘 하는지도 잘 모르겠다. 그래서 ‘은닉’이라는 이름이 붙은 것이다. 하지만 사실 이 은닉층이야말로 딥러닝의 핵심이며, 뇌로 따지자면 생각하고, 추론하고, 개념을 만드는 공간이다. 겉에서 드러나는 건 아니지만, 모든 진짜 일은 이 안에서 벌어진다고 해도 과언이 아니다.</p><p>딥러닝 전체 구조를 다시 생각해보자. 입력층은 데이터를 받는다. 예를 들어 이미지를 보면, 그건 수많은 숫자로 이루어진 픽셀의 집합이다. 출력층은 우리가 알고 싶은 정답을 말해준다. 이건 고양이야, 이건 강아지야, 또는 이 이메일은 스팸이야, 아니야, 같은 것들이다. 그런데 이 입력에서 출력까지 바로 이어진다면, 그건 사실 너무 단순하다. 사람의 사고 과정도 그런 식은 아니다. 우리는 무언가를 볼 때 곧바로 결론에 도달하지 않는다. 먼저 보고, 생각하고, 판단하고, 경험과 연결하고, 그제야 결론에 도달한다. 이 가운데 있는 ‘생각의 과정’이 바로 은닉층에 해당한다.</p><p>딥러닝의 은닉층도 마찬가지다. 입력층에서 받은 정보를, 곧바로 출력층에 전달하는 것이 아니라, 여러 개의 은닉층을 거치면서 점점 더 추상적인 정보로 바꾸는 것이다. 예를 들어 이미지를 입력으로 받는다고 하자. 처음에는 단순히 픽셀 값들, 즉 빛의 밝기 정도를 본다. 그 다음 은닉층은 이 픽셀들 사이에서 선이나 모서리 같은 단순한 모양을 파악한다. 그 다음 은닉층은 이 선들을 모아 귀, 눈, 입처럼 조금 더 복잡한 형태를 만들어낸다. 그 다음 층은 이것들을 조합해서 ‘얼굴’이라는 개념을 만든다. 마지막 은닉층은 여러 얼굴 중에서 “이 얼굴은 고양이야”라고 결론을 내리기 직전의 상태를 만들어낸다. 이처럼 은닉층은 단순한 정보에서 점점 더 의미 있는 정보를 만드는 역할을 한다.</p><p>한층만 가지고 이런 일을 할 수 있을까? 물론 이론적으로는 가능하다고 한다. 단 하나의 은닉층만으로도 모든 연산을 할 수 있다는 수학적 증명도 있다. 하지만 현실은 다르다. 단 하나의 층으로는 너무 많은 연산을 해야 하고, 복잡한 구조를 학습하기엔 비효율적이다. 그래서 우리는 여러 개의 은닉층을 쌓는다. 한 층이 하는 일을 나눠서 여러 층이 분담하면, 각각이 단순한 계산만 해도 전체적으로 매우 똑똑한 판단을 내릴 수 있게 된다. 이는 마치 공장에서 물건을 만들 때, 한 명이 처음부터 끝까지 만드는 것보다 여러 명이 단계별로 나눠서 만드는 것이 더 효율적인 것과 같다. 은닉층 하나하나가 전체 문제를 나눠서 해결하는 작은 작업장인 셈이다.</p><p>그렇다면 왜 은닉층은 ‘은닉’이라고 부를까? 그건 사용자가 직접적으로 관찰하거나 조작하지 않기 때문이다. 우리는 입력은 알고, 출력도 안다. 하지만 은닉층이 정확히 어떤 중간 단계를 만들었는지는 알기 어렵다. 그래서 마치 검은 상자처럼 느껴진다. 하지만 딥러닝의 실제 인식 능력은 이 검은 상자 안에서 나온다. 다시 말해, 입력과 출력만 보면 마술처럼 보이는 것도, 사실은 은닉층에서 모든 마법이 벌어지기 때문이다.</p><p>은닉층이 많을수록 좋은 걸까? 꼭 그렇지는 않다. 층이 많으면 많을수록 더 복잡한 문제를 풀 수 있는 건 맞다. 하지만 층이 너무 많아지면, 계산 비용이 커지고, 학습이 어려워지며, 과적합(overfitting)이라는 문제도 생길 수 있다. 과적합이란 훈련 데이터에만 너무 맞춰져서, 새로운 데이터를 잘 처리하지 못하는 현상이다. 마치 시험 공부를 너무 문제집 위주로 해서, 진짜 시험에서 처음 보는 문제가 나오면 당황하는 것과 비슷하다. 그래서 은닉층의 개수는 무작정 늘리는 것이 아니라, 문제의 복잡도에 맞게 조절해야 한다.</p><p>은닉층이 정보를 어떻게 바꾸는지도 중요한데, 각 은닉층은 앞의 층으로부터 받은 숫자들을 가중치(weight)와 편향(bias)을 이용해 계산하고, 활성화 함수라는 수학적 장치를 거쳐 다음 층으로 보낸다. 이 과정은 뉴런 하나하나가 담당하며, 각 뉴런은 자신만의 가중치와 편향을 가지고 있다. 이 가중치와 편향이 조절되는 과정이 바로 딥러닝이 학습을 통해 똑똑해지는 과정이다. 은닉층은 이런 뉴런들이 모여서 ‘생각을 나누는 팀’을 구성하고 있는 것이다. 입력층이 정보를 받아오고, 출력층이 결정을 발표한다면, 은닉층은 그 중간에서 회의하고, 분석하고, 조정하고, 최종 판단을 돕는 고문 역할을 한다.</p><p>또 흥미로운 점은, 은닉층이 많아질수록 ‘추상화’가 진행된다는 것이다. 즉, 입력 정보에서 점점 더 개념적인 정보로 올라가는 것이다. 처음엔 그냥 점이었고, 그다음엔 선, 다음엔 모양, 다음엔 물체, 다음엔 상황, 마지막엔 의미다. 이는 마치 우리가 글을 읽을 때처럼 진행된다. 우리는 한 글자 한 글자를 보고, 단어를 이해하고, 문장을 해석하고, 전체 문맥을 파악하고, 결국 그 글이 말하고자 하는 바를 이해하게 된다. 딥러닝도 똑같다. 은닉층을 지나면서 점점 더 높은 수준의 의미를 이해하게 되는 것이다. 바로 이 능력이 딥러닝이 기존 기계학습과 구별되는 핵심이다.</p><p>딥러닝이 ‘딥(deep)’하다는 말 자체가 은닉층이 깊다는 말이다. 과거에는 은닉층이 한두 개에 불과했지만, 요즘의 딥러닝은 수십, 수백 개의 은닉층을 가진다. 그래서 ‘딥러닝’이라는 이름이 붙은 것이다. 사람 뇌의 피질처럼, 이 층들이 많을수록 더 정교한 판단과 추론이 가능해진다. 물론 단순히 깊기만 하면 안 되고, 잘 설계되고 훈련되어야 의미가 있다. 층이 많아질수록 그 안에서 ‘정보가 사라지는 현상(vanishing gradient)’ 같은 문제가 생기기도 한다. 그래서 이를 해결하기 위한 다양한 기술도 함께 발전해왔다. 예를 들어 배치 정규화(batch normalization), 잔차 연결(residual connection), 드롭아웃(dropout) 같은 기법들이 있다. 이들은 모두 은닉층이 많아졌을 때 생기는 문제를 완화시켜주는 기술이다.</p><p>은닉층은 딥러닝의 뇌 속 생각실이다. 입력은 눈과 귀처럼 정보를 받아오고, 출력은 입처럼 결과를 말한다면, 은닉층은 머릿속에서 정보를 조합하고 판단하는 역할을 한다. 단순한 데이터를 의미 있는 정보로 바꾸는 마법이 바로 여기서 일어난다. 우리가 무엇인가를 보고 “아, 이건 고양이다”라고 말할 때, 그 판단은 단지 눈에 들어온 정보 때문이 아니라, 그 정보를 바탕으로 머릿속에서 일어난 수많은 추론과 연결 덕분이다. 딥러닝도 마찬가지다. 은닉층이 없다면, 단지 입력을 받아 출력하는 기계에 불과하지만, 은닉층이 있기 때문에 학습하고 판단하며, 새로운 상황에 적응할 수 있다.</p><p>결국 은닉층이란 딥러닝의 ‘생각’을 담당하는 공간이다. 이 공간에서 입력은 의미로 변하고, 감각은 인식으로 진화하며, 데이터는 개념으로 추상화된다. 보이지 않지만, 모든 핵심은 바로 이곳에서 일어난다. 그래서 우리는 이 ‘숨겨진 층’을 가장 중요하게 다루고, 그 설계와 학습 방식에 가장 많은 주의를 기울이는 것이다. 딥러닝이 진정한 지능에 가까워지기 위해서는, 이 은닉층이 단순한 계산기들의 집합이 아니라, 진짜 생각을 할 수 있는 구조로 발전해 나가야 한다. 그리고 바로 그 점에서, 은닉층은 단순한 계산 단위를 넘어, 딥러닝의 철학과 미래를 품고 있는 공간이라 할 수 있다.</p><h3 id=7-출력층-설계>7. 출력층 설계
<a class=anchor href=#7-%ec%b6%9c%eb%a0%a5%ec%b8%b5-%ec%84%a4%ea%b3%84>#</a></h3><p>딥러닝의 출력층(Output Layer)은 신경망의 마지막 단계에서 우리가 궁금해하는 정답을 말해주는 부분이다. 뉴런들이 앞단에서 열심히 일하고, 수많은 가중치와 활성화 함수를 거쳐 정보를 점점 더 의미 있는 형태로 바꾸어가면서 최종적으로 도착하는 곳이 바로 출력층이다. 말하자면 이건 딥러닝이 우리에게 내리는 결론이라고 볼 수 있다. 그래서 출력층이 하는 일은 아주 단순하면서도 매우 중요하다. 뉴런 전체의 긴 여정을 마친 정보가 여기서 “이건 고양이야” 혹은 “이건 7이야” 또는 “내일은 비가 와”라고 말해주는 것이다.</p><p>그렇다면 출력층은 어떻게 생겼을까? 사실 앞에 있는 은닉층과 구조는 비슷하다. 뉴런이 있고, 그 뉴런이 앞단의 뉴런들과 연결되어 있으며, 역시 입력을 받아서 가중치를 곱하고 편향을 더한 뒤 어떤 함수를 거쳐 결과를 출력한다. 그런데 이 출력층은 다른 층과는 조금 다른 목적을 가지고 있다. 앞의 은닉층들은 점점 더 추상적인 특징을 추출하는 데 집중하는 반면, 출력층은 그 모든 정보를 바탕으로 최종적인 예측을 해야 한다. 마치 긴 논리적 추론 과정을 거친 뒤 “그래서 결론은 이거야”라고 말하는 부분이다.</p><p>이 출력층을 어떻게 설계하느냐는 우리가 풀고자 하는 문제의 종류에 따라 달라진다. 문제는 크게 둘로 나눌 수 있다. 하나는 분류 문제(classification)고, 다른 하나는 회귀 문제(regression)다.</p><p>먼저 분류 문제를 생각해보자. 예를 들어 고양이와 강아지를 구분하는 문제다. 이럴 때 우리는 출력층에서 “이건 고양이야” 혹은 “이건 강아지야” 같은 대답을 듣고 싶다. 이건 어떤 선택지를 고르는 문제니까, 출력층에서는 각 선택지에 대한 확률을 출력하면 좋다. 예를 들어 [고양이일 확률: 0.8, 강아지일 확률: 0.2] 같은 식이다. 이렇게 하려면 출력층의 뉴런 수는 클래스의 수만큼 있어야 한다. 고양이와 강아지면 2개, 0~9까지 숫자 구분이면 10개가 필요하다.</p><p>여기서 중요한 건 출력층에서 사용하는 함수, 즉 활성화 함수다. 분류 문제에서는 보통 소프트맥스(Softmax) 라는 함수를 쓴다. 이 함수는 모든 출력값을 0과 1 사이의 값으로 바꾸고, 그 합이 1이 되도록 정규화해준다. 다시 말해, 각각의 출력값이 확률처럼 보이도록 만들어주는 것이다. 소프트맥스는 이렇게 작동한다. 각 뉴런의 z값(입력값과 가중치의 곱에 편향을 더한 값)을 지수함수로 바꿔서 모두 더한 뒤, 각 값에 대해 전체 합으로 나누는 것이다. 즉, yᵢ = exp(zᵢ) / Σ exp(zⱼ) 이런 식이다. 이렇게 하면 어떤 출력은 0.7, 어떤 출력은 0.2, 또 다른 출력은 0.1이 될 수 있는데, 합은 항상 1이다. 그러면 우리는 가장 높은 값을 가진 출력을 예측값으로 선택하면 된다. 딥러닝은 이런 방식으로 &ldquo;이건 고양이일 확률이 0.8"이라고 말하는 셈이다.</p><p>반면 회귀 문제는 숫자를 예측하는 문제다. 예를 들어 내일의 온도는 몇 도일까? 혹은 아파트 가격이 얼마일까? 같은 질문에 답하는 것이다. 이럴 땐 선택지가 아니라 정확한 값을 알고 싶으니까 확률이 아니라 연속적인 수치를 출력해야 한다. 따라서 회귀 문제에서는 출력층에서 그냥 선형 함수(linear function) 를 사용하는 경우가 많다. 즉, 활성화 함수를 쓰지 않거나, 항등 함수(identity function)를 쓴다. 이 함수는 입력을 그대로 출력하는 함수다. z를 받으면 그냥 z를 출력한다. 이유는 간단하다. 어떤 숫자든 제한 없이 출력할 수 있어야 하기 때문이다. 소프트맥스나 시그모이드처럼 출력 범위가 0~1로 제한되면, 실제 숫자를 예측할 수 없기 때문이다.</p><p>그럼 좀 더 복잡한 경우는 어떨까? 예를 들어 이미지를 보고 자동차가 어디 있는지 좌표로 알려달라고 한다면, 출력층은 좌표값(x, y)을 출력해야 하니까 두 개의 숫자를 연속적으로 예측해야 한다. 이럴 땐 출력 뉴런이 2개고, 각각 항등 함수를 거쳐 최종 값을 출력한다. 또는 한 문장을 예측해야 하는 자연어 처리(NLP) 문제에서는 출력층이 단어 사전의 크기만큼 커지기도 한다. 만약 단어 사전에 10,000개 단어가 있다면, 출력층 뉴런이 10,000개라는 뜻이다. 이 모든 뉴런에 대해 소프트맥스를 적용해서 가장 높은 확률을 갖는 단어를 다음 단어로 예측하는 식이다. 그래서 NLP에서는 출력층이 매우 커지는 일이 흔하다.</p><p>출력층의 또 다른 역할은 손실 함수(loss function) 와 잘 연결되는 것이다. 딥러닝은 예측을 한 뒤, 정답과 얼마나 차이가 나는지를 손실 함수로 계산하고, 이 값을 줄이는 방향으로 학습한다. 손실 함수는 출력층의 형태에 따라 달라진다. 예를 들어 분류 문제에서는 보통 크로스 엔트로피 손실(cross-entropy loss) 를 쓰고, 회귀 문제에서는 평균 제곱 오차(MSE) 를 쓴다. 그리고 이 손실 값을 줄이기 위해 역전파(backpropagation)를 통해 가중치와 편향을 업데이트한다. 결국 출력층의 구조가 어떻게 생겼느냐에 따라 손실 함수도 달라지고, 학습 방법도 달라지는 것이다.</p><p>여기서 흥미로운 점은, 출력층이 반드시 하나의 값을 출력해야 하는 건 아니라는 것이다. 예를 들어 다중 레이블 분류(multi-label classification)라는 게 있다. 한 이미지에 고양이도 있고 강아지도 있을 수 있다면, 정답이 하나가 아니라 여러 개다. 이럴 땐 출력층의 각 뉴런이 각각의 클래스에 대해 “있다/없다”를 판단해야 한다. 이런 경우에는 시그모이드(sigmoid) 함수를 출력층에 써서, 각 클래스마다 독립적으로 확률을 출력하게 만든다. 이렇게 하면 “고양이 0.9, 강아지 0.7, 사람 0.1” 같은 예측이 가능하다. 이때는 각 확률이 독립적이기 때문에 전체가 1이 될 필요는 없다. 반면 소프트맥스는 확률 전체가 1이 되도록 만들기 때문에, “하나만 선택해야 하는 문제”에 적합하다.</p><p>출력층은 딥러닝 모델의 문장 끝 마침표와 같다. 앞에서 얼마나 복잡하고 정교한 표현을 했는지와 관계없이, 마지막 마침표가 어색하면 전체 문장이 이상해진다. 출력층이 엉뚱한 구조거나 잘못된 함수를 쓰면, 아무리 앞단이 열심히 일해도 결과가 틀린다. 그래서 출력층 설계는 모델 설계에서 매우 중요하다. 문제의 종류를 명확히 파악하고, 그에 맞는 출력 구조와 함수, 손실 함수를 함께 고려해야 딥러닝 모델이 잘 작동한다.</p><p>정리하자면, 출력층은 딥러닝이 어떤 문제를 푸는지에 따라 형태와 기능이 달라진다. 분류 문제라면 소프트맥스를 통해 확률을 출력하고, 회귀 문제라면 항등 함수를 통해 수치를 출력한다. 그리고 이 출력값을 손실 함수와 연결해 학습 방향을 잡아간다. 출력층은 단순해 보이지만, 모델 전체의 목적과 정답을 가장 잘 반영하는 부분이다. 마치 사람의 판단에서 마지막에 “그래서 난 이렇게 생각해”라고 말하는 것과 같다. 따라서 출력층은 단순한 계산의 끝이 아니라, 딥러닝이라는 생각 기계가 세상에 내놓는 한 마디 말인 셈이다.</p><p>출력층은 딥러닝이 &ldquo;내가 열심히 생각한 결과는 이거야!&ldquo;라고 말하는 마지막 문장이다. 이 문장을 제대로 쓰기 위해선 문제를 제대로 이해하고, 그에 맞는 표현 방식을 택해야 한다. 그래서 우리는 출력층에서 어떤 함수와 구조를 사용할지를 고민하며, 딥러닝이 우리 대신 정확한 답을 내리도록 돕는 것이다. 딥러닝은 계산하는 기계지만, 출력층은 그 계산의 의미를 해석해주는 통역기이기도 하다. 이 통역기가 잘 작동해야, 우리는 딥러닝이 진짜 무엇을 이해했는지를 알 수 있다.</p><h3 id=8-모델-구조의-표현과-설계>8. 모델 구조의 표현과 설계
<a class=anchor href=#8-%eb%aa%a8%eb%8d%b8-%ea%b5%ac%ec%a1%b0%ec%9d%98-%ed%91%9c%ed%98%84%ea%b3%bc-%ec%84%a4%ea%b3%84>#</a></h3><p>딥러닝에서 ‘모델 구조(Architecture)’라는 말은, 쉽게 말해 신경망이 어떤 모양을 하고 있는지를 의미한다. 마치 집을 지을 때 “1층은 거실, 2층은 침실”처럼 설계를 하듯이, 딥러닝에서도 “입력층에는 몇 개의 뉴런을 두고, 은닉층은 몇 개, 출력층은 무엇을 하게 할까?” 같은 전체 설계를 정하는 것이다. 신경망이란 이름 그대로, 뉴런들이 층을 이루어 연결되어 있는 구조이기 때문에, 이 구조가 어떻게 생겼는지에 따라 신경망이 어떤 문제를 잘 푸는지가 달라진다. 그러니까 모델 구조를 잘 설계하는 일은, 딥러닝에서 무척 중요한 작업이다.</p><p>먼저 아주 간단한 예부터 생각해보자. 어떤 아이가 고양이와 개를 구분하는 법을 배우고 싶다고 하자. 그 아이의 뇌에는 수많은 뉴런이 있지만, 우리가 만들 인공신경망은 그보다 훨씬 작고 단순하다. 이미지가 들어오면, 그것을 해석해서 “고양이”인지 “개”인지 맞히는 신경망을 만들고 싶다. 그럼 가장 먼저 할 일은 입력이 몇 개인지를 정하는 것이다. 이게 바로 입력층(input layer)이다. 예를 들어, 흑백 사진이라면 각 픽셀의 밝기 값이 하나의 숫자가 되고, 전체 28×28 크기라면 총 784개의 숫자가 입력값이 된다. 이 784개의 숫자가 바로 입력층의 뉴런 784개에 들어가는 셈이다. 여기까지는 단순히 데이터를 집어넣는 단계다.</p><p>그다음은 ‘은닉층(hidden layer)’이다. 이름이 왜 은닉층이냐 하면, 입력층도 아니고 출력층도 아니기 때문에, 그 중간에서 조용히 계산을 하는 층이라서 그렇다. 이 은닉층이 몇 층이 있는지, 각 층에 뉴런이 몇 개 있는지가 신경망의 복잡도와 표현력에 아주 큰 영향을 미친다. 만약 은닉층이 아예 없거나 하나만 있다면, 그 신경망은 선형적인 문제만 풀 수 있다. 즉, 선으로 나눌 수 있는 문제만 풀 수 있다는 뜻이다. 하지만 세상은 그렇게 단순하지 않다. 고양이와 개는 단순한 선으로 구분되지 않는다. 그래서 우리는 은닉층을 여러 개 쌓아서 비선형 문제, 즉 복잡한 경계를 가진 문제를 풀 수 있게 만든다. 층이 많아질수록 신경망은 더 깊어지고, 그래서 ‘딥러닝(Deep Learning)’이라는 이름이 붙은 것이다.</p><p>각 은닉층에 몇 개의 뉴런을 넣을지도 결정해야 한다. 뉴런이 너무 적으면, 신경망이 문제를 풀 능력이 부족하고, 너무 많으면 오히려 불필요한 계산이 많아져서 학습이 느려지고, 과적합(overfitting)이 일어날 수 있다. 그러니까 적당히, 문제의 복잡도에 맞게 층의 수와 뉴런의 개수를 정해야 한다. 이건 마치 퍼즐을 풀기 위해 적당한 도구 상자를 꾸리는 것과 같다. 간단한 퍼즐에는 작은 도구 상자면 되고, 복잡한 퍼즐에는 좀 더 다양한 도구가 필요하다. 하지만 너무 많은 도구를 갖고 있으면, 오히려 선택이 어렵고, 혼란스러울 수 있다.</p><p>이제 마지막은 출력층(output layer)이다. 출력층은 문제의 목적에 따라 모양이 달라진다. 예를 들어 이미지가 고양이인지 개인지를 판단하는 이진 분류(binary classification) 문제라면, 출력층에 뉴런이 1개만 있으면 된다. 이 뉴런은 ‘이게 고양이일 확률’을 출력하면 된다. 만약 세 가지 동물을 구분하는 문제라면, 출력층에 뉴런이 3개 있어야 하고, 각각 고양이, 개, 토끼일 확률을 출력해야 한다. 이처럼 출력층의 구조는 우리가 푸는 문제의 정답 형식과 딱 맞아떨어져야 한다.</p><p>그럼 어떤 모델 구조가 좋은 구조일까? 사실 이건 문제마다 다르다. 사람 얼굴을 구분하는 문제라면, 이미지의 공간적인 특징을 잘 파악할 수 있는 구조가 필요하다. 그래서 ‘합성곱 신경망(Convolutional Neural Network, CNN)’이라는 구조가 자주 쓰인다. 이 구조는 이미지에서 눈, 코, 입 같은 국소적인 특징을 잘 잡아내고, 그 특징들을 점점 더 추상화해서 전체 얼굴을 파악한다. 반면, 시간이 흐름에 따라 변하는 데이터를 다룰 때는 ‘순환 신경망(Recurrent Neural Network, RNN)’이나 ‘LSTM(Long Short-Term Memory)’ 같은 구조를 쓴다. 이들은 앞에서 나온 정보가 뒤에 영향을 주도록 설계되어 있다. 예를 들어 문장이나 음성처럼 앞뒤 맥락이 중요한 데이터를 처리할 수 있다.</p><p>요즘은 트랜스포머(Transformer)라는 구조가 특히 인기가 많다. 챗GPT 같은 모델도 이 구조를 쓴다. 트랜스포머는 입력의 순서를 고려하면서도 병렬 계산이 가능하도록 만든 구조다. 그래서 문장, 코드, 음악, 이미지 등 다양한 데이터를 빠르게 처리할 수 있다. 이처럼 모델 구조는 정답이 정해져 있는 게 아니라, 문제에 따라 적절히 고르는 것이다.</p><p>그럼 모델 구조를 표현할 때는 어떻게 할까? 실제로 우리는 도표나 코드를 이용해서 구조를 표현한다. 예를 들어 “784 → 128 → 64 → 10”이라고 적으면, 입력층에 784개, 첫 은닉층에 128개, 두 번째 은닉층에 64개, 출력층에 10개의 뉴런이 있다는 뜻이다. 이 구조는 숫자 이미지 분류 문제(MNIST)에서 자주 쓰이는 간단한 예다. 또 다른 방법으로는 구조를 그림으로 그리기도 한다. 각 층을 원이나 네모로 표시하고, 그 사이에 선을 연결해서 흐름을 나타낸다. 이걸 보면 정보가 어디서 들어와서, 어떻게 계산되고, 어디로 나가는지를 직관적으로 알 수 있다.</p><p>요즘은 더 복잡한 구조를 만들기 위해 ‘모듈화’도 한다. 즉, 자주 쓰는 계산 덩어리를 하나의 블록처럼 만들어서 조립하듯이 구조를 만든다. CNN에서는 합성곱 블록, 풀링(pooling) 블록, 완전연결층(fully connected layer) 같은 블록들이 대표적이다. 트랜스포머에서는 self-attention 블록, feed-forward 블록 등이 있다. 이런 블록들을 여러 번 반복하면서 전체 구조를 구성한다. 그래서 거대한 딥러닝 모델도 사실은 몇 가지 반복되는 패턴의 조합인 경우가 많다.</p><p>그런데 모델 구조를 설계한다고 해서 그것이 무조건 잘 작동하는 건 아니다. 우리가 짠 구조가 데이터를 잘 이해하지 못하면, 예측도 엉망이 된다. 그래서 우리는 구조를 만들고 나서, 데이터를 넣어 학습시키고, 그 성능을 평가하면서, 구조를 계속 바꾸고 조정한다. 이 과정을 ‘모델 튜닝’이라고 한다. 예를 들어 층의 수를 줄였다 늘렸다 해보고, 뉴런 수를 바꿔보고, 혹은 다른 활성화 함수를 써보기도 한다. 어떤 경우에는 dropout 같은 기법으로 일부 뉴런을 임시로 꺼서 과적합을 방지하기도 한다. 마치 건물 설계에서 기둥을 더 넣거나 창문을 바꾸는 것처럼, 모델 구조도 유연하게 바꿔가며 실험해야 한다.</p><p>게다가 최근에는 사람이 직접 구조를 설계하는 것 대신, 컴퓨터가 스스로 구조를 찾게 하는 방법도 있다. 이걸 NAS(Neural Architecture Search)라고 부른다. 컴퓨터가 수많은 구조를 시도하고, 그중에서 성능이 좋은 구조를 자동으로 찾아주는 것이다. 마치 로봇이 집을 지어보고, 튼튼한 구조를 스스로 찾아내는 느낌이다.</p><p>결국 딥러닝에서 모델 구조를 설계한다는 것은, 문제를 잘 풀 수 있는 계산의 흐름과 모양을 만들어주는 일이다. 입력이 들어와서, 여러 단계를 거치고, 최종적으로 어떤 정답을 뱉을지를 설계하는 것이다. 이 구조가 너무 단순하면 문제를 잘 이해하지 못하고, 너무 복잡하면 계산 비용이 너무 크거나 과적합이 일어난다. 그래서 적절한 구조를 만드는 일은 딥러닝에서 가장 창의적인 작업 중 하나다. 사람의 감각, 경험, 실험 결과가 모두 필요하다.</p><p>딥러닝 모델 구조는 계산이 흐르는 도로망을 짜는 일이다. 입력은 자동차처럼 들어오고, 은닉층에서는 여러 신호등과 분기점이 그 입력을 다듬고 처리한다. 출력층은 최종 목적지다. 이 도로망을 얼마나 효율적으로, 잘 연결해놓았느냐에 따라 자동차가 빠르고 정확하게 도착할 수 있는지가 달라진다. 그리고 이 도로망은 우리가 직접 설계할 수도 있고, 스스로 학습하는 AI에게 맡길 수도 있다. 중요한 건, 문제를 해결할 수 있는 올바른 경로를 만들어주는 것. 그것이 바로 딥러닝에서 모델 구조를 설계한다는 뜻이다.</p><h3 id=9-정방향-연산>9. 정방향 연산
<a class=anchor href=#9-%ec%a0%95%eb%b0%a9%ed%96%a5-%ec%97%b0%ec%82%b0>#</a></h3><p>딥러닝에서 &lsquo;정방향 연산(Forward Propagation)&lsquo;이라는 개념은 이름만 보면 조금 어려워 보일 수 있지만, 사실은 매우 단순한 계산 흐름을 뜻한다. 마치 물이 높은 곳에서 낮은 곳으로 흐르듯, 데이터가 딥러닝 모델의 한 층에서 다음 층으로 흘러가며 계산되는 과정을 말한다. 이 흐름은 입력에서 시작해서 최종 출력까지 한 방향으로만 흐르기 때문에 ‘정방향’이라 부른다. 이제 이 개념을 파인만 식으로, 즉 아주 쉽게, 그림을 그리듯 이야기해 보자.</p><p>딥러닝 모델을 커다란 계산 공장이라고 생각해 보자. 이 공장에는 여러 개의 작업대가 있고, 각 작업대마다 일꾼들이 나란히 앉아 있다. 이 일꾼들이 바로 ‘뉴런(Neuron)’들이다. 공장의 가장 앞쪽에는 입력 작업대가 있고, 그 뒤로 여러 개의 은닉층 작업대가 있으며, 가장 마지막에는 출력 작업대가 있다. 하나의 데이터가 이 공장에 들어오면, 앞의 작업대에서부터 시작해서 각 층을 거치며 점점 더 복잡한 처리를 거쳐 최종적인 결과로 바뀐다. 이 전체 과정이 바로 정방향 연산이다.</p><p>그럼, 예를 들어 고양이 사진 한 장을 이 공장에 넣었다고 해 보자. 처음에는 이 사진이 수많은 픽셀 숫자로 바뀌어서 입력층에 들어간다. 입력층의 각 뉴런은 이 숫자를 그대로 받는다. 예를 들어 첫 번째 뉴런은 사진의 왼쪽 위 픽셀 값을 받고, 두 번째 뉴런은 그 옆 픽셀 값을 받고, 이런 식으로 수천 개의 숫자가 입력된다. 이제 이 숫자들은 첫 번째 은닉층으로 전달된다. 하지만 그냥 전달되는 게 아니라, 계산을 거친다.</p><p>은닉층의 뉴런들은 각각 입력값에 자신만의 ‘가중치(weight)’를 곱한다. 마치 일꾼이 물건에 라벨을 붙이듯, “이 입력은 중요하니까 0.9배 해주고, 저 입력은 덜 중요하니까 0.1배 해줘”라고 가중치를 곱하는 것이다. 그런 다음 곱해진 모든 값을 더하고, 여기에 ‘편향(bias)’이라는 작은 조정 값을 더한다. 마지막으로, 이 숫자는 ‘활성화 함수(activation function)’라는 작은 필터를 통과한다. 이 필터는 일종의 조건 검사기처럼 작동해서, 입력이 어느 수준 이상이면 통과시키고, 그렇지 않으면 줄이거나 끊는다.</p><p>예를 들어, 뉴런 하나가 받는 입력이 세 개라고 하자. 각각 x₁, x₂, x₃이고, 가중치는 w₁, w₂, w₃이다. 그럼 이 뉴런은 먼저 x₁·w₁ + x₂·w₂ + x₃·w₃를 계산하고, 여기에 편향 b를 더한다. 이걸 z라고 부르자. 그리고 그다음에는 y = f(z)라는 계산을 한다. 여기서 f는 활성화 함수다. 이 y가 바로 뉴런의 출력이며, 다음 층으로 전달될 값이다. 이 과정을 각 뉴런마다 반복하면, 하나의 층 전체에서 계산이 완료된다.</p><p>그다음에는 이 출력 값들이 다시 다음 층의 입력으로 들어간다. 그리고 똑같은 과정을 반복한다. 입력에 가중치를 곱하고, 다 더하고, 편향을 더하고, 활성화 함수를 적용해서 결과를 만든다. 이렇게 데이터를 한 층 한 층 통과시켜 가며, 점점 더 복잡하고 추상적인 특징을 잡아내는 것이다. 앞쪽 층에서는 단순한 선이나 점 같은 정보를 파악하지만, 뒤쪽으로 갈수록 눈, 귀, 얼굴 전체 같은 더 큰 구조를 인식하게 된다. 이렇게 해서 마지막 출력층에 도달하면, 그 층은 결국 우리가 원하는 정답을 예측해낸다. “이건 고양이야”, “이건 강아지야” 같은 식이다.</p><p>정방향 연산을 파인만 식으로 다시 표현하면, 마치 비행기가 정해진 경로를 따라 차례차례 비행장들을 지나가며 연료를 충전하고, 엔진을 점검하면서 최종 목적지로 향하는 것과 같다. 각 층은 하나의 비행장이고, 그곳에서 연산이라는 점검을 받는다. 한 번 연산된 출력은 다시 다음 비행장으로 넘어가고, 또다시 점검과 충전을 받으며 목적지까지 간다. 이렇게 출발지에서 도착지까지 한 방향으로만 나아가는 것이 바로 ‘정방향’의 의미다.</p><p>그럼 왜 이렇게 층을 나눠서 계산할까? 이유는 간단하다. 한 번에 모든 특징을 파악하는 건 너무 어려운 일이기 때문이다. 사람도 뭔가를 이해할 때 한 단계씩 생각한다. 예를 들어, 우리가 동물을 볼 때도 처음엔 색깔, 다음엔 윤곽, 마지막엔 전체 형상을 본다. 딥러닝도 마찬가지다. 처음 층에서는 색깔, 다음 층에서는 모양, 그다음엔 구조 같은 걸 알아내면서 점점 더 똑똑한 판단을 한다. 층이 깊을수록, 즉 신경망이 깊을수록 복잡한 개념도 알아낼 수 있다.</p><p>또 하나 중요한 건, 이 모든 연산은 숫자로만 이루어진다는 것이다. 이미지도 숫자, 소리도 숫자, 글자도 숫자로 바꿔서 넣는다. 그리고 계산도 곱셈, 덧셈, 함수 적용만으로 구성된다. 딥러닝의 똑똑함은 사실 숫자를 잘 다루는 능력에서 오는 것이다. 그리고 그 모든 숫자 연산이 층을 따라 한 방향으로 흘러가며 이루어지기 때문에, 이걸 ‘정방향 연산’이라고 부른다.</p><p>이 연산은 학습이 끝난 후에도 계속 사용된다. 학습이란 건, 이 정방향 연산의 중간에 있는 가중치와 편향 값들을 계속 조정하는 과정이다. 처음엔 아무렇게나 계산해서 결과가 틀릴 수 있다. 그럼 뒤에서 &lsquo;정답&rsquo;과 비교해서 &ldquo;얼마나 틀렸는지"를 계산하고, 이걸 바탕으로 가중치와 편향을 조금씩 바꾼다. 이렇게 바꾸는 건 역방향 연산(Backpropagation)이라고 하는데, 이건 정방향 연산을 반대로 따라가면서 &ldquo;여기서 조금 고치자"라고 알려주는 것이다. 하지만 학습이 끝나고 나면, 다시 정방향 연산만 사용해서 입력을 받아 예측을 하게 된다. 그러니까 정방향 연산은 딥러닝이 예측을 할 때 사용하는 메인 루트이며, 우리가 딥러닝 모델을 통해 어떤 문제든 해결할 때 항상 이 경로를 따라간다.</p><p>이쯤 되면, 정방향 연산은 그냥 하나의 거대한 계산 흐름이라고 이해할 수 있다. 입력이 들어오면, 정해진 규칙에 따라 각 층을 거쳐 가며 계산되고, 그 결과가 출력으로 나간다. 복잡하게 보일 수 있지만, 본질은 단순하다. 입력을 받아서, 각 층에서 반복적으로 곱하고 더하고 함수를 적용하면서, 마지막까지 흘려보내는 것. 이 흐름 안에 딥러닝의 모든 똑똑함이 담겨 있다.</p><p>마치 요리를 한다고 생각해보자. 감자, 당근, 고기, 양파 같은 재료들이 입력이라면, 우리는 재료를 썰고, 볶고, 끓이고, 양념을 더하는 여러 단계를 거쳐 맛있는 스튜를 만들어낸다. 이 과정에서 각 단계는 재료를 조금씩 더 나은 형태로 바꾸어 주는 층이고, 그 각 단계에서의 조리 방식은 활성화 함수라고 볼 수 있다. 그리고 최종 결과물은 딥러닝 모델의 출력이다. 즉, 정방향 연산은 입력 재료를 받아 단계별 조리를 거쳐 완성된 요리를 만들어내는 일련의 흐름이다.</p><p>결국 정방향 연산은 딥러닝의 가장 기본적인 동작 방식이며, 모델이 입력을 처리하고 예측을 생성하는 데 꼭 필요한 계산 흐름이다. 이 흐름 속에서 각각의 뉴런은 자신에게 들어온 정보를 가중치와 편향을 통해 조정하고, 활성화 함수를 거쳐 출력을 만들어낸다. 그리고 그 출력이 다음 뉴런의 입력이 되고, 이런 식으로 하나의 거대한 계산 네트워크가 움직이게 된다. 이 단순한 원리를 반복해서 적용함으로써, 우리는 복잡한 문제도 해결할 수 있게 되는 것이다.</p><p>정방향 연산은 수많은 작은 계산기들이 정보를 차례대로 주고받으며 문제를 해결하는 협업 과정이다. 각각의 계산기는 자기 할 일만 잘하고, 정해진 순서에 따라 다음 사람에게 결과를 넘긴다. 이 단순하고도 질서정연한 흐름이 바로 딥러닝이 작동하는 방식이며, 우리가 보는 AI의 똑똑함 뒤에 숨은 아주 기초적인 원리다.</p><h3 id=10-손실-함수의-정의와-선택>10. 손실 함수의 정의와 선택
<a class=anchor href=#10-%ec%86%90%ec%8b%a4-%ed%95%a8%ec%88%98%ec%9d%98-%ec%a0%95%ec%9d%98%ec%99%80-%ec%84%a0%ed%83%9d>#</a></h3><p>딥러닝에서 손실 함수(Loss Function)를 이해하는 가장 좋은 방법은 이렇게 생각하는 것이다. 어떤 학생이 시험을 본다고 해보자. 시험을 보고 나면 점수를 받는다. 이 점수는 “얼마나 잘했는가”를 나타내는 수치다. 딥러닝도 마찬가지다. 딥러닝은 학습을 통해 무언가를 맞히려고 한다. 예를 들어 이미지를 보고 “이건 고양이다”라고 말하는 모델이 있다고 하자. 그런데 실제로는 그 이미지가 강아지였다면, 딥러닝은 틀린 것이다. 그럼 얼마나 틀렸는지를 알려줘야 다시 고칠 수 있다. 이 “얼마나 틀렸는가”를 수치로 표현한 것이 바로 손실 함수다.</p><p>딥러닝은 이 손실 함수를 통해 배운다. 결과가 틀리면 손실 값이 크고, 정답과 가까우면 손실 값이 작아진다. 딥러닝은 손실 값을 줄이기 위해 가중치와 편향 같은 내부 값을 조금씩 바꿔가며 학습을 진행한다. 마치 학생이 틀린 문제를 보고 “이 문제는 이런 식으로 푸는 거였구나”라고 하면서 다시 공부하는 것과 같다. 그러니까 손실 함수는 딥러닝에게 &ldquo;지금 너는 얼마나 잘하고 있는지"를 알려주는 채점표 같은 역할을 하는 셈이다.</p><p>손실 함수가 없다면, 딥러닝은 무얼 기준으로 학습해야 할지 모르게 된다. 어떤 결과가 좋은지 나쁜지를 판단할 기준이 없기 때문이다. 마치 학생에게 시험은 봤는데 점수를 안 주면, 무엇을 복습해야 할지도 모르는 것처럼 말이다. 그래서 손실 함수는 딥러닝의 학습 과정에서 반드시 필요한 요소다.</p><p>그럼 손실 함수는 어떤 식으로 계산될까? 가장 간단한 예로, 예측 값과 실제 정답의 차이를 그대로 계산하는 방식이 있다. 예를 들어 딥러닝이 “고양이일 확률이 0.7이야”라고 예측했는데, 실제로는 고양이였다면 정답은 1이다. 이 경우, 손실 함수는 1과 0.7의 차이, 즉 0.3이 된다. 이것을 더 넓게 일반화하면 &lsquo;오차(error)&lsquo;라는 개념이 된다. 이 오차를 제곱하거나 절댓값을 취해서 전체 손실을 계산하는 것이다. 왜 제곱이나 절댓값을 쓰냐고? 오차가 플러스일 수도 있고 마이너스일 수도 있기 때문에, 단순히 더하면 상쇄되어 버릴 수 있다. 그러면 실제로 얼마나 틀렸는지를 제대로 알 수 없기 때문에, 보통은 제곱이나 절댓값으로 바꿔서 손실을 계산한다.</p><p>그럼 실제로는 어떤 손실 함수가 많이 쓰일까? 사용하는 문제의 종류에 따라 손실 함수도 달라진다. 크게 분류하면 회귀(regression) 문제와 분류(classification) 문제가 있다. 회귀 문제는 숫자를 예측하는 것이다. 예를 들어 집값, 온도, 판매량처럼 연속된 값을 맞추는 문제다. 이런 문제에서는 예측 값과 실제 값의 차이를 그대로 측정하는 &lsquo;평균 제곱 오차(Mean Squared Error, MSE)&lsquo;가 가장 많이 쓰인다. 이 함수는 예측 값과 실제 값의 차이를 제곱해서 평균을 낸 것이다. 즉, 예측이 정답에서 얼마나 떨어져 있는지를 부드럽게 측정하는 방식이다. 왜 제곱하느냐고? 차이가 클수록 더 큰 벌을 주기 위해서다. 예측이 조금 틀리면 괜찮지만, 너무 많이 틀리면 그만큼 손실도 크게 만들겠다는 의도다.</p><p>다음으로는 분류 문제가 있다. 예를 들어 &ldquo;이 이메일은 스팸인가요, 아닌가요?&rdquo;, &ldquo;이 사진은 개입니까, 고양이입니까?&rdquo; 같은 문제다. 이런 경우는 &lsquo;교차 엔트로피 손실 함수(Cross Entropy Loss)&lsquo;를 많이 쓴다. 이 함수는 확률 예측을 기준으로 손실을 계산한다. 딥러닝이 “이건 고양이일 확률이 90%야”라고 말했을 때 실제로 고양이였다면 손실이 작고, 만약 “10%밖에 안 돼”라고 했다면 손실이 크게 나온다. 즉, 정답일 확률을 얼마나 높게 예측했는가가 기준이 된다.</p><p>이제 잠시 수학적인 표현을 아주 쉽게 풀어보자. 교차 엔트로피는 다음과 같은 아이디어를 갖고 있다. 우리가 기대하는 정답 확률은 1이고, 나머지는 0이다. 그런데 딥러닝의 출력은 0.9일 수도 있고 0.1일 수도 있다. 이때 손실 함수는 정답에 가까운 값을 출력했는가를 체크한다. 교차 엔트로피 함수는 정답에 가까우면 손실이 거의 0에 가깝고, 멀어지면 급격히 손실이 커진다. 마치 &ldquo;너는 이걸 99%로 맞췄으니 거의 완벽해!&ldquo;라고 칭찬해주고, &ldquo;넌 1%밖에 안 줬네, 완전 틀렸어!&ldquo;라고 꾸짖는 느낌이다. 그래서 이 손실 함수는 딥러닝에게 아주 강한 피드백을 줄 수 있어서 빠르게 학습할 수 있게 도와준다.</p><p>여기서 중요한 질문이 생긴다. 왜 손실 함수가 이렇게 많고, 상황마다 다른 걸 써야 할까? 그 이유는 문제마다 “틀림”의 의미가 다르기 때문이다. 예를 들어, 어떤 경우에는 예측이 조금만 틀려도 큰 문제가 되는 경우가 있다. 예를 들어 병의 유무를 예측하는 AI라면, 실제로 병이 있는데 &ldquo;아니야, 병 없어"라고 말하는 건 큰 손해를 불러온다. 이런 상황에선 단순한 평균 제곱 오차로는 부족하다. 좀 더 정교하게, 그리고 민감하게 반응할 수 있는 손실 함수가 필요하다. 또 어떤 경우는 예측 값이 아주 정확하진 않아도 괜찮은 경우가 있다. 예를 들어 영화 추천 시스템은 사용자가 좋아할 만한 영화를 대략 맞히면 된다. 이런 경우엔 손실 함수가 너무 예민하면 오히려 학습이 어렵다.</p><p>또 하나 중요한 건, 손실 함수는 학습의 방향을 결정한다. 딥러닝은 손실 값을 기준으로 가중치와 편향을 어떻게 바꿀지를 결정하는데, 이때 수학적으로 &lsquo;기울기(gradient)&lsquo;라는 개념을 사용한다. 손실 함수는 이 기울기를 계산할 수 있어야 한다. 즉, 손실 함수가 매끄럽고, 연속적이고, 기울기를 잘 계산할 수 있어야 딥러닝이 제대로 학습할 수 있다. 그래서 손실 함수를 고를 때는 단지 결과만 보는 게 아니라, 계산하기 쉬운지도 고려해야 한다.</p><p>이처럼 손실 함수는 단순한 채점표가 아니다. 그것은 딥러닝에게 어떤 방향으로 가야 할지를 알려주는 나침반이자, 지금 잘하고 있는지를 점검하는 체온계이며, 학습의 핵심 중추다. 딥러닝은 수많은 예측과 비교를 통해 손실을 계산하고, 그 손실을 줄이는 방향으로 똑똑해진다. 손실 함수는 딥러닝이 “나는 잘하고 있나요?”라고 물었을 때 “응, 거의 정답이야” 혹은 “아니야, 완전 틀렸어”라고 대답해주는 선생님이다. 이 선생님이 없다면 딥러닝은 어디로 가야 할지도 모르고, 얼마나 틀렸는지도 모른 채 길을 잃게 된다. 좋은 손실 함수는 딥러닝이 효율적으로 배우고, 빠르게 성장하게 해주는 핵심 열쇠다.</p><p>딥러닝의 본질은 수많은 입력과 계산, 그리고 결과 비교를 통해 더 나은 방향으로 나아가는 여정이다. 이 여정에서 손실 함수는 현재 위치를 알려주는 지표다. 우리가 얼마나 목적지에서 멀어졌는지를 알려주고, 어느 방향으로 가면 더 가까워질 수 있는지를 수학적으로 계산해주는 것이다. 결국 딥러닝은 손실을 줄이기 위한 싸움이다. 손실이 작아질수록 모델은 더 똑똑해지고, 더 정확해지며, 더 인간처럼 판단하게 된다. 그렇기에 손실 함수를 단순한 계산 공식이 아니라, 딥러닝의 눈과 귀, 그리고 방향타로 이해해야 한다.</p><p>결론적으로 손실 함수란, 딥러닝에게 세상의 틀림을 수치로 설명해주는 도구다. 이 숫자가 높으면 &ldquo;많이 틀렸구나"라는 뜻이고, 낮으면 &ldquo;거의 맞았네"라는 뜻이다. 딥러닝은 이 숫자를 보고 고개를 끄덕이며 생각한다. &ldquo;좋아, 이번에는 이 숫자를 더 줄이기 위해서 내 가중치를 이렇게 바꿔봐야겠어.&rdquo; 그렇게 수많은 계산과 시행착오 끝에, 딥러닝은 점점 더 정답에 가까워지고, 결국 놀라운 수준의 성능을 갖게 된다. 이 모든 것의 중심에는, 바로 손실 함수라는 조용하지만 똑똑한 선생님이 존재하고 있다.</p><h3 id=11-미분-가능성differentiability과-학습을-위한-전제-조건>11. 미분 가능성(Differentiability)과 학습을 위한 전제 조건
<a class=anchor href=#11-%eb%af%b8%eb%b6%84-%ea%b0%80%eb%8a%a5%ec%84%b1differentiability%ea%b3%bc-%ed%95%99%ec%8a%b5%ec%9d%84-%ec%9c%84%ed%95%9c-%ec%a0%84%ec%a0%9c-%ec%a1%b0%ea%b1%b4>#</a></h3><p>딥러닝을 잘 이해하려면 “학습”이라는 말이 도대체 무슨 뜻인지부터 살펴보는 것이 중요하다. 학습이란 아주 간단히 말해서, 처음에는 잘 못하던 걸 점점 잘하게 되는 과정이다. 컴퓨터에게 “고양이 사진을 구분해봐”라고 하면, 처음에는 무작위로 아무거나 고양이라고 하다가, 점점 정답을 맞히는 비율이 높아지게 된다. 그러면 자연스럽게 이런 의문이 생긴다. 도대체 컴퓨터는 어떻게 더 잘하게 되는 걸까? 정답을 알려주면 마법처럼 점점 정확해지는 그 비밀은 무엇일까?</p><p>그 핵심에는 ‘미분 가능성’이라는 개념이 숨어 있다. 미분이란, 어떤 함수의 변화율, 즉 조금 움직였을 때 결과가 얼마나 변하는지를 알아보는 수학적인 도구다. 어려운 것 같지만 실제로는 아주 직관적인 개념이다. 예를 들어 어떤 길을 걷고 있는데, 내 위치를 조금만 바꾸면 풍경이 얼마나 변하는지를 보는 것과 같다. 아주 가파른 산을 오르면 한 걸음만 가도 풍경이 확 바뀌고, 평지에서는 한참 걸어야 조금 바뀐다. 이것이 바로 변화율이다. 딥러닝에서 이 변화율을 이용해서, 모델이 잘못된 부분을 조금씩 수정해나가는 것이 바로 ‘학습’이다.</p><p>딥러닝 모델은 어떤 입력에 대해 예측을 하고, 그 예측이 얼마나 틀렸는지를 계산한다. 이 틀림 정도를 수치로 표현한 것이 바로 ‘손실 함수(Loss Function)’다. 이 값이 크면 예측이 많이 틀렸다는 뜻이고, 작으면 잘 맞췄다는 뜻이다. 딥러닝의 목표는 이 손실 함수를 가능한 작게 만드는 것이다. 마치 골프공을 가장 낮은 지점인 홀컵에 떨어뜨리려는 것과 같다. 그런데 딥러닝 모델은 입력과 출력 사이에 수많은 계산 단계를 거치기 때문에, 손실 값이 최종적으로 어떻게 나왔는지 그 과정을 되짚어가며 잘못된 부분을 고쳐야 한다. 이때 사용하는 도구가 바로 ‘미분’이다.</p><p>딥러닝의 학습은 손실 함수가 줄어들도록 뉴런 안의 ‘가중치’(입력 값에 곱해지는 숫자)와 ‘편향’(출력에 더해지는 상수)을 조절하는 과정이다. 그런데 이 조절을 아무렇게나 해서는 안 된다. 손실 함수의 값을 줄이기 위해서는, 어떤 방향으로 얼마나 가야 할지를 정확히 계산해야 한다. 이때 방향과 속도를 알려주는 나침반이 바로 미분이다. 현재의 가중치를 기준으로 손실 함수가 어떤 방향으로 가장 빠르게 줄어드는지를 알려주는 값, 그것이 미분값, 즉 기울기(gradient)다.</p><p>이 과정을 조금 더 직관적으로 설명해보자. 눈을 감고 산속에서 가장 낮은 지점을 찾는다고 해보자. 우리는 지금 어디가 낮은지 모른다. 그래서 한 발자국씩 움직여보면서 발밑이 오르막인지 내리막인지 감지하고, 내리막 방향으로 계속 걸어간다. 이 내리막 방향을 알려주는 것이 바로 미분값이다. 만약 우리가 있는 곳이 미분 불가능한, 즉 갑자기 단절된 절벽 같은 지형이라면, 어디로 가야 할지 방향을 알 수 없다. 따라서 우리가 학습을 하려면, 산의 지형처럼 모든 곳이 부드럽게 이어져 있어야 하고, 발밑의 경사를 항상 알 수 있어야 한다. 이것이 바로 ‘미분 가능성’이다.</p><p>딥러닝에서 사용하는 함수들, 예를 들어 활성화 함수나 손실 함수는 대부분 미분 가능한 함수들이다. 예를 들어 ReLU 함수는 0보다 작을 때는 0을, 0보다 크면 그대로 값을 넘기는 함수다. 이 함수는 0이라는 한 점에서는 미분이 정의되지 않지만, 대부분의 영역에서는 아주 간단한 미분값을 가진다. 그래서 실제 학습에는 큰 문제가 되지 않는다. 반면에, 완전히 계단처럼 뚝 끊기는 함수는 어디로 이동해야 손실이 줄어드는지 방향을 알려주지 못한다. 그런 함수는 딥러닝에서 학습이 되지 않게 만든다.</p><p>이처럼 딥러닝에서는 모든 계산 과정을 수식으로 표현하고, 그것이 부드럽게 이어져 있어야 한다. 입력에서 출력까지 이어지는 그 모든 계산이 하나의 함수라고 볼 수 있다. 이 함수는 복잡한 층(layer)들을 통과하면서 가중치와 편향을 포함한 계산들을 계속 수행하는데, 그 결과로 손실 함수가 나오는 것이다. 학습을 하려면, 손실 함수를 기준으로 각 가중치가 손실에 어떤 영향을 주었는지를 계산해야 한다. 이걸 가능하게 해주는 것이 바로 ‘연쇄 법칙(chain rule)’이라는 미분의 규칙이다.</p><p>연쇄 법칙은 복잡한 함수가 여러 개 연결되어 있을 때, 그 전체에 대한 미분을 구하는 방법이다. 예를 들어 y = f(g(h(x))) 같은 구조가 있다면, 우리는 h(x), g(), f() 각각의 변화율을 곱해서 전체의 변화율을 계산할 수 있다. 딥러닝은 이런 구조로 되어 있기 때문에, 출력에서 입력으로 거꾸로 따라가면서 각 지점의 미분값을 계산하고, 이 정보를 바탕으로 각 가중치를 얼마나 바꿔야 할지를 결정할 수 있다. 이 과정을 우리는 ‘역전파(backpropagation)’라고 부른다.</p><p>역전파는 말 그대로 오차를 거꾸로 전달하는 것이다. 모델이 예측을 하고 손실을 계산한 후, 이 손실이 어디에서 얼마나 발생했는지를 계산해 다시 각 층으로 돌려보낸다. 각 층의 뉴런은 그 신호를 받아서, 자신의 가중치가 손실에 얼마나 영향을 주었는지를 계산하고, 그에 따라 가중치를 조금씩 조정한다. 이 과정을 반복하면 손실 값은 점점 작아지고, 예측은 점점 정확해진다. 이 모든 과정은 미분 가능성이 보장될 때만 가능하다. 만약 중간 어딘가에서 미분이 불가능한 함수가 있다면, 그 지점에서 역전파가 멈추고, 학습이 제대로 되지 않게 된다.</p><p>이처럼 딥러닝에서 미분 가능성은 학습의 생명줄이다. 컴퓨터는 눈이 없고 감각이 없기 때문에, 어떤 방향으로 더 나아가야 손실이 줄어드는지를 수학적으로 계산해야 한다. 이때 필요한 정보가 바로 변화율, 즉 미분값이다. 그리고 이 미분값을 구하려면, 모든 계산이 연속적이고 부드럽게 이어져 있어야 한다. 이것이 바로 미분 가능성이다. 마치 실로 이어진 줄을 당기듯, 어느 한 곳이라도 끊어져 있다면 전체가 흔들리지 않고, 조정이 불가능해지는 것이다.</p><p>딥러닝의 설계자들은 그래서 항상 함수가 미분 가능한지를 가장 먼저 고려한다. 활성화 함수도, 손실 함수도, 뉴런의 계산 방식도 모두 미분 가능해야 한다. 이를 위해 수많은 수학적 기법들이 개발되었고, 아주 복잡한 신경망도 미분 가능성을 유지할 수 있도록 구성된다. 심지어 최근에는 미분 가능한 정렬 함수나 샘플링 방법까지 연구되고 있다. 왜냐하면 모든 단계가 미분 가능해야 학습이 이루어지기 때문이다.</p><p>마지막으로 정리하자면, 딥러닝의 학습이란 손실을 줄이는 방향으로 가중치를 조금씩 바꾸는 과정이고, 이 방향을 알려주는 것이 바로 미분값이다. 미분 가능성이 없으면 우리는 방향을 잃고, 아무리 많은 데이터를 줘도 모델은 배울 수 없다. 마치 나침반 없이 어두운 숲을 걷는 것처럼, 미분 없이는 학습도 없다. 그래서 딥러닝에서는 “미분 가능성”이 가장 기본적이고 필수적인 전제조건이다. 이 조건 위에서만 우리가 손실을 줄이고, 모델을 발전시키고, 복잡한 문제를 풀어낼 수 있게 된다. 파인만 식으로 말하자면, 딥러닝은 고양이를 잘 알아보게 만드는 훈련소이고, 미분은 그 훈련소에서 방향을 잡아주는 선생님이다. 이 선생님이 있어야만 학습이 가능하고, 실수한 것을 바로잡을 수 있으며, 점점 더 똑똑해지는 딥러닝 모델을 만들어낼 수 있는 것이다.</p><h1><a class=anchor href=#>#</a></h1><p>#출처</p><p><a href=https://chatgpt.com/share/6870f244-68e4-8000-a8d4-099abb173816>https://chatgpt.com/share/6870f244-68e4-8000-a8d4-099abb173816</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#1-딥러닝>1. 딥러닝?</a></li><li><a href=#2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조</a></li><li><a href=#3-가중치와-편향의-역할>3. 가중치와 편향의 역할</a></li><li><a href=#4-활성화-함수의-필요성>4. 활성화 함수의 필요성</a></li><li><a href=#5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들</a></li><li><a href=#6-은닉층의-개념과-역할>6. 은닉층의 개념과 역할</a></li><li><a href=#7-출력층-설계>7. 출력층 설계</a></li><li><a href=#8-모델-구조의-표현과-설계>8. 모델 구조의 표현과 설계</a></li><li><a href=#9-정방향-연산>9. 정방향 연산</a></li><li><a href=#10-손실-함수의-정의와-선택>10. 손실 함수의 정의와 선택</a></li><li><a href=#11-미분-가능성differentiability과-학습을-위한-전제-조건>11. 미분 가능성(Differentiability)과 학습을 위한 전제 조건</a></li></ul></li></ul></nav></div></aside></main></body></html>