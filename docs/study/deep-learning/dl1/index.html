<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  #4 신경망
  #

#2025-07-07


  1. 딥러닝?
  #

딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/deep-learning/dl1/"><meta property="og:site_name" content=" "><meta property="og:title" content="#4 신경망"><meta property="og:description" content="#4 신경망 # #2025-07-07
1. 딥러닝? # 딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-11T00:00:00+00:00"><meta property="article:tag" content="2025-07"><title>#4 신경망 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/deep-learning/dl1/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.dee3eb3a08f378cc5e7a7e7338342f9c260867a1da7504237707e49bfa1ce6e8.js integrity="sha256-3uPrOgjzeMxeen5zODQvnCYIZ6HadQQjdwfkm/oc5ug=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/baking/>베이킹</a><ul></ul></li><li><a href=/docs/hobby/favorite/>🌸</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/bioinformatics/>생물정보학</a><ul></ul></li><li><a href=/docs/study/algorithm/>코테</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li><li><a href=/docs/study/etc/>기타</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>#4 신경망</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#1-딥러닝>1. 딥러닝?</a></li><li><a href=#2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조</a></li><li><a href=#3-가중치와-편향의-역할>3. 가중치와 편향의 역할</a></li><li><a href=#4-활성화-함수의-필요성>4. 활성화 함수의 필요성</a></li><li><a href=#5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=4-신경망>#4 신경망
<a class=anchor href=#4-%ec%8b%a0%ea%b2%bd%eb%a7%9d>#</a></h1><p>#2025-07-07</p><hr><h3 id=1-딥러닝>1. 딥러닝?
<a class=anchor href=#1-%eb%94%a5%eb%9f%ac%eb%8b%9d>#</a></h3><p>딥러닝은 사실 우리 뇌를 본떠 만든 일종의 흉내다. 인간의 뇌는 수십억 개의 뉴런이 서로 얽히고설켜서 작동한다. 우리가 개를 보면 “아, 개구나!” 하고 알아보는 건, 뇌 속의 뉴런들이 전기신호를 주고받으면서 수많은 과거 경험과 연결된 정보를 꺼내오는 과정 덕분이다. 딥러닝도 이처럼 입력된 정보에서 패턴을 찾아내고, 그것이 무엇인지 스스로 판단하는 구조를 갖는다. 다만 진짜 뇌처럼 복잡하고 유기적이지는 않다. 우리는 이를 아주 단순화된 수학적 구조로 흉내낼 뿐이다.</p><p>예를 들어, 딥러닝에게 고양이 사진을 수만 장 보여주면서 “이게 고양이야”라고 알려준다. 그러면 딥러닝은 그 수만 장의 공통점, 즉 고양이에게만 있는 특징을 찾아낸다. 귀의 모양, 눈의 위치, 털의 질감, 얼굴 비율 등을 조금씩 파악하면서 학습한다. 이렇게 학습이 끝나면, 처음 보는 고양이 사진을 줘도 “이건 고양이야”라고 스스로 말할 수 있게 된다. 중요한 점은, 사람이 일일이 고양이의 특징을 알려주지 않아도, 딥러닝이 스스로 그 특징을 찾아낸다는 것이다. 이것이 바로 딥러닝의 힘이다.</p><p>그럼 이 딥러닝이 실제로 어떻게 작동하는 걸까? 딥러닝은 ‘인공신경망’이라는 구조를 바탕으로 한다. 이것은 여러 층(layer)으로 구성되는데, 가장 앞에는 입력층(input layer), 가장 뒤에는 출력층(output layer), 그리고 그 사이에는 수많은 은닉층(hidden layer)이 존재한다. 각각의 층에는 ‘뉴런’이라는 작은 계산 단위가 가득 들어 있다. 하나의 뉴런은 아주 단순한 계산만 할 수 있지만, 이 뉴런이 수백 개, 수천 개 모여 층을 이루고, 층이 다시 여러 겹 쌓이면 매우 복잡한 문제도 풀 수 있게 된다. 말하자면, 뉴런 하나는 벽돌이고, 벽돌을 쌓아 집을 짓는다고 보면 된다.</p><p>입력층은 말 그대로 정보가 들어오는 곳이다. 예를 들어 이미지가 입력되면, 그 이미지는 픽셀이라는 아주 작은 점들의 집합이고, 각 점에는 숫자가 있다. 밝으면 큰 숫자, 어두우면 작은 숫자다. 이 숫자들이 입력층으로 들어간다. 그리고 각 입력은 첫 번째 은닉층의 뉴런들과 연결되어 있다. 연결되어 있다는 건, 숫자가 전달된다는 뜻이고, 이 전달에는 ‘가중치(weight)’라는 것이 함께 붙는다. 이 가중치는 각각의 입력이 얼마나 중요한지를 나타내는 숫자다. 예를 들어 어떤 입력은 0.9의 가중치를 받고, 어떤 입력은 0.1의 가중치를 받을 수 있다. 이 가중치가 딥러닝의 핵심이다.</p><p>입력과 가중치를 곱한 뒤, 그 값을 더하고, 마지막으로 ‘편향(bias)’이라는 값을 추가한다. 그리고 이 결과값을 활성화 함수(activation function)라는 계산에 넣는다. 이 함수는 일종의 필터다. 이 값을 어느 정도 이상이면 다음 뉴런에 신호를 전달하고, 그렇지 않으면 무시한다. 이런 식으로 정보가 첫 번째 은닉층을 지나고, 두 번째 은닉층을 지나고, 계속해서 다음 층으로 전달되면서 점점 복잡한 특징을 추출하게 된다. 처음에는 단순한 선이나 색깔을 파악하다가, 나중에는 귀의 윤곽, 눈의 대칭성, 얼굴의 구조 같은 추상적인 개념도 파악하게 된다.</p><p>이런 모든 과정을 거쳐 출력층에 도달하면, 딥러닝은 최종적으로 “이건 고양이다”, “이건 강아지다” 같은 판단을 내린다. 이 판단이 맞았는지 틀렸는지는 사람이 알려준다. 틀렸다면, 딥러닝은 “왜 틀렸지?” 하고 스스로의 계산을 되짚어보고, 잘못된 가중치를 조금씩 조정한다. 이 과정을 ‘역전파(backpropagation)’라고 하는데, 이는 마치 시험을 보고 틀린 문제를 복습해서 다시 공부하는 것과 비슷하다. 이런 공부 과정을 반복할수록 딥러닝은 점점 더 정답에 가까워지고, 결국 사람처럼 판단하게 된다.</p><p>중요한 건, 딥러닝은 우리가 직접 가르치는 게 아니라 ‘보여주고 실수하게 하면서’ 배우게 한다는 점이다. 그리고 그 학습은 수천 번, 수만 번 반복된다. 사람도 어릴 때 수없이 넘어지면서 걷는 법을 배운다. 딥러닝도 마찬가지다. 실수를 하고, 그 실수를 수정하면서 조금씩 정답에 가까워지는 것이다. 이런 특성 때문에, 딥러닝은 수많은 데이터가 있어야 하고, 그 데이터를 바탕으로 학습하면서 비로소 쓸 만한 모델이 된다.</p><p>그렇다면 딥러닝은 어떤 일을 할 수 있을까? 앞서 말한 이미지 분류 외에도, 딥러닝은 자율주행차의 눈 역할을 하기도 하고, 음성 인식으로 우리 말을 컴퓨터가 이해하게 만들기도 하며, 글을 읽고 이해하거나 번역하는 데에도 쓰인다. 심지어 예술작품을 그리거나 작곡을 할 수도 있다. 바둑을 두고 사람을 이길 수도 있고, 의료 영상에서 암세포를 찾아내는 데에도 활용된다. 이런 일을 하려면 기존의 컴퓨터 알고리즘으로는 한계가 있지만, 딥러닝은 데이터를 직접 보며 스스로 규칙을 찾아내기 때문에 훨씬 유연하게 문제를 해결할 수 있다.</p><p>그럼에도 불구하고 딥러닝이 만능은 아니다. 딥러닝은 설명력이 약하다. 즉, 왜 그런 판단을 내렸는지를 설명하기 어렵다. 예를 들어 고양이와 강아지를 구분해냈다고 하자. 딥러닝이 왜 고양이라고 판단했는지를 우리는 정확히 알 수 없다. 인간은 눈이 크니까 고양이라고 판단했는지, 귀가 뾰족해서 그런지, 우리가 생각하는 방식으로 설명하지 않는다. 그래서 이를 ‘블랙박스’ 모델이라고 부르기도 한다. 최근에는 이런 문제를 해결하기 위해 ‘설명 가능한 AI(Explainable AI)’에 대한 연구도 활발히 진행되고 있다.</p><p>또한 딥러닝은 많은 데이터를 요구한다. 적은 데이터로는 제대로 배우지 못한다. 어린아이는 고양이 한두 번 보면 고양이를 알아보지만, 딥러닝은 수천 장, 수만 장을 봐야 같은 수준에 도달한다. 이처럼 데이터가 풍부한 환경에서는 매우 강력하지만, 데이터가 부족하면 힘을 발휘하지 못하는 것이 단점이다.</p><p>마지막으로, 딥러닝은 계산 비용이 크다. 뉴런이 많고, 층이 깊을수록, 계산할 것도 많아지고 시간이 오래 걸린다. 그래서 딥러닝을 잘 작동시키려면 GPU 같은 강력한 컴퓨터 장비가 필요하다. 마치 수많은 문제를 빠르게 푸는 수학 천재가 필요하듯이 말이다.</p><p>결국 딥러닝은, 아주 많은 계산을 통해 패턴을 학습하고, 그 패턴을 바탕으로 새로운 것을 판단하는 시스템이다. 사람의 두뇌를 흉내 내지만, 여전히 우리는 그 원리와 한계에 대해 계속해서 탐구하고 있다. 딥러닝은 &ldquo;입력을 넣으면 출력을 내놓는 똑똑한 계산기"지만, 그 계산기의 뇌 속에서 무슨 일이 벌어지는지는 우리가 명확히 이해하려면 아직 시간이 필요하다. 그렇기 때문에 지금도 우리는 딥러닝을 가르치고, 실수하게 하고, 더 나은 판단을 할 수 있도록 조금씩 성장시키는 중이다. 그리고 언젠가는 그 계산기가 스스로 학습하고, 설명하고, 새로운 창조까지 하는 세상도 올지 모른다. 바로 그것이 딥러닝의 현재이며, 앞으로 우리가 함께 만들어갈 미래다.</p><h3 id=2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조
<a class=anchor href=#2-%eb%89%b4%eb%9f%b0-%eb%aa%a8%eb%8d%b8%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ea%b5%ac%ec%a1%b0>#</a></h3><p>딥러닝에서 뉴런(Neuron) 모델이란, 아주 작고 단순한 계산 단위라고 할 수 있다. 마치 사람 뇌의 신경세포 하나처럼 생각하면 되는데, 실제 뇌의 뉴런이 전기 신호를 받고 다음 뉴런에게 신호를 전달하듯, 인공 신경망 속 뉴런도 어떤 입력 값을 받아서 처리한 뒤, 그 결과를 다음 단계로 넘긴다. 하지만 인공 뉴런은 실제 뉴런과 달리 아주 단순하다. 본질적으로는 숫자를 받아서 계산한 뒤, 또 다른 숫자를 내보내는 계산기 하나라고 보면 된다. 그럼 이 계산기 안에서는 어떤 일이 벌어질까?</p><p>가장 먼저 뉴런은 입력을 받는다. 예를 들어 &ldquo;고양이 사진이 들어왔다"고 하면, 실제로는 사진 속 픽셀들의 숫자가 뉴런에게 입력되는 것이다. 한 장의 흑백 이미지가 있다면, 각 픽셀은 0에서 255 사이의 숫자일 수 있다. 이 숫자들이 여러 개 모여 뉴런에게 전달된다. 예를 들어 총 5개의 입력 값이 들어온다고 하자. 각각 x₁, x₂, x₃, x₄, x₅라고 부르자. 이 숫자들은 우리가 처리해야 할 데이터다. 그런데 이 입력들이 모두 똑같이 중요한 건 아니다. 예를 들어 눈의 위치는 중요할 수 있지만, 배경의 회색 그림자는 덜 중요할 수 있다. 그래서 각각의 입력에는 ‘가중치’라는 숫자를 곱해준다. 이를 각각 w₁, w₂, w₃, w₄, w₅라고 하자. 이 가중치는 입력이 얼마나 중요한지를 알려주는 역할을 한다.</p><p>이제 뉴런은 각 입력과 가중치를 곱하고, 그 결과를 모두 더한다. 이 과정은 x₁·w₁ + x₂·w₂ + x₃·w₃ + x₄·w₄ + x₅·w₅ 처럼 표현된다. 이렇게 곱하고 더하는 이유는 간단하다. 입력이 클수록, 그리고 그 입력의 가중치가 클수록 결과에 더 많은 영향을 주기 때문이다. 마치 시험에서 국어보다 수학이 2배 중요하다고 하면, 수학 점수에 더 큰 가중치를 주는 것과 비슷하다. 이렇게 입력과 가중치를 곱해서 모두 더한 값을 우리는 흔히 z라고 부른다. 수학적으로는 z = Σ(xᵢ·wᵢ)라고 쓸 수 있다. 그런데 여기서 하나 더, 뉴런은 항상 일정한 방향으로만 판단하지 않는다. 어느 정도 기준점이 필요하다. 그래서 우리는 이 계산 결과에 ‘편향(bias)’이라는 숫자를 하나 더해준다. 편향은 말 그대로 기준선을 조금 위로 올리거나 아래로 내리는 역할을 한다. 즉, z = Σ(xᵢ·wᵢ) + b가 되는 것이다.</p><p>이제 이 값을 그대로 출력하면 될까? 아니다. 딥러닝은 복잡하고 다양한 패턴을 찾아야 하기 때문에, 이 z 값을 그대로 쓰지 않고, 특별한 수학적 함수를 한 번 더 거친다. 이 함수를 ‘활성화 함수(activation function)’라고 부른다. 왜냐하면 이 함수가 뉴런이 ‘활성화’될지를 결정하기 때문이다. 가장 기본적인 활성화 함수는 ‘계단 함수’다. z가 어떤 기준보다 크면 1을 출력하고, 아니면 0을 출력한다. 마치 &ldquo;이 정도 점수 이상이면 합격, 아니면 불합격&rdquo; 같은 느낌이다. 하지만 이 함수는 너무 뚝 끊긴 느낌이라, 요즘은 부드럽게 동작하는 함수들을 많이 쓴다. 예를 들어 ‘시그모이드(sigmoid)’ 함수는 z의 값을 0과 1 사이의 값으로 바꿔준다. z가 아주 크면 1에 가까워지고, 아주 작으면 0에 가까워진다. 즉, 흑백처럼 판단하는 것이 아니라 회색 지대를 인정하면서, &ldquo;이건 고양이일 확률이 85%쯤 돼&rdquo; 같은 식으로 표현하게 된다. 또 다른 활성화 함수로는 ReLU(Rectified Linear Unit)가 있다. 이건 z가 0보다 작으면 0, 크면 그대로 통과시키는 아주 간단한 함수인데, 복잡한 계산 없이도 성능이 좋아서 널리 쓰인다.</p><p>그럼 다시 한 번 정리해보자. 하나의 인공 뉴런이 하는 일은, (1) 여러 입력 값을 받고, (2) 각 입력에 가중치를 곱해서 더한 뒤, (3) 편향을 더하고, (4) 그 결과를 활성화 함수에 넣어서 (5) 출력 값을 만드는 것이다. 이 출력은 다음 뉴런으로 전달된다. 그리고 이 과정은 한 층(layer)에서 또 다음 층으로, 그리고 또 그다음 층으로 계속 이어지면서 전체 신경망이 작동하게 된다.</p><p>그런데 중요한 질문이 하나 있다. “이 가중치 w와 편향 b는 누가 정하나?” 처음엔 아무도 모른다. 그래서 딥러닝은 이 값을 무작위로 시작하고, 점점 더 좋은 값으로 바꿔나간다. 이 과정이 바로 학습이다. 딥러닝은 정답을 알고 있는 데이터를 바탕으로 예측을 해보고, 그 예측이 얼마나 틀렸는지를 계산한 뒤, 그 오차를 줄이기 위해 w와 b를 조금씩 수정한다. 이 과정을 반복하면서 w와 b는 점점 더 정답에 가까운 값을 갖게 된다. 결국 하나의 뉴런이 똑똑해진다는 건, 이 w와 b가 똑똑해졌다는 뜻이다.</p><p>이렇게 보면, 인공 뉴런은 생각보다 단순하다. 단지 입력을 받고 곱하고 더하고, 함수를 한 번 통과시킬 뿐이다. 그런데 이런 뉴런이 수천 개, 수만 개 모여서 층을 이루고, 그 층이 다시 깊이 쌓이면, 고양이 사진을 알아보거나, 사람의 말을 인식하거나, 심지어 장면을 보고 다음 장면을 예측하는 일까지 할 수 있다. 이는 마치 하나의 퍼즐 조각은 단순하지만, 그것들이 모이면 아름다운 그림이 되는 것과 비슷하다.</p><p>한 가지 더 재미있는 점은, 사람의 뇌도 비슷한 방식으로 작동한다는 것이다. 물론 훨씬 복잡하지만, 기본 원리는 비슷하다. 눈으로 들어온 이미지가 시각 뉴런들을 따라 전달되면서, 점점 더 고차원적인 특징을 추출하게 된다. 초기에 단순한 선이나 모양을 인식하다가, 점점 얼굴, 표정, 감정까지 인식하게 된다. 인공신경망에서도 초기 뉴런은 단순한 패턴을 배우고, 깊은 층의 뉴런일수록 복잡한 의미를 배운다. 그래서 “깊은 신경망(deep neural network)”이라는 말이 나온 것이다.</p><p>요약하자면, 뉴런은 단순한 계산기다. 입력을 받고, 가중치를 곱하고, 모두 더한 다음 편향을 더하고, 활성화 함수를 거쳐 출력한다. 이 모든 과정은 수학적으로는 아주 간단한 식 하나로 표현할 수 있다. y = f(Σ(xᵢ·wᵢ) + b), 여기서 f는 활성화 함수다. 이 간단한 식이 모이고 모여, 우리 눈에는 마법처럼 보이는 AI 시스템이 만들어지는 것이다. 마치 셀 수 없이 많은 작은 기계 부품들이 모여 자동차를 움직이듯, 딥러닝도 수많은 뉴런이 정해진 방식으로 작동하면서 전체 시스템이 학습하고, 예측하고, 판단하게 된다.</p><p>인공 뉴런은 수를 받아 계산하는 작은 친구일 뿐이다. 그런데 이 친구들이 손을 맞잡고 함께 일하면, 놀라운 일을 할 수 있다. 한 명은 선을 보고, 한 명은 곡선을 보고, 또 한 명은 얼굴을 보고, 마지막엔 “이건 고양이야”라고 말하는 팀이 되는 것이다. 그래서 우리는 이 단순한 뉴런 하나하나를 정성껏 설계하고, 그 연결을 조정하면서 더 똑똑한 AI를 만들어가고 있는 중이다. 딥러닝은 그런 수많은 조그마한 뉴런들의 팀워크로 이루어진 거대한 생각 기계라 할 수 있다.</p><h3 id=3-가중치와-편향의-역할>3. 가중치와 편향의 역할
<a class=anchor href=#3-%ea%b0%80%ec%a4%91%ec%b9%98%ec%99%80-%ed%8e%b8%ed%96%a5%ec%9d%98-%ec%97%ad%ed%95%a0>#</a></h3><p>인공신경망에서 가중치(Weights)와 편향(Bias)의 역할은, 우리가 친구에게 “이게 중요하고, 저건 별로 중요하지 않아”라고 말하는 것과 비슷하다고 볼 수 있다. 인공신경망이라는 건 사실 수많은 입력 값을 받아서 “이건 고양이야” 혹은 “이건 숫자 3이야”라고 판단하는 계산기인데, 이 계산기의 핵심은 무엇을 얼마나 중요하게 여길지 스스로 판단할 수 있다는 점이다. 바로 그 판단을 가능하게 해주는 것이 가중치와 편향이다.</p><p>먼저 가중치부터 이야기해보자. 어떤 뉴런이 입력을 여러 개 받는다고 상상해보자. 예를 들어 이미지라면 각각의 픽셀이 입력이 될 수 있다. 그 입력값 하나하나가 그냥 있는 그대로 다 중요하냐고 묻는다면, 그렇지 않다. 어떤 픽셀은 진짜 핵심일 수 있고, 어떤 픽셀은 배경이거나 노이즈일 수 있다. 그래서 우리는 각 입력에 ‘중요도’를 부여하는데, 그게 바로 가중치다. 예를 들어 어떤 입력이 0.9의 가중치를 가진다면, 그건 매우 중요한 입력이라는 뜻이고, 0.01의 가중치를 가진다면 별로 신경 쓸 필요 없는 입력이라는 뜻이다. 즉, 가중치는 입력의 영향력을 조절하는 손잡이다.</p><p>그럼 이걸 좀 더 일상적인 예로 풀어보자. 당신이 친구와 햄버거를 먹으러 갈지, 피자를 먹으러 갈지 고민한다고 해보자. 선택은 여러 요인에 따라 달라질 수 있다. 예를 들어 당신은 배가 고프고, 날씨는 덥고, 돈은 얼마 없고, 어제는 피자를 먹었다고 하자. 이때 각각의 요인은 입력이다. 그리고 각 요인이 당신의 선택에 얼마나 영향을 미치느냐는 다를 수 있다. 배고픔은 매우 큰 요인일 수 있으니 가중치가 크고, 날씨는 별로 상관없다면 가중치는 작을 것이다. 이런 식으로 당신은 마음속에서 입력 × 중요도를 계산해서 결정을 내린다. 인공신경망도 마찬가지다. 각 입력에 가중치를 곱해서 얼마나 중요한지를 판단한다.</p><p>계산 과정은 간단하다. 입력 값 x가 들어오면, 이에 가중치 w를 곱해서 wx를 만든다. 이 과정을 모든 입력에 대해 수행하고, 그 결과를 다 더한다. 이걸 z라고 부르자. 그러니까 z = x₁·w₁ + x₂·w₂ + &mldr; + xₙ·wₙ이다. 이 z는 아직 판단 결과가 아니다. 말하자면 지금까지 계산된 총합일 뿐이다. 그런데 여기서 “이 정도 점수가 넘어가면 고양이다” 같은 기준선을 정해주어야 하는데, 이때 등장하는 게 편향이다.</p><p>편향(bias)은 말 그대로, 계산된 총합에 더하거나 빼는 하나의 값이다. 이것이 왜 중요할까? 편향이 없다면, 입력이 전혀 없을 땐 항상 0을 출력해야 할지도 모른다. 그런데 현실은 다르다. 예를 들어 어떤 학생이 시험을 보면 항상 최소한 30점은 맞는다. 이 30점은 공부와 상관없이 기본으로 갖고 있는 능력이라고 할 수 있다. 마찬가지로, 편향은 입력 값이 아무리 작거나 없다 해도, 어느 정도 기본값을 유지하거나 기준선을 조정할 수 있게 해준다.</p><p>편향의 역할은 직관적으로 말해 ‘문턱을 조절하는 스위치’다. 예를 들어 시그모이드 같은 함수에서는 z가 0보다 크면 0.5 이상, 작으면 0.5 미만으로 결과가 나온다. 이때 편향을 더하면 이 문턱을 옮길 수 있다. 즉, 더 엄격하게 판단할 수도 있고, 더 관대하게 판단할 수도 있는 것이다. 쉽게 말해서 어떤 뉴런이 “조금만 자극이 와도 반응할지” 아니면 “강한 자극이 올 때만 반응할지”를 조정하는 건 편향이 하는 일이다. 이렇게 생각하면, 편향은 뉴런의 성격을 바꾼다고도 할 수 있다. 어떤 뉴런은 예민하고, 어떤 뉴런은 둔감하게 만드는 조절기다.</p><p>조금 더 수학적으로 보면, 인공신경망의 뉴런은 y = f(wx + b)라는 구조로 작동한다. 여기서 x는 입력, w는 가중치, b는 편향, f는 활성화 함수다. 이 식에서 w는 입력의 크기를 조절하고, b는 기준점을 조절한다. 결과적으로 이 두 요소는 입력이 어떻게 처리될지를 완전히 결정짓는다.</p><p>이제, 가중치와 편향은 처음에는 아무렇게나 설정된다. 우리가 무작위로 시작한다. 그러고 나서 딥러닝은 데이터를 보면서 정답과 예측이 얼마나 다른지 비교하고, 그 차이를 줄이기 위해 w와 b를 조금씩 바꾼다. 예를 들어, 어떤 입력이 결과에 큰 영향을 줘야 했는데 실제로는 반영이 잘 안 됐다면, 그 입력의 가중치를 키운다. 혹은 기준점이 너무 높아서 뉴런이 자꾸 반응을 못 했다면, 편향을 낮춘다. 이런 식으로 w와 b를 조정하면서 딥러닝은 더 똑똑해진다. 마치 사람이 문제를 틀리면 “아, 여긴 더 신경 써야겠다” 하고 다음에 잘 풀 수 있도록 마음을 고쳐먹는 것처럼, 신경망도 w와 b를 고쳐가면서 학습한다.</p><p>가중치는 마치 각 입력에 대한 스피커의 볼륨이라고 보면 된다. 어떤 소리는 크게 들려야 하고, 어떤 소리는 작게 들려야 한다. 편향은 스피커의 기본 출력 세기라고 할 수 있다. 아무 소리도 없는데도 약간의 잡음이 나올 수 있고, 그것이 필요할 때가 있다. 예를 들어 의사가 환자를 진료할 때, 병력이나 외부 증상 외에도 항상 어느 정도는 “혹시 모를 경우”라는 기본 경계심을 갖는다. 이게 일종의 편향이다. 아무런 입력이 없더라도 어느 정도의 경계심을 갖고 판단을 내리는 것. 딥러닝도 마찬가지다. 완전한 입력이 없더라도, 편향을 통해 기준을 설정하고 판단을 조절한다.</p><p>좀 더 직관적인 비유를 하나 더 해보자. 인공신경망은 마치 요리사다. 다양한 재료들(입력)을 받고, 그 재료들이 얼마나 중요한지를 판단해서 맛을 조절한다. 이때 가중치는 각 재료의 비율을 나타낸다. 예를 들어 짜장면을 만들려면 춘장은 많이 넣고, 물은 적게 넣는다. 재료가 많다고 다 중요한 건 아니다. 가중치를 잘못 주면 맛이 이상해진다. 편향은 마치 요리의 기본 양념이다. 모든 요리에 약간의 소금이 들어가듯, 어떤 결과를 내기 위한 기본값이 존재하는 것이다. 요리를 잘하는 요리사는 재료의 양도 조절하고, 기본 양념도 정확히 넣는다. 딥러닝도 마찬가지다. 가중치와 편향을 잘 조절해야 훌륭한 결과를 낼 수 있다.</p><p>결국, 가중치와 편향은 인공신경망이 세상을 이해하는 방식의 핵심 열쇠다. 가중치는 입력을 어떻게 다룰지 결정하고, 편향은 언제 반응할지를 조절한다. 이 두 가지가 정확히 맞아떨어질 때, 신경망은 복잡한 문제를 정확하게 해결할 수 있다. 그래서 딥러닝에서 학습이란 건 사실 이 두 값을 찾아가는 여정이다. 이 입력은 얼마나 중요하고, 어느 선에서 판단을 바꿔야 할지를 끊임없이 조율해가는 과정이다. 가중치와 편향을 통해 신경망은 단순한 숫자 계산을 넘어, 고양이와 강아지를 구분하고, 언어의 의미를 이해하며, 심지어 예술적인 감각까지 모방할 수 있는 존재로 진화한다.</p><p>이 모든 것이 단지 입력에 숫자를 곱하고 더하는 간단한 수식에서 시작된다는 건 정말 놀라운 일이다. 하지만 이 단순한 수식이 반복되고 조정되고 조합되면서, 우리는 인간처럼 사고하는 기계를 만들어가고 있다. “그냥 숫자 몇 개 곱하고 더하는 건데, 그게 세상을 이해하는 방법이라니. 참 멋진 일이잖아!”</p><h3 id=4-활성화-함수의-필요성>4. 활성화 함수의 필요성
<a class=anchor href=#4-%ed%99%9c%ec%84%b1%ed%99%94-%ed%95%a8%ec%88%98%ec%9d%98-%ed%95%84%ec%9a%94%ec%84%b1>#</a></h3><p>딥러닝에서 활성화 함수(activation function)가 왜 필요한지를 이해하려면, 먼저 아주 단순한 신경망을 떠올려보는 것이 좋다. 입력이 있고, 그 입력에 어떤 가중치를 곱해서 더한 다음, 결과를 출력하는 구조를 생각해보자. 수학적으로는 이런 구조를 &ldquo;선형(linear) 모델"이라고 부른다. 즉, 출력은 입력의 선형 조합이라는 뜻이다. 예를 들어 x₁이라는 입력에 w₁이라는 가중치를 곱하고, x₂에 w₂를 곱한 뒤, 이 둘을 더하고, 마지막으로 편향 b를 더한 것이 출력이 되는 구조다. 아무리 이 과정을 반복해서 층을 쌓고 쌓아도, 그 결과는 여전히 선형(linear)의 범주를 벗어나지 않는다. 왜냐하면 선형끼리 아무리 곱하고 더해도 결국 또 하나의 선형 함수일 뿐이기 때문이다.</p><p>그럼 이게 왜 문제가 될까? 실제 세상은 선형이 아니다. 아주 단순한 예를 들어 보자. 어떤 사람이 배가 고플 때는 음식을 보고 기뻐하지만, 너무 배부를 때는 오히려 그 음식을 보면 짜증이 난다. 즉, 입력과 출력의 관계가 단순한 직선이 아니라, 어떤 구부러진 곡선이 된다. 또는 고양이와 개를 구분할 때, 귀의 크기만으로는 판단이 안 되고, 귀의 크기와 눈의 간격이 특정 조합일 때만 “아, 고양이다!” 하고 판단할 수 있을지도 모른다. 이런 건 선형 함수로는 도저히 설명할 수 없다. 이럴 때 필요한 것이 바로 &ldquo;비선형성(non-linearity)&ldquo;이다. 그리고 이 비선형성을 도입하는 도구가 바로 활성화 함수다.</p><p>한마디로 말해, 활성화 함수는 뉴런에게 생각의 자유를 주는 문이다. 이 문이 없다면 뉴런은 무조건 정해진 선형식대로만 계산을 하게 된다. 이 경우 아무리 층을 많이 쌓아도, 결국 전체 모델은 하나의 선형 모델과 다를 바 없다. 층이 많아봤자 아무 소용이 없는 것이다. 하지만 활성화 함수가 있으면, 그 층마다 새로운 비선형 특성이 들어가고, 그렇게 복잡하고 다양한 패턴을 인식할 수 있는 능력이 생긴다. 다시 말해, 활성화 함수는 단순한 계산기인 뉴런에게 “생각의 전환점”을 제공한다.</p><p>예를 들어, 아주 간단한 활성화 함수 중 하나인 계단 함수(step function)를 보자. 이 함수는 입력이 어떤 기준값보다 크면 출력은 1, 작으면 0을 출력한다. 이 함수는 뉴런에게 “이 정도 자극이 오면 반응해, 아니면 조용히 있어”라는 규칙을 주는 것이다. 그런데 이 함수는 갑자기 뚝 끊어지듯 동작하기 때문에, 현재는 잘 쓰이지 않는다. 대신 좀 더 부드러운 함수들이 널리 쓰인다. 대표적인 예가 시그모이드 함수(sigmoid function)다. 이 함수는 입력이 작을 때는 0에 가까운 값을, 클 때는 1에 가까운 값을 출력하고, 중간에서는 완만하게 상승하는 S자 곡선을 그린다. 이것은 뉴런이 단순히 &ldquo;반응한다, 안 한다"로 나뉘는 것이 아니라, &ldquo;얼마나 강하게 반응할까?&ldquo;를 연속적인 값으로 표현하게 해준다.</p><p>또 다른 유명한 함수는 ReLU(Rectified Linear Unit) 함수다. 이 함수는 입력이 0보다 작으면 0, 크면 그대로 출력한다. 이 단순한 구조는 계산이 빠르고, 깊은 신경망에서도 효과적으로 작동하기 때문에 딥러닝의 표준이 되었다. ReLU는 특히 음수에 대해서는 반응하지 않고, 양수에 대해서만 반응하므로, 뉴런이 “이건 별로 중요하지 않아”라고 판단할 수 있는 기회를 주기도 한다. 이는 마치 우리 뇌가 어떤 소리를 들었을 때 “그냥 백색소음이야”라고 무시하고, 중요한 말소리는 반응하는 것과 비슷하다.</p><p>이렇게 다양한 활성화 함수들은 딥러닝 모델에게 유연함을 선사한다. 이미지 인식, 음성 분석, 언어 이해 같은 문제들은 모두 단순한 직선으로 설명할 수 없는 복잡한 구조를 가지고 있다. 입력 값 사이의 관계도 비선형적이고, 다양한 조건과 조합에 따라 결과가 달라진다. 이러한 현실 세계의 복잡성을 담기 위해선, 신경망이 반드시 비선형성을 갖춰야 하고, 그 핵심이 바로 활성화 함수다.</p><p>활성화 함수는 또 다른 중요한 역할을 한다. 바로 출력 값을 일정한 범위로 제한하는 것이다. 만약 활성화 함수가 없다면, 뉴런의 출력은 입력의 크기에 따라 무한히 커질 수 있다. 예를 들어, 입력이 10이고 가중치도 10이라면 출력은 100이 될 수 있다. 이렇게 점점 커지면 나중에 숫자가 너무 커져서 계산이 어려워진다. 하지만 시그모이드 함수처럼 출력이 항상 0과 1 사이에 머무는 함수는 숫자가 폭주하는 것을 막아준다. 이는 학습의 안정성에도 매우 중요하다. 물론 ReLU처럼 제한 없이 커질 수 있는 함수도 있지만, 이는 각 층마다 정규화를 추가하거나 적절한 초기화 기법으로 조절이 가능하다.</p><p>또한, 활성화 함수는 네트워크가 “결정 경계(decision boundary)”를 학습하게 도와준다. 예를 들어 어떤 데이터가 두 개의 클래스(예: 고양이 vs 강아지)로 나뉜다고 할 때, 그 둘을 나누는 선이나 면을 잘 학습하는 것이 목표다. 선형 함수만으로는 직선이나 평면밖에 만들 수 없다. 그러나 실제 데이터는 복잡하게 뒤얽혀 있기 때문에, 곡선이나 비선형 경계가 필요하다. 이때 활성화 함수가 뉴런의 출력을 비틀어 줌으로써, 모델은 더욱 복잡하고 세밀한 결정 경계를 만들 수 있다. 덕분에 데이터가 뒤얽혀 있어도 올바르게 분류할 수 있게 된다.</p><p>활성화 함수의 존재는 딥러닝의 핵심이자, 신경망을 단순한 선형 회귀와 구분 짓는 근본적인 이유다. 만약 활성화 함수가 없다면, 아무리 층을 쌓아도 선형 모델의 틀을 벗어나지 못한다. 딥러닝이라는 말이 의미 있으려면, &lsquo;딥&rsquo; 즉 층이 깊어질수록 더 풍부한 표현력을 가져야 하고, 그걸 가능하게 해주는 것이 바로 이 작은 비선형 함수들이다.</p><p>이제 파인만 식으로 정말 쉽게 말해보자. 뉴런은 전기를 받는 전구와 같다. 입력이라는 전압이 들어오면, 일정 수준 이상일 때 불이 켜진다. 너무 약하면 불이 안 켜진다. 그런데 이 전구가 단순히 켜지거나 꺼지는 것만 할 수 있다면, 우리는 아주 복잡한 전기 회로를 만들 수 없을 것이다. 그래서 전구의 밝기가 입력 전압에 따라 달라지도록 만든다고 상상해보자. 아주 약하면 어둡고, 점점 밝아지고, 아주 강하면 최대 밝기까지 올라간다. 이것이 바로 시그모이드다. 또는 입력이 없으면 불이 꺼져 있고, 일정 전압 이상이 되면 그 전압만큼 밝아진다고 하자. 이건 ReLU다. 이런 방식으로 전구가 단순히 ‘켜짐/꺼짐’을 넘어서, 입력의 차이에 따라 다양한 반응을 보이게 된다면, 우리는 더 섬세하고 정교한 회로를 만들 수 있게 된다. 딥러닝도 똑같다. 뉴런들이 입력에 따라 다양한 방식으로 반응해야, 복잡한 세상의 패턴을 잘 따라갈 수 있다. 바로 그 역할을 활성화 함수가 해주는 것이다.</p><p>딥러닝의 발전 역사에서도 활성화 함수의 변화는 매우 중요한 순간들이었다. 초기에는 시그모이드나 tanh 함수가 널리 쓰였지만, 이 함수들은 입력이 크거나 작을 때 출력이 포화(saturation)되어, 그 구간에서는 거의 반응을 하지 않게 된다는 단점이 있었다. 이 때문에 학습이 매우 느려지거나 멈추는 문제가 있었다. 이를 해결하기 위해 ReLU 같은 새로운 함수가 제안되었고, 이는 깊은 네트워크를 효율적으로 학습시키는 데 큰 도움이 되었다. 이후에도 Leaky ReLU, ELU, Swish 등 다양한 함수들이 제안되며, 딥러닝 모델의 성능을 끌어올리는 데 기여하고 있다.</p><p>결론적으로, 활성화 함수는 뉴런에게 생각할 수 있는 힘을 준다. 입력을 단순히 더하고 곱하는 것만으로는 세상의 복잡함을 따라갈 수 없다. 하지만 그 계산 결과에 비선형 함수를 적용하면, 뉴런은 다양한 방식으로 반응하게 되고, 그 결과 우리는 사진을 인식하고, 말을 이해하고, 글을 생성하고, 음악을 작곡하는 AI를 만들 수 있다. 이 작은 함수 하나하나가 딥러닝의 진짜 마법을 일으키는 열쇠인 셈이다. 뉴런은 입력을 받아 반응하지만, 그 반응의 방식이 얼마나 다양하고 부드럽냐에 따라 전체 모델이 얼마나 똑똑해질 수 있는지가 결정된다. 그래서 활성화 함수는 단순한 수학 함수 그 이상으로, 딥러닝의 두뇌를 살아 있게 만드는 숨결과도 같다.</p><h3 id=5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들
<a class=anchor href=#5-%eb%8c%80%ed%91%9c%ec%a0%81%ec%9d%b8-%ed%99%9c%ec%84%b1%ed%99%94-%ed%95%a8%ec%88%98%eb%93%a4>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#1-딥러닝>1. 딥러닝?</a></li><li><a href=#2-뉴런-모델의-수학적-구조>2. 뉴런 모델의 수학적 구조</a></li><li><a href=#3-가중치와-편향의-역할>3. 가중치와 편향의 역할</a></li><li><a href=#4-활성화-함수의-필요성>4. 활성화 함수의 필요성</a></li><li><a href=#5-대표적인-활성화-함수들>5. 대표적인 활성화 함수들</a></li></ul></li></ul></nav></div></aside></main></body></html>