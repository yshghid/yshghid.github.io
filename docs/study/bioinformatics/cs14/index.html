<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  [ë”¥ëŸ¬ë‹] ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT
  #


  ëª©ë¡
  #

2024-12-31 â‹¯ 17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)
2024-12-31 â‹¯ 17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
2024-12-31 â‹¯ 17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
2024-12-31 â‹¯ 17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
2024-12-31 â‹¯ 17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡


  17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)
  #


BERT?

BERTëŠ” ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©(Contextual Embedding)ì„ ì‚¬ìš©í•¨. ì´ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ëœ ë°©ì‹.
ì…/ì¶œë ¥ êµ¬ì¡°

ì…ë ¥ì€ ê° ë‹¨ì–´ë¥¼ 768ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•œ ê²ƒ. ex) [CLS], I, love, you â†’ ê°ê° 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜.
ì¶œë ¥ì€ BERTì˜ ë‚´ë¶€ ì—°ì‚°ì„ ê±°ì³, ë¬¸ë§¥ì„ ë°˜ì˜í•œ 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ëœ ê²ƒ.
ë¬¸ë§¥ ë°˜ì˜? ì…ë ¥ëœ ë‹¨ì–´ì˜ ë²¡í„°ì— ëŒ€í•œ ì¶œë ¥ ì„ë² ë”©ì€ ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ ë²¡í„°. [CLS] ë²¡í„°ëŠ” ë¬¸ì¥ì˜ ì „ì²´ ì •ë³´ë¥¼ ìš”ì•½í•œ ë²¡í„°ë¡œ í™œìš©ëœë‹¤.


êµ¬ì¡°ì™€ ì—°ì‚°

BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¥¼ 12ì¸µ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°.
ê° ì¸µì—ì„œ ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜(Multi-Head Self-Attention)**ê³¼ í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(Position-wise Feed Forward Network) ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì…ë ¥ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•œë‹¤.




BERTì˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €: WordPiece"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/bioinformatics/cs14/"><meta property="og:site_name" content=" "><meta property="og:title" content="ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT"><meta property="og:description" content="[ë”¥ëŸ¬ë‹] ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT # ëª©ë¡ # 2024-12-31 â‹¯ 17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)
2024-12-31 â‹¯ 17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
2024-12-31 â‹¯ 17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
2024-12-31 â‹¯ 17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
2024-12-31 â‹¯ 17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT) # BERT?
BERTëŠ” ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©(Contextual Embedding)ì„ ì‚¬ìš©í•¨. ì´ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ëœ ë°©ì‹. ì…/ì¶œë ¥ êµ¬ì¡° ì…ë ¥ì€ ê° ë‹¨ì–´ë¥¼ 768ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•œ ê²ƒ. ex) [CLS], I, love, you â†’ ê°ê° 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜. ì¶œë ¥ì€ BERTì˜ ë‚´ë¶€ ì—°ì‚°ì„ ê±°ì³, ë¬¸ë§¥ì„ ë°˜ì˜í•œ 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ëœ ê²ƒ. ë¬¸ë§¥ ë°˜ì˜? ì…ë ¥ëœ ë‹¨ì–´ì˜ ë²¡í„°ì— ëŒ€í•œ ì¶œë ¥ ì„ë² ë”©ì€ ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ ë²¡í„°. [CLS] ë²¡í„°ëŠ” ë¬¸ì¥ì˜ ì „ì²´ ì •ë³´ë¥¼ ìš”ì•½í•œ ë²¡í„°ë¡œ í™œìš©ëœë‹¤. êµ¬ì¡°ì™€ ì—°ì‚° BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¥¼ 12ì¸µ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°. ê° ì¸µì—ì„œ ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜(Multi-Head Self-Attention)**ê³¼ í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(Position-wise Feed Forward Network) ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì…ë ¥ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•œë‹¤. BERTì˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €: WordPiece"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-12-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-31T00:00:00+00:00"><meta property="article:tag" content="2024-12"><title>ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/bioinformatics/cs14/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.317f060c7b872e5b96e3450f326e7ec9cc17aec290730b7e83b9ca8a0aa2c606.js integrity="sha256-MX8GDHuHLluW40UPMm5+ycwXrsKQcwt+g7nKigqixgY=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>ê¸°ë¡</span><ul><li><a href=/docs/hobby/daily/>ì¼ìƒ</a><ul></ul></li><li><a href=/docs/hobby/book/>ê¸€</a><ul></ul></li><li><a href=/docs/hobby/favorite/>ğŸ¤</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/docs/study/>ê³µë¶€</a><ul><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/sw/>SW</a><ul></ul></li><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/algorithm/>ì½”í…Œ</a><ul></ul></li><li><a href=/docs/study/career/>ì·¨ì—…</a><ul></ul></li><li><a href=/docs/study/github/>ê¹ƒí—ˆë¸Œ</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#ëª©ë¡>ëª©ë¡</a></li><li><a href=#17-02-ë²„íŠ¸bidirectional-encoder-representations-from-transformers-bert>17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)</a></li><li><a href=#17-03-êµ¬ê¸€-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></li><li><a href=#17-04-í•œêµ­ì–´-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></li><li><a href=#17-05-êµ¬ê¸€-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></li><li><a href=#17-06-í•œêµ­ì–´-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=ë”¥ëŸ¬ë‹-ë”¥ëŸ¬ë‹ì„-ì´ìš©í•œ-ìì—°ì–´-ì²˜ë¦¬-ì…ë¬¸--bert>[ë”¥ëŸ¬ë‹] ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ | BERT
<a class=anchor href=#%eb%94%a5%eb%9f%ac%eb%8b%9d-%eb%94%a5%eb%9f%ac%eb%8b%9d%ec%9d%84-%ec%9d%b4%ec%9a%a9%ed%95%9c-%ec%9e%90%ec%97%b0%ec%96%b4-%ec%b2%98%eb%a6%ac-%ec%9e%85%eb%ac%b8--bert>#</a></h1><h2 id=ëª©ë¡>ëª©ë¡
<a class=anchor href=#%eb%aa%a9%eb%a1%9d>#</a></h2><p><em>2024-12-31</em> â‹¯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert>17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)</a></p><p><em>2024-12-31</em> â‹¯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></p><p><em>2024-12-31</em> â‹¯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></p><p><em>2024-12-31</em> â‹¯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></p><p><em>2024-12-31</em> â‹¯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></p><hr><h2 id=17-02-ë²„íŠ¸bidirectional-encoder-representations-from-transformers-bert>17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)
<a class=anchor href=#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert>#</a></h2><blockquote><p><strong>BERT?</strong></p><ol><li>BERTëŠ” ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©(Contextual Embedding)ì„ ì‚¬ìš©í•¨. ì´ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ëœ ë°©ì‹.</li><li>ì…/ì¶œë ¥ êµ¬ì¡°<ul><li>ì…ë ¥ì€ ê° ë‹¨ì–´ë¥¼ 768ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•œ ê²ƒ. ex) [CLS], I, love, you â†’ ê°ê° 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜.</li><li>ì¶œë ¥ì€ BERTì˜ ë‚´ë¶€ ì—°ì‚°ì„ ê±°ì³, ë¬¸ë§¥ì„ ë°˜ì˜í•œ 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ëœ ê²ƒ.</li><li>ë¬¸ë§¥ ë°˜ì˜? ì…ë ¥ëœ ë‹¨ì–´ì˜ ë²¡í„°ì— ëŒ€í•œ ì¶œë ¥ ì„ë² ë”©ì€ ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ ë²¡í„°. [CLS] ë²¡í„°ëŠ” ë¬¸ì¥ì˜ ì „ì²´ ì •ë³´ë¥¼ ìš”ì•½í•œ ë²¡í„°ë¡œ í™œìš©ëœë‹¤.</li></ul></li><li>êµ¬ì¡°ì™€ ì—°ì‚°<ul><li>BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¥¼ 12ì¸µ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°.</li><li>ê° ì¸µì—ì„œ ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜(Multi-Head Self-Attention)**ê³¼ í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(Position-wise Feed Forward Network) ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì…ë ¥ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•œë‹¤.</li></ul></li></ol></blockquote><blockquote><p><strong>BERTì˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €: WordPiece</strong></p><ol><li>ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €: ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ, ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì„œë¸Œì›Œë“œ(subword) ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” ë°©ì‹ì˜ í† í¬ë‚˜ì´ì €.</li><li>WordPieceì˜ ì‘ë™ ì›ë¦¬<ul><li>í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ë‹¨ì–´ ì§‘í•©ì„ ìƒì„±í•˜ëŠ”ë°, ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¶”ê°€í•˜ê³  ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë” ì‘ì€ ë‹¨ìœ„(ì„œë¸Œì›Œë“œ)ë¡œ ìª¼ê°œì–´ ì¶”ê°€í•œë‹¤.</li><li>í† í°í™”: ë‹¨ì–´ê°€ ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  ë‹¨ì–´ê°€ ë‹¨ì–´ ì§‘í•©ì— ì—†ìœ¼ë©´ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬í•œë‹¤.
ex) ë‹¨ì–´ &ldquo;embeddings"ê°€ ë‹¨ì–´ ì§‘í•©ì— ì—†ìœ¼ë©´ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬: em, ##bed, ##ding, #s. ##ëŠ” ë‹¨ì–´ì˜ ì¤‘ê°„ì´ë‚˜ ëì—ì„œ ì˜¨ ì„œë¸Œì›Œë“œë¼ëŠ” í‘œì‹œì´ë‹¤.</li></ul></li><li>BERTëŠ” ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ í† í°í™”ë¥¼ ìˆ˜í–‰í•œ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.</li></ol></blockquote><blockquote><p><strong>ìš”ì•½</strong></p><ul><li>BERTëŠ” ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œë¥¼ ì°¸ê³ í•˜ë„ë¡ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”(ì…€í”„ ì–´í…ì…˜)ë¥¼ í™œìš©í•´ ë¬¸ë§¥ ì •ë³´ë¥¼ í¬í•¨í•œ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.</li><li>WordPiece í† í¬ë‚˜ì´ì €ëŠ” ë‹¨ì–´ë¥¼ ìì£¼ ë“±ì¥ ì—¬ë¶€ì— ë”°ë¼ ë‹¨ì–´ ë˜ëŠ” ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ”ë° ì„œë¸Œì›Œë“œ í‘œê¸°(##)ë¥¼ í†µí•´ ë‹¨ì–´ ë³µì›ì´ ê°€ëŠ¥í•˜ë©°, ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ í‘œí˜„ë ¥ì„ ë†’ì¸ë‹¤.</li></ul></blockquote><ol><li>transformers íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸°</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>) <span style=color:#75715e># Bert-baseì˜ í† í¬ë‚˜ì´ì €</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>tokenize(<span style=color:#e6db74>&#39;Here is the sentence I want embeddings for.&#39;</span>)
</span></span><span style=display:flex><span>print(result)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>vocab[<span style=color:#e6db74>&#39;here&#39;</span>])
</span></span><span style=display:flex><span><span style=color:#75715e>#print(tokenizer.vocab[&#39;embeddings&#39;])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;sentence&#39;, &#39;i&#39;, &#39;want&#39;, &#39;em&#39;, &#39;##bed&#39;, &#39;##ding&#39;, &#39;##s&#39;, &#39;for&#39;, &#39;.&#39;]
</span></span><span style=display:flex><span>2182
</span></span></code></pre></div><ul><li>&lsquo;Here is the sentence I want embeddings for.&lsquo;ë¼ëŠ” ë¬¸ì¥ì„ BERTì˜ í† í¬ë‚˜ì´ì €ê°€ ì–´ë–»ê²Œ í† í°í™”í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸°.</li><li>embeddingsë¼ëŠ” ë‹¨ì–´ëŠ” ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ em, ##bed, ##ding, #së¡œ ë¶„ë¦¬ë˜ì—ˆë‹¤.</li><li>BERTì˜ ë‹¨ì–´ ì§‘í•©ì— &ldquo;here"ê°€ ìˆëŠ”ì§€ ì¡°íšŒ -> ë‹¨ì–´ hereì´ ì •ìˆ˜ ì¸ì½”ë”©ì„ ìœ„í•´ì„œ ë‹¨ì–´ ì§‘í•© ë‚´ë¶€ì ìœ¼ë¡œ 2182ë¼ëŠ” ì •ìˆ˜ë¡œ ë§µí•‘ë˜ì–´ì ¸ ìˆë‹¤.</li><li>&ldquo;embeddings"ê°€ ìˆëŠ”ì§€ ì¡°íšŒ -> KeyError: &rsquo;embeddings&rsquo; ë°œìƒ.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># BERTì˜ ë‹¨ì–´ ì§‘í•©ì„ vocabulary.txtì— ì €ì¥</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#39;vocabulary.txt&#39;</span>, <span style=color:#e6db74>&#39;w&#39;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokenizer<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>        f<span style=color:#f92672>.</span>write(token <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_fwf(<span style=color:#e6db74>&#39;vocabulary.txt&#39;</span>, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :&#39;</span>,len(df))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : 30522
</span></span></code></pre></div><blockquote><p><strong>ìë£Œ ì¶œì²˜</strong></p><p><a href=https://wikidocs.net/115055>https://wikidocs.net/115055</a></p></blockquote><hr><h2 id=17-03-êµ¬ê¸€-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
<a class=anchor href=#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForMaskedLM, AutoTokenizer
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-large-uncased&#39;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-large-uncased&#34;</span>)
</span></span></code></pre></div><ul><li>TFBertForMaskedLM: ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸(Masked Language Model, MLM)ì„ ìœ„í•œ BERT êµ¬ì¡°</li><li>AutoTokenizer: í•´ë‹¹ ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í† í¬ë‚˜ì´ì €.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#39;Soccer is a really fun [MASK].&#39;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)
</span></span></code></pre></div><ul><li>ì‚¬ì „ í•™ìŠµëœ BERTë¡œ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ ìƒì„±í•¨.</li><li>ì˜ˆì œ ë¬¸ì¥: &lsquo;Soccer is a really fun [MASK].&lsquo;ì— ëŒ€í•´ í† í¬ë‚˜ì´ì €ë¡œ ì •ìˆ˜ ì¸ì½”ë”©ì„ ìˆ˜í—¹.</li><li>[MASK] í† í° ì˜ˆì¸¡í•˜ê¸°?</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> FillMaskPipeline
</span></span><span style=display:flex><span>pip <span style=color:#f92672>=</span> FillMaskPipeline(model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>pip(<span style=color:#e6db74>&#39;Soccer is a really fun [MASK].&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[{&#39;score&#39;: 0.7621169686317444,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4368,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;sport&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun sport.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.2034207135438919,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2208,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;game&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun game.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.01220863126218319,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2518,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;thing&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun thing.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.001863038633018732,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4023,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;activity&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun activity.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.0013354964321479201,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2492,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;field&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun field.&#39;}]
</span></span></code></pre></div><ul><li>ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ íŒŒì´í”„ë¼ì¸ì— ì§€ì •.<ul><li>FillMaskPipelineì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì—ì„œ [MASK] ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡</li><li>ê²°ê³¼ëŠ” [MASK]ì— ë“¤ì–´ê°ˆ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ 5ê°œì™€ ê° ë‹¨ì–´ì˜ ê´€ë ¨ ì •ë³´</li></ul></li><li>ì˜ˆì œ ê²°ê³¼<ol><li>sportê°€ ê°€ì¥ ë†’ì€ í™•ë¥  0.7621ì„ ê°€ì§. ë¬¸ì¥ì´ ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë§¥ìƒ ê°€ì¥ ì í•©í•˜ê¸° ë•Œë¬¸ì— MLM ëª¨ë¸ì´ ì´ë¥¼ ì²« ë²ˆì§¸ í›„ë³´ë¡œ ì˜ˆì¸¡í–ˆë‹¤.</li><li>gameì€ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ í™•ë¥  0.2034ì„ ê°€ì§.</li><li>thing, activity, fieldëŠ” 1.2%, 0.19, 0.13% í™•ë¥ ì„ ê°€ì§.</li></ol></li></ul><blockquote><p><strong>ìë£Œ ì¶œì²˜</strong></p><p><a href=https://wikidocs.net/153992>https://wikidocs.net/153992</a></p></blockquote><hr><h2 id=17-04-í•œêµ­ì–´-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸
<a class=anchor href=#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>#</a></h2><blockquote><p>&lsquo;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤&rsquo;ë¥¼ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ìœ¼ë©´, ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì€ [MASK]ì˜ ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForMaskedLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;klue/bert-base&#39;</span>, from_pt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;klue/bert-base&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤.&#39;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;attention_mask&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)
</span></span><span style=display:flex><span>tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)
</span></span><span style=display:flex><span>tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)
</span></span></code></pre></div><ul><li>klue/bert-baseì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì„œ &lsquo;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤&rsquo;ë¥¼ ë³€í™˜.</li><li>í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜ëœ ê²°ê³¼: inputs<ul><li>input_ids: ì •ìˆ˜ë¡œ ë³€í™˜ëœ í† í° ì‹œí€€ìŠ¤.</li><li>token_type_ids: ë¬¸ì¥ êµ¬ë¶„ (í•œ ê°œ ë¬¸ì¥ì´ë¯€ë¡œ ëª¨ë‘ 0).</li><li>attention_mask: íŒ¨ë”© í† í° êµ¬ë¶„ (íŒ¨ë”© ì—†ìŒ â†’ ëª¨ë‘ 1).</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> FillMaskPipeline
</span></span><span style=display:flex><span>pip <span style=color:#f92672>=</span> FillMaskPipeline(model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>pip(<span style=color:#e6db74>&#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤.&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[{&#39;score&#39;: 0.8963565230369568,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4559,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;ìŠ¤í¬ì¸ &#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” ìŠ¤í¬ì¸  ë‹¤.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.025957893580198288,
</span></span><span style=display:flex><span>  &#39;token&#39;: 568,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;ê±°&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” ê±° ë‹¤.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.010034064762294292,
</span></span><span style=display:flex><span>  &#39;token&#39;: 3682,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;ê²½ê¸°&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” ê²½ê¸° ë‹¤.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.007924459874629974,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4713,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;ì¶•êµ¬&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” ì¶•êµ¬ ë‹¤.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.007844261825084686,
</span></span><span style=display:flex><span>  &#39;token&#39;: 5845,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;ë†€ì´&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” ë†€ì´ ë‹¤.&#39;}]
</span></span></code></pre></div><ul><li>FillMaskPipelineìœ¼ë¡œ [MASK] ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ìƒìœ„ 5ê°œ í›„ë³´ ë‹¨ì–´ ì˜ˆì¸¡.</li><li>&ldquo;ìŠ¤í¬ì¸ "ê°€ ë¬¸ë§¥ìƒ ê°€ì¥ ì í•©í•œ ë‹¨ì–´ë¡œ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ë‹¤.</li></ul><blockquote><p><strong>ìë£Œ ì¶œì²˜</strong></p><p><a href=https://wikidocs.net/152922>https://wikidocs.net/152922</a></p></blockquote><hr><h2 id=17-05-êµ¬ê¸€-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
<a class=anchor href=#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForNextSentencePrediction, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForNextSentencePrediction<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>cls_token, <span style=color:#e6db74>&#39;:&#39;</span>, tokenizer<span style=color:#f92672>.</span>cls_token_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>sep_token, <span style=color:#e6db74>&#39;:&#39;</span> , tokenizer<span style=color:#f92672>.</span>sep_token_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>decode(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>][<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor(
</span></span><span style=display:flex><span>[[  101  1999  3304  1010 10733  2366  1999  5337 10906  1010  2107  2004
</span></span><span style=display:flex><span>   2012  1037  4825  1010  2003  3591  4895 14540  6610  2094  1012   102
</span></span><span style=display:flex><span>  10733  2003  8828  2007  1996  2224  1997  1037  5442  1998  9292  1012
</span></span><span style=display:flex><span>   1999 10017 10906  1010  2174  1010  2009  2003  3013  2046 17632  2015
</span></span><span style=display:flex><span>   2000  2022  8828  2096  2218  1999  1996  2192  1012   102]], shape=(1, 58), dtype=int32)
</span></span><span style=display:flex><span>[CLS] : 101
</span></span><span style=display:flex><span>[SEP] : 102
</span></span><span style=display:flex><span>[CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP]
</span></span><span style=display:flex><span>tf.Tensor(
</span></span><span style=display:flex><span>[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1
</span></span><span style=display:flex><span>  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32)
</span></span></code></pre></div><ul><li>ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , í† í¬ë‚˜ì´ì €ë¡œ ë‘ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”©í–ˆë‹¤.</li><li>input_idsëŠ” ì •ìˆ˜ë¡œ ë³€í™˜ëœ í† í° ì‹œí€€ìŠ¤ì´ë‹¤.<ul><li>ì—¬ê¸°ì„œ 101ê³¼ 102ëŠ” íŠ¹ë³„ í† í°ì¸ [CLS] í† í°ê³¼ [SEP] í† í°ì´ë‹¤.</li><li>ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë””ì½”ë”©í•´ì„œ í˜„ì¬ ì…ë ¥ì˜ êµ¬ì„±ì„ í™•ì¸í•´ë³´ë©´ BERTì—ì„œ ë‘ ê°œì˜ ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ê²½ìš°ì— ë§¨ ì•ì—ëŠ” [CLS] í† í°, ë¬¸ì¥ì´ ëë‚˜ë©´ [SEP] í† í°, ë‘ë²ˆì§¸ ë¬¸ì¥ì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ë‹¤ì‹œ [SEP] í† í°ì´ ì¶”ê°€ëœë‹¤</li></ul></li><li>token_type_idsëŠ” ë‘ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”©ì´ë‹¤.<ul><li>ì²« ë²ˆì§¸ ë¬¸ì¥ì€ 0, ë‘ ë²ˆì§¸ ë¬¸ì¥ì€ 1.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(probs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[9.9999714e-01 2.8381860e-06]], shape=(1, 2), dtype=float32)
</span></span><span style=display:flex><span>ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” : [0]
</span></span></code></pre></div><ul><li>ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡í•˜ê¸°<ol><li>BERT ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì–´ logits(ì˜ˆì¸¡ ì ìˆ˜)ë¥¼ ë°˜í™˜</li><li>ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•´ ê° ë ˆì´ë¸”(0 ë˜ëŠ” 1)ì— ëŒ€í•œ í™•ë¥  ê³„ì‚°.</li></ol></li><li>ì˜ˆì¸¡ ê²°ê³¼<ul><li>ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¼ í™•ë¥ (ë ˆì´ë¸” 0): 99.9997%</li><li>ì´ì–´ì§€ì§€ ì•ŠëŠ” ë¬¸ì¥ì¼ í™•ë¥ (ë ˆì´ë¸” 1) 0.00028%</li><li>ìµœì¢… ì˜ˆì¸¡ì€ ë ˆì´ë¸” 0ìœ¼ë¡œì¨ ë‘ ë¬¸ì¥ì´ ì´ì–´ì§„ë‹¤ê³  íŒë‹¨í•¨.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># ìƒê´€ì—†ëŠ” ë‘ ê°œì˜ ë¬¸ì¥</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;The sky is blue due to the shorter wavelength of blue light.&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” : [1]
</span></span></code></pre></div><ul><li>ì´ì–´ì§€ì§€ ì•ŠëŠ” ë‘ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸</li><li>ì˜ˆì¸¡ ê²°ê³¼: ì´ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨.</li></ul><blockquote><p><strong>ìë£Œ ì¶œì²˜</strong></p><p><a href=https://wikidocs.net/156767>https://wikidocs.net/156767</a></p></blockquote><hr><h2 id=17-06-í•œêµ­ì–´-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
<a class=anchor href=#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForNextSentencePrediction
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForNextSentencePrediction<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;klue/bert-base&#39;</span>, from_pt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;klue/bert-base&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ì´ì–´ì§€ëŠ” ë‘ ê°œì˜ ë¬¸ì¥</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒëŠ” ì¼ë³¸ê³¼ ê³µë™ìœ¼ë¡œ ê°œìµœë˜ì—ˆë˜ ì„¸ê³„ì ì¸ í° ì”ì¹˜ì…ë‹ˆë‹¤.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ì—¬í–‰ì„ ê°€ë³´ë‹ˆ í•œêµ­ì˜ 2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒì˜ ì¤€ë¹„ëŠ” ì™„ë²½í–ˆìŠµë‹ˆë‹¤.&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” : [0]
</span></span></code></pre></div><ul><li><p>ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ</p><ul><li>TFBertForNextSentencePrediction.from_pretrained(&lsquo;BERT ëª¨ë¸ ì´ë¦„&rsquo;)ì„ ë„£ìœ¼ë©´ ë‘ ê°œì˜ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ ê´€ê³„ì¸ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” BERT êµ¬ì¡°ë¥¼ ë¡œë“œ.</li><li>AutoTokenizer.from_pretrained(&lsquo;ëª¨ë¸ ì´ë¦„&rsquo;)ì„ ë„£ìœ¼ë©´ í•´ë‹¹ ëª¨ë¸ì´ í•™ìŠµë˜ì—ˆì„ ë‹¹ì‹œì— ì‚¬ìš©ë˜ì—ˆë˜ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ.</li></ul></li><li><p>ì˜ˆì¸¡ ê²°ê³¼: ë‘ ë¬¸ì¥ì´ ì´ì–´ì§„ë‹¤ê³  íŒë‹¨.</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># ìƒê´€ì—†ëŠ” ë‘ ê°œì˜ ë¬¸ì¥</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒëŠ” ì¼ë³¸ê³¼ ê³µë™ìœ¼ë¡œ ê°œìµœë˜ì—ˆë˜ ì„¸ê³„ì ì¸ í° ì”ì¹˜ì…ë‹ˆë‹¤.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ê·¹ì¥ê°€ì„œ ë¡œë§¨ìŠ¤ ì˜í™”ë¥¼ ë³´ê³ ì‹¶ì–´ìš”&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” : [1]
</span></span></code></pre></div><ul><li>ì´ì–´ì§€ì§€ ì•ŠëŠ” ë‘ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸</li><li>ì˜ˆì¸¡ ê²°ê³¼: ì´ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨.</li></ul><blockquote><p><strong>ìë£Œ ì¶œì²˜</strong></p><p><a href=https://wikidocs.net/156774>https://wikidocs.net/156774</a></p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#ëª©ë¡>ëª©ë¡</a></li><li><a href=#17-02-ë²„íŠ¸bidirectional-encoder-representations-from-transformers-bert>17-02 ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)</a></li><li><a href=#17-03-êµ¬ê¸€-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-03 êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></li><li><a href=#17-04-í•œêµ­ì–´-bertì˜-ë§ˆìŠ¤í¬ë“œ-ì–¸ì–´-ëª¨ë¸>17-04 í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸</a></li><li><a href=#17-05-êµ¬ê¸€-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-05 êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></li><li><a href=#17-06-í•œêµ­ì–´-bertì˜-ë‹¤ìŒ-ë¬¸ì¥-ì˜ˆì¸¡>17-06 í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡</a></li></ul></nav></div></aside></main></body></html>