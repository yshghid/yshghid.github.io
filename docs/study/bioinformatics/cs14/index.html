<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  [딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT
  #


  목록
  #

2024-12-31 ⋯ 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
2024-12-31 ⋯ 17-03 구글 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-04 한국어 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-05 구글 BERT의 다음 문장 예측
2024-12-31 ⋯ 17-06 한국어 BERT의 다음 문장 예측


  17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
  #


BERT?

BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.
입/출력 구조

입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환.
출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것.
문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다.


구조와 연산

BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조.
각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다.




BERT의 서브워드 토크나이저: WordPiece"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/bioinformatics/cs14/"><meta property="og:site_name" content=" "><meta property="og:title" content="딥러닝을 이용한 자연어 처리 입문 | BERT"><meta property="og:description" content="[딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT # 목록 # 2024-12-31 ⋯ 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
2024-12-31 ⋯ 17-03 구글 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-04 한국어 BERT의 마스크드 언어 모델
2024-12-31 ⋯ 17-05 구글 BERT의 다음 문장 예측
2024-12-31 ⋯ 17-06 한국어 BERT의 다음 문장 예측
17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) # BERT?
BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식. 입/출력 구조 입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환. 출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것. 문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다. 구조와 연산 BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조. 각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다. BERT의 서브워드 토크나이저: WordPiece"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-12-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-31T00:00:00+00:00"><meta property="article:tag" content="2024-12"><title>딥러닝을 이용한 자연어 처리 입문 | BERT |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/bioinformatics/cs14/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.70c87b2f75d4696caa9a5833f44c14ad7fbf0c196b5f5ec8a23416a3eade314b.js integrity="sha256-cMh7L3XUaWyqmlgz9EwUrX+/DBlrX17IojQWo+reMUs=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/algorithm/>Algorithm</a><ul></ul></li><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/sw/>SW</a><ul></ul></li><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/devops/>DevOps</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>딥러닝을 이용한 자연어 처리 입문 | BERT</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#목록>목록</a></li><li><a href=#17-02-버트bidirectional-encoder-representations-from-transformers-bert>17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)</a></li><li><a href=#17-03-구글-bert의-마스크드-언어-모델>17-03 구글 BERT의 마스크드 언어 모델</a></li><li><a href=#17-04-한국어-bert의-마스크드-언어-모델>17-04 한국어 BERT의 마스크드 언어 모델</a></li><li><a href=#17-05-구글-bert의-다음-문장-예측>17-05 구글 BERT의 다음 문장 예측</a></li><li><a href=#17-06-한국어-bert의-다음-문장-예측>17-06 한국어 BERT의 다음 문장 예측</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=딥러닝-딥러닝을-이용한-자연어-처리-입문--bert>[딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT
<a class=anchor href=#%eb%94%a5%eb%9f%ac%eb%8b%9d-%eb%94%a5%eb%9f%ac%eb%8b%9d%ec%9d%84-%ec%9d%b4%ec%9a%a9%ed%95%9c-%ec%9e%90%ec%97%b0%ec%96%b4-%ec%b2%98%eb%a6%ac-%ec%9e%85%eb%ac%b8--bert>#</a></h1><h2 id=목록>목록
<a class=anchor href=#%eb%aa%a9%eb%a1%9d>#</a></h2><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert>17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>17-03 구글 BERT의 마스크드 언어 모델</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>17-04 한국어 BERT의 마스크드 언어 모델</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>17-05 구글 BERT의 다음 문장 예측</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs14/#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>17-06 한국어 BERT의 다음 문장 예측</a></p><hr><h2 id=17-02-버트bidirectional-encoder-representations-from-transformers-bert>17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
<a class=anchor href=#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert>#</a></h2><blockquote><p><strong>BERT?</strong></p><ol><li>BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.</li><li>입/출력 구조<ul><li>입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환.</li><li>출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것.</li><li>문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다.</li></ul></li><li>구조와 연산<ul><li>BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조.</li><li>각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다.</li></ul></li></ol></blockquote><blockquote><p><strong>BERT의 서브워드 토크나이저: WordPiece</strong></p><ol><li>서브워드 토크나이저: 자주 등장하는 단어는 단어 단위로, 드물게 등장하는 단어는 서브워드(subword) 단위로 분리하는 방식의 토크나이저.</li><li>WordPiece의 작동 원리<ul><li>훈련 데이터로부터 단어 집합을 생성하는데, 자주 등장하는 단어는 단어 단위로 추가하고 드물게 등장하는 단어는 더 작은 단위(서브워드)로 쪼개어 추가한다.</li><li>토큰화: 단어가 단어 집합에 존재하면 그대로 사용하고 단어가 단어 집합에 없으면 서브워드로 분리한다.
ex) 단어 &ldquo;embeddings"가 단어 집합에 없으면 서브워드로 분리: em, ##bed, ##ding, #s. ##는 단어의 중간이나 끝에서 온 서브워드라는 표시이다.</li></ul></li><li>BERT는 서브워드 단위로 토큰화를 수행한 입력 데이터를 받아 문맥 정보를 반영한 임베딩을 생성한다.</li></ol></blockquote><blockquote><p><strong>요약</strong></p><ul><li>BERT는 모든 단어가 서로를 참고하도록 트랜스포머 인코더(셀프 어텐션)를 활용해 문맥 정보를 포함한 임베딩을 생성한다.</li><li>WordPiece 토크나이저는 단어를 자주 등장 여부에 따라 단어 또는 서브워드로 분리하여 토큰화를 수행하는데 서브워드 표기(##)를 통해 단어 복원이 가능하며, 단어 집합의 크기를 줄이면서 표현력을 높인다.</li></ul></blockquote><ol><li>transformers 패키지를 사용하여 BERT 토크나이저 사용하기</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>) <span style=color:#75715e># Bert-base의 토크나이저</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>tokenize(<span style=color:#e6db74>&#39;Here is the sentence I want embeddings for.&#39;</span>)
</span></span><span style=display:flex><span>print(result)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>vocab[<span style=color:#e6db74>&#39;here&#39;</span>])
</span></span><span style=display:flex><span><span style=color:#75715e>#print(tokenizer.vocab[&#39;embeddings&#39;])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;sentence&#39;, &#39;i&#39;, &#39;want&#39;, &#39;em&#39;, &#39;##bed&#39;, &#39;##ding&#39;, &#39;##s&#39;, &#39;for&#39;, &#39;.&#39;]
</span></span><span style=display:flex><span>2182
</span></span></code></pre></div><ul><li>&lsquo;Here is the sentence I want embeddings for.&lsquo;라는 문장을 BERT의 토크나이저가 어떻게 토큰화하는지 확인하기.</li><li>embeddings라는 단어는 단어 집합에 존재하지 않으므로 em, ##bed, ##ding, #s로 분리되었다.</li><li>BERT의 단어 집합에 &ldquo;here"가 있는지 조회 -> 단어 here이 정수 인코딩을 위해서 단어 집합 내부적으로 2182라는 정수로 맵핑되어져 있다.</li><li>&ldquo;embeddings"가 있는지 조회 -> KeyError: &rsquo;embeddings&rsquo; 발생.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># BERT의 단어 집합을 vocabulary.txt에 저장</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#39;vocabulary.txt&#39;</span>, <span style=color:#e6db74>&#39;w&#39;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokenizer<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>        f<span style=color:#f92672>.</span>write(token <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_fwf(<span style=color:#e6db74>&#39;vocabulary.txt&#39;</span>, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;단어 집합의 크기 :&#39;</span>,len(df))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>단어 집합의 크기 : 30522
</span></span></code></pre></div><blockquote><p><strong>자료 출처</strong></p><p><a href=https://wikidocs.net/115055>https://wikidocs.net/115055</a></p></blockquote><hr><h2 id=17-03-구글-bert의-마스크드-언어-모델>17-03 구글 BERT의 마스크드 언어 모델
<a class=anchor href=#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForMaskedLM, AutoTokenizer
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-large-uncased&#39;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-large-uncased&#34;</span>)
</span></span></code></pre></div><ul><li>TFBertForMaskedLM: 마스크드 언어 모델(Masked Language Model, MLM)을 위한 BERT 구조</li><li>AutoTokenizer: 해당 모델 학습 시 사용된 토크나이저.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#39;Soccer is a really fun [MASK].&#39;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)
</span></span></code></pre></div><ul><li>사전 학습된 BERT로 마스크드 언어 모델 생성함.</li><li>예제 문장: &lsquo;Soccer is a really fun [MASK].&lsquo;에 대해 토크나이저로 정수 인코딩을 수헹.</li><li>[MASK] 토큰 예측하기?</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> FillMaskPipeline
</span></span><span style=display:flex><span>pip <span style=color:#f92672>=</span> FillMaskPipeline(model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>pip(<span style=color:#e6db74>&#39;Soccer is a really fun [MASK].&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[{&#39;score&#39;: 0.7621169686317444,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4368,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;sport&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun sport.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.2034207135438919,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2208,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;game&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun game.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.01220863126218319,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2518,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;thing&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun thing.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.001863038633018732,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4023,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;activity&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun activity.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.0013354964321479201,
</span></span><span style=display:flex><span>  &#39;token&#39;: 2492,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;field&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;soccer is a really fun field.&#39;}]
</span></span></code></pre></div><ul><li>모델과 토크나이저를 파이프라인에 지정.<ul><li>FillMaskPipeline을 사용하여 문장에서 [MASK] 위치에 들어갈 단어를 예측</li><li>결과는 [MASK]에 들어갈 가능성이 높은 단어 5개와 각 단어의 관련 정보</li></ul></li><li>예제 결과<ol><li>sport가 가장 높은 확률 0.7621을 가짐. 문장이 자연스럽고 문맥상 가장 적합하기 때문에 MLM 모델이 이를 첫 번째 후보로 예측했다.</li><li>game은 두 번째로 높은 확률 0.2034을 가짐.</li><li>thing, activity, field는 1.2%, 0.19, 0.13% 확률을 가짐.</li></ol></li></ul><blockquote><p><strong>자료 출처</strong></p><p><a href=https://wikidocs.net/153992>https://wikidocs.net/153992</a></p></blockquote><hr><h2 id=17-04-한국어-bert의-마스크드-언어-모델>17-04 한국어 BERT의 마스크드 언어 모델
<a class=anchor href=#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>#</a></h2><blockquote><p>&lsquo;축구는 정말 재미있는 [MASK]다&rsquo;를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측한다.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForMaskedLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;klue/bert-base&#39;</span>, from_pt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;klue/bert-base&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#39;축구는 정말 재미있는 [MASK]다.&#39;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])
</span></span><span style=display:flex><span>print(inputs[<span style=color:#e6db74>&#39;attention_mask&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)
</span></span><span style=display:flex><span>tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)
</span></span><span style=display:flex><span>tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)
</span></span></code></pre></div><ul><li>klue/bert-base의 토크나이저를 사용해서 &lsquo;축구는 정말 재미있는 [MASK]다&rsquo;를 변환.</li><li>토크나이저로 변환된 결과: inputs<ul><li>input_ids: 정수로 변환된 토큰 시퀀스.</li><li>token_type_ids: 문장 구분 (한 개 문장이므로 모두 0).</li><li>attention_mask: 패딩 토큰 구분 (패딩 없음 → 모두 1).</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> FillMaskPipeline
</span></span><span style=display:flex><span>pip <span style=color:#f92672>=</span> FillMaskPipeline(model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>pip(<span style=color:#e6db74>&#39;축구는 정말 재미있는 [MASK]다.&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>[{&#39;score&#39;: 0.8963565230369568,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4559,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;스포츠&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;축구는 정말 재미있는 스포츠 다.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.025957893580198288,
</span></span><span style=display:flex><span>  &#39;token&#39;: 568,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;거&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;축구는 정말 재미있는 거 다.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.010034064762294292,
</span></span><span style=display:flex><span>  &#39;token&#39;: 3682,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;경기&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;축구는 정말 재미있는 경기 다.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.007924459874629974,
</span></span><span style=display:flex><span>  &#39;token&#39;: 4713,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;축구&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;축구는 정말 재미있는 축구 다.&#39;},
</span></span><span style=display:flex><span> {&#39;score&#39;: 0.007844261825084686,
</span></span><span style=display:flex><span>  &#39;token&#39;: 5845,
</span></span><span style=display:flex><span>  &#39;token_str&#39;: &#39;놀이&#39;,
</span></span><span style=display:flex><span>  &#39;sequence&#39;: &#39;축구는 정말 재미있는 놀이 다.&#39;}]
</span></span></code></pre></div><ul><li>FillMaskPipeline으로 [MASK] 위치에 들어갈 수 있는 상위 5개 후보 단어 예측.</li><li>&ldquo;스포츠"가 문맥상 가장 적합한 단어로 높은 점수를 받았다.</li></ul><blockquote><p><strong>자료 출처</strong></p><p><a href=https://wikidocs.net/152922>https://wikidocs.net/152922</a></p></blockquote><hr><h2 id=17-05-구글-bert의-다음-문장-예측>17-05 구글 BERT의 다음 문장 예측
<a class=anchor href=#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForNextSentencePrediction, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForNextSentencePrediction<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>print(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>cls_token, <span style=color:#e6db74>&#39;:&#39;</span>, tokenizer<span style=color:#f92672>.</span>cls_token_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>sep_token, <span style=color:#e6db74>&#39;:&#39;</span> , tokenizer<span style=color:#f92672>.</span>sep_token_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>decode(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>][<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor(
</span></span><span style=display:flex><span>[[  101  1999  3304  1010 10733  2366  1999  5337 10906  1010  2107  2004
</span></span><span style=display:flex><span>   2012  1037  4825  1010  2003  3591  4895 14540  6610  2094  1012   102
</span></span><span style=display:flex><span>  10733  2003  8828  2007  1996  2224  1997  1037  5442  1998  9292  1012
</span></span><span style=display:flex><span>   1999 10017 10906  1010  2174  1010  2009  2003  3013  2046 17632  2015
</span></span><span style=display:flex><span>   2000  2022  8828  2096  2218  1999  1996  2192  1012   102]], shape=(1, 58), dtype=int32)
</span></span><span style=display:flex><span>[CLS] : 101
</span></span><span style=display:flex><span>[SEP] : 102
</span></span><span style=display:flex><span>[CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP]
</span></span><span style=display:flex><span>tf.Tensor(
</span></span><span style=display:flex><span>[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1
</span></span><span style=display:flex><span>  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32)
</span></span></code></pre></div><ul><li>모델과 토크나이저를 로드하고, 토크나이저로 두 문장을 정수 인코딩했다.</li><li>input_ids는 정수로 변환된 토큰 시퀀스이다.<ul><li>여기서 101과 102는 특별 토큰인 [CLS] 토큰과 [SEP] 토큰이다.</li><li>정수 인코딩 결과를 다시 디코딩해서 현재 입력의 구성을 확인해보면 BERT에서 두 개의 문장이 입력으로 들어갈 경우에 맨 앞에는 [CLS] 토큰, 문장이 끝나면 [SEP] 토큰, 두번째 문장이 종료되었을 때 다시 [SEP] 토큰이 추가된다</li></ul></li><li>token_type_ids는 두 문장을 구분하기 위한 세그먼트 인코딩이다.<ul><li>첫 번째 문장은 0, 두 번째 문장은 1.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(probs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;최종 예측 레이블 :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tf.Tensor([[9.9999714e-01 2.8381860e-06]], shape=(1, 2), dtype=float32)
</span></span><span style=display:flex><span>최종 예측 레이블 : [0]
</span></span></code></pre></div><ul><li>다음 문장 예측하기<ol><li>BERT 모델에 입력 데이터를 넣어 logits(예측 점수)를 반환</li><li>소프트맥스를 적용해 각 레이블(0 또는 1)에 대한 확률 계산.</li></ol></li><li>예측 결과<ul><li>이어지는 문장일 확률(레이블 0): 99.9997%</li><li>이어지지 않는 문장일 확률(레이블 1) 0.00028%</li><li>최종 예측은 레이블 0으로써 두 문장이 이어진다고 판단함.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 상관없는 두 개의 문장</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;The sky is blue due to the shorter wavelength of blue light.&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;최종 예측 레이블 :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>최종 예측 레이블 : [1]
</span></span></code></pre></div><ul><li>이어지지 않는 두 개의 문장으로 테스트</li><li>예측 결과: 이어지지 않는다고 판단.</li></ul><blockquote><p><strong>자료 출처</strong></p><p><a href=https://wikidocs.net/156767>https://wikidocs.net/156767</a></p></blockquote><hr><h2 id=17-06-한국어-bert의-다음-문장-예측>17-06 한국어 BERT의 다음 문장 예측
<a class=anchor href=#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertForNextSentencePrediction
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFBertForNextSentencePrediction<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;klue/bert-base&#39;</span>, from_pt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;klue/bert-base&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 이어지는 두 개의 문장</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;최종 예측 레이블 :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>최종 예측 레이블 : [0]
</span></span></code></pre></div><ul><li><p>모델과 토크나이저 로드</p><ul><li>TFBertForNextSentencePrediction.from_pretrained(&lsquo;BERT 모델 이름&rsquo;)을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드.</li><li>AutoTokenizer.from_pretrained(&lsquo;모델 이름&rsquo;)을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드.</li></ul></li><li><p>예측 결과: 두 문장이 이어진다고 판단.</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 상관없는 두 개의 문장</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.&#34;</span>
</span></span><span style=display:flex><span>next_sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;극장가서 로맨스 영화를 보고싶어요&#34;</span>
</span></span><span style=display:flex><span>encoding <span style=color:#f92672>=</span> tokenizer(prompt, next_sentence, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> model(encoding[<span style=color:#e6db74>&#39;input_ids&#39;</span>], token_type_ids<span style=color:#f92672>=</span>encoding[<span style=color:#e6db74>&#39;token_type_ids&#39;</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>softmax <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Softmax()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;최종 예측 레이블 :&#39;</span>, tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(probs, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>최종 예측 레이블 : [1]
</span></span></code></pre></div><ul><li>이어지지 않는 두 개의 문장으로 테스트</li><li>예측 결과: 이어지지 않는다고 판단.</li></ul><blockquote><p><strong>자료 출처</strong></p><p><a href=https://wikidocs.net/156774>https://wikidocs.net/156774</a></p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#목록>목록</a></li><li><a href=#17-02-버트bidirectional-encoder-representations-from-transformers-bert>17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)</a></li><li><a href=#17-03-구글-bert의-마스크드-언어-모델>17-03 구글 BERT의 마스크드 언어 모델</a></li><li><a href=#17-04-한국어-bert의-마스크드-언어-모델>17-04 한국어 BERT의 마스크드 언어 모델</a></li><li><a href=#17-05-구글-bert의-다음-문장-예측>17-05 구글 BERT의 다음 문장 예측</a></li><li><a href=#17-06-한국어-bert의-다음-문장-예측>17-06 한국어 BERT의 다음 문장 예측</a></li></ul></nav></div></aside></main></body></html>