<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  [딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT
  #


  목록
  #

2024-12-31 ⋯ 4.1 ALBERT
2024-12-31 ⋯ 4.3 RoBERTa
2024-12-31 ⋯ 4.4 ELECTRA


  4.1 ALBERT
  #


ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.

  크로스 레이어 변수 공유
  #


BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다.
ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.


  펙토라이즈 임베딩 변수화
  #


BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.
ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션).

행렬1: vocab_size x embedding_size
행렬2: embedding_size x hidden_size




  문장 순서 예측
  #



BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/bioinformatics/cs17/"><meta property="og:site_name" content=" "><meta property="og:title" content="구글 BERT의 정석 | BERT의 파생 모델"><meta property="og:description" content="[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 2024-12-31 ⋯ 4.1 ALBERT
2024-12-31 ⋯ 4.3 RoBERTa
2024-12-31 ⋯ 4.4 ELECTRA
4.1 ALBERT # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.
크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다. 펙토라이즈 임베딩 변수화 # BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다. ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션). 행렬1: vocab_size x embedding_size 행렬2: embedding_size x hidden_size 문장 순서 예측 # BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-12-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-31T00:00:00+00:00"><meta property="article:tag" content="2024-12"><title>구글 BERT의 정석 | BERT의 파생 모델 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/bioinformatics/cs17/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.469ad498b78163b8c3f1977ea7d41c895353aa3033059c28d321cfa8daacc65d.js integrity="sha256-RprUmLeBY7jD8Zd+p9QciVNTqjAzBZwo0yHPqNqsxl0=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/shopping/>쇼핑</a><ul></ul></li><li><a href=/docs/hobby/youtube/>유튜브</a><ul></ul></li><li><a href=/docs/hobby/music/>음악</a><ul></ul></li><li><a href=/docs/hobby/baking/>베이킹</a><ul></ul></li><li><a href=/docs/hobby/movie/>영화</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/sw/>SW</a><ul></ul></li><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/algorithm/>코테</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li><li><a href=/docs/study/github/>깃허브</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>구글 BERT의 정석 | BERT의 파생 모델</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#목록>목록</a></li><li><a href=#41-albert>4.1 ALBERT</a><ul><li><a href=#크로스-레이어-변수-공유>크로스 레이어 변수 공유</a></li><li><a href=#펙토라이즈-임베딩-변수화>펙토라이즈 임베딩 변수화</a></li><li><a href=#문장-순서-예측>문장 순서 예측</a></li><li><a href=#albert와-bert-비교>ALBERT와 BERT 비교</a></li><li><a href=#albert에서-임베딩-추출>ALBERT에서 임베딩 추출</a></li></ul></li><li><a href=#43-roberta>4.3 RoBERTa</a><ul><li><a href=#정적-마스크-대신-동적-마스크-사용>정적 마스크 대신 동적 마스크 사용</a></li><li><a href=#nsp-테스크-제거>NSP 테스크 제거</a></li><li><a href=#더-많은-데이터로-학습>더 많은 데이터로 학습</a></li><li><a href=#큰-배치-크기로-학습>큰 배치 크기로 학습</a></li><li><a href=#bbpe-토크나이저-사용>BBPE 토크나이저 사용</a></li></ul></li><li><a href=#44-electra>4.4 ELECTRA</a><ul><li><a href=#교체한-토큰-판별-테스크>교체한 토큰 판별 테스크</a></li><li><a href=#electra의-생성자와-판별자>ELECTRA의 생성자와 판별자</a></li><li><a href=#electra-모델-학습>ELECTRA 모델 학습</a></li><li><a href=#효율적인-학습-방법-탐색>효율적인 학습 방법 탐색</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=딥러닝-구글-bert의-정석--bert의-파생-모델-albert-roberta-electra-spanbert>[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT
<a class=anchor href=#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%ec%a0%95%ec%84%9d--bert%ec%9d%98-%ed%8c%8c%ec%83%9d-%eb%aa%a8%eb%8d%b8-albert-roberta-electra-spanbert>#</a></h1><h2 id=목록>목록
<a class=anchor href=#%eb%aa%a9%eb%a1%9d>#</a></h2><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs17/#41-albert>4.1 ALBERT</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs17/#43-roberta>4.3 RoBERTa</a></p><p><em>2024-12-31</em> ⋯ <a href=https://yshghid.github.io/docs/study/cs/cs17/#44-electra>4.4 ELECTRA</a></p><hr><h2 id=41-albert>4.1 ALBERT
<a class=anchor href=#41-albert>#</a></h2><blockquote><p>ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.</p></blockquote><h3 id=크로스-레이어-변수-공유>크로스 레이어 변수 공유
<a class=anchor href=#%ed%81%ac%eb%a1%9c%ec%8a%a4-%eb%a0%88%ec%9d%b4%ec%96%b4-%eb%b3%80%ec%88%98-%ea%b3%b5%ec%9c%a0>#</a></h3><ul><li>BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다.</li><li>ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.</li></ul><h3 id=펙토라이즈-임베딩-변수화>펙토라이즈 임베딩 변수화
<a class=anchor href=#%ed%8e%99%ed%86%a0%eb%9d%bc%ec%9d%b4%ec%a6%88-%ec%9e%84%eb%b2%a0%eb%94%a9-%eb%b3%80%ec%88%98%ed%99%94>#</a></h3><ul><li>BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.</li><li>ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션).<ul><li>행렬1: vocab_size x embedding_size</li><li>행렬2: embedding_size x hidden_size</li></ul></li></ul><h3 id=문장-순서-예측>문장 순서 예측
<a class=anchor href=#%eb%ac%b8%ec%9e%a5-%ec%88%9c%ec%84%9c-%ec%98%88%ec%b8%a1>#</a></h3><ul><li><p>BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용.</p><ul><li>NSP은 두 문장이 연속적으로 존재하는지를 예측하는 태스크. 문장 간 관계를 학습하는 데 사용됨.</li></ul></li><li><p>ALBERT는 문장 순서 예측 (SOP, Sentence Order Prediction) 라는 새로운 학습 태스크를 도입.</p><ul><li>두 문장이 주어졌을 때, 두 문장의 순서가 올바른지를 예측함.</li></ul></li><li><p>SOP는 문장 간의 순서 관계를 이해하는 데 NSP보다 적합하다.</p></li></ul><h3 id=albert와-bert-비교>ALBERT와 BERT 비교
<a class=anchor href=#albert%ec%99%80-bert-%eb%b9%84%ea%b5%90>#</a></h3><ol><li>크로스 레이어 변수 공유: ALBERT는 여러 레이어에서 파라미터를 공유하여 파라미터 수를 크게 줄임. BERT는 각 레이어마다 독립적인 파라미터 집합을 사용.</li><li>펙토라이즈 임베딩: ALBERT는 임베딩 행렬을 분해하여 파라미터 수를 줄임. BERT는 한 번에 큰 임베딩 행렬을 사용.</li><li>문장 순서 예측 (SOP): ALBERT는 NSP 대신 SOP를 사용하여 문장 순서를 더 잘 예측할 수 있게 하여, 문장 간 관계 학습을 개선.</li></ol><h3 id=albert에서-임베딩-추출>ALBERT에서 임베딩 추출
<a class=anchor href=#albert%ec%97%90%ec%84%9c-%ec%9e%84%eb%b2%a0%eb%94%a9-%ec%b6%94%ec%b6%9c>#</a></h3><ol><li><p>단어 임베딩</p><ul><li>입력 텍스트의 각 단어를 vocab_size x embedding_size 크기의 행렬을 사용하여 고차원 벡터로 변환함. 이 벡터는 각 단어의 의미를 반영하는 고차원적인 특징을 갖고있다.</li></ul></li><li><p>레벨 별 임베딩 추출 (Layer-wise Embedding Extraction)</p><ul><li>입력 텍스트가 Transformer 모델을 통과하면서 각 레이어에서 벡터 표현이 점진적으로 변환된다.</li><li>ALBERT에서는 주로 첫 번째 레이어 또는 최종 레이어에서 추출된 임베딩을 사용할 수 있음.</li><li>첫 번째 레이어에는 주로 단어의 기본적인 의미와 구조적 특징 정보.</li><li>최종 레이어에는 문장 전체의 복합적인 의미와 문맥이 결합되어, 더 구체화되고 세부적인 정보.</li></ul></li><li><p>중간 레이어에서의 임베딩 추출</p></li></ol><ul><li>ALBERT는 다중 레이어 구조를 갖기 때문에, 중간 레이어의 출력도 사용할 수 있음.</li><li>문장 내 특정 단어의 문맥을 더 잘 반영하는 중간 레이어의 임베딩을 추출할 수 있다.</li><li>사용자가 수행하는 작업에 따라, 특정 작업에 적합한 레이어의 출력을 선택.<ul><li>예를 들어, 문장 분류 작업에서는 모델의 최종 레이어에서 추출된 임베딩이 더 중요할 수 있으며, 개체명 인식(NER) 작업에서는 중간 레이어에서 나온 임베딩이 더 유용할 수 있다.</li></ul></li></ul><hr><h2 id=43-roberta>4.3 RoBERTa
<a class=anchor href=#43-roberta>#</a></h2><h3 id=정적-마스크-대신-동적-마스크-사용>정적 마스크 대신 동적 마스크 사용
<a class=anchor href=#%ec%a0%95%ec%a0%81-%eb%a7%88%ec%8a%a4%ed%81%ac-%eb%8c%80%ec%8b%a0-%eb%8f%99%ec%a0%81-%eb%a7%88%ec%8a%a4%ed%81%ac-%ec%82%ac%ec%9a%a9>#</a></h3><ul><li>BERT는 정적 마스크 (Static Masking) 방식을 사용하여 훈련함. 훈련 데이터에서 마스킹할 단어를 고르고, 그 마스크를 모든 훈련 단계에서 동일하게 유지한다. 즉 같은 단어가 훈련 내내 계속 마스크된다.</li><li>RoBERTa는 동적 마스크 (Dynamic Masking) 방식을 사용. 즉, 각 훈련 배치마다 문장에서 마스크되는 단어가 랜덤하게 변경된다.</li><li>정적 마스크에서는 동일한 문맥을 반복해서 학습하므로, 모델이 특정 단어의 패턴을 암기할 수 있는데, 동적 마스크에서는 훈련마다 마스크가 달라져 모델이 더 다양한 방식으로 문맥을 학습할 수 있도록 돕고, 일관된 마스크 패턴에 의한 편향을 줄여 모델이 더 일반화된 특징을 학습할 수 있게 해준다.</li></ul><h3 id=nsp-테스크-제거>NSP 테스크 제거
<a class=anchor href=#nsp-%ed%85%8c%ec%8a%a4%ed%81%ac-%ec%a0%9c%ea%b1%b0>#</a></h3><ul><li>BERT 모델은 훈련 과정에서 MLM, NSP 테스크를 사용한다.</li><li>RoBERTa는 NSP 대신 MLM만을 사용하여 훈련을 진행. NSP 제거의 이유는 문장 간의 관계 학습에 NSP가 크게 기여하지 않으며 제거 시 훈련이 더 간단해지고, 모델이 더욱 집중해서 문맥을 학습할 수 있음.</li></ul><h3 id=더-많은-데이터로-학습>더 많은 데이터로 학습
<a class=anchor href=#%eb%8d%94-%eb%a7%8e%ec%9d%80-%eb%8d%b0%ec%9d%b4%ed%84%b0%eb%a1%9c-%ed%95%99%ec%8a%b5>#</a></h3><ul><li>RoBERTa는 BERT보다 훨씬 더 많은 데이터로 훈련. BERT는 16GB 크기의 BooksCorpus와 English Wikipedia로 훈련되었지만, RoBERTa는 여기에 추가로 Common Crawl 데이터, CC-News, OpenWebText, Stories 등의 더 많은 데이터를 포함하여 훈련됨.</li></ul><h3 id=큰-배치-크기로-학습>큰 배치 크기로 학습
<a class=anchor href=#%ed%81%b0-%eb%b0%b0%ec%b9%98-%ed%81%ac%ea%b8%b0%eb%a1%9c-%ed%95%99%ec%8a%b5>#</a></h3><ul><li>RoBERTa는 훈련에 더 큰 배치 크기를 사용합니다. BERT는 일반적으로 배치 크기를 32 또는 64로 설정하여 훈련하지만, RoBERTa는 배치 크기 8,000까지 사용하여 훈련했습니다.</li></ul><blockquote><p><strong>큰 배치 크기?</strong></p><ul><li>배치가 크면 모델이 더 많은 데이터를 한 번에 처리할 수 있게 해주고, 훈련 속도를 높이는 데 기여함.</li><li>학습 안정성을 높여, 학습 과정에서 발생할 수 있는 불안정한 그래디언트 문제를 완화하는 데 도움을 줌.</li></ul></blockquote><h3 id=bbpe-토크나이저-사용>BBPE 토크나이저 사용
<a class=anchor href=#bbpe-%ed%86%a0%ed%81%ac%eb%82%98%ec%9d%b4%ec%a0%80-%ec%82%ac%ec%9a%a9>#</a></h3><ul><li>BERT는 WordPiece 토크나이저를 사용하여 텍스트를 서브워드 단위로 분할.</li><li>RoBERTa는 BBPE (Byte Pair Encoding) 토크나이저를 사용. 단어를 자주 발생하는 문자쌍으로 분할하여 서브워드 토큰을 만든다.<ul><li>이는 드문 단어나 외래어가 포함된 텍스트에서 더욱 효과적임.</li></ul></li><li>BBPE는 단어를 더 작은 조각으로 나누고, 이를 더 자주 사용되는 문자쌍으로 합치는 방식으로 작동함.</li><li>효과: 어휘 집합 크기를 줄이면서도 다양한 단어를 처리할 수 있게 해주며, 모델의 효율성을 높이고, 모든 언어에서 유연한 처리가 가능.</li></ul><hr><h2 id=44-electra>4.4 ELECTRA
<a class=anchor href=#44-electra>#</a></h2><h3 id=교체한-토큰-판별-테스크>교체한 토큰 판별 테스크
<a class=anchor href=#%ea%b5%90%ec%b2%b4%ed%95%9c-%ed%86%a0%ed%81%b0-%ed%8c%90%eb%b3%84-%ed%85%8c%ec%8a%a4%ed%81%ac>#</a></h3><ul><li>BERT와 같은 기존 모델들은 일부 단어를 마스킹하고 예측하는 방식(Masked Language Modeling, MLM)을 사용해서 모델을 학습.</li><li>이 방식은 마스크된 단어의 예측이 실제 문맥을 잘 반영하지 않게 될 수 있다는 단점이 있다.</li><li>ELECTRA는 교체한 토큰 판별 테스크 (Replaced Token Detection) 를 사용.</li><li>이 방식은 문장을 구성하는 각 토큰이 원래의 문장에서 그대로 있었는지 아니면 다른 토큰으로 교체되었는지를 구분하는 문제이며 이렇게 하면 모델은 교체된 단어를 구별하는 법을 배운다.</li></ul><h3 id=electra의-생성자와-판별자>ELECTRA의 생성자와 판별자
<a class=anchor href=#electra%ec%9d%98-%ec%83%9d%ec%84%b1%ec%9e%90%ec%99%80-%ed%8c%90%eb%b3%84%ec%9e%90>#</a></h3><ul><li>ELECTRA는 두 가지 모델로 구성된다.</li></ul><ol><li><p>생성자 (Generator)</p><ul><li>기존 BERT와 같은 Masked Language Model (MLM) 구조.</li><li>입력 문장에서 일부 단어를 [MASK]로 변환한 후, 이를 생성자의 예측 값으로 대체함.</li></ul></li><li><p>판별자 (Discriminator)</p><ul><li>문장 내 각 토큰이 진짜인지(fake) 가짜인지(real)를 분류하는 이진 분류(Binary Classification) 문제를 해결.</li></ul></li></ol><h3 id=electra-모델-학습>ELECTRA 모델 학습
<a class=anchor href=#electra-%eb%aa%a8%eb%8d%b8-%ed%95%99%ec%8a%b5>#</a></h3><ol><li><p>생성자 학습</p><ul><li>문장에서 일부 단어를 마스킹한 후, 생성자가 그 단어를 예측.</li><li>예측된 단어는 원래 단어 대신 교체된 단어(replaced token)로 사용됨.</li></ul></li><li><p>판별자 학습</p><ul><li>생성자가 만든 교체된 단어를 포함한 문장을 입력받음.</li><li>판별자는 문장 내 각 단어가 원래 단어인지, 교체된 단어인지 판별하는 작업을 수행.</li><li>판별자가 더 정확한 예측을 할수록 모델의 언어 이해 능력이 향상됨.</li></ul></li><li><p>손실 함수 계산</p><ul><li>생성자는 Cross-Entropy Loss (MLM 방식)</li><li>판별자는 Binary Classification Loss (Replaced Token Detection 방식)</li></ul></li><li><p>반복 학습</p><ul><li>생성자의 성능이 향상될수록 판별자의 분류 작업이 더 어려워짐.</li><li>결국 판별자가 더 정교한 문맥 이해 능력을 갖도록 최적화됨.</li></ul></li></ol><h3 id=효율적인-학습-방법-탐색>효율적인 학습 방법 탐색
<a class=anchor href=#%ed%9a%a8%ec%9c%a8%ec%a0%81%ec%9d%b8-%ed%95%99%ec%8a%b5-%eb%b0%a9%eb%b2%95-%ed%83%90%ec%83%89>#</a></h3><blockquote><p>ELECTRA 모델을 효율적으로 학습시키기 위해서 생성자와 판별자의 가중치를 공유한다.</p></blockquote><ul><li><p>기존 BERT는 마스킹된 토큰만 학습에 사용하지만, ELECTRA는 모든 토큰을 판별 작업에 사용하여 훨씬 더 높은 학습 데이터 활용률을 가짐.</p></li><li><p>기존의 MLM 방식보다 80% 적은 연산량으로 동일한 성능을 유지, 동일한 연산량을 사용했을 때 BERT보다 2~4배 더 빠르게 학습 가능.</p></li><li><p>생성자는 BERT와 같은 크기를 사용할 필요가 없어서, 생성자를 작은 크기의 모델로 설정하여 연산량을 절감.</p></li><li><p>ELECTRA-Small (14M parameters) → BERT-Small보다 86% 더 높은 성능 / ELECTRA-Large는 BERT-Large보다 적은 연산량으로 더 높은 성능을 보임.</p></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#목록>목록</a></li><li><a href=#41-albert>4.1 ALBERT</a><ul><li><a href=#크로스-레이어-변수-공유>크로스 레이어 변수 공유</a></li><li><a href=#펙토라이즈-임베딩-변수화>펙토라이즈 임베딩 변수화</a></li><li><a href=#문장-순서-예측>문장 순서 예측</a></li><li><a href=#albert와-bert-비교>ALBERT와 BERT 비교</a></li><li><a href=#albert에서-임베딩-추출>ALBERT에서 임베딩 추출</a></li></ul></li><li><a href=#43-roberta>4.3 RoBERTa</a><ul><li><a href=#정적-마스크-대신-동적-마스크-사용>정적 마스크 대신 동적 마스크 사용</a></li><li><a href=#nsp-테스크-제거>NSP 테스크 제거</a></li><li><a href=#더-많은-데이터로-학습>더 많은 데이터로 학습</a></li><li><a href=#큰-배치-크기로-학습>큰 배치 크기로 학습</a></li><li><a href=#bbpe-토크나이저-사용>BBPE 토크나이저 사용</a></li></ul></li><li><a href=#44-electra>4.4 ELECTRA</a><ul><li><a href=#교체한-토큰-판별-테스크>교체한 토큰 판별 테스크</a></li><li><a href=#electra의-생성자와-판별자>ELECTRA의 생성자와 판별자</a></li><li><a href=#electra-모델-학습>ELECTRA 모델 학습</a></li><li><a href=#효율적인-학습-방법-탐색>효율적인 학습 방법 탐색</a></li></ul></li></ul></nav></div></aside></main></body></html>