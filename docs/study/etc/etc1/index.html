<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='
  #1 DBSCAN
  #

#2025-06-25


  개념
  #

DBSCAN은 밀도 기반 클러스터링 알고리즘으로

데이터가 밀집된 영역을 클러스터로 인식하고
밀도가 낮은 영역은 노이즈(이상치)로 간주하는 방법.

KMeans와 달리, 군집 수를 미리 정하지 않아도 되며,

비선형 구조나 잡음이 있는 데이터에서 잘 작동한다.


  파라미터와 핵심 용어
  #

주요 파라미터는 2개

eps: 반지름 거리. 한 점에서 eps 거리 내에 있는 점들을 &ldquo;이웃"이라고 판단.
min_samples: core point로 인정되기 위해 필요한 최소 이웃 수

핵심 용어는 3개'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/etc/etc1/"><meta property="og:site_name" content="Lifelog 2025"><meta property="og:title" content="#1 DBSCAN"><meta property="og:description" content='#1 DBSCAN # #2025-06-25
개념 # DBSCAN은 밀도 기반 클러스터링 알고리즘으로
데이터가 밀집된 영역을 클러스터로 인식하고 밀도가 낮은 영역은 노이즈(이상치)로 간주하는 방법. KMeans와 달리, 군집 수를 미리 정하지 않아도 되며,
비선형 구조나 잡음이 있는 데이터에서 잘 작동한다. 파라미터와 핵심 용어 # 주요 파라미터는 2개
eps: 반지름 거리. 한 점에서 eps 거리 내에 있는 점들을 “이웃"이라고 판단. min_samples: core point로 인정되기 위해 필요한 최소 이웃 수 핵심 용어는 3개'><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-06-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-25T00:00:00+00:00"><meta property="article:tag" content="2025-06"><title>#1 DBSCAN | Lifelog 2025</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/etc/etc1/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.638f780875a95184805390f20ab5d46973689c62ab93f9548de8fba33cfa9f26.js integrity="sha256-Y494CHWpUYSAU5DyCrXUaXNonGKrk/lUjej7ozz6nyY=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span>Lifelog 2025</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/etc/>기타</a><ul></ul></li><li><a href=/docs/hobby/favorite/>🤍</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/tech/>연구실</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li><li><a href=/docs/study/etc/>기타</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>#1 DBSCAN</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#개념>개념</a></li><li><a href=#파라미터와-핵심-용어>파라미터와 핵심 용어</a></li><li><a href=#장점과-단점>장점과 단점</a></li><li><a href=#qa>Q&amp;A</a></li><li><a href=#성능-평가>성능 평가</a></li><li><a href=#파이썬-구현---dbscan>파이썬 구현 - DBSCAN</a></li><li><a href=#파이썬-구현---k-distance-plot>파이썬 구현 - k distance plot</a></li><li><a href=#파이썬-구현---silhouette-score>파이썬 구현 - silhouette score</a></li><li><a href=#전체-파이프라인-실행>전체 파이프라인 실행</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=1-dbscan>#1 DBSCAN
<a class=anchor href=#1-dbscan>#</a></h1><p>#2025-06-25</p><hr><h2 id=개념>개념
<a class=anchor href=#%ea%b0%9c%eb%85%90>#</a></h2><p>DBSCAN은 밀도 기반 클러스터링 알고리즘으로</p><ul><li>데이터가 밀집된 영역을 클러스터로 인식하고</li><li>밀도가 낮은 영역은 노이즈(이상치)로 간주하는 방법.</li></ul><p>KMeans와 달리, 군집 수를 미리 정하지 않아도 되며,</p><ul><li>비선형 구조나 잡음이 있는 데이터에서 잘 작동한다.</li></ul><h2 id=파라미터와-핵심-용어>파라미터와 핵심 용어
<a class=anchor href=#%ed%8c%8c%eb%9d%bc%eb%af%b8%ed%84%b0%ec%99%80-%ed%95%b5%ec%8b%ac-%ec%9a%a9%ec%96%b4>#</a></h2><p>주요 파라미터는 2개</p><ul><li>eps: 반지름 거리. 한 점에서 eps 거리 내에 있는 점들을 &ldquo;이웃"이라고 판단.</li><li>min_samples: core point로 인정되기 위해 필요한 최소 이웃 수</li></ul><p>핵심 용어는 3개</p><ul><li>Core Point (중심점): eps 거리 내에 min_samples 이상 이웃이 있는 점</li><li>Border Point (경계점): core point의 eps 거리 내에 있으나, 자기 자신은 core point가 아닌 점</li><li>Noise Point (잡음점): 어떤 core point의 eps 안에도 포함되지 않는 점</li></ul><h2 id=장점과-단점>장점과 단점
<a class=anchor href=#%ec%9e%a5%ec%a0%90%ea%b3%bc-%eb%8b%a8%ec%a0%90>#</a></h2><p>장점 4개</p><ul><li>자동 군집수 결정</li><li>이상치 탐지 가능</li><li>복잡한 클러스터 형태 탐지</li><li>비지도 학습</li></ul><p>단점 3개</p><ul><li>eps 값 설정이 민감함</li><li>밀도가 다른 클러스터는 잘 분리 못함 (밀도 기준이 하나뿐이라 불균형 분포에 약함)</li><li>고차원 데이터에선 거리 개념이 희석되므로 차원 축소(t-SNE, PCA 등) 필요.</li></ul><h2 id=qa>Q&amp;A
<a class=anchor href=#qa>#</a></h2><p>Q1) DBSCAN은 몇차원에서 제일 효율적인가?</p><p>A1)</p><p>2(~3)차원에서 가장 효율적.</p><ul><li>거리 개념이 명확하고 시각화 가능</li><li>시각화 가능 -> 시각화 통해 군집 구조 확인 가능 -> eps 직관적으로 조정 가능</li></ul><p>4~10차원에서 점점 어려워짐.</p><ul><li>거리 분포가 평평해지고, core point 조건을 충족시키기 어려움</li><li>유클리드 거리 기반 eps 조정이 매우 민감해짐</li><li>차원 축소(PCA, t-SNE, UMAP) 후 사용 추천</li></ul><p>10차원 이상</p><ul><li>거리 희소성(dimensionality curse): 모든 점 간 거리가 비슷해져 밀도 기반 판별이 어려워짐</li><li>eps와 min_samples 조합이 성능에 큰 영향을 주며, 조정이 어렵고 불안정함</li><li>고차원에선 DBSCAN보다 HDBSCAN, Spectral Clustering, 또는 Spherical KMeans 등을 고려 / 또는 차원 축소를 선행한 후 DBSCAN 사용</li></ul><p>Q2) 파라미터 선택법?</p><p>A2)</p><ol><li>이론적 기준으로 min_samples=2*d를 적용해서 min_samples 후보값을 정함</li><li>k = min_samples-1로 설정하여 k-distance plot을 그림</li><li>elbow point을 찾아 eps를 결정</li><li>다양한 min_samples로 그래프를 여러 번 그려보고 -> 가장 뚜렷한 elbow point을 주는 min_samples를 선택</li></ol><h2 id=성능-평가>성능 평가
<a class=anchor href=#%ec%84%b1%eb%8a%a5-%ed%8f%89%ea%b0%80>#</a></h2><p>DBSCAN은 비지도 학습 알고리즘이기 때문에, 성능 평가에 있어서 supervised 방식과는 다른 접근이 필요</p><p>내부 평가 지표</p><ul><li>Silhouette Score<ul><li>각 점이 속한 클러스터 내부 응집도와, 가장 가까운 다른 클러스터와의 거리 차이를 비교</li><li>-1 ~ 1 (1: 잘 클러스터됨, 0: 경계에 있음)</li></ul></li><li>Davies-Bouldin Index<ul><li>클러스터 간 간격이 멀고, 내부 응집도가 높을수록 좋은 값</li><li>값이 작을수록 우수</li></ul></li><li>Calinski-Harabasz Index<ul><li>클러스터 간 분산 / 클러스터 내 분산 비율</li><li>값이 클수록 좋은 클러스터링</li></ul></li></ul><p>외부 평가 지표 (만약 정답 레이블이 있다면 다음 지표들도 사용 가능)</p><ul><li>Adjusted Rand Index (ARI): 무작위 군집과 비교하여 클러스터 일치 정도 확인 (1에 가까울수록 좋음)</li><li>Normalized Mutual Information (NMI): 군집 정보가 얼마나 label과 유사한지 확인</li><li>Fowlkes–Mallows index (FMI): TP 기준 군집 일치 정도</li></ul><p>시각화 기반 평가</p><ul><li>2D나 t-SNE로 클러스터링 결과 시각화해서, 클러스터 모양, 분리 정도 노이즈의 위치 분포 군집 수가 과도하지 않은지 등을 확인</li><li>1D 데이터에서는 사용 불가</li></ul><h2 id=파이썬-구현---dbscan>파이썬 구현 - DBSCAN
<a class=anchor href=#%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ea%b5%ac%ed%98%84---dbscan>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>euclidean_distance</span>(p1, p2):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> math<span style=color:#f92672>.</span>sqrt(sum((a <span style=color:#f92672>-</span> b) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#66d9ef>for</span> a, b <span style=color:#f92672>in</span> zip(p1, p2)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>region_query</span>(data, point_idx, eps):
</span></span><span style=display:flex><span>    neighbors <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx, point <span style=color:#f92672>in</span> enumerate(data):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> euclidean_distance(data[point_idx], point) <span style=color:#f92672>&lt;=</span> eps:
</span></span><span style=display:flex><span>            neighbors<span style=color:#f92672>.</span>append(idx)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> neighbors
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>expand_cluster</span>(data, labels, point_idx, neighbors, cluster_id, eps, min_samples):
</span></span><span style=display:flex><span>    labels[point_idx] <span style=color:#f92672>=</span> cluster_id
</span></span><span style=display:flex><span>    i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> i <span style=color:#f92672>&lt;</span> len(neighbors):
</span></span><span style=display:flex><span>        n_idx <span style=color:#f92672>=</span> neighbors[i]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> labels[n_idx] <span style=color:#f92672>==</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:  <span style=color:#75715e># noise → now becomes part of a cluster</span>
</span></span><span style=display:flex><span>            labels[n_idx] <span style=color:#f92672>=</span> cluster_id
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> labels[n_idx] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            labels[n_idx] <span style=color:#f92672>=</span> cluster_id
</span></span><span style=display:flex><span>            n_neighbors <span style=color:#f92672>=</span> region_query(data, n_idx, eps)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(n_neighbors) <span style=color:#f92672>&gt;=</span> min_samples:
</span></span><span style=display:flex><span>                neighbors <span style=color:#f92672>+=</span> n_neighbors
</span></span><span style=display:flex><span>        i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dbscan</span>(data, eps, min_samples):
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> len(data)  <span style=color:#75715e># 0 = unvisited, -1 = noise, ≥1 = cluster id</span>
</span></span><span style=display:flex><span>    cluster_id <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(len(data)):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> labels[idx] <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>  <span style=color:#75715e># 이미 방문한 점</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        neighbors <span style=color:#f92672>=</span> region_query(data, idx, eps)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(neighbors) <span style=color:#f92672>&lt;</span> min_samples:
</span></span><span style=display:flex><span>            labels[idx] <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>  <span style=color:#75715e># noise</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            cluster_id <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            expand_cluster(data, labels, idx, neighbors, cluster_id, eps, min_samples)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> labels
</span></span></code></pre></div><h2 id=파이썬-구현---k-distance-plot>파이썬 구현 - k distance plot
<a class=anchor href=#%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ea%b5%ac%ed%98%84---k-distance-plot>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>euclidean_distance</span>(p1, p2):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>sum((p1 <span style=color:#f92672>-</span> p2) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_k_distances</span>(X, k):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    각 포인트에 대해 k번째 최근접 이웃까지의 거리 계산
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - X: (n_samples, n_features) ndarray
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - k: 이웃의 수 (k = min_samples - 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - k_distances: 각 포인트의 k번째 최근접 이웃 거리 리스트
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>    k_distances <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>        distances <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> i <span style=color:#f92672>!=</span> j:
</span></span><span style=display:flex><span>                dist <span style=color:#f92672>=</span> euclidean_distance(X[i], X[j])
</span></span><span style=display:flex><span>                distances<span style=color:#f92672>.</span>append(dist)
</span></span><span style=display:flex><span>        distances<span style=color:#f92672>.</span>sort()
</span></span><span style=display:flex><span>        k_distances<span style=color:#f92672>.</span>append(distances[k <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>])  <span style=color:#75715e># k번째 작은 거리</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sort(k_distances)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_k_distance_manual</span>(X, k):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    sklearn 없이 k-distance plot 그리기
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - X: (n_samples, n_features) ndarray
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - k: int, 이웃 수 (= min_samples - 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    k_distances <span style=color:#f92672>=</span> compute_k_distances(X, k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>4</span>))
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(k_distances)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>-th nearest neighbor distance&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Points sorted by distance&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Manual k-distance plot (k=</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h2 id=파이썬-구현---silhouette-score>파이썬 구현 - silhouette score
<a class=anchor href=#%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ea%b5%ac%ed%98%84---silhouette-score>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>silhouette_score_manual</span>(X, labels):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Silhouette Score를 직접 계산하는 함수
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - X: (n_samples, n_features) ndarray
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - labels: (n_samples,) 클러스터 ID, 노이즈는 제외되어 있어야 함 (-1 제거 필수)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - 평균 Silhouette Score (float)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    unique_labels <span style=color:#f92672>=</span> set(labels)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(unique_labels) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;클러스터가 1개 이하입니다. Silhouette Score를 계산할 수 없습니다.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>    silhouette_values <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>        own_cluster <span style=color:#f92672>=</span> labels[i]
</span></span><span style=display:flex><span>        same_cluster_indices <span style=color:#f92672>=</span> [j <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(n_samples) <span style=color:#66d9ef>if</span> labels[j] <span style=color:#f92672>==</span> own_cluster <span style=color:#f92672>and</span> j <span style=color:#f92672>!=</span> i]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># a(i): 같은 클러스터 내 평균 거리</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> same_cluster_indices:
</span></span><span style=display:flex><span>            a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean([np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(X[i] <span style=color:#f92672>-</span> X[j]) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> same_cluster_indices])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            a <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>  <span style=color:#75715e># 고립된 점</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># b(i): 가장 가까운 다른 클러스터와의 평균 거리</span>
</span></span><span style=display:flex><span>        b <span style=color:#f92672>=</span> float(<span style=color:#e6db74>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> other_cluster <span style=color:#f92672>in</span> unique_labels:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> other_cluster <span style=color:#f92672>==</span> own_cluster:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            other_indices <span style=color:#f92672>=</span> [j <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(n_samples) <span style=color:#66d9ef>if</span> labels[j] <span style=color:#f92672>==</span> other_cluster]
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> other_indices:
</span></span><span style=display:flex><span>                b_dist <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean([np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(X[i] <span style=color:#f92672>-</span> X[j]) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> other_indices])
</span></span><span style=display:flex><span>                b <span style=color:#f92672>=</span> min(b, b_dist)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># s(i): silhouette score for point i</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> max(a, b) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            s <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            s <span style=color:#f92672>=</span> (b <span style=color:#f92672>-</span> a) <span style=color:#f92672>/</span> max(a, b)
</span></span><span style=display:flex><span>        silhouette_values<span style=color:#f92672>.</span>append(s)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(silhouette_values)
</span></span></code></pre></div><h2 id=전체-파이프라인-실행>전체 파이프라인 실행
<a class=anchor href=#%ec%a0%84%ec%b2%b4-%ed%8c%8c%ec%9d%b4%ed%94%84%eb%9d%bc%ec%9d%b8-%ec%8b%a4%ed%96%89>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 1. 데이터 생성</span>
</span></span><span style=display:flex><span>X, _ <span style=color:#f92672>=</span> make_moons(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>, noise<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. k-distance plot</span>
</span></span><span style=display:flex><span>min_samples <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>plot_k_distance_manual(X, k<span style=color:#f92672>=</span>min_samples <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. 클러스터링 (여기선 elbow 보고 eps=0.125 정도 선택)</span>
</span></span><span style=display:flex><span>eps <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.15</span>
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> dbscan(X, eps<span style=color:#f92672>=</span>eps, min_samples<span style=color:#f92672>=</span>min_samples)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 4. 클러스터 시각화</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X[:, <span style=color:#ae81ff>0</span>], X[:, <span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>labels, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tab10&#39;</span>, s<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;DBSCAN Clustering (eps=</span><span style=color:#e6db74>{</span>eps<span style=color:#e6db74>}</span><span style=color:#e6db74>, min_samples=</span><span style=color:#e6db74>{</span>min_samples<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;X1&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;X2&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 5. Silhouette Score 계산</span>
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> silhouette_score_manual(X, labels)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Silhouette Score: </span><span style=color:#e6db74>{</span>score<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/0dacd3df-037c-40a1-84f8-7ea8012e61fa alt=image>
<img src=https://github.com/user-attachments/assets/aed95c7b-432d-4299-bd50-052775a34760 alt=image></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Silhouette Score: 0.3327
</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#개념>개념</a></li><li><a href=#파라미터와-핵심-용어>파라미터와 핵심 용어</a></li><li><a href=#장점과-단점>장점과 단점</a></li><li><a href=#qa>Q&amp;A</a></li><li><a href=#성능-평가>성능 평가</a></li><li><a href=#파이썬-구현---dbscan>파이썬 구현 - DBSCAN</a></li><li><a href=#파이썬-구현---k-distance-plot>파이썬 구현 - k distance plot</a></li><li><a href=#파이썬-구현---silhouette-score>파이썬 구현 - silhouette score</a></li><li><a href=#전체-파이프라인-실행>전체 파이프라인 실행</a></li></ul></nav></div></aside></main></body></html>