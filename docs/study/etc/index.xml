<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>etc on</title><link>https://yshghid.github.io/docs/study/etc/</link><description>Recent content in etc on</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 26 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://yshghid.github.io/docs/study/etc/index.xml" rel="self" type="application/rss+xml"/><item><title>#2 Explainable AI</title><link>https://yshghid.github.io/docs/study/etc/etc2/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/etc/etc2/</guid><description>&lt;h1 id="2-explainable-ai"&gt;
 #2 Explainable AI
 &lt;a class="anchor" href="#2-explainable-ai"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;#2025-06-26&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id="1-explainable-ai란"&gt;
 1. Explainable AI란?
 &lt;a class="anchor" href="#1-explainable-ai%eb%9e%80"&gt;#&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Explainable AI는 인공지능(AI) 또는 머신러닝(ML) 모델이 어떤 방식으로 특정 결과를 도출했는지 사람이 이해할 수 있도록 설명하는 기술과 방법론.&lt;/p&gt;
&lt;h3 id="2-xai-기법-분류"&gt;
 2. XAI 기법 분류
 &lt;a class="anchor" href="#2-xai-%ea%b8%b0%eb%b2%95-%eb%b6%84%eb%a5%98"&gt;#&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;모델 구조&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intrinsic:	모델 자체가 설명 가능한 구조 (예: 의사결정나무, 선형회귀 등)&lt;/li&gt;
&lt;li&gt;Post-hoc:	모델 학습 후 별도로 설명 생성 (예: SHAP, LIME)
대상&lt;/li&gt;
&lt;li&gt;Global:	전체 모델의 작동 원리를 설명&lt;/li&gt;
&lt;li&gt;Local:	특정 샘플의 예측 결과를 설명&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3-주요-post-hoc-설명-기법"&gt;
 3. 주요 Post-hoc 설명 기법
 &lt;a class="anchor" href="#3-%ec%a3%bc%ec%9a%94-post-hoc-%ec%84%a4%eb%aa%85-%ea%b8%b0%eb%b2%95"&gt;#&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;LIME (Local Interpretable Model-Agnostic Explanations): 주변 입력을 랜덤하게 생성하고, 단순 모델(선형 회귀 등)을 학습해 근사&lt;/p&gt;</description></item><item><title>#3 Random Forest</title><link>https://yshghid.github.io/docs/study/etc/etc3/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/etc/etc3/</guid><description>&lt;h1 id="3-random-forest"&gt;
 #3 Random Forest
 &lt;a class="anchor" href="#3-random-forest"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;#2025-06-26&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id="1-random-forest의-분류와-회귀"&gt;
 1. Random Forest의 분류와 회귀
 &lt;a class="anchor" href="#1-random-forest%ec%9d%98-%eb%b6%84%eb%a5%98%ec%99%80-%ed%9a%8c%ea%b7%80"&gt;#&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;랜덤 포레스트(Random Forest)는&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RandomForestClassifier: 분류용&lt;/li&gt;
&lt;li&gt;RandomForestRegressor: 회귀용 이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;분류와 회귀의 핵심 차이는&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;분류는 각 leaf node에 속한 클래스의 비율을 기반으로 확률 예측&lt;/li&gt;
&lt;li&gt;회귀는 leaf node에 있는 target 값들의 평균을 예측값으로 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;랜덤 포레스트의 트리 구조(= 리프 분기 방식)는 분류나 회귀나 똑같고&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;단지 리프 노드에 어떤 데이터 형식이 들어가느냐에 따라
&lt;ul&gt;
&lt;li&gt;분류이면 라벨 비율(확률 분포)&lt;/li&gt;
&lt;li&gt;회귀이면 값의 평균으로 예측을 내놓는다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2-트리-기반-모델과-클러스터링의-차이"&gt;
 2. 트리 기반 모델과 클러스터링의 차이
 &lt;a class="anchor" href="#2-%ed%8a%b8%eb%a6%ac-%ea%b8%b0%eb%b0%98-%eb%aa%a8%eb%8d%b8%ea%b3%bc-%ed%81%b4%eb%9f%ac%ec%8a%a4%ed%84%b0%eb%a7%81%ec%9d%98-%ec%b0%a8%ec%9d%b4"&gt;#&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;랜덤 포레스트(혹은 결정 트리)의 리프 분기 방식은 &amp;lsquo;거리 기반&amp;rsquo;이 아님&lt;/p&gt;</description></item><item><title>#1 DBSCAN</title><link>https://yshghid.github.io/docs/study/etc/etc1/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/etc/etc1/</guid><description>&lt;h1 id="1-dbscan"&gt;
 #1 DBSCAN
 &lt;a class="anchor" href="#1-dbscan"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;#2025-06-25&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="개념"&gt;
 개념
 &lt;a class="anchor" href="#%ea%b0%9c%eb%85%90"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;DBSCAN은 밀도 기반 클러스터링 알고리즘으로&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;데이터가 밀집된 영역을 클러스터로 인식하고&lt;/li&gt;
&lt;li&gt;밀도가 낮은 영역은 노이즈(이상치)로 간주하는 방법.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KMeans와 달리, 군집 수를 미리 정하지 않아도 되며,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;비선형 구조나 잡음이 있는 데이터에서 잘 작동한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="파라미터와-핵심-용어"&gt;
 파라미터와 핵심 용어
 &lt;a class="anchor" href="#%ed%8c%8c%eb%9d%bc%eb%af%b8%ed%84%b0%ec%99%80-%ed%95%b5%ec%8b%ac-%ec%9a%a9%ec%96%b4"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;주요 파라미터는 2개&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;eps: 반지름 거리. 한 점에서 eps 거리 내에 있는 점들을 &amp;ldquo;이웃&amp;quot;이라고 판단.&lt;/li&gt;
&lt;li&gt;min_samples: core point로 인정되기 위해 필요한 최소 이웃 수&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;핵심 용어는 3개&lt;/p&gt;</description></item></channel></rss>