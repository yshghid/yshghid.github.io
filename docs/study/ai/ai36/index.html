<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  AI #1 ML 방법론 기초
  #

#2025-09-19

#1 ML 방법론

통계기반 방법론은?

linear regression이나 logistic regression 같은걸 말함
가설과 근거가 명확히 세워져 있고
데이터가 알고리즘에 맞게 정제돼있고
통계적 유의성으로 결과가 나오는 깔끔한 방식


ML 방법론은?

작은 경연을 열듯 시행착오를 거치며 가장 적합한 모델을 찾는다는 컨셉이다.




  
  #

#2 지도 비지도 준지도

모두 입력 데이터에 존재하는 구조를 추론함
준지도

이상 탐지: 처럼 라벨링 비용이 클때
딥러닝: 은 파라미터 수가 많아 안정적인 학습을 위해 충분한 데이터가 필요한데

우선 라벨이 있는 데이터로 기본 학습을 진행하고 -> 라벨이 없는 데이터의 구조나 의사결정 경계를 활용해 모델을 보완함






  
  #

#3 regression, instance based algorithm"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/ai/ai36/"><meta property="og:site_name" content=" "><meta property="og:title" content="AI #1 ML 방법론 기초"><meta property="og:description" content="AI #1 ML 방법론 기초 # #2025-09-19
#1 ML 방법론
통계기반 방법론은? linear regression이나 logistic regression 같은걸 말함 가설과 근거가 명확히 세워져 있고 데이터가 알고리즘에 맞게 정제돼있고 통계적 유의성으로 결과가 나오는 깔끔한 방식 ML 방법론은? 작은 경연을 열듯 시행착오를 거치며 가장 적합한 모델을 찾는다는 컨셉이다. # #2 지도 비지도 준지도
모두 입력 데이터에 존재하는 구조를 추론함 준지도 이상 탐지: 처럼 라벨링 비용이 클때 딥러닝: 은 파라미터 수가 많아 안정적인 학습을 위해 충분한 데이터가 필요한데 우선 라벨이 있는 데이터로 기본 학습을 진행하고 -> 라벨이 없는 데이터의 구조나 의사결정 경계를 활용해 모델을 보완함 # #3 regression, instance based algorithm"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-09-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-19T00:00:00+00:00"><meta property="article:tag" content="2025-09"><title>AI #1 ML 방법론 기초 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/ai/ai36/><link rel=stylesheet href=/book.min.30a7836b6a89342da3b88e7afd1036166aeced16c8de12df060ded2031837886.css integrity="sha256-MKeDa2qJNC2juI56/RA2Fmrs7RbI3hLfBg3tIDGDeIY=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.0dfd47c08131a59b54e33e8119df6c89c4bfd7c71ef87eae3f24c2ee6d1b5f10.js integrity="sha256-Df1HwIExpZtU4z6BGd9sicS/18ce+H6uPyTC7m0bXxA=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/book/>글</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/sw/>SW</a><ul></ul></li><li><a href=/docs/study/algorithm/>알고리즘</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>AI #1 ML 방법론 기초</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=ai-1-ml-방법론-기초>AI #1 ML 방법론 기초
<a class=anchor href=#ai-1-ml-%eb%b0%a9%eb%b2%95%eb%a1%a0-%ea%b8%b0%ec%b4%88>#</a></h1><p>#2025-09-19</p><hr><p>#1 ML 방법론</p><ul><li>통계기반 방법론은?<ul><li>linear regression이나 logistic regression 같은걸 말함</li><li>가설과 근거가 명확히 세워져 있고</li><li>데이터가 알고리즘에 맞게 정제돼있고</li><li>통계적 유의성으로 결과가 나오는 깔끔한 방식</li></ul></li><li>ML 방법론은?<ul><li>작은 경연을 열듯 시행착오를 거치며 가장 적합한 모델을 찾는다는 컨셉이다.</li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#2 지도 비지도 준지도</p><ul><li>모두 입력 데이터에 존재하는 구조를 추론함</li><li>준지도<ul><li>이상 탐지: 처럼 라벨링 비용이 클때</li><li>딥러닝: 은 파라미터 수가 많아 안정적인 학습을 위해 충분한 데이터가 필요한데<ul><li>우선 라벨이 있는 데이터로 기본 학습을 진행하고 -> 라벨이 없는 데이터의 구조나 의사결정 경계를 활용해 모델을 보완함</li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#3 regression, instance based algorithm</p><ul><li>알고리즘을 일하는 방식의 유사성에 따라 묶으면 regression, instance based algorithm.</li><li>regression<ul><li>선형 회귀<ul><li>모델이 예측한 값과 실제 값의 오차를 측정하는 전형적인 선형 기반 방법이고</li></ul></li><li>로지스틱 회귀<ul><li>작은 선형 회귀들을 이어붙여 분류 문제를 푸는 방식.</li></ul></li></ul></li><li>instance based algorithm<ul><li>유사 사례를 구축해놓고 내가풀려는 케이스랑 유사한케이스를 찾아서 그걸기반으로 의사결정.</li><li>정석적인정의는?<ul><li>데이터 그 자체를 중요한 정보로 삼아 의사결정을 내리고</li><li>유사 사례를 저장해 두었다가 새로운 입력이 들어오면 가장 비슷한 사례를 찾아 예측에 활용.</li></ul></li><li>k-최근접 이웃(kNN)<ul><li>데이터 불균형 조정에도 활용</li></ul></li><li>Lazy Learning<ul><li>데이터만 잘 저장해 두면 학습이 끝난 것으로 볼 수 있지만 예측 시에는 거리 계산을 반복해야 하므로 데이터가 많을수록 연산이 무거워질 수 있다.</li><li>instance based algorithm의 대표는 kNN이고 느슨하게 해석하면 서포트 벡터만을 활용해 예측하는 SVM도 포함될 수 있다.<ul><li>다만 SVM은 정확히는 거리 기반이 아니라 커널 기반 모델이지만&mldr;</li><li>전체데이터를 다 바라보는게 아니라 소수의 인스턴스(서포트벡터)에 집중한다는 점에서 유사하게 분류되기도 한다.</li></ul></li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#4 불편 추정량 (p.11)</p><ul><li>안정적인 추정량을 얻으려면?<ul><li>주어진 데이터로 기울기와 절편을 예측할건데</li></ul><ol><li>기울기와 절편 추정치들의 분산이 작게 나와야한다.</li></ol><ul><li>이를 위해서는 분자는 작고 분모는 커야 하고</li><li>표본 수 n이 많을수록 1/(n-1)이 작아져 (기울기와 절편의) 분산이 줄어들어 더 안정적인 추정이 가능하다.</li></ul><ol start=2><li>입력 변수 x의 분산은 충분히 커야 예측력이 높아진다.</li></ol><ul><li>다만 x의 분산이 지나치게 넓으면 분류가 어렵고 반대로 값들이 한곳에 몰려 있으면 y를 구분하기 힘들다.</li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#5 linear 모델의 강건성</p><ul><li>강건성<ul><li>선형 모델을 선택하는이유?<ul><li>복잡한 모델에 비해 가정들을 잘 충족하며 다양한 상황에서 안정적으로 작동하기 때문에.</li></ul></li><li>복잡도를 높이면?<ul><li>학습 데이터에서는 성능이 향상되지만 실제 테스트 데이터에서는 어느 시점 이후 오히려 성능이 떨어지면서 오버피팅이 발생한다.</li><li>데이터 분포가 조금만 바뀌어도 성능이 무너질수있는데 단순한 모델은 이런 변화에도 비교적 강건하게 대응한다<ul><li>즉 조건이 바뀌거나 노이즈가 생기더라도 입력의 작은 변화가 출력에 크게 영향을 주지 않기 때문에 모델의 성과가 오래 유지된다.</li></ul></li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#6 knn (p.14)</p><ul><li>knn은 사람의 의사결정 방식에서 착안한 사례 기반 추론 알고리즘이다.<ul><li>새로운 사례가 등장했을 때 과거의 유사한 문제와 그 답을 참고해 판단을 내린다는 아이디어에 기반하는데</li><li>예를 들어 분류 문제에서.<ul><li>어떤 점이 별 모양인지 삼각형인지 결정하고 싶다고할때 그림에서 기준을 K=3으로 두면 가장 가까운 세 개 중 다수가 삼각형이므로 삼각형으로 분류되고 K=7로 두면 별이 더 많아져 별로 분류된다.</li></ul></li><li>회귀 문제에선<ul><li>입력 변수가 하나일 때 테스트 포인트가 주어지면 가장 가까운 K개의 값을 찾아 그 평균이 예측값이된다</li><li>K=3이라면 가까운 세 개의 y값을 평균내어 예측하고, K=1이라면 가장 가까운 하나의 값이 그대로 예측 결과가 된다<ul><li>즉 K가 클수록 추정은 부드럽지만 세밀함이 줄고 K가 작을수록 개별 사례의 영향을 크게 받아 예측이 민감해진다</li></ul></li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#7 knn에서 좋은 이웃?</p><ul><li>좋은 이웃을 어떻게 정할까?<ol><li>어떤 유사도를 측정할까</li></ol><ul><li>보통 데이터를 벡터로 변환한 뒤 거리 기반으로 유사도를 평가함<ul><li>이때 어떤 거리 메트릭을 쓸지?</li><li>기본적으로 많이 쓰이는 것은 유클리디언 거리. 맨해튼 거리도 있고 이를 일반화*한 것이 민코프스키 거리.</li><li>일반화?<ul><li>맨해튼은 두점사이 x축 y축 평행 거리. 유클리디언은 직선 거리.</li><li>민코프스키는 맨해튼, 유클리디언 둘다에 해당하는 공식. 차수 p를 어떻게 주느냐에 따라 다른 거리가 나오고 p=1이면 맨해튼 p=2이면 유클리디언 p=∞이면 체비셰프 거리.</li></ul></li></ul></li><li>cf)<ul><li>어떤 속성을 거리 계산에 포함할지 얼마나 반영할지는 전처리 단계에서 결정되고 중요하지 않은 변수를 제거하거나 가중치를 달리 부여해 조정할 수 있다.<ul><li>어떤 속성을 거리 계산에 포함할지(중요하지 않은 변수를 제거)<ul><li>KNN은 “학습으로 규칙을 만들어내는” 모델이 아니라 “그대로 두고 거리만 재서 판단하는” 모델이라서 예측의 성패가 모델 내부 파라미터가 아니라 우리가 미리 만들어 놓은 좌표계—즉 어떤 축들(특성)을 쓸지, 각 축을 얼마나 길게 혹은 짧게 잡을지—에 달려 있다.</li></ul></li><li>얼마나 반영할지<ul><li>특정 축을 스케일링해서 더 길거나 짧게 만드는 일.</li><li>중요한 특성에는 자를 늘려 그 방향 차이가 크게 반영되게 하고, 덜 중요한 특성에는 자를 줄여 그 차이가 작게 반영되게 만든다.</li><li>수식으로 보면 특성마다 계수(스케일)를 곱해 좌표를 변환한 뒤 민코프스키 같은 거리 공식을 적용하는 것과 같다. 좌표계를 바꾸면 같은 두 점이라도 거리가 달라지고, 거리가 달라지면 “가까운 이웃”의 순위가 바뀌고, 결국 예측이 달라진다.</li></ul></li></ul></li></ul></li></ul><ol start=2><li>가중치를 적용할것인가?</li></ol><ul><li>단순 다수결(voting)을 쓰면 모든 이웃을 똑같이 취급하지만 실제로는 가까운 이웃이 더 중요하다고 보고 거리 기반 가중치를 적용할 수 있다<ul><li>(가장 가까운 두세 개 이웃은 크게 반영하고, 나머지는 약하게 반영하는 식)</li><li>사이킷런 같은 라이브러리에서는 기본값이 uniform(모두 동일)이고 distance 옵션을 선택하면 거리에 반비례해 가중치를줄수있다.</li></ul></li><li>cf)<ul><li>거리 기반 말고 다른것도있나?<ul><li>“거리 기반”도 형태가 매우 다양하고, 랭크/커널/밀도/시간/클래스 비용 등 목적에 맞게 이웃의 표를 설계할 수 있다 데이터가 불균형·노이즈·개정 주기가 크다면 단순 distance기반보다 다른 전략들이 더 견고하게 먹히는 경우가 많다.</li><li>순위(랭크) 가중치: 거리값 대신 “가까운 순서”로만 가중치 부여. 예) 1등=1, 2등=1/2, 3등=1/3 …처럼 내림 가중.</li><li>커널 가중치: 가우시안, Epanechnikov, 삼각형 등 커널을 써서 부드럽게 감쇠. 수학적으론 거리 함수지만, 1/d 타입보다 훨씬 유연한 모양을 가짐.</li><li>클래스/코스트 가중치: 불균형 완화를 위해 희소 클래스 표에 더 큰 가중. 실전에서는 리샘플링(SMOTE/ENN/CNN 등)이나 사후 의사결정 임계값 조정과 함께 씀.</li><li>밀도/신뢰도 가중치: 이웃 점의 로컬 밀도(또는 LOF 같은 이상치 점수), 지역 정확도(leave-one-out 성능)로 신뢰 높은 이웃 표를 키우고, 의심스러운 이웃 표를 줄임.</li><li>시간 감쇠 가중치: 시계열·온라인 데이터에서 최신 사례에 더 큰 표를 주는 방식.</li><li>공유 최근접 이웃(SNN) 기반: 두 점이 “공유하는 이웃 수”로 유사도를 정의해 그 값으로 가중. 순수 거리 대신 그래프적 근접성을 씀</li></ul></li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#8 svm</p><ul><li>서포트벡터?<ul><li>svm의 핵심은 두 집단을 가장 크게 벌려 나누는 선형 경계를 찾는 것인데<ul><li>이때 경계에 가장 가까이 붙어 있는 점들이 ‘서포트 벡터’이고</li><li>마진을 최대로 하는 최적화 문제를 풀면 자연스럽게 어떤 점들이 서포트 벡터로 선택된다.</li><li>마진을 최대로 한다?<ul><li>“두 집단을 가르는 결정경계(직선/평면)를 중심으로, 양쪽 클래스가 비어 있는 완충지대(버퍼)를 가장 두껍게 만들자”.</li><li>완충지대의 두께가 마진인데 마진이 두꺼울수록 경계가 흔들려도(노이즈·분포 미세 변화) 오분류로 넘어가기 어렵기 때문에 일반화가 좋아진다.</li></ul></li></ul></li></ul></li><li>&ldquo;데이터 전체를 안 쓴다"의 의미?<ul><li>해의 형태가 서포트 벡터에만 의존하므로 모든 표본이 아닌 경계 부근의 소수 표본만이 결정에 실질적으로 기여한다는 의미.</li><li>덕분에 경계에서 멀리 떨어진 외곽 이상치의 영향은 상대적으로 작아 강건성이 생긴다.</li></ul></li><li>소프트 마진?<ul><li>현실 데이터의 노이즈를 허용하기 위해서 소프트 마진을 씀</li><li>위반 정도를 나타내는 슬랙 변수의 총합에 패널티를 주는 C를 함께 최소화한다. C를 크게 잡으면 위반에 대한 벌점이 커져 오류를 덜 허용하는 경계가, 작게 잡으면 더 너그러운 경계가 나온다.</li></ul></li><li>svm에서 하이퍼파라미터?<ul><li>하이퍼파라미터는 모델이 학습을 통해 스스로 조정하는 값(예: 회귀계수, 신경망의 가중치)과 달리 학습 전에 사람이 직접 정해줘야 하는 설정값.</li><li>svm에서 C는 &ldquo;오류를 얼마나 용인할것인지&rdquo;</li><li>결정 방법은?<ul><li>보통 validation set으로 성능을 비교하거나 교차검증을 돌리면서 가장좋은성능을주는값을 선택하거나</li><li>도메인 지식을 쓴다: 데이터가 매우 노이즈가 많다 하면 C를 크게 두는건 불리하니 오히려 작은 C가 적합할수있다</li></ul></li></ul></li><li>단점?<ul><li>기본적으로 이진 분류를 위한 알고리즘이어서 다중 클래스 문제를 다루기 위해서는 one-vs-one이나 one-vs-rest 같은 확장 방식을 사용해야하는데<ul><li>피쳐 수가 많거나 라벨 종류가 늘어나면 학습과 예측 속도가 급격히 떨어질 수 있다.</li></ul></li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#9 decision tree 기반 앙상블모델</p><ul><li>결정트리 기반 앙상블(random forest, gradient boosting)은 각 특성의 실제 값에 따라 분기를 만들어 규칙을 쌓아 가고<ul><li>분류에서는 불순도(지니·엔트로피)를 줄이고 회귀에서는 리프의 예측 오차(MSE·분산)를 줄이도록 학습한다.</li></ul></li><li>분류와 회귀의 이해<ul><li>회귀를 “선을 긋는 것”, 분류를 “가까운 것끼리 묶는 것”으로 단순화하기보다는</li><li>회귀는 수치 오차를 최소화하는 함수 추정, 분류는 손실(또는 불순도)을 최소화하는 경계 학습으로 이해하는 것이 정확하다.</li></ul></li><li>과적합을 막기<ul><li>svm<ul><li>svr(회귀) 과적합 제어는 ‘ε-무감도 손실(ε-insensitive)’과 C(위반 페널티)로 수행</li><li>svc(분류) 에서는 ε를 쓰지 않고 소프트 마진 + C로 마진 위반을 얼마나 허용할지 제어</li><li>C 커지면 복잡도 커져서 과적합 위험.</li></ul></li><li>트리 계열은 최대 깊이·리프 최소 표본 수·가지치기·학습률(부스팅)·트리 수(앙상블) 같은 복잡도 제어로 과적합을 막는다.</li><li>결론<ul><li>SVR은 ε와 C(그리고 커널 파라미터)로, SVC는 C(와 커널 파라미터)로 과적합을 조절하고 트리 계열은 깊이·노드 최소 표본·가지치기·샘플링·학습률·트리 수(및 조기 종료)로 모델 복잡도를 관리한다.</li></ul></li></ul></li><li>svm과 트리앙상블 비교 결론<ul><li>트리·앙상블은 값 기반 분기와 모델 복잡도 패널티로, SVM은 마진 최대화와 슬랙, C 조절로 강건성을 확보한다</li><li>둘 다 분류와 회귀에 쓸 수 있지만 과적합 제어의 수단과 최적화 목표가 다르다.</li></ul></li></ul><h3><a class=anchor href=#>#</a></h3><p>#10 불순도</p><ul><li>불순도가 낮다는 건 한 그룹 안에 같은 클래스가 많이 모여 있어 훨씬 명확하다는 뜻. 불순도가 높다는 건 한 그룹 안에 서로 다른 클래스가 많이 섞여 있어서 결과를 이해하기 어렵다는 뜻.</li><li>어떤 방식으로 데이터를 나눠야 불순도가 더 많이 줄어들까? 해보기.<ol><li>처음 데이터의 Gini 지수가 0.42라면?</li></ol><ul><li>꽤 섞여 있어서 완전히 깨끗하지 않은 상태.</li></ul><ol start=2><li>특성 A로 분할을 시도해 본다.</li></ol><ul><li>왼쪽 그룹과 오른쪽 그룹으로 나누고 나서 다시 각 그룹의 불순도를 계산했는데 -> 두 그룹이 완전히 한쪽 클래스만 포함하게 되어 Gini 지수가 0이됨<ul><li>원래 0.42였던 불순도가 0으로 줄었으니까 0.42만큼의 불순도가 줄어들었고 0.42만큼 정보를 얻었다.</li></ul></li></ul><ol start=3><li>특성 B로 나눠본다.</li></ol><ul><li>나누고 나니 -> 그룹 내부에 여전히 섞임이 남아 있고 Gini 지수가 0.342.</li><li>원래 0.42에서 0.342로 줄었으니 0.078만큼의 불순도가 줄어들었다</li></ul><ol start=4><li>결론</li></ol><ul><li>불순도를 줄인 양을 봤을때 즉 정보 이득을 봤을때 특성 A로 나누는것이 정보 이득이 훨씬 크다고 결론내려서 해당 노드에서 분할A를 선택한다</li></ul></li><li>결론<ul><li>부모는 섞여 있었는데 자식으로 갈수록 덜 섞여 있으면, 그만큼 정보를 더 알아낸 것.</li><li>분할을 통해 트리 성장 = 섞인 것을 덜 섞이게 만드는 방향으로 선을 긋고, 그 과정을 반복해서 더 순수한 그룹을 만드는 것.</li></ul></li></ul><h1><a class=anchor href=#>#</a></h1></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul></li></ul></nav></div></aside></main></body></html>