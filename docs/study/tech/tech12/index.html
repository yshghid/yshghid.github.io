<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼
  #

#2025-05-28

#introduction

ë°ì´í„°ì…‹: Kaggle - Stallion ë°ì´í„°ì…‹
ëª©ì : Temporal Fusion Transformer(TFT)ë¥¼ í™œìš©í•˜ì—¬ ìŒë£Œ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡

#install
$ nvidia-smi
Wed May 28 14:00:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A6000               Off | 00000000:3B:00.0 Off |                  Off |
| 30%   59C    P2             204W / 300W |   8339MiB / 49140MiB |     95%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A6000               Off | 00000000:5E:00.0 Off |                  Off |
| 30%   60C    P2             213W / 300W |   6897MiB / 49140MiB |     94%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A6000               Off | 00000000:B1:00.0 Off |                  Off |
| 30%   60C    P2             203W / 300W |   6799MiB / 49140MiB |     94%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A6000               Off | 00000000:D9:00.0 Off |                  Off |
| 32%   63C    P2             212W / 300W |   6885MiB / 49140MiB |     96%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     20199      C   ...dg/miniconda3/envs/woodg/bin/python      664MiB |
|    0   N/A  N/A    860801      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
|    0   N/A  N/A   1201205      C   ...u1098/anaconda3/envs/dna/bin/python     6198MiB |
|    0   N/A  N/A   1216286      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
|    0   N/A  N/A   1225349      C   python                                      782MiB |
|    1   N/A  N/A   1201206      C   ...u1098/anaconda3/envs/dna/bin/python     6104MiB |
|    1   N/A  N/A   1224607      C   python                                      782MiB |
|    2   N/A  N/A   1201207      C   ...u1098/anaconda3/envs/dna/bin/python     6006MiB |
|    2   N/A  N/A   1224848      C   python                                      782MiB |
|    3   N/A  N/A   1201208      C   ...u1098/anaconda3/envs/dna/bin/python     6092MiB |
|    3   N/A  N/A   1225121      C   python                                      782MiB |
+---------------------------------------------------------------------------------------+


NVIDIA ë“œë¼ì´ë²„ ë²„ì „: 545.23.08"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/tech/tech12/"><meta property="og:site_name" content="Lifelog 2025"><meta property="og:title" content="TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼"><meta property="og:description" content="TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼ # #2025-05-28
#introduction
ë°ì´í„°ì…‹: Kaggle - Stallion ë°ì´í„°ì…‹ ëª©ì : Temporal Fusion Transformer(TFT)ë¥¼ í™œìš©í•˜ì—¬ ìŒë£Œ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡ #install
$ nvidia-smi Wed May 28 14:00:07 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 545.23.08 Driver Version: 545.23.08 CUDA Version: 12.3 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA RTX A6000 Off | 00000000:3B:00.0 Off | Off | | 30% 59C P2 204W / 300W | 8339MiB / 49140MiB | 95% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA RTX A6000 Off | 00000000:5E:00.0 Off | Off | | 30% 60C P2 213W / 300W | 6897MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA RTX A6000 Off | 00000000:B1:00.0 Off | Off | | 30% 60C P2 203W / 300W | 6799MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 3 NVIDIA RTX A6000 Off | 00000000:D9:00.0 Off | Off | | 32% 63C P2 212W / 300W | 6885MiB / 49140MiB | 96% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 0 N/A N/A 20199 C ...dg/miniconda3/envs/woodg/bin/python 664MiB | | 0 N/A N/A 860801 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1201205 C ...u1098/anaconda3/envs/dna/bin/python 6198MiB | | 0 N/A N/A 1216286 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1225349 C python 782MiB | | 1 N/A N/A 1201206 C ...u1098/anaconda3/envs/dna/bin/python 6104MiB | | 1 N/A N/A 1224607 C python 782MiB | | 2 N/A N/A 1201207 C ...u1098/anaconda3/envs/dna/bin/python 6006MiB | | 2 N/A N/A 1224848 C python 782MiB | | 3 N/A N/A 1201208 C ...u1098/anaconda3/envs/dna/bin/python 6092MiB | | 3 N/A N/A 1225121 C python 782MiB | +---------------------------------------------------------------------------------------+ NVIDIA ë“œë¼ì´ë²„ ë²„ì „: 545.23.08"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-05-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-28T00:00:00+00:00"><meta property="article:tag" content="2025-05"><title>TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼ | Lifelog 2025</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/tech/tech12/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.4917d83dad1618f3cd9cf909429c645207f0cbfed875c50178d2713904866572.js integrity="sha256-SRfYPa0WGPPNnPkJQpxkUgfwy/7YdcUBeNJxOQSGZXI=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span>Lifelog 2025</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>ê¸°ë¡</span><ul><li><a href=/docs/hobby/book/>ì±…</a><ul></ul></li><li><a href=/docs/hobby/movie/>ì˜í™”</a><ul></ul></li><li><a href=/docs/hobby/daily/>ì¼ìƒ</a><ul></ul></li></ul></li><li class=book-section-flat><span>ê³µë¶€</span><ul><li><a href=/docs/study/tech/>tech</a><ul></ul></li><li><a href=/docs/study/etc/>etc</a><ul></ul></li></ul></li><li><a href=/docs/about/>ğ“‚‚ ğ“ˆ’ğ“¸ï»¿</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><h1 id=tft-pytorch-forecasting---stallion-íŠœí† ë¦¬ì–¼>TFT PyTorch Forecasting - Stallion íŠœí† ë¦¬ì–¼
<a class=anchor href=#tft-pytorch-forecasting---stallion-%ed%8a%9c%ed%86%a0%eb%a6%ac%ec%96%bc>#</a></h1><p>#2025-05-28</p><hr><p>#introduction</p><ul><li>ë°ì´í„°ì…‹: <a href=https://www.kaggle.com/datasets/utathya/future-volume-prediction>Kaggle - Stallion ë°ì´í„°ì…‹</a></li><li>ëª©ì : Temporal Fusion Transformer(TFT)ë¥¼ í™œìš©í•˜ì—¬ ìŒë£Œ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡</li></ul><p>#install</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ nvidia-smi
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Wed May 28 14:00:07 2025       
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
</span></span><span style=display:flex><span>|-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span style=display:flex><span>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
</span></span><span style=display:flex><span>|                                         |                      |               MIG M. |
</span></span><span style=display:flex><span>|=========================================+======================+======================|
</span></span><span style=display:flex><span>|   0  NVIDIA RTX A6000               Off | 00000000:3B:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   59C    P2             204W / 300W |   8339MiB / 49140MiB |     95%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   1  NVIDIA RTX A6000               Off | 00000000:5E:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   60C    P2             213W / 300W |   6897MiB / 49140MiB |     94%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   2  NVIDIA RTX A6000               Off | 00000000:B1:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   60C    P2             203W / 300W |   6799MiB / 49140MiB |     94%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   3  NVIDIA RTX A6000               Off | 00000000:D9:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 32%   63C    P2             212W / 300W |   6885MiB / 49140MiB |     96%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>                                                                                         
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>| Processes:                                                                            |
</span></span><span style=display:flex><span>|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
</span></span><span style=display:flex><span>|        ID   ID                                                             Usage      |
</span></span><span style=display:flex><span>|=======================================================================================|
</span></span><span style=display:flex><span>|    0   N/A  N/A     20199      C   ...dg/miniconda3/envs/woodg/bin/python      664MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A    860801      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1201205      C   ...u1098/anaconda3/envs/dna/bin/python     6198MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1216286      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1225349      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    1   N/A  N/A   1201206      C   ...u1098/anaconda3/envs/dna/bin/python     6104MiB |
</span></span><span style=display:flex><span>|    1   N/A  N/A   1224607      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    2   N/A  N/A   1201207      C   ...u1098/anaconda3/envs/dna/bin/python     6006MiB |
</span></span><span style=display:flex><span>|    2   N/A  N/A   1224848      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    3   N/A  N/A   1201208      C   ...u1098/anaconda3/envs/dna/bin/python     6092MiB |
</span></span><span style=display:flex><span>|    3   N/A  N/A   1225121      C   python                                      782MiB |
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span></code></pre></div><ul><li><p>NVIDIA ë“œë¼ì´ë²„ ë²„ì „: 545.23.08</p></li><li><p>CUDA ë²„ì „: 12.3</p></li><li><p>PyTorch ë° ê´€ë ¨ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•  ë•Œ CUDA 12.3ì„ ì§€ì›í•˜ëŠ” ë²„ì „ìœ¼ë¡œ ë§ì¶°ì•¼ GPU ì‚¬ìš©ì´ ê°€ëŠ¥.</p><ul><li>CUDA 12.3ì„ ê·¸ëŒ€ë¡œ ì“°ëŠ” ê²½ìš° PyTorch GPU ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì´ ë‚®ê±°ë‚˜ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆì–´ CUDA 11.7ë¡œ ì„¤ì¹˜í•´ì¤€ë‹¤</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install torch<span style=color:#f92672>==</span>1.13.1+cu117 torchvision<span style=color:#f92672>==</span>0.14.1+cu117 torchaudio<span style=color:#f92672>==</span>0.13.1 -f https://download.pytorch.org/whl/torch_stable.html
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Looking in links: https://download.pytorch.org/whl/torch_stable.html
</span></span><span style=display:flex><span>Collecting torch<span style=color:#f92672>==</span>1.13.1+cu117
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>1801.9 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.8/1.8 GB 1.5 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Collecting torchvision<span style=color:#f92672>==</span>0.14.1+cu117
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>24.3 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 24.3/24.3 MB 42.5 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Collecting torchaudio<span style=color:#f92672>==</span>0.13.1
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/rocm5.2/torchaudio-0.13.1%2Brocm5.2-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>3.9 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.9/3.9 MB 60.0 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Requirement already satisfied: typing-extensions in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torch<span style=color:#f92672>==</span>1.13.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>4.7.1<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: numpy in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>1.21.6<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: requests in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>2.31.0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: pillow!<span style=color:#f92672>=</span>8.3.*,&gt;<span style=color:#f92672>=</span>5.3.0 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>9.5.0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: charset-normalizer&lt;4,&gt;<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>3.3.2<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: certifi&gt;<span style=color:#f92672>=</span>2017.4.17 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>2022.12.7<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: urllib3&lt;3,&gt;<span style=color:#f92672>=</span>1.21.1 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>1.26.20<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: idna&lt;4,&gt;<span style=color:#f92672>=</span>2.5 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>3.7<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Installing collected packages: torch, torchvision, torchaudio
</span></span><span style=display:flex><span>  Attempting uninstall: torch
</span></span><span style=display:flex><span>    Found existing installation: torch 1.13.1
</span></span><span style=display:flex><span>    Uninstalling torch-1.13.1:
</span></span><span style=display:flex><span>      Successfully uninstalled torch-1.13.1
</span></span><span style=display:flex><span>Successfully installed torch-1.13.1+cu117 torchaudio-0.13.1+rocm5.2 torchvision-0.14.1+cu117
</span></span></code></pre></div><p>ì •ìƒ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -c <span style=color:#e6db74>&#34;import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else &#39;No GPU&#39;)&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>1.13.1+cu117
</span></span><span style=display:flex><span>True
</span></span><span style=display:flex><span>NVIDIA RTX A6000
</span></span></code></pre></div><p>ë¬¸ì œì—†ì´ ì„¤ì¹˜ë˜ì—ˆë‹¤!</p><p>#load package</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install lightning
</span></span><span style=display:flex><span>$ pip install pytorch-forecasting
</span></span><span style=display:flex><span>$ pip install pyarrow
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>warnings<span style=color:#f92672>.</span>filterwarnings(<span style=color:#e6db74>&#34;ignore&#34;</span>)  <span style=color:#75715e># avoid printing out absolute paths</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> copy
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> lightning.pytorch <span style=color:#66d9ef>as</span> pl
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.callbacks <span style=color:#f92672>import</span> EarlyStopping, LearningRateMonitor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.loggers <span style=color:#f92672>import</span> TensorBoardLogger
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting <span style=color:#f92672>import</span> Baseline, TemporalFusionTransformer, TimeSeriesDataSet
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.data <span style=color:#f92672>import</span> GroupNormalizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.metrics <span style=color:#f92672>import</span> MAE, SMAPE, PoissonLoss, QuantileLoss
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.temporal_fusion_transformer.tuning <span style=color:#f92672>import</span> (
</span></span><span style=display:flex><span>    optimize_hyperparameters,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>#load data</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.data.examples <span style=color:#f92672>import</span> get_stallion_data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> get_stallion_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add time index</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;time_idx&#34;</span>] <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;date&#34;</span>]<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>year <span style=color:#f92672>*</span> <span style=color:#ae81ff>12</span> <span style=color:#f92672>+</span> data[<span style=color:#e6db74>&#34;date&#34;</span>]<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>month
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;time_idx&#34;</span>] <span style=color:#f92672>-=</span> data[<span style=color:#e6db74>&#34;time_idx&#34;</span>]<span style=color:#f92672>.</span>min()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add additional features</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;month&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>date<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>month<span style=color:#f92672>.</span>astype(str)<span style=color:#f92672>.</span>astype(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;category&#34;</span>
</span></span><span style=display:flex><span>)  <span style=color:#75715e># categories have be strings</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;log_volume&#34;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>log(data<span style=color:#f92672>.</span>volume <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-8</span>)
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;avg_volume_by_sku&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>groupby(
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>], observed<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>volume<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;avg_volume_by_agency&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>groupby(
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;agency&#34;</span>], observed<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>volume<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># we want to encode special days as one variable and thus need to first reverse one-hot encoding</span>
</span></span><span style=display:flex><span>special_days <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;easter_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;good_friday&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;new_year&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;christmas&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;labor_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;independence_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;revolution_day_memorial&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;regional_games&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;fifa_u_17_world_cup&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;football_gold_cup&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;beer_capital&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;music_fest&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>data[special_days] <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    data[special_days]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>map({<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;-&#34;</span>, <span style=color:#ae81ff>1</span>: x<span style=color:#f92672>.</span>name}))<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;category&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>data<span style=color:#f92672>.</span>sample(<span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>521</span>)
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/e27cc10a-d51a-4182-9324-04f009ffb41b alt=image></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data<span style=color:#f92672>.</span>describe()
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/fbffee77-5b44-4004-8f9b-0d3d3698a439 alt=image></p><p>#Create dataset and dataloaders</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>max_prediction_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>max_encoder_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>training_cutoff <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;time_idx&#34;</span>]<span style=color:#f92672>.</span>max() <span style=color:#f92672>-</span> max_prediction_length
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>training <span style=color:#f92672>=</span> TimeSeriesDataSet(
</span></span><span style=display:flex><span>    data[<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>time_idx <span style=color:#f92672>&lt;=</span> training_cutoff],
</span></span><span style=display:flex><span>    time_idx<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;time_idx&#34;</span>,
</span></span><span style=display:flex><span>    target<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;volume&#34;</span>,
</span></span><span style=display:flex><span>    group_ids<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>],
</span></span><span style=display:flex><span>    min_encoder_length<span style=color:#f92672>=</span>max_encoder_length
</span></span><span style=display:flex><span>    <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>,  <span style=color:#75715e># keep encoder length long (as it is in the validation set)</span>
</span></span><span style=display:flex><span>    max_encoder_length<span style=color:#f92672>=</span>max_encoder_length,
</span></span><span style=display:flex><span>    min_prediction_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    max_prediction_length<span style=color:#f92672>=</span>max_prediction_length,
</span></span><span style=display:flex><span>    static_categoricals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>],
</span></span><span style=display:flex><span>    static_reals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;avg_population_2017&#34;</span>, <span style=color:#e6db74>&#34;avg_yearly_household_income_2017&#34;</span>],
</span></span><span style=display:flex><span>    time_varying_known_categoricals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;special_days&#34;</span>, <span style=color:#e6db74>&#34;month&#34;</span>],
</span></span><span style=display:flex><span>    variable_groups<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;special_days&#34;</span>: special_days
</span></span><span style=display:flex><span>    },  <span style=color:#75715e># group of categorical variables can be treated as one variable</span>
</span></span><span style=display:flex><span>    time_varying_known_reals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;price_regular&#34;</span>, <span style=color:#e6db74>&#34;discount_in_percent&#34;</span>],
</span></span><span style=display:flex><span>    time_varying_unknown_categoricals<span style=color:#f92672>=</span>[],
</span></span><span style=display:flex><span>    time_varying_unknown_reals<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;log_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;industry_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;soda_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_max_temp&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_volume_by_agency&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_volume_by_sku&#34;</span>,
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    target_normalizer<span style=color:#f92672>=</span>GroupNormalizer(
</span></span><span style=display:flex><span>        groups<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>], transformation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softplus&#34;</span>
</span></span><span style=display:flex><span>    ),  <span style=color:#75715e># use softplus and normalize by group</span>
</span></span><span style=display:flex><span>    add_relative_time_idx<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    add_target_scales<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    add_encoder_length<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># create validation set (predict=True) which means to predict the last max_prediction_length points in time</span>
</span></span><span style=display:flex><span><span style=color:#75715e># for each series</span>
</span></span><span style=display:flex><span>validation <span style=color:#f92672>=</span> TimeSeriesDataSet<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training, data, predict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, stop_randomization<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># create dataloaders for model</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>  <span style=color:#75715e># set this between 32 to 128</span>
</span></span><span style=display:flex><span>train_dataloader <span style=color:#f92672>=</span> training<span style=color:#f92672>.</span>to_dataloader(
</span></span><span style=display:flex><span>    train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span>batch_size, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>val_dataloader <span style=color:#f92672>=</span> validation<span style=color:#f92672>.</span>to_dataloader(
</span></span><span style=display:flex><span>    train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, batch_size<span style=color:#f92672>=</span>batch_size <span style=color:#f92672>*</span> <span style=color:#ae81ff>10</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>#Create baseline model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># calculate baseline mean absolute error, i.e. predict next value as the last available value from the history</span>
</span></span><span style=display:flex><span>baseline_predictions <span style=color:#f92672>=</span> Baseline()<span style=color:#f92672>.</span>predict(val_dataloader, return_y<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>MAE()(baseline_predictions<span style=color:#f92672>.</span>output, baseline_predictions<span style=color:#f92672>.</span>y)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>---------------------------------------------------------------------------
</span></span><span style=display:flex><span>TypeError                                 Traceback (most recent call last)
</span></span><span style=display:flex><span>/tmp/ipykernel_1239848/2174382858.py in &lt;module&gt;
</span></span><span style=display:flex><span>      1 # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history
</span></span><span style=display:flex><span>----&gt; 2 baseline_predictions = Baseline().predict(val_dataloader, return_y=True)
</span></span><span style=display:flex><span>      3 MAE()(baseline_predictions.output, baseline_predictions.y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>~/miniconda3/envs/workspace/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in predict(self, data, mode, return_index, return_decoder_lengths, batch_size, num_workers, fast_dev_run, show_progress_bar, return_x, mode_kwargs, **kwargs)
</span></span><span style=display:flex><span>   1157 
</span></span><span style=display:flex><span>   1158                 # make prediction
</span></span><span style=display:flex><span>-&gt; 1159                 out = self(x, **kwargs)  # raw output is dictionary
</span></span><span style=display:flex><span>   1160 
</span></span><span style=display:flex><span>   1161                 lengths = x[&#34;decoder_lengths&#34;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>~/miniconda3/envs/workspace/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
</span></span><span style=display:flex><span>   1192         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
</span></span><span style=display:flex><span>   1193                 or _global_forward_hooks or _global_forward_pre_hooks):
</span></span><span style=display:flex><span>-&gt; 1194             return forward_call(*input, **kwargs)
</span></span><span style=display:flex><span>   1195         # Do not call functions when jit is used
</span></span><span style=display:flex><span>   1196         full_backward_hooks, non_full_backward_hooks = [], []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TypeError: forward() got an unexpected keyword argument &#39;return_y&#39;
</span></span></code></pre></div><ul><li>ì™œì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ <code>baseline_predictions = Baseline().predict(val_dataloader, return_y=True)</code>ì—ì„œ return_yë¼ëŠ” ì¸ìëŠ” ì•ˆë°›ëŠ”ë‹¤ê³ í•¨.<ul><li>return_y=Trueë¥¼ ë¹¼ê³  <code>Baseline().predict(val_dataloader)</code>ë§Œ ì‚¬ìš©í•˜ë©´ ì˜ˆì¸¡ê°’(prediction)ë§Œ ë°˜í™˜ë˜ê³  ì‹¤ì œê°’(y)ì€ ë°˜í™˜ë˜ì§€ ì•ŠìŒ</li><li>return_y=True ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ì¦‰ MAEë¥¼ ê³„ì‚°í•˜ë ¤ë©´ ì§ì ‘ val_dataloaderì—ì„œ y ê°’ì„ êº¼ë‚´ë„ë¡ ì½”ë“œ ìˆ˜ì •</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.metrics <span style=color:#f92672>import</span> MAE
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.baseline <span style=color:#f92672>import</span> Baseline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>baseline_model <span style=color:#f92672>=</span> Baseline()
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> baseline_model<span style=color:#f92672>.</span>predict(val_dataloader)
</span></span><span style=display:flex><span>y_true <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([batch[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;decoder_target&#34;</span>] <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> val_dataloader])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mae <span style=color:#f92672>=</span> MAE()(y_pred, y_true)
</span></span><span style=display:flex><span>mae
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tensor(293.0088)
</span></span></code></pre></div><p>#Train the Temporal Fusion Transformer</p><p>##Find optimal learning rate</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># configure network and trainer</span>
</span></span><span style=display:flex><span>pl<span style=color:#f92672>.</span>seed_everything(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> pl<span style=color:#f92672>.</span>Trainer(
</span></span><span style=display:flex><span>    accelerator<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># clipping gradients is a hyperparameter and important to prevent divergance</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># of the gradient for recurrent neural networks</span>
</span></span><span style=display:flex><span>    gradient_clip_val<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tft <span style=color:#f92672>=</span> TemporalFusionTransformer<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training,
</span></span><span style=display:flex><span>    <span style=color:#75715e># not meaningful for finding the learning rate but otherwise very important</span>
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>,
</span></span><span style=display:flex><span>    hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,  <span style=color:#75715e># most important hyperparameter apart from learning rate</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># number of attention heads. Set to up to 4 for large datasets</span>
</span></span><span style=display:flex><span>    attention_head_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,  <span style=color:#75715e># between 0.1 and 0.3 are good values</span>
</span></span><span style=display:flex><span>    hidden_continuous_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,  <span style=color:#75715e># set to &lt;= hidden_size</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>=</span>QuantileLoss(),
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ranger&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># reduce learning rate if no improvement in validation loss after x epochs</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># reduce_on_plateau_patience=1000,</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of parameters in network: </span><span style=color:#e6db74>{</span>tft<span style=color:#f92672>.</span>size() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1e3</span><span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>k&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Global seed set to 42
</span></span><span style=display:flex><span>GPU available: True (cuda), used: False
</span></span><span style=display:flex><span>TPU available: False, using: 0 TPU cores
</span></span><span style=display:flex><span>IPU available: False, using: 0 IPUs
</span></span><span style=display:flex><span>HPU available: False, using: 0 HPUs
</span></span><span style=display:flex><span>Number of parameters in network: 13.5k
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># find optimal learning rate</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.tuner <span style=color:#f92672>import</span> Tuner
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> Tuner(trainer)<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;suggested learning rate: </span><span style=color:#e6db74>{</span>res<span style=color:#f92672>.</span>suggestion()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>plot(show<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, suggest<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>---------------------------------------------------------------------------
</span></span><span style=display:flex><span>ImportError                               Traceback (most recent call last)
</span></span><span style=display:flex><span>/tmp/ipykernel_1239848/4268711780.py in &lt;module&gt;
</span></span><span style=display:flex><span>      1 # find optimal learning rate
</span></span><span style=display:flex><span>----&gt; 2 from lightning.pytorch.tuner import Tuner
</span></span><span style=display:flex><span>      3 
</span></span><span style=display:flex><span>      4 res = Tuner(trainer).lr_find(
</span></span><span style=display:flex><span>      5     tft,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ImportError: cannot import name &#39;Tuner&#39; from &#39;lightning.pytorch.tuner&#39; (/home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages/lightning/pytorch/tuner/__init__.py)
</span></span></code></pre></div><ul><li><code>pip show lightning</code>ìœ¼ë¡œ í™•ì¸ ê²°ê³¼ lightning ë²„ì „ì´ 1.9.5ì´ê³  lightning.pytorch íŒ¨í‚¤ì§€ êµ¬ì¡°ê°€ ë„ì…ë˜ê¸° ì „ ë²„ì „ì´ì–´ì„œ ì˜¤ë¥˜ê°€ ë‚¬ë‹¤</li><li>importë¥¼ ìˆ˜ì •í•´ì£¼ê³  importí•œê±°ì— ë§ì¶°ì„œ ì½”ë“œë„ ìˆ˜ì •<ul><li>lr_finder ì¢…ë£Œ í›„ ìë™ ë³µì›, í•™ìŠµë¥  ìë™ ì—…ë°ì´íŠ¸ &#171;ë¥¼ ì¶©ì¡±í•˜ë„ë¡ ìˆ˜ì •í–ˆë‹¤.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># find optimal learning rate</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_lightning <span style=color:#f92672>import</span> Trainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_lightning.tuner.tuning <span style=color:#f92672>import</span> Tuner
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tuner <span style=color:#f92672>=</span> Tuner(trainer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lr_finder <span style=color:#f92672>=</span> tuner<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>    update_attr<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><ul><li>ì™œì¸ì§€ ëª¨ë¥´ê²ŸëŠ”ë° ìˆ˜ì •í•œ ì½”ë“œì—ì„œëŠ” &lsquo;ranger&rsquo;ê°€ ì•ˆë¨¹ì–´ì„œ, í•™ìŠµë¥  ì„ íƒì€ ë‹¤ë¥¸ optimizerë¡œ í•˜ê³  ì„ íƒí•œ í•™ìŠµë¥ ì„ ranger optimizer ì“°ëŠ” ì›ë˜ ëª¨ë¸ì— ì ìš©ì‹œí‚¤ëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì±—ì§€í”¼í‹°ê°€ ì¶”ì²œí•´ì¤¬ë‹¤</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.temporal_fusion_transformer.tuning <span style=color:#f92672>import</span> optimize_hyperparameters
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting <span style=color:#f92672>import</span> TemporalFusionTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ranger ëŒ€ì‹  Adam ì‚¬ìš© (lr ì°¾ê¸° ì „ìš©)</span>
</span></span><span style=display:flex><span>tft_tmp <span style=color:#f92672>=</span> TemporalFusionTransformer<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training,
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>,
</span></span><span style=display:flex><span>    hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    attention_head_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>    hidden_continuous_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>=</span>QuantileLoss(),
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;adam&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># lr_find()</span>
</span></span><span style=display:flex><span>tuner <span style=color:#f92672>=</span> Tuner(trainer)
</span></span><span style=display:flex><span>lr_finder <span style=color:#f92672>=</span> tuner<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft_tmp,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>suggested_lr <span style=color:#f92672>=</span> lr_finder<span style=color:#f92672>.</span>suggestion()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Suggested LR: </span><span style=color:#e6db74>{</span>suggested_lr<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> lr_finder<span style=color:#f92672>.</span>plot(suggest<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Suggested LR: 0.0019498445997580445
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/7a9f568a-21a1-4b23-9617-169c02f79c66 alt=image></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tft<span style=color:#f92672>.</span>hparams<span style=color:#f92672>.</span>learning_rate <span style=color:#f92672>=</span> suggested_lr <span style=color:#75715e>#ranger ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ì›ë˜ tft ëª¨ë¸ì— ì´ í•™ìŠµë¥ ì„ ì ìš©</span>
</span></span></code></pre></div><p>ê·¼ë° ì´ê²Œ ë§ë‚˜.. ì‹¶ì–´ì„œ ë²„ì „ ë§ì¶°ì„œ ë‹¤ì‹œí•´ë³¼ì˜ˆì •.</p><blockquote><p>ì½”ë“œ: <a href=https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html>https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html</a></p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>