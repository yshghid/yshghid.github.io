<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  TFT PyTorch Forecasting - Stallion 튜토리얼
  #

#2025-05-28

#introduction

데이터셋: Kaggle - Stallion 데이터셋
목적: Temporal Fusion Transformer(TFT)를 활용하여 음료 판매량을 예측

#install
$ nvidia-smi
Wed May 28 14:00:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A6000               Off | 00000000:3B:00.0 Off |                  Off |
| 30%   59C    P2             204W / 300W |   8339MiB / 49140MiB |     95%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A6000               Off | 00000000:5E:00.0 Off |                  Off |
| 30%   60C    P2             213W / 300W |   6897MiB / 49140MiB |     94%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A6000               Off | 00000000:B1:00.0 Off |                  Off |
| 30%   60C    P2             203W / 300W |   6799MiB / 49140MiB |     94%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A6000               Off | 00000000:D9:00.0 Off |                  Off |
| 32%   63C    P2             212W / 300W |   6885MiB / 49140MiB |     96%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     20199      C   ...dg/miniconda3/envs/woodg/bin/python      664MiB |
|    0   N/A  N/A    860801      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
|    0   N/A  N/A   1201205      C   ...u1098/anaconda3/envs/dna/bin/python     6198MiB |
|    0   N/A  N/A   1216286      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
|    0   N/A  N/A   1225349      C   python                                      782MiB |
|    1   N/A  N/A   1201206      C   ...u1098/anaconda3/envs/dna/bin/python     6104MiB |
|    1   N/A  N/A   1224607      C   python                                      782MiB |
|    2   N/A  N/A   1201207      C   ...u1098/anaconda3/envs/dna/bin/python     6006MiB |
|    2   N/A  N/A   1224848      C   python                                      782MiB |
|    3   N/A  N/A   1201208      C   ...u1098/anaconda3/envs/dna/bin/python     6092MiB |
|    3   N/A  N/A   1225121      C   python                                      782MiB |
+---------------------------------------------------------------------------------------+


NVIDIA 드라이버 버전: 545.23.08"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://yshghid.github.io/docs/study/tech/tech12/"><meta property="og:site_name" content=" "><meta property="og:title" content="TFT PyTorch Forecasting - Stallion 튜토리얼"><meta property="og:description" content="TFT PyTorch Forecasting - Stallion 튜토리얼 # #2025-05-28
#introduction
데이터셋: Kaggle - Stallion 데이터셋 목적: Temporal Fusion Transformer(TFT)를 활용하여 음료 판매량을 예측 #install
$ nvidia-smi Wed May 28 14:00:07 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 545.23.08 Driver Version: 545.23.08 CUDA Version: 12.3 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA RTX A6000 Off | 00000000:3B:00.0 Off | Off | | 30% 59C P2 204W / 300W | 8339MiB / 49140MiB | 95% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA RTX A6000 Off | 00000000:5E:00.0 Off | Off | | 30% 60C P2 213W / 300W | 6897MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA RTX A6000 Off | 00000000:B1:00.0 Off | Off | | 30% 60C P2 203W / 300W | 6799MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 3 NVIDIA RTX A6000 Off | 00000000:D9:00.0 Off | Off | | 32% 63C P2 212W / 300W | 6885MiB / 49140MiB | 96% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 0 N/A N/A 20199 C ...dg/miniconda3/envs/woodg/bin/python 664MiB | | 0 N/A N/A 860801 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1201205 C ...u1098/anaconda3/envs/dna/bin/python 6198MiB | | 0 N/A N/A 1216286 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1225349 C python 782MiB | | 1 N/A N/A 1201206 C ...u1098/anaconda3/envs/dna/bin/python 6104MiB | | 1 N/A N/A 1224607 C python 782MiB | | 2 N/A N/A 1201207 C ...u1098/anaconda3/envs/dna/bin/python 6006MiB | | 2 N/A N/A 1224848 C python 782MiB | | 3 N/A N/A 1201208 C ...u1098/anaconda3/envs/dna/bin/python 6092MiB | | 3 N/A N/A 1225121 C python 782MiB | +---------------------------------------------------------------------------------------+ NVIDIA 드라이버 버전: 545.23.08"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-05-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-28T00:00:00+00:00"><meta property="article:tag" content="2025-05"><title>TFT PyTorch Forecasting - Stallion 튜토리얼 |</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://yshghid.github.io/docs/study/tech/tech12/><link rel=stylesheet href=/book.min.6217d077edb4189fd0578345e84bca1a884dfdee121ff8dc9a0f55cfe0852bc9.css integrity="sha256-YhfQd+20GJ/QV4NF6EvKGohN/e4SH/jcmg9Vz+CFK8k=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.63530f1cf01249c89efe8f085cf4a4b588cee710908effd47540440b98aff6ee.js integrity="sha256-Y1MPHPASScie/o8IXPSktYjO5xCQjv/UdUBEC5iv9u4=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo class=book-icon><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>기록</span><ul><li><a href=/docs/hobby/book/>글</a><ul></ul></li><li><a href=/docs/hobby/daily/>일상</a><ul></ul></li><li><a href=/docs/hobby/shopping/>쇼핑</a><ul></ul></li><li><a href=/docs/hobby/youtube/>유튜브</a><ul></ul></li><li><a href=/docs/hobby/music/>음악</a><ul></ul></li><li><a href=/docs/hobby/baking/>베이킹</a><ul></ul></li><li><a href=/docs/hobby/movie/>영화</a><ul></ul></li></ul></li><li class=book-section-flat><span>공부</span><ul><li><a href=/docs/study/ai/>AI</a><ul></ul></li><li><a href=/docs/study/sw/>SW</a><ul></ul></li><li><a href=/docs/study/bioinformatics/>Bioinformatics</a><ul></ul></li><li><a href=/docs/study/algorithm/>코테</a><ul></ul></li><li><a href=/docs/study/career/>취업</a><ul></ul></li><li><a href=/docs/study/github/>깃허브</a><ul></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>TFT PyTorch Forecasting - Stallion 튜토리얼</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><h1 id=tft-pytorch-forecasting---stallion-튜토리얼>TFT PyTorch Forecasting - Stallion 튜토리얼
<a class=anchor href=#tft-pytorch-forecasting---stallion-%ed%8a%9c%ed%86%a0%eb%a6%ac%ec%96%bc>#</a></h1><p>#2025-05-28</p><hr><p>#introduction</p><ul><li>데이터셋: <a href=https://www.kaggle.com/datasets/utathya/future-volume-prediction>Kaggle - Stallion 데이터셋</a></li><li>목적: Temporal Fusion Transformer(TFT)를 활용하여 음료 판매량을 예측</li></ul><p>#install</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ nvidia-smi
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Wed May 28 14:00:07 2025       
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
</span></span><span style=display:flex><span>|-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span style=display:flex><span>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
</span></span><span style=display:flex><span>|                                         |                      |               MIG M. |
</span></span><span style=display:flex><span>|=========================================+======================+======================|
</span></span><span style=display:flex><span>|   0  NVIDIA RTX A6000               Off | 00000000:3B:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   59C    P2             204W / 300W |   8339MiB / 49140MiB |     95%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   1  NVIDIA RTX A6000               Off | 00000000:5E:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   60C    P2             213W / 300W |   6897MiB / 49140MiB |     94%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   2  NVIDIA RTX A6000               Off | 00000000:B1:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 30%   60C    P2             203W / 300W |   6799MiB / 49140MiB |     94%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>|   3  NVIDIA RTX A6000               Off | 00000000:D9:00.0 Off |                  Off |
</span></span><span style=display:flex><span>| 32%   63C    P2             212W / 300W |   6885MiB / 49140MiB |     96%      Default |
</span></span><span style=display:flex><span>|                                         |                      |                  N/A |
</span></span><span style=display:flex><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style=display:flex><span>                                                                                         
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>| Processes:                                                                            |
</span></span><span style=display:flex><span>|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
</span></span><span style=display:flex><span>|        ID   ID                                                             Usage      |
</span></span><span style=display:flex><span>|=======================================================================================|
</span></span><span style=display:flex><span>|    0   N/A  N/A     20199      C   ...dg/miniconda3/envs/woodg/bin/python      664MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A    860801      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1201205      C   ...u1098/anaconda3/envs/dna/bin/python     6198MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1216286      C   ...jyj/miniconda3/envs/TiCC/bin/python      338MiB |
</span></span><span style=display:flex><span>|    0   N/A  N/A   1225349      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    1   N/A  N/A   1201206      C   ...u1098/anaconda3/envs/dna/bin/python     6104MiB |
</span></span><span style=display:flex><span>|    1   N/A  N/A   1224607      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    2   N/A  N/A   1201207      C   ...u1098/anaconda3/envs/dna/bin/python     6006MiB |
</span></span><span style=display:flex><span>|    2   N/A  N/A   1224848      C   python                                      782MiB |
</span></span><span style=display:flex><span>|    3   N/A  N/A   1201208      C   ...u1098/anaconda3/envs/dna/bin/python     6092MiB |
</span></span><span style=display:flex><span>|    3   N/A  N/A   1225121      C   python                                      782MiB |
</span></span><span style=display:flex><span>+---------------------------------------------------------------------------------------+
</span></span></code></pre></div><ul><li><p>NVIDIA 드라이버 버전: 545.23.08</p></li><li><p>CUDA 버전: 12.3</p></li><li><p>PyTorch 및 관련 패키지를 설치할 때 CUDA 12.3을 지원하는 버전으로 맞춰야 GPU 사용이 가능.</p><ul><li>CUDA 12.3을 그대로 쓰는 경우 PyTorch GPU 버전과의 호환성이 낮거나 불안정할 수 있어 CUDA 11.7로 설치해준다</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install torch<span style=color:#f92672>==</span>1.13.1+cu117 torchvision<span style=color:#f92672>==</span>0.14.1+cu117 torchaudio<span style=color:#f92672>==</span>0.13.1 -f https://download.pytorch.org/whl/torch_stable.html
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Looking in links: https://download.pytorch.org/whl/torch_stable.html
</span></span><span style=display:flex><span>Collecting torch<span style=color:#f92672>==</span>1.13.1+cu117
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>1801.9 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 GB 1.5 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Collecting torchvision<span style=color:#f92672>==</span>0.14.1+cu117
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>24.3 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/24.3 MB 42.5 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Collecting torchaudio<span style=color:#f92672>==</span>0.13.1
</span></span><span style=display:flex><span>  Downloading https://download.pytorch.org/whl/rocm5.2/torchaudio-0.13.1%2Brocm5.2-cp37-cp37m-linux_x86_64.whl <span style=color:#f92672>(</span>3.9 MB<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 60.0 MB/s eta 0:00:00
</span></span><span style=display:flex><span>Requirement already satisfied: typing-extensions in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torch<span style=color:#f92672>==</span>1.13.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>4.7.1<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: numpy in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>1.21.6<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: requests in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>2.31.0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: pillow!<span style=color:#f92672>=</span>8.3.*,&gt;<span style=color:#f92672>=</span>5.3.0 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>9.5.0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: charset-normalizer&lt;4,&gt;<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>3.3.2<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: certifi&gt;<span style=color:#f92672>=</span>2017.4.17 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>2022.12.7<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: urllib3&lt;3,&gt;<span style=color:#f92672>=</span>1.21.1 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>1.26.20<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Requirement already satisfied: idna&lt;4,&gt;<span style=color:#f92672>=</span>2.5 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages <span style=color:#f92672>(</span>from requests-&gt;torchvision<span style=color:#f92672>==</span>0.14.1+cu117<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>3.7<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Installing collected packages: torch, torchvision, torchaudio
</span></span><span style=display:flex><span>  Attempting uninstall: torch
</span></span><span style=display:flex><span>    Found existing installation: torch 1.13.1
</span></span><span style=display:flex><span>    Uninstalling torch-1.13.1:
</span></span><span style=display:flex><span>      Successfully uninstalled torch-1.13.1
</span></span><span style=display:flex><span>Successfully installed torch-1.13.1+cu117 torchaudio-0.13.1+rocm5.2 torchvision-0.14.1+cu117
</span></span></code></pre></div><p>정상 설치 여부 확인</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -c <span style=color:#e6db74>&#34;import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else &#39;No GPU&#39;)&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>1.13.1+cu117
</span></span><span style=display:flex><span>True
</span></span><span style=display:flex><span>NVIDIA RTX A6000
</span></span></code></pre></div><p>문제없이 설치되었다!</p><p>#load package</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install lightning
</span></span><span style=display:flex><span>$ pip install pytorch-forecasting
</span></span><span style=display:flex><span>$ pip install pyarrow
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>warnings<span style=color:#f92672>.</span>filterwarnings(<span style=color:#e6db74>&#34;ignore&#34;</span>)  <span style=color:#75715e># avoid printing out absolute paths</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> copy
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> lightning.pytorch <span style=color:#66d9ef>as</span> pl
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.callbacks <span style=color:#f92672>import</span> EarlyStopping, LearningRateMonitor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.loggers <span style=color:#f92672>import</span> TensorBoardLogger
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting <span style=color:#f92672>import</span> Baseline, TemporalFusionTransformer, TimeSeriesDataSet
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.data <span style=color:#f92672>import</span> GroupNormalizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.metrics <span style=color:#f92672>import</span> MAE, SMAPE, PoissonLoss, QuantileLoss
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.temporal_fusion_transformer.tuning <span style=color:#f92672>import</span> (
</span></span><span style=display:flex><span>    optimize_hyperparameters,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>#load data</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.data.examples <span style=color:#f92672>import</span> get_stallion_data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> get_stallion_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add time index</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;time_idx&#34;</span>] <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;date&#34;</span>]<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>year <span style=color:#f92672>*</span> <span style=color:#ae81ff>12</span> <span style=color:#f92672>+</span> data[<span style=color:#e6db74>&#34;date&#34;</span>]<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>month
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;time_idx&#34;</span>] <span style=color:#f92672>-=</span> data[<span style=color:#e6db74>&#34;time_idx&#34;</span>]<span style=color:#f92672>.</span>min()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add additional features</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;month&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>date<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>month<span style=color:#f92672>.</span>astype(str)<span style=color:#f92672>.</span>astype(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;category&#34;</span>
</span></span><span style=display:flex><span>)  <span style=color:#75715e># categories have be strings</span>
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;log_volume&#34;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>log(data<span style=color:#f92672>.</span>volume <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-8</span>)
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;avg_volume_by_sku&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>groupby(
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>], observed<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>volume<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#34;avg_volume_by_agency&#34;</span>] <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>groupby(
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;agency&#34;</span>], observed<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>volume<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># we want to encode special days as one variable and thus need to first reverse one-hot encoding</span>
</span></span><span style=display:flex><span>special_days <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;easter_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;good_friday&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;new_year&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;christmas&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;labor_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;independence_day&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;revolution_day_memorial&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;regional_games&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;fifa_u_17_world_cup&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;football_gold_cup&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;beer_capital&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;music_fest&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>data[special_days] <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    data[special_days]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>map({<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;-&#34;</span>, <span style=color:#ae81ff>1</span>: x<span style=color:#f92672>.</span>name}))<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;category&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>data<span style=color:#f92672>.</span>sample(<span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>521</span>)
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/e27cc10a-d51a-4182-9324-04f009ffb41b alt=image></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data<span style=color:#f92672>.</span>describe()
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/fbffee77-5b44-4004-8f9b-0d3d3698a439 alt=image></p><p>#Create dataset and dataloaders</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>max_prediction_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>max_encoder_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>training_cutoff <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;time_idx&#34;</span>]<span style=color:#f92672>.</span>max() <span style=color:#f92672>-</span> max_prediction_length
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>training <span style=color:#f92672>=</span> TimeSeriesDataSet(
</span></span><span style=display:flex><span>    data[<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>time_idx <span style=color:#f92672>&lt;=</span> training_cutoff],
</span></span><span style=display:flex><span>    time_idx<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;time_idx&#34;</span>,
</span></span><span style=display:flex><span>    target<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;volume&#34;</span>,
</span></span><span style=display:flex><span>    group_ids<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>],
</span></span><span style=display:flex><span>    min_encoder_length<span style=color:#f92672>=</span>max_encoder_length
</span></span><span style=display:flex><span>    <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>,  <span style=color:#75715e># keep encoder length long (as it is in the validation set)</span>
</span></span><span style=display:flex><span>    max_encoder_length<span style=color:#f92672>=</span>max_encoder_length,
</span></span><span style=display:flex><span>    min_prediction_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    max_prediction_length<span style=color:#f92672>=</span>max_prediction_length,
</span></span><span style=display:flex><span>    static_categoricals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>],
</span></span><span style=display:flex><span>    static_reals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;avg_population_2017&#34;</span>, <span style=color:#e6db74>&#34;avg_yearly_household_income_2017&#34;</span>],
</span></span><span style=display:flex><span>    time_varying_known_categoricals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;special_days&#34;</span>, <span style=color:#e6db74>&#34;month&#34;</span>],
</span></span><span style=display:flex><span>    variable_groups<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;special_days&#34;</span>: special_days
</span></span><span style=display:flex><span>    },  <span style=color:#75715e># group of categorical variables can be treated as one variable</span>
</span></span><span style=display:flex><span>    time_varying_known_reals<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;time_idx&#34;</span>, <span style=color:#e6db74>&#34;price_regular&#34;</span>, <span style=color:#e6db74>&#34;discount_in_percent&#34;</span>],
</span></span><span style=display:flex><span>    time_varying_unknown_categoricals<span style=color:#f92672>=</span>[],
</span></span><span style=display:flex><span>    time_varying_unknown_reals<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;log_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;industry_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;soda_volume&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_max_temp&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_volume_by_agency&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;avg_volume_by_sku&#34;</span>,
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    target_normalizer<span style=color:#f92672>=</span>GroupNormalizer(
</span></span><span style=display:flex><span>        groups<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;agency&#34;</span>, <span style=color:#e6db74>&#34;sku&#34;</span>], transformation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softplus&#34;</span>
</span></span><span style=display:flex><span>    ),  <span style=color:#75715e># use softplus and normalize by group</span>
</span></span><span style=display:flex><span>    add_relative_time_idx<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    add_target_scales<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    add_encoder_length<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># create validation set (predict=True) which means to predict the last max_prediction_length points in time</span>
</span></span><span style=display:flex><span><span style=color:#75715e># for each series</span>
</span></span><span style=display:flex><span>validation <span style=color:#f92672>=</span> TimeSeriesDataSet<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training, data, predict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, stop_randomization<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># create dataloaders for model</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>  <span style=color:#75715e># set this between 32 to 128</span>
</span></span><span style=display:flex><span>train_dataloader <span style=color:#f92672>=</span> training<span style=color:#f92672>.</span>to_dataloader(
</span></span><span style=display:flex><span>    train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span>batch_size, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>val_dataloader <span style=color:#f92672>=</span> validation<span style=color:#f92672>.</span>to_dataloader(
</span></span><span style=display:flex><span>    train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, batch_size<span style=color:#f92672>=</span>batch_size <span style=color:#f92672>*</span> <span style=color:#ae81ff>10</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>#Create baseline model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># calculate baseline mean absolute error, i.e. predict next value as the last available value from the history</span>
</span></span><span style=display:flex><span>baseline_predictions <span style=color:#f92672>=</span> Baseline()<span style=color:#f92672>.</span>predict(val_dataloader, return_y<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>MAE()(baseline_predictions<span style=color:#f92672>.</span>output, baseline_predictions<span style=color:#f92672>.</span>y)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>---------------------------------------------------------------------------
</span></span><span style=display:flex><span>TypeError                                 Traceback (most recent call last)
</span></span><span style=display:flex><span>/tmp/ipykernel_1239848/2174382858.py in &lt;module&gt;
</span></span><span style=display:flex><span>      1 # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history
</span></span><span style=display:flex><span>----&gt; 2 baseline_predictions = Baseline().predict(val_dataloader, return_y=True)
</span></span><span style=display:flex><span>      3 MAE()(baseline_predictions.output, baseline_predictions.y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>~/miniconda3/envs/workspace/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in predict(self, data, mode, return_index, return_decoder_lengths, batch_size, num_workers, fast_dev_run, show_progress_bar, return_x, mode_kwargs, **kwargs)
</span></span><span style=display:flex><span>   1157 
</span></span><span style=display:flex><span>   1158                 # make prediction
</span></span><span style=display:flex><span>-&gt; 1159                 out = self(x, **kwargs)  # raw output is dictionary
</span></span><span style=display:flex><span>   1160 
</span></span><span style=display:flex><span>   1161                 lengths = x[&#34;decoder_lengths&#34;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>~/miniconda3/envs/workspace/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
</span></span><span style=display:flex><span>   1192         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
</span></span><span style=display:flex><span>   1193                 or _global_forward_hooks or _global_forward_pre_hooks):
</span></span><span style=display:flex><span>-&gt; 1194             return forward_call(*input, **kwargs)
</span></span><span style=display:flex><span>   1195         # Do not call functions when jit is used
</span></span><span style=display:flex><span>   1196         full_backward_hooks, non_full_backward_hooks = [], []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TypeError: forward() got an unexpected keyword argument &#39;return_y&#39;
</span></span></code></pre></div><ul><li>왜인지 모르겠지만 <code>baseline_predictions = Baseline().predict(val_dataloader, return_y=True)</code>에서 return_y라는 인자는 안받는다고함.<ul><li>return_y=True를 빼고 <code>Baseline().predict(val_dataloader)</code>만 사용하면 예측값(prediction)만 반환되고 실제값(y)은 반환되지 않음</li><li>return_y=True 결과를 얻으려면 즉 MAE를 계산하려면 직접 val_dataloader에서 y 값을 꺼내도록 코드 수정</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.metrics <span style=color:#f92672>import</span> MAE
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.baseline <span style=color:#f92672>import</span> Baseline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>baseline_model <span style=color:#f92672>=</span> Baseline()
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> baseline_model<span style=color:#f92672>.</span>predict(val_dataloader)
</span></span><span style=display:flex><span>y_true <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([batch[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;decoder_target&#34;</span>] <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> val_dataloader])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mae <span style=color:#f92672>=</span> MAE()(y_pred, y_true)
</span></span><span style=display:flex><span>mae
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>tensor(293.0088)
</span></span></code></pre></div><p>#Train the Temporal Fusion Transformer</p><p>##Find optimal learning rate</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># configure network and trainer</span>
</span></span><span style=display:flex><span>pl<span style=color:#f92672>.</span>seed_everything(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> pl<span style=color:#f92672>.</span>Trainer(
</span></span><span style=display:flex><span>    accelerator<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># clipping gradients is a hyperparameter and important to prevent divergance</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># of the gradient for recurrent neural networks</span>
</span></span><span style=display:flex><span>    gradient_clip_val<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tft <span style=color:#f92672>=</span> TemporalFusionTransformer<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training,
</span></span><span style=display:flex><span>    <span style=color:#75715e># not meaningful for finding the learning rate but otherwise very important</span>
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>,
</span></span><span style=display:flex><span>    hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,  <span style=color:#75715e># most important hyperparameter apart from learning rate</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># number of attention heads. Set to up to 4 for large datasets</span>
</span></span><span style=display:flex><span>    attention_head_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,  <span style=color:#75715e># between 0.1 and 0.3 are good values</span>
</span></span><span style=display:flex><span>    hidden_continuous_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,  <span style=color:#75715e># set to &lt;= hidden_size</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>=</span>QuantileLoss(),
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ranger&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># reduce learning rate if no improvement in validation loss after x epochs</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># reduce_on_plateau_patience=1000,</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of parameters in network: </span><span style=color:#e6db74>{</span>tft<span style=color:#f92672>.</span>size() <span style=color:#f92672>/</span> <span style=color:#ae81ff>1e3</span><span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>k&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Global seed set to 42
</span></span><span style=display:flex><span>GPU available: True (cuda), used: False
</span></span><span style=display:flex><span>TPU available: False, using: 0 TPU cores
</span></span><span style=display:flex><span>IPU available: False, using: 0 IPUs
</span></span><span style=display:flex><span>HPU available: False, using: 0 HPUs
</span></span><span style=display:flex><span>Number of parameters in network: 13.5k
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># find optimal learning rate</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lightning.pytorch.tuner <span style=color:#f92672>import</span> Tuner
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> Tuner(trainer)<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;suggested learning rate: </span><span style=color:#e6db74>{</span>res<span style=color:#f92672>.</span>suggestion()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>plot(show<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, suggest<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>---------------------------------------------------------------------------
</span></span><span style=display:flex><span>ImportError                               Traceback (most recent call last)
</span></span><span style=display:flex><span>/tmp/ipykernel_1239848/4268711780.py in &lt;module&gt;
</span></span><span style=display:flex><span>      1 # find optimal learning rate
</span></span><span style=display:flex><span>----&gt; 2 from lightning.pytorch.tuner import Tuner
</span></span><span style=display:flex><span>      3 
</span></span><span style=display:flex><span>      4 res = Tuner(trainer).lr_find(
</span></span><span style=display:flex><span>      5     tft,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ImportError: cannot import name &#39;Tuner&#39; from &#39;lightning.pytorch.tuner&#39; (/home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages/lightning/pytorch/tuner/__init__.py)
</span></span></code></pre></div><ul><li><code>pip show lightning</code>으로 확인 결과 lightning 버전이 1.9.5이고 lightning.pytorch 패키지 구조가 도입되기 전 버전이어서 오류가 났다</li><li>import를 수정해주고 import한거에 맞춰서 코드도 수정<ul><li>lr_finder 종료 후 자동 복원, 학습률 자동 업데이트 &#171;를 충족하도록 수정했다.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># find optimal learning rate</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_lightning <span style=color:#f92672>import</span> Trainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_lightning.tuner.tuning <span style=color:#f92672>import</span> Tuner
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tuner <span style=color:#f92672>=</span> Tuner(trainer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lr_finder <span style=color:#f92672>=</span> tuner<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>    update_attr<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><ul><li>왜인지 모르겟는데 수정한 코드에서는 &lsquo;ranger&rsquo;가 안먹어서, 학습률 선택은 다른 optimizer로 하고 선택한 학습률을 ranger optimizer 쓰는 원래 모델에 적용시키는 아래 코드를 챗지피티가 추천해줬다</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting.models.temporal_fusion_transformer.tuning <span style=color:#f92672>import</span> optimize_hyperparameters
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pytorch_forecasting <span style=color:#f92672>import</span> TemporalFusionTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ranger 대신 Adam 사용 (lr 찾기 전용)</span>
</span></span><span style=display:flex><span>tft_tmp <span style=color:#f92672>=</span> TemporalFusionTransformer<span style=color:#f92672>.</span>from_dataset(
</span></span><span style=display:flex><span>    training,
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>,
</span></span><span style=display:flex><span>    hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    attention_head_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>    hidden_continuous_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>=</span>QuantileLoss(),
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;adam&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># lr_find()</span>
</span></span><span style=display:flex><span>tuner <span style=color:#f92672>=</span> Tuner(trainer)
</span></span><span style=display:flex><span>lr_finder <span style=color:#f92672>=</span> tuner<span style=color:#f92672>.</span>lr_find(
</span></span><span style=display:flex><span>    tft_tmp,
</span></span><span style=display:flex><span>    train_dataloaders<span style=color:#f92672>=</span>train_dataloader,
</span></span><span style=display:flex><span>    val_dataloaders<span style=color:#f92672>=</span>val_dataloader,
</span></span><span style=display:flex><span>    min_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>    max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>10.0</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>suggested_lr <span style=color:#f92672>=</span> lr_finder<span style=color:#f92672>.</span>suggestion()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Suggested LR: </span><span style=color:#e6db74>{</span>suggested_lr<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> lr_finder<span style=color:#f92672>.</span>plot(suggest<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>Suggested LR: 0.0019498445997580445
</span></span></code></pre></div><p><img src=https://github.com/user-attachments/assets/7a9f568a-21a1-4b23-9617-169c02f79c66 alt=image></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tft<span style=color:#f92672>.</span>hparams<span style=color:#f92672>.</span>learning_rate <span style=color:#f92672>=</span> suggested_lr <span style=color:#75715e>#ranger 옵티마이저를 사용하는 원래 tft 모델에 이 학습률을 적용</span>
</span></span></code></pre></div><p>근데 이게 맞나.. 싶어서 버전 맞춰서 다시해볼예정.</p><blockquote><p>코드: <a href=https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html>https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html</a></p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments><script src=https://giscus.app/client.js data-repo=yshghid/yshghid.github.io data-repo-id=R_kgDONkMkNg data-category-id=DIC_kwDONkMkNs4CloJh data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko crossorigin=anonymous async></script></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>