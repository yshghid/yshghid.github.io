[{"id":0,"href":"/docs/about/","title":"#﻿","section":"Docs","content":" # #블로그 소개\n본 블로그의 목적은 관심사에 대한 기록이다\n관심사는 장기 관심사와 단기 관심사로 나눌 수 있는데\n장기 관심사는 이제까지 장기적으로 관심이 지속되어왔던 것들 단기 관심사는 흥미를 느낌에 따라 시간이나 돈을 많이 쓴 것들 인데 최근에 시작한 것들/최근에는 안하는것들 로 볼수있다! (즉, 관심의 척도=시간과 돈..)\n둘의 차이는\n장기 관심사는 카테고리가 공개 단기 관심사는 카테고리가 숨김 처리 etc에서 한꺼번에 확인 가능하게 구성 예정 그안에서도 최근에 시작한 것들은 상단 배치 최근에 안하는 것들은 하단 배치 그리고 장/단기 관심사는 1-2달에 한번씩 재검토 예정이다. #블로그 히스토리\n#2025-04-22\n#2024-12-31\n"},{"id":1,"href":"/docs/collection/","title":"수집함","section":"Docs","content":" 수집함 # 책 # "},{"id":2,"href":"/docs/hobby/book/","title":"글","section":"기록","content":" 글 # 읽었던 재밌는글 모음.\n2025 # 07-01 ⋯ 변하지 않기 위해서는 변해야 한다\n06-25 ⋯ 충족감\n06-02 ⋯ 불행에 대한 수비력\n04-21 ⋯ 사건의 복리효과\n04-10 ⋯ 비효율을 견디는 능력\n2024 # 12-31 ⋯ 필승법\n12-31 ⋯ 생존법\n12-31 ⋯ 아이러니서클\n12-31 ⋯ 어른들의지휘\n12-31 ⋯ 새롭게얻은 부와 충동\n12-31 ⋯ 다른 행성의 관찰자\n12-31 ⋯ 위기모드\n"},{"id":3,"href":"/docs/hobby/book/_index_prev/","title":"글","section":"글","content":" 글 # 읽었던 재밌는글 모음.\n2025 # 06-02 ⋯ 해소되지 않은 기분은 성격이 된다.\n04-10 ⋯ 사건의 복리효과\n2024 # 12-31 ⋯ 자전거\n12-31 ⋯ summer\n12-31 ⋯ 필승법\n12-31 ⋯ 생존법\n12-31 ⋯ 아이러니서클\n12-31 ⋯ 어른들의지휘\n12-31 ⋯ 새롭게얻은 부와 충동\n12-31 ⋯ 인간의 사교적인 행동을 배우려는 다른 행성의 관찰자\n12-31 ⋯ 위기모드\n12-31 ⋯ 결혼과 행복\n12-31 ⋯ 좋은 것들이 기다리고 있다는 약속\n12-31 ⋯ 그릿을 획득하기 vs 진실로의 창을 열어놓기.\n12-31 ⋯ 자연은 인간의 사정을 봐주지 않는다 vs 운명의 형태를 만드는 것은 사람의 의지다.\n12-31 ⋯ 인테그리티\n12-31 ⋯ 잘할 수 있는 일을 찾기 vs 일을 잘하기.\n12-31 ⋯ 공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다.\n12-31 ⋯ 결핍과 그에 대한 애도의 기간(라디오스타 김영철)\n12-31 ⋯ 모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까?\n12-31 ⋯ 깔끔한 상자 모서리는 든든하지만 환상일 뿐이다.\n12-31 ⋯ 불행한 것과 우울한 것.\n12-31 ⋯ 애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다.\n12-31 ⋯ 진전의 가시화\n12-31 ⋯ 전문가의 세상으로 나가는것에 대한 두려움\n12-31 ⋯ 결국 상황은 나아질것이다\n12-31 ⋯ 우리가 빛의 속도로 갈 수 없다면\n"},{"id":4,"href":"/docs/hobby/movie/","title":"영화","section":"기록","content":" 영화 # 2025 # 2024 # 12-31 ⋯ 콜 미 바이 유어 네임 (2023)\n"},{"id":5,"href":"/docs/study/tech/","title":"생물정보학","section":"공부","content":" 생물정보학 # 2025 # 05-29 ⋯ TFT PyTorch Forecasting - Stallion 튜토리얼 #2\n05-28 ⋯ TFT PyTorch Forecasting - Stallion 튜토리얼\n05-28 ⋯ bashrc script (local / server)\n04-21 ⋯ RNA-seq 전처리 파이프라인 비교 (TopHat2+HTSeq vs Rsubread)\n04-21 ⋯ Bismark로 WGBS 전처리\n04-21 ⋯ ChIP-seq 전처리 파이프라인\n04-21 ⋯ RNA-seq 전처리 꿀조합 (Rsubread, edgeR)\n04-21 ⋯ RNA-seq 전처리 파이프라인 (TopHat, SAMtools, HTSeq)\n04-21 ⋯ Enrichment 분석 - 버블 플롯 시각화 (gProfiler/ggplot2)\n04-21 ⋯ Sleuth 코드\n04-21 ⋯ Kallisto Pseudoalignment 작업\n2024 # 12-31 ⋯ EndNote 사용법\n12-31 ⋯ EBV 유전체 RNA-seq 전처리\n12-31 ⋯ DESeq2 워크플로우\n연구실 메모장 # ﹂Project 1: Density based local clustering algorithm\n﹂Project 2: Antibiotics TFT model\n"},{"id":6,"href":"/docs/study/bioinformatics/","title":"생물정보학","section":"공부","content":" 생물정보학 # 2025 # 05-28 ⋯ bashrc script (local / server)\n04-21 ⋯ RNA-seq 전처리 파이프라인 비교 (TopHat2+HTSeq vs Rsubread)\n04-21 ⋯ Bismark로 WGBS 전처리\n04-21 ⋯ ChIP-seq 전처리 파이프라인\n04-21 ⋯ RNA-seq 전처리 꿀조합 (Rsubread, edgeR)\n04-21 ⋯ RNA-seq 전처리 파이프라인 (TopHat, SAMtools, HTSeq)\n04-21 ⋯ Enrichment 분석 - 버블 플롯 시각화 (gProfiler/ggplot2)\n04-21 ⋯ Sleuth 코드\n04-21 ⋯ Kallisto Pseudoalignment 작업\n2024 # 12-31 ⋯ EndNote 사용법\n12-31 ⋯ EBV 유전체 RNA-seq 전처리\n12-31 ⋯ DESeq2 워크플로우\n생물정보학 메모장 # ﹂Project 1: Density based local clustering algorithm\n﹂Project 2: Antibiotics TFT model\n"},{"id":7,"href":"/docs/study/career/","title":"취업","section":"공부","content":" 취업 # 2025 # 07-01 ⋯ 첫면접\n06-17 ⋯ 6월 17일\n06-16 ⋯ 6월 16일\n06-15 ⋯ 6월 15일\n06-14 ⋯ 6월 14일\n06-11 ⋯ 6월 11일\n06-10 ⋯ 6월 10일\n06-09 ⋯ 6월 9일\n06-08 ⋯ 6월 8일 (+스트레스 받을 이유가 없는이유)\n06-07 ⋯ 6월 7일\n06-06 ⋯ 6월 6일\n06-05 ⋯ 6월 5일 (특이점:외부에쫌많이 흔들림)\n06-05 ⋯ ADsP 45회 응시결과\n01-01 ⋯ 대학원생 면접대비캠프\n취업 메모장 # ﹂원서정리\n"},{"id":8,"href":"/docs/hobby/daily/","title":"일상","section":"기록","content":" 일상 # 카페가고 여행가는 일상\n2025 # 06-21 ⋯ 비오는날의 카페 페이스포포\n06-08 ⋯ 여름경주🍡🌿\n05-20 ⋯ 초여름 부산˚‧｡🐋\n04-28 ⋯ 카페 스페이스임원\n04-12 ⋯ 카페 오퐁드부아 이터리\n03-29 ⋯ 카페 오딘\n02-20 ⋯ 오타루☃️\n2024 # 12-31 ⋯ 블로그 시작 (부제: 제발열심히살자..)\n11-03 ⋯ 경주☘️\n08-26 ⋯ 진도🌾\n06-21 ⋯ 수원/여주🦜🧡\n05-01 ⋯ 제주🏝️\n03-09 ⋯ 엄마랑 갑자기 서울!!\n2023 # 08-18 ⋯ 졸업식 2023\n일상 메모장 # ﹂생각들\n"},{"id":9,"href":"/docs/hobby/baking/","title":"베이킹","section":"기록","content":" 베이킹 # 2024 # 09-10 ⋯ 크림치즈스콘\n08-24 ⋯ 주말아침의 대파치즈스콘\n08-15 ⋯ 황치즈 비스코티\n08-15 ⋯ 포카치아\n08-11 ⋯ 소금빵\n08-08 ⋯ 버터롤빵\n08-07 ⋯ 레몬 쿠키\n08-04 ⋯ 통밀쿠키 / 빼곰스튜디오 쿠키커터\n08-04 ⋯ 휘낭시에\n07-30 ⋯ 무품곰 (무화과 품은 곰) 쿠키\n07-13 ⋯ 홈메이드 그래놀라\n"},{"id":10,"href":"/docs/hobby/favorite/","title":"🌸","section":"기록","content":" 🌸 # 영화/음악/책/영상/쇼핑 취향 모음집 !!\n2025 # 🎧 # 06-21 ⋯ Peder Elias - When I´m Still Getting Over You\n♟️ # 06-25 ⋯ 강지영과 저스틴 민의 속마음ㅣ고나리자 EP.66\n06-21 ⋯ 김하린 harin 데블스플랜2 비하인드 | Q\u0026amp;A\n🛒 # 06-26 ⋯ byemypie 뮤땅이 폰케이스\n🧸 # 07-01 ⋯ 정희원의 라디오 쉼표\n"},{"id":11,"href":"/docs/hobby/etc/","title":"기타","section":"기록","content":" 기타 # 베이킹 # 2024 # 09-10 ⋯ 크림치즈스콘\n08-24 ⋯ 주말아침의 대파치즈스콘\n08-15 ⋯ 황치즈 비스코티\n08-15 ⋯ 포카치아\n08-11 ⋯ 소금빵\n08-08 ⋯ 버터롤빵\n08-07 ⋯ 레몬 쿠키\n08-04 ⋯ 통밀쿠키 / 빼곰스튜디오 쿠키커터\n08-04 ⋯ 휘낭시에\n07-30 ⋯ 무품곰 (무화과 품은 곰) 쿠키\n07-13 ⋯ 홈메이드 그래놀라\n"},{"id":12,"href":"/docs/study/etc/","title":"기타","section":"공부","content":" 기타 # 코테 # 2025 # 06-20 ⋯ #4 완전범죄\n06-17 ⋯ #3 네트워크\n06-17 ⋯ #2 베스트앨범\n06-16 ⋯ #1 완주하지 못한 선수\nML/DL # 06-27 ⋯ #4 Random Forest pseudocode로 이해하기\n06-26 ⋯ #3 Random Forest\n06-26 ⋯ #2 Explainable AI\n06-25 ⋯ #1 DBSCAN\n05-29 ⋯ TFT PyTorch Forecasting - Stallion 튜토리얼 #2\n05-28 ⋯ TFT PyTorch Forecasting - Stallion 튜토리얼\n깃허브 블로그 만들기 # 2025 # 04-09 ⋯ 깃허브 오류 Ubuntu 20.04 brownout\n2024 # 12-31 ⋯ 깃허브 오류 There was an error committing your changes: File could not be edited\n12-31 ⋯ Favicon 변경, Giscus 댓글창 추가\n12-31 ⋯ 사이트 생성, 깃허브 배포\n"},{"id":13,"href":"/docs/study/luck/","title":"﹂#","section":"공부","content":"#취준기록\n06-17 ⋯ 6월 17일\n06-16 ⋯ 6월 16일\n06-15 ⋯ 6월 15일\n06-14 ⋯ 6월 14일\n06-11 ⋯ 6월 11일\n06-10 ⋯ 6월 10일\n06-09 ⋯ 6월 9일\n06-08 ⋯ 6월 8일 (+스트레스 받을 이유가 없는이유)\n06-07 ⋯ 6월 7일\n06-06 ⋯ 6월 6일\n06-05 ⋯ 6월 5일 (특이점:외부에쫌많이 흔들림)\n#시험일정정리\n#원서정리\n#취준수칙 1.외부에 흔들리지말기 2.우울해하기전에 계획을지킬것 3.외로워도조금만참을것\n"},{"id":14,"href":"/docs/hobby/daily/daily2/","title":"너무많은일","section":"일상","content":" 너무많은일 # #2025-04-10\n사람때문에 힘들어진일을 사람한테 치유받고 정신차려서 일상에다시 올라탄 다음에 책을 읽어서 완전한 치유를 받고 +1 성장하고 다음 사이클로 들어가기\n를 마냥 반복중인데 .. 매번 조금씩 베리에이션이 있기때문에 기록은 항상 하는게 좋은거같음\n"},{"id":15,"href":"/docs/hobby/daily/daily1/","title":"블로그 시작 (부제: 제발열심히살자..)","section":"일상","content":" 블로그 시작 (부제: 제발열심히살자..) # #2024-12-31\n최근에 무기력한 기분이 너무 오래가서\u0026hellip; 느슨해지다못해 일시정지해버린 일상에 긴장감을 주기 위해 블로그를 시작한다. 공부도 하기싫고 취준도 하기싫고 구냥 아무것도 하고싶지않다 ㅠㅠ\n오늘도 사실 랩미팅 피피티만들어야되는데 하기싫어서, 전에 오류나서 엎었던 블로그 다시 만들었다. 정말이지 일하는것빼고 다 재밌는듯.\n그리고 독감걸린동안 아무것도 안했는데 내일이면 휴가 끝나니까 그것도 너무 두렵다. 이제 몸은 안아픈데 정신이 아픈거같음.. ㅋㅋ\n일단 지금 해야되는일은\nSQL 공부 (시험일: 3.8) 빅분기 필기 공부 (시험일: 4.5) ODE 모델 논문 찾아놓은거 읽고 피피티 만들기 (제일 급함) TCR-bert 모델 논문 보내주신거 읽기 (제일 많음) 면접대비캠프 신청해놓음 (1.7-1.11) 이정도이고\n이번주 토요일 오전에 미용실가야하고 헬스장은 2일 연속 갔으니까 오늘은 안가도되지만 안가면 내일도 안갈거같긴한데 모르겠다. 근데 피피티 내일 하루만에 만들수있나? 절대 못만들거같은데 오늘은 안하고싶다.\n그래두 앉아서 뭐라도하니깐 무기력한기분은 조금 가시는것같다. 넘조급하지말고 할수있는일을 하자..!!\n"},{"id":16,"href":"/docs/hobby/daily/blog34/","title":"경주🍀","section":"일상","content":" 경주🍀 # #2024-11-03\n"},{"id":17,"href":"/docs/hobby/daily/blog33/","title":"진도🌾","section":"일상","content":" 진도🌾 # #2024-08-26\n"},{"id":18,"href":"/docs/hobby/daily/blog32/","title":"수원/여주🦜🧡","section":"일상","content":" 수원/여주🦜🧡 # #2024-06-21\n"},{"id":19,"href":"/docs/hobby/daily/blog31/","title":"제주🏝️","section":"일상","content":" 제주🏝️ # #2024-05-01\n"},{"id":20,"href":"/docs/hobby/daily/blog35/","title":"엄마랑 갑자기 서울!!","section":"일상","content":" 엄마랑 갑자기 서울!! # #2024-03-09\n"},{"id":21,"href":"/docs/hobby/daily/blog36/","title":"졸업식","section":"일상","content":" 졸업식 # #2023-08-18\n"},{"id":22,"href":"/docs/hobby/book/book43/","title":"*","section":"글","content":" * # #2025-07-02\n#1\n나는왜커리어욕심을 내는지? 내가 중요하게 생각하는것은 사람과 성취감이다. 살면서 주변으로 접하는사람들이랑 좋은시간을 보내는게 행복하고 무언가를 노력해서 얻었을때 행복하다 그리고 그 성취라는게 대단한게 아니라 이쁜쿠키를 구웠을때도 마싯는커피를 내렷을때도 성취감은 든다. 꼭 NLP 모델링이어야하는 이유가 있나?\n면접이 애매했던 이유도 그건거같다 지원동기가 애매했다. 면접관들도 의아했을것이다 지원동기가 명확하지 않은데 어떻게이렇게 의지가명확한지? 사실 이번\u0026rsquo;면접\u0026rsquo;에 붙고싶던 이유는 명확했다 \u0026lsquo;나도 대기업 갈수있는 사람인걸 인정받고 싶어\u0026rsquo; \u0026lsquo;이 연구실에 남아있는것 말고는 할수있는게 없는 사람이 아닌걸 증명하고싶어\u0026rsquo; \u0026lsquo;내가 이정도라는걸 보여주고싶어\u0026rsquo; 하지만 그게 해당 교육과정과 기업에 대한 동기였냐 하면 아니었다 그냥 석사와 취준을하면서 조금내려간 자존감을 이번지원을 통해서 해소하고싶었던거같다. 난내가 입신양명이 꿈이고 그래서 누구보다 독기있게 면접을 봤다고 생각했는데 사실은 무언가 끔찍한것으로부터 도망칠 명분을 얻고싶었던거같다 그게 취업해야하는 상황이든 난이정도는 대체가능한인력이라는사실이라는거든\n#2\n나는 딥러닝, 대기업이 목적인 사람이 아니라 그런 강한 커리어를 통해 내가 강해보이고 싶었던거같다 남들로부터 나를 보호할수있는막이 돼준다고 생각한거같다.\n그런 동기로 뭔가를 시작하는것도 나쁘진 않지만\n내가 커리어를 결정할때마다 남들이 생각했을때 준비한다고 말할때, 붙었다고 말할때 좋게 생각하는 분야여서 이게 나한테 장착됐을때 그만큼 나를 강하게 만들어줄지 남들이 좋아해줄지 그런 것들을 위주로 유행따라 진로를 결정해온거같다(의대-\u0026gt;약대-\u0026gt;AI-\u0026gt;딥러닝). 이 과정이 100% 유행따라일수도 있지만 어느정도 내 적성이 반영된걸수도있고 아닐수도있는건 현재 진로탐색의 한가운데에서 온갖감정과 현실을 정통으로 맞고있는내가 판단할수있는 일은 아닌거같다.\n그치만 그와중에도 가장중요하게고려해야할게 여기라는느낌이든다 남들이 선망하는 직업을 갖춰 나를 강하게 만들어서 주변으로부터 나를 보호하고 내 주변을 외부로부터 보호하려는건 좋지만 내 적성이랑 맞는지? 몰두해서 성취한후 커리어에 반영해도 좋을 일인지는 정말 고민이 필요할거같다.\n"},{"id":23,"href":"/docs/hobby/book/book42/","title":"변하지 않기 위해서는 변해야 한다","section":"글","content":" 변하지 않기 위해서는 변해야 한다 # #2025-07-01\n#1\n난 그냥 나에게 맞는 직업을 찾아 이동했을 뿐이다. 내 주관적인 적성 그 외에는 어떠한 의미 부여도 가치 판단도 하고 싶지 않다.\n#2\n이직을 고민하면서 가장 핵심으로 생각한 질문은 이것이다. ‘내 인생에서 직장과 관련하여 단 하나를 잡는다면 무엇을 잡을 것이냐?’ 돈인가, 명예인가, 여유인가, 전문성인가, 꿈인가.\n난 신이 아니기에 일을 하면서 모든 것을 얻을 수 없다. 돈도 명예도 여유도 전문성도 꿈도 모든 것을 갖고 싶지만 불완전한 인간이기에 당연히 무엇인가는 놓칠 수밖에 없다. 하지만 인간이기에 모든 것을 가지고 싶어 하는 마음도 무시할 수는 없다. 그렇기에 잡지도 못하면서 돈도 명예도 시간도 안정성도 이것저것 다 가져가려고 억지로 잡고 버티다가 고통받고 넘어졌다고 생각한다. 그래서 생각했다. 내가 정말 여유가 없고 힘들고 생존이 위기인 상황이라도 일을 통해서 얻고 싶은 것은 무엇일까? 잡고 있는 것만으로도 행복한 무엇인가를 찾아야 하지 않을까.\n내가 어떠한 고통의 과정과 고민의 과정을 겪고 무엇을 내려놓았는지 그리고 반대로 내가 무엇을 잡았는지 그리고 어떠한 이유로 지금 잡고 있는 ‘단 하나’를 선택하게 되었는지를 허심탄회하게 이야기해 보고자 한다. 적성과 진로, 더 나아가 삶에 고민이 있는 모든 분이 이 책을 읽으면서 잠시나마 공감이 되고 저자의 불행과 행복의 이야기를 통해 본인들의 불행과 행복을 돌아보고 삶의 가장 중요한 ‘단 하나’를 찾는 데에 자그마한 나비의 날갯짓이라도 되었으면 한다\n#3\n인문계를 가긴 싫고 영어를 못하니 외고는 못 가고 과고를 목표로 준비했는데 운이 좋아서 붙었다. 아니, 그만큼 간절하기도 했다. 과학이 하고 싶어서라는 긍정적인 동기부여가 아닌 ‘인문계 가기 싫다’라는 현실적이고 쫓기는 압박이었기 때문에 그런 단기간의 성과를 거둘 수 있었다고 생각한다. 사람은 목숨이 걸려 있을 때 기적적인 힘을 발휘한다고 하는 것을 이때 깨달았다. 학교도 가지 않고 하루에 3시간씩만 자면서 공부했던 이 단기적 성공의 경험은 내 인생에 있어서 큰 도움이 되기도 했지만 때로는 큰 독으로 작용하게 된다.\n*사람은 좋아하는걸 쫓아갈때보다 끔찍한것에서부터 도망칠때 제일 강해지는것같다고 느꼈는데 이사람두 그렇게 적었네..\n실제 대학교 입시 때가 되니 꿈과 희망이 없이 자라 온 자신을 원망하기도 했다. 다들 직업적 꿈을 따라 목표를 따라 자기소개서 쓰고 스펙을 쌓고 가고 싶은 과를 넣는데, 나는 이러한 것들이 아무것도 없었다. 진로와 적성과 관련해서 꿈도 희망도 없는 나는 대학이 원하는 인재가 아니었다. 그렇게 개인적으로는 입시에서 실패와 고통을 겪고 대학에 진학하게 되었다\n#4\n휴학은 예상치도 못하게 1년 반으로 늘어났고, 인생을 허비하고 있단 생각에 군대조차 못 가는 병신이란 생각에 내 자존감을 바닥을 기었다. 친구들은 이미 대학원을 다니고 있었다. 우여곡절 끝에 헌혈을 해서 가산점을 받고 나서야 14년 6월에 입대를 할 수 있었다.\n당시의 나는 모든 것이 너무 늦었다 생각했다. 그래서 너무 조급했다. 군대에서 빨리 나약한 정신머리를 고치고 내 길을 찾겠다고 생각했다. 누구보다 열심히 훈련에 임하였고, 글씨체 연습도 하고, 독서도 다시 열심히 하면서 내 모든 것을 갈아엎었다. 그러나 결국은 또다시 선택에 있어서 비겁한 선택을 하고 말았다.\n생각의 시작부터가 너무 현실적이었고 어떻게 보면 비겁했다. 내가 남들보다 잘하는 것 그리고 한 방에 끝날 수 있는 것을 선택하자. 하지만 그 당시 내 선택이 잘못되었다고 생각하지 않는다. 그냥 경험이 부족했다 생각한다. 내 인생 자체가 저렇게 살아왔으니까. 항상 내가 할 수 있는 노력만큼 투자해서 성과를 내 왔으니까. 그렇게만 목표를 잡아 왔고 그러한 성공의 맛만 봤으니까. 이번에도 그러고 싶었다. 계속해서 꾸준하게 하는 것이 아닌 한 방! 한 번만 또 열심히 올인해서 끝내고 싶었다. 이 진로의 고민을 아니 앞으로의 인생의 고민을….\n그 결과 정말 나에게 어울리는 고시를 보기로 결심했다. 내 학창 시절을 곁에서 접한 사람이면 모두가 이해 못 할 결정이었다. 시험이 문제가 아니라 공무원 생활에 전혀 맞지 않는 사람이기 때문이다. 하지만 나는 이번에도 그런 적성과 관련한 문제는 뒤로 밀어 두었다. 내가 노력하면 고시는 붙을 것 같으니까. 대한민국에서 공부로 인생 역전할 수 있는 방법이라고 들었으니까. 앞서가는 친구들을 따라잡을 유일한 방법이니까. 비싼 로스쿨 등록금, 의전 등록금 없이도 빠르게 사회적 계급을 얻을 수 있으니까. 붙으면 안 잘리고 연금도 나온다니까. 공부는 자신 있었고 공무원이 어떤 직업인지는 몰랐고 알고 싶지도 않았다. 그렇게 사무관이 뭔지도 모르고 공무원이 되고 싶은 것이 아닌 합격자가 되고 싶은 괴상한 고시생이 탄생했다.\n지금 글을 쓰면서 다시 생각해 보니 한심해서 뒤통수를 한 대 치고 싶다. 아니, 그 수준을 넘어서 너무 어이가 없어서 웃음이 나온다. 앞으로 평생을 할 직업인데, 무슨 일을 하는지, 생활은 어떠한지 아무것도 모른 상태에서 그냥 공부가 할 만하니까 결정한 상황. 그런데 슬프게도 한 번 더 생각하니 어이가 없지 않고 너무 합리적이다. 왜냐하면 나는 학교에서 자라면서 한 번도 내 적성에 관한 제대로 된 교육을 받거나 멘토링을 받아 본 적이 없다. 시험기간이라서 공부하고, 고등학교 가야 해서 공부하고, 대학교 가야 해서 공부했을 뿐이다. 그러니 당연히 지금도 직장을 얻기 위해 공부를 한다는 생각뿐이었고 그럼 공부를 할 만한 직장을 택한 것뿐이다. 너무나도 합리적이고 너무나도 이성적이었기에 벌어진 일이었다.\n#5\n어떠한 성향이 옳고 그르다는 것이 아니라 이 단체는 이러한 성향이었는데 나는 그와 다른 성향이었다는 사실을 이야기하고자 한다. 실제로 현실에서 누군가는 직장에 잘 다니고 누군가는 힘들어하는 현상이 발생하는 것은 사람의 성향이 그만큼 다양하고 차이가 크기 때문이라고 생각한다. 각 조직마다 메인 특성이 존재하고 이와 다른 특성을 가진 사람들은 무언가 맞지 않아 직장을 다니는 데 고통을 느끼기 마련이다.\n#6\n연수원 입교 전 맞지 않는 점을 말하며, 나열한 특징들이 모두 연수원 입교 후 공무원이라는 단체가 되면서 엄청나게 더 강해졌다. 분명 300명이 넘는 동기들은 각양각색의 특색을 지니고 있을 테고 일부가 저러한 거지 전부 저러한 것이 아닐 텐데 왜 더 강해지고 그쪽으로 극단적으로 발달했을까. ‘자기주장이 강하지 않으며 눈치를 많이 보고 이미지 관리를 중요시한다’, ‘관습과 규칙을 중요시하고 의문 없이 일단 따른다’ 이 두 가지 특성 때문이다.\n공무원이라는 단체가 되니 개인의 색깔이 더욱 지워지고 모두가 하나의 거대한 단체의 특성을 갖게 되는 것이었다. 나는 불가능한 일이었다. 나와 맞지 않는 단체의 특성을 받아들일 만한 성향이 아니었다. 그래서 더욱 힘들었다. 밖에서 1대1로 만났으면 그래도 어느 정도 적절한 관계가 형성되었을 수도 있는 사이이지만 연수원에서 ‘나’와 ‘공무원 집단’으로 만나게 되니 그 누구와도 친해지기가 힘들었다. 그러한 의심마저 들었다. 지금 친한 사람들도 만약 연수원에서 만나게 되었어도 친해졌을까.\n이러한 상황에 대해 그래프로 설명을 드리는 게 편할 것 같다. 사람의 성향을 나타내는 가상의 X축 그래프를 머릿속으로 상상해 보자. 나는 좌측 끝단에 있는 사람이다. 내가 연수원 입교 전에 만난 사람들은 그래프의 중앙 정도에 있는 사람이다. 그리고 나와 놀아 주느라 내가 있는 좌측으로 더욱 다가왔었을 것이다. 그들의 성향은 어울리는 것이기 때문이다. 나도 어느 정도 물론 중앙 쪽으로 다가갔다. 그렇기에 친해질 수 있었고 큰 고통은 없었다. 하지만 공직이란 단체에 들어가게 되었고 단체의 특성은 우측 끝단에 존재했다. 중앙에 있던 사람조차 모두 우측으로 흡수가 되었다. 그리고 집단이기에 움직이지 않는다. 나는 좌측 끝단에 있기에 집단과 나는 이제 절대 가까워질 수 없는 관계가 된 것이다\n연수원 때 있던 재밌는 일화도 있다. 연수원 중 우즈베키스탄으로 해외 연수를 간 적이 있다. 당시 심정은 국외추방 당하는 기분이었다. 맞지 않는 조원들과 10일간 해외에서 같이 생활해야 한다니 너무 답답하고 스트레스가 극심했다. 실제로 스트레스로 현지에서 장염에 걸려 고열과 설사에 시달리기도 하였다. 아무튼 우즈베키스탄에 도착해서 현지 유적지를 탐방하는 시간이었다. 나는 당연하게도 무리와 떨어져 있었고, 혼자 구경하다가 나무 그늘이 있는 벤치에 앉아 있었다. 그때 그곳에서 일하시는 현지인 3분 정도가 내 주위로 다가와서 말을 걸기 시작했다. 나도 그분들도 짧은 영어로 겨우겨우 의사소통을 했지만 너무나도 즐거웠고, 서로 소리 내며 웃기까지 했다. 그 모습을 보고 조원 중에 기존부터 친하던 친구가 ‘영어 잘하나 보다. 어떻게 그렇게 빨리 친해지냐? 넌 참 신기하다’ 이런 식으로 얘기를 했다. 그리고 그때 나는 많은 것을 느꼈다. 같은 언어를 써도 말이 통하지 않는 사람이 있는 반면, 언어는 통하지 않아도 이렇게 마음속으로 통하는 사람이 있다는 것을 몸으로 느낀 것이다. ‘사람의 성향이라는 것이 이렇게 중요하구나’라고 더더욱 체감한 순간이었다.\n*진짜외로웠을듯 글만봣는데도 느껴짐 ..\n아직까지 나는 속으로 ‘그냥 적당히 일하고 안정적으로 살면 되겠지’, ‘뭐 내가 언제나 여러 사람하고 친하게 지냈나. 대학 때도 친구 없었는데 혼자 잘 지내면 되지’ 하고 있었다. 스스로 외로움을 타지 않고 혼자서 잘 놀고 또 놀면서 잘 살 수 있을 거라 너무 편하게 생각했다. 진짜 외로움이 얼마나 힘든지 몰랐고 나 스스로의 적성과 욕망을 아직 깨닫지 못했기 때문이다.\n#7\n안정이란 \u0026lsquo;변하지 않음\u0026rsquo;에서 오는 것이다. 변하지 않는다는 것은 어쩌면 매력적인 것일지도 모른다. 나 역시 처음에는 변하지 않는 일상, 변하지 않는 수입, 변하지 않는 직책 속에서 위안을 얻으려 했다. 그러나 시간이 지나면서 깨달았다. 진정한 안정이란 단지 외부 조건이 바뀌지 않는 상태가 아니라, 변화하는 세상 속에서도 나 스스로를 지키고 성장시킬 수 있는 능력을 갖추는 것임을. 변화를 수용하고 새로운 것에 적응할 수 있는 힘이 없이는 진정한 안정에 도달할 수 없다는 사실이 마음속에 자리잡기 시작했다. 왜 안정적 조건이 나를 불안하게 하고 불행하게 하는지 점차 이해가 가기 시작했다.\n이렇게 안정과 불안정을 고민하던 중, 나는 한 문구가 떠올랐다. \u0026ldquo;변하지 않기 위해서는 변해야 한다.\u0026rdquo; 그 문장은 내가 나아갈 방향을 제시해 주었다. 나는 나 자신을 지키고, 내가 진정 원하는 삶을 살기 위해 더 큰 변화 속으로 들어가야 한다고 결심했다. 그것이 내가 공직을 떠나기로 한 이유 중 하나였다. 안정된 환경 속에서 나를 정체시키기 보다는, 불안정한 도전 속에서 나를 성장시키는 것이 훨씬 나답다는 것을 깨달았고, 그게 나에게 있어 진정한 안정감을 줄 수 있다고 생각했다. 나에게 있어 불안정이란, 세상과 비교해 변화할 수 없어 뒤처지는 그 상태였고, 반대로 나에게 있어 안정이란 끊임없이 변화를 받아들이고 성장할 수 있는 상태란 것을 깨달았기 때문이다.\n#8\n모든 것을 나 자신이 책임져야 하는 무거운 부담이 있었지만, 동시에 그 속에서 나만의 안정감을 만들어가는 방법을 찾게 되었다. 나에게 안정이란 이제 정해진 조건이나 자리가 아니다. 나의 가치관과 성장, 그리고 나를 끊임없이 변화시키는 도전이 안정의 또 다른 형태임을 깨닫게 되었다. 세상이 변해도 내가 나를 지킬 수 있는 힘, 그것이 내가 찾은 새로운 안정이었다.\n# #출처\n책 5급 사무관을 때려치우다\n"},{"id":24,"href":"/docs/hobby/favorite/favorite5/","title":"정희원의 라디오 쉼표","section":"🌸","content":" 정희원의 라디오 쉼표 # #2025-07-01\n시기적절하게 제2의 왕날편이 생겼다 ㅠㅠ\n인생힘든시기2차가 온거같은 느낌이 들었는데 이번엔 요거 빨로 이겨보면되겠다 근데 사진은 ㅋㅋㅋ 좀너무한거같네 ㅋㅋㅋㅋ\n"},{"id":25,"href":"/docs/study/career/career5/","title":"첫면접","section":"취업","content":" 첫면접 # #2025-07-01\n새로산기여운케이스랑 마고플레인이랑 저속노화좌 없었으면 멘탈 부셔졌을거같은데 다행히 마무리까지끝냈다\n준비하는동안 24기광수책이랑 정희원의저속노화 너무 읽고싶었는데 이제읽을수있으니까 좋다.\n"},{"id":26,"href":"/docs/study/etc/etc4/","title":"#4 Random Forest pseudocode로 이해하기","section":"기타","content":" #4 Random Forest pseudocode로 이해하기 # #2025-06-27\n1. Random Forest 분류 슈도코드 # class RandomForestClassifier: def __init__(self, n_trees, max_features, max_depth): self.n_trees = n_trees # 트리 개수 self.max_features = max_features # 각 노드에서 무작위로 선택할 feature 수 self.max_depth = max_depth # 트리 최대 깊이 self.trees = [] # 의사결정트리 저장 리스트 def fit(self, X, y): for _ in range(self.n_trees): # 1. 부트스트랩 샘플링 (데이터 중복 허용 샘플링) X_sample, y_sample = bootstrap_sample(X, y) # 2. 의사결정트리 학습 (노드마다 무작위 feature 선택) tree = DecisionTree(max_features=self.max_features, max_depth=self.max_depth) tree.fit(X_sample, y_sample) self.trees.append(tree) def predict(self, X): # 각 트리로부터 예측 결과 수집 tree_preds = [tree.predict(X) for tree in self.trees] # 각 샘플에 대해 다수결(Majority Vote) final_preds = [] for i in range(len(X)): votes = [pred[i] for pred in tree_preds] final_preds.append(majority_vote(votes)) return final_preds # 보조 함수 (부트스트랩 샘플링) def bootstrap_sample(X, y): n_samples = len(X) indices = np.random.choice(n_samples, size=n_samples, replace=True) return X[indices], y[indices] # 보조 함수 (다수결) def majority_vote(votes): return most_common_label(votes) 데이터 샘플링 -\u0026gt; 트리 학습 -\u0026gt; 트리들의 예측 결과 수집, 다수결.\n2. Decision Tree 슈도코드 # class DecisionTree: def __init__(self, max_depth=None, max_features=None): self.max_depth = max_depth self.max_features = max_features self.root = None def fit(self, X, y): self.root = self._build_tree(X, y, depth=0) def _build_tree(self, X, y, depth): # 종료 조건: 최대 깊이 도달 또는 y가 모두 동일 if depth == self.max_depth or len(set(y)) == 1: return LeafNode(predicted_class=most_common_label(y)) # 사용할 feature 무작위 선택 features = random_subset(X.shape[1], self.max_features) # 가장 좋은 분할 찾기 best_feat, best_thresh = find_best_split(X, y, features) # 분할 실행 left_indices = X[:, best_feat] \u0026lt; best_thresh right_indices = ~left_indices # 자식 노드 재귀적으로 생성 left = self._build_tree(X[left_indices], y[left_indices], depth + 1) right = self._build_tree(X[right_indices], y[right_indices], depth + 1) return DecisionNode(feature=best_feat, threshold=best_thresh, left=left, right=right) def predict(self, X): return [self._predict_one(x, self.root) for x in X] def _predict_one(self, x, node): # 리프 노드이면 예측값 반환 if isinstance(node, LeafNode): return node.predicted_class # 분기 조건에 따라 왼쪽 또는 오른쪽으로 이동 if x[node.feature] \u0026lt; node.threshold: return self._predict_one(x, node.left) else: return self._predict_one(x, node.right) # 결정 노드 class DecisionNode: def __init__(self, feature, threshold, left, right): self.feature = feature self.threshold = threshold self.left = left self.right = right # 리프 노드 class LeafNode: def __init__(self, predicted_class): self.predicted_class = predicted_class 모든 샘플에 대해 최적 분기(feature, threshold)를 찾음 어떤 feature의 어떤 기준값(숫자)으로 데이터를 양쪽(왼쪽/오른쪽)으로 나누면 가장 ‘좋은’ 분류 결과를 얻을 수 있을까?” 이걸 모든 feature마다, 가능한 threshold마다 다 계산해보고 그 중 가장 좋은 분할을 고른다 좋은 분할 = Gini 감소량 가장 큰 경우 선택 2)Gini 기준으로 impurity가 가장 크게 감소하는 조건 선택\n3)트리를 재귀적으로 확장 (최대 깊이 or 순도가 높을 때 정지)\n4)예측 시, 루트부터 분기 조건 따라 하위 노드로 이동\n5)최종 리프 노드의 라벨이 예측 결과\n"},{"id":27,"href":"/docs/study/etc/etc5/","title":"#5 Confusion matrix","section":"기타","content":" #5 Confusion matrix # #2025-06-27\n1. 정의 # confusion matrix는\n분류 모델의 예측 성능을 상세하게 평가하는 도구 특히 이진 분류에서 매우 유용함 실제: 정상 (0) 실제: 중증 (1) 예측: 정상 (0) TN (True Negative) FN (False Negative) 예측: 중증 (1) FP (False Positive) TP (True Positive) 2. 예시 # 실제: 정상 실제: 중증 예측: 정상 50 10 예측: 중증 5 35 confusion matrix는\n단순한 정확도만으로는 보이지 않는 문제점(FP, FN)을 보여줌 특히 의료나 보안 분야처럼 FN이 위험한 경우 매우 중요하다 (FN: 중증을 정상으로 잘못 예측) 3. 정확도 정밀도 재현율 # 정확도는 높고 정밀도는 높지만 재현율은 낮은 경우\n모델은 예측할 때 매우 신중함 → 중증이라고 판단한 경우는 거의 맞음 하지만 많은 중증 환자를 놓침 → 중증을 적게 예측함 오진율은 낮지만 중요한 대상을 놓침 FN이 많음 비용 큰 치료나 확신이 필요한 상황에 필요!! 정확도는 높고 정밀도는 낮지만 재현율은 높은 경우\n모델은 많은 중증 환자를 잡으려 시도 → 놓침(FN)은 적음 대신 중증이 아닌 것도 중증으로 잘못 예측 → FP 많음 과잉 진료 중증을 절대 놓치면 안 되는 상황에 필요!! 정리\n정밀도: 예측 중 실제 재현율: 실제 중 맞힘 F1 score\n정밀도와 재현율의 조화평균 조화평균인 이유 산술평균은 둘중 하나만 높으면 나쁘지않은 점수가 나옴 조화평균은 균형이 깨지면 페널티 많이 부과. 따라서 F1 Score는 모델이 Precision과 Recall을 동시에 잘하는가?를 평가하는 가장 공정한 방법. "},{"id":28,"href":"/docs/study/tech/tech37/","title":"HLA-peptide interaction affected by mutation of c315 and c442","section":"생물정보학","content":" HLA-peptide interaction affected by mutation of c315 and c442 # #2025-06-27\n1. Load package # import pandas as pd import numpy as np 2. Load affinity data # with open(\u0026#39;/data/home/ysh980101/2411/data-mhc/patient_id.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: patients = [line.strip() for line in file] len(patients) 388 #including reference 3. Merge affinity tables # hotspot = \u0026#34;c315\u0026#34; dfs = [] for pid in patients: file_path = f\u0026#34;/data/home/ysh980101/2411/data-mhc/{hotspot}/{pid}/binding_affinities_HLA-I.csv\u0026#34; df = pd.read_csv(file_path) df.rename(columns={\u0026#39;Affinity\u0026#39;: f\u0026#39;{pid}\u0026#39;}, inplace=True) df.rename(columns={\u0026#39;Peptide\u0026#39;: f\u0026#39;Peptide_{pid}\u0026#39;}, inplace=True) if pid == \u0026#39;reference\u0026#39;: dfs.append(df) else: dfs.append(df[[f\u0026#39;{pid}\u0026#39;]]) #dfs.append(df[[f\u0026#39;{pid}\u0026#39;, f\u0026#39;Peptide_{pid}\u0026#39;]]) res_df = pd.concat(dfs, axis=1) res_df = res_df.set_index(\u0026#39;Allele\u0026#39;) res_df res_df.iloc[:, 1:] = res_df.iloc[:, 1:].subtract(res_df[\u0026#39;reference\u0026#39;], axis=0) res_df res_df.to_csv(f\u0026#34;/data/home/ysh980101/2411/data/{hotspot}/aff-table.csv\u0026#34;) 만든건 저장.\n4. Load peptide data # hotspot = \u0026#34;c315\u0026#34; peptide_df_list = [] for patient in patients: peptide_df = pd.read_csv(f\u0026#34;/data/home/ysh980101/2411/data-mhc/{hotspot}/{patient}/peptides_HLA-I.csv\u0026#34;) peptide_df = peptide_df[~peptide_df[\u0026#39;Peptide\u0026#39;].str.contains(\u0026#39;[-*]\u0026#39;, regex=True, na=False)] patient_name = patient peptide_df = peptide_df[[\u0026#39;Peptide\u0026#39;]] peptide_df.columns = [patient_name] peptide_df_list.append(peptide_df) 5. Merge peptide data # merged_df = pd.concat(peptide_df_list, axis = 1) merged_df.index = f\u0026#34;{hotspot}.\u0026#34; + merged_df.index.astype(str) merged_df merged_df.to_csv(f\u0026#34;/data/home/ysh980101/2412/result/epitope_{hotspot}.csv\u0026#34;) 6. Check affinity between moderate and severe group # "},{"id":29,"href":"/docs/study/career/career3/","title":"SK AX 면접 준비","section":"취업","content":" SK AX 면접 준비 # #2025-06-27\n"},{"id":30,"href":"/docs/study/tech/tech36/","title":"﹂슈도코드","section":"생물정보학","content":" ﹂슈도코드 # #2025-06-26\n#Clustering # def DBSCAN(sequence, eps, min_samples): cores = [] clusters = [] for nt in sequence: neighbors = find_neighbors(nt, eps) if len(neighbors) \u0026gt;= min_samples: label of nt = 1 #core append nt in cores for core in cores: label, clusters = expand_cluster(core, neighbors, eps, min_samples) label of nt = -1 for nt in sequence if not in clusters #noise not in cluster return clusters def MUTCLUST(sequence, eps_scaler, dim_factor, min_samples): ccms = [] hscore = [] deps = [] label = [] clusters = [] for nt in sequence: hscore[nt], deps[nt] = calculate_hscore(nt), calculate_deps(nt) append nt in ccms if select_ccm(hscore, deps, min_samples) for ccm in ccms: label of ccm = 1 #core clusters = expand_cluster(ccm, sequence, eps, min_samples, eps_scaler, dim_factor) label of nt = -1 for nt in sequence if not in clusters #noise return hscore, ccms, clusters #functions used in dbscan() def expand_cluster(cur_nt, cur_neighbors, min_samples, clusters): #expand cluster of cur_nt for ne in cur_neighbors: ne_neighbors = find_neighbors(ne, eps) if ne_neighbors \u0026gt;= min_samples: label of ne = 0 #border append ne in clusters[cur_nt] append ne in cur_neighbors else: label of nt = -1 #noise in cluster append ne in clusters[cur_nt] return label, clusters def find_neighbors(nt, eps): for potential_ne in sequence: append potential_ne in neighbors if euclidean distance \u0026lt;= eps return neighbors #functions used in mutclust() def expand_cluster(cur_nt, cur_neighbors, min_samples, clusters): #expand cluster of cur_nt eps = [] cur_deps = deps[cur_nt] ne = cur_nt while cur_deps \u0026lt; min_samples: ne = next_ne(ne) label of ne = 0 #border append ne in clusters[cur_nt] ne_deps = deps[ne] cur_deps = diminish_deps(cur_deps, ne_deps, dim_factor) #diminish cur_deps by ne_deps eps[cur_nt] = cur_deps return label, clusters def calculate_hscore(): freq, ent, ratio = info.freq, info.ent, info.ratio #frequency, entropy, ratio are pre-calculated hscore = np.log2(ratio * ent * 100 + 1) return hscore def calculate_deps(hscore): #params #EPS_SCALER = 10 deps = ceil(eps_scaler * hscore) return deps def select_ccm(): #params #MIN_MUTATIONS = 5 #CCM_MIN_HSCORE_SUM = 0.05 #CCM_MIN_HSCORE_AVR = 0.01 #CCM_MIN_HSCORE = 0.03 eps_temp = deps[nt] #calculate statistics within eps_temp of nt if count of mutation \u0026lt; MIN_MUTATIONS: return False if sum of hscore \u0026lt; CCM_MIN_HSCORE_SUM: return False if average of hscore \u0026lt; CCM_MIN_HSCORE_AVR: return False if min of hscore \u0026lt; CCM_MIN_HSCORE: return False return True def next_ne(ne): return next nt def diminish_deps(): #params #EPS_SCALER = 10 #DIMINISHING_FACTOR = 3 "},{"id":31,"href":"/docs/hobby/book/book41/","title":"*","section":"글","content":" * # #2025-06-26\n갑자기 튀어나온 나의 솔직한 마음.\n연구실 와서 많이 느낀건데 나는 얼마나 성격이 이상한거지? 싶다. 내딴에는 남들을 불쾌하게 하지 않고 어떻게 생각할까를 엄청 고민해서 말하고 말조심하려고 노력하는데도 평판이 딱히 좋은 편이 아닌거같아서 \u0026hellip; 성격이 얼마나 안좋길래 이렇게 신경을 써도 남들이 불편하게 느끼는 부분이 많은걸까 ㅠㅠ\n어느정도나면 남을 대하는 행위가 검열해야할것이 너무 많은 일이라 피로도가 높아서, 할일이 많거나 기분이 안좋거나 하는 이유로 1) 기분을 좋게하거나 할일을 하기 2) 사람을 대하기 이 두가지를 동시에 하는게 버거운 날이 있고\n그런 날이면 남들에게 혹여나 실수할까봐 남들이 없는곳으로 도망가서 조용히 있는다. 난 이렇게까지 하는데도 왜 이질감이 들까?\n하고싶은 말을 안하고 하면 좋을것같은 말만 하는게 스트레스긴하지만 남들한테 거슬리고 피해주고 불쾌하게하고싶지 않아서 그로부터 오는 스트레스는 내가 기꺼이 감수하고 싶다. 남한테 피해를 준다는 사실이 장기적으로는 나한테 더 우울한 일이 될것이기 때문에.\n근데 한번씩은 너무 힘든거같다\n오빠랑 얘기를 하면 오빠는 너는너무평범한데 먼소리지 하고 무시를하는데 그말을 들으면 마음이 너무너무 편해져서 매일 하루에 10번씩 물어보고 답을 듣고싶은 정도다. 그렇다고 진짜 10번 물을순없고 한번씩 내가 좋아하는 사람 입에서 나온 그말을 들을 수 있어서 정말 다행이고 감사하다. 정말 필요할때는 그 말을 들을 수 있다는 사실 자체만으로 평소에 힘들때 많이 힘이 되는거같다.\n문제가 많은 사람으로 사는건 좀 힘든일이다 틀어막고 쌓아두고 사는건 생각보다 숨차는 일이다.\n그래도 한번씩 혼자 답답한게 터지고 외롭고 슬프고 그래도 그럴때 책을읽거나 좋은유튜브영상을보거나 울거나 하면 또 다음사이클 돌기전까진 괜찮으니까 아직까진 내 선에서 처리하는게 마음이 편한것같다.\n섞여있을때 편안한 기분이 들면 그것도 한번씩은 도움이 된다.\n근데 요즘처럼 여유가 없고 힘들고 슬플때는 사람들 사이에 있는게 좀 버겁긴 한거같다. 좀 힘들어서 이 기간이 얼른 사그라들었으면 좋겠다.\n근데 왜 갑자기 쓰고싶어졌냐? 하면 황온후라고 나솔에 광수로나온 사람이 있는데 그사람 말하는게 너무 이해되고 공감돼서 눈에띄었었고 인스타도 조금씩 구경하고 했는데 이사람이 책을 썼길래,\n안그래도 오늘 울적하고 마음이혼란하고 슬펐는데 이건 읽어야해!!!!! 하고 밀리의 서재에 있길래 바로 읽었다. 라운지에 누워서 한 한시간 넘게 읽으면서 엄청 울었고 모든 부분에 밑줄을 그을수가 없어서 그냥 계속 눈에 담았다.\n그사람이 위로를 건네고 싶은게 느껴져서 눈물이 엄청 났다. 힘들었던 얘기를 솔직하게 털어놓으면서 마음을 정리하고 상황을 이해하면 남과 세상을 미워하는게 아니라 나한테 맞는 옷을 찾아 입고 내 마음을 뒤틀리지 않게 활자로 뱉어놓을 수 있다. 그걸 느껴서 밤에 갑자기 생각을 쓰게된거같다.\n방금까지는 있는그대로의 나를 원하는 곳이 아무데도 없구나 라고 생각하면서 우울하고 불안정하고 슬펐는데 글로 적고 나니까 상황은 그대로지만 감정이 약간은 사그라든것같다\n젤중요한건\n판단력이 너무 흐려지지 말자 원인이 너무 즐거움이든 너무 불행함이든 판단력을 지켜야 나를지킬수있다\n"},{"id":32,"href":"/docs/study/etc/etc2/","title":"#2 Explainable AI","section":"기타","content":" #2 Explainable AI # #2025-06-26\n1. Explainable AI란? # Explainable AI는 인공지능(AI) 또는 머신러닝(ML) 모델이 어떤 방식으로 특정 결과를 도출했는지 사람이 이해할 수 있도록 설명하는 기술과 방법론.\n2. XAI 기법 분류 # 모델 구조\nIntrinsic:\t모델 자체가 설명 가능한 구조 (예: 의사결정나무, 선형회귀 등) Post-hoc:\t모델 학습 후 별도로 설명 생성 (예: SHAP, LIME) 대상 Global:\t전체 모델의 작동 원리를 설명 Local:\t특정 샘플의 예측 결과를 설명 3. 주요 Post-hoc 설명 기법 # LIME (Local Interpretable Model-Agnostic Explanations): 주변 입력을 랜덤하게 생성하고, 단순 모델(선형 회귀 등)을 학습해 근사\nSHAP (SHapley Additive exPlanations): 게임 이론의 샤플리 값 기반\n각 피처가 기여한 정도를 공정하게 분배하여 설명 장점: 수학적으로 정당성 확보, 일관된 설명 제공 단점: 계산 비용 큼 Permutation Importance: 입력 피처를 무작위로 섞은 후 예측 성능 감소 정도 측정\n예측 성능이 크게 감소하면, 중요한 피처로 판단 Saliency Maps (이미지 분야)\n4. SHAP이란? # 각 feature(입력 변수)가 모델의 예측값에 얼마나 기여했는지 정량적으로 계산.\n원래는 협력 게임 이론에서\n여러 플레이어가 팀을 이뤄 보상을 받았을 때, 각 플레이어가 전체 보상에 얼마나 기여했는지를 계산하는 방법인데 예시로 축구 게임을 하여 팀 전체가 100점을 획득했다고 가정 팀에는 선수 A, B, C가 있다 누가 더 중요한 선수인지, 각 선수가 점수에 얼마나 기여했는지를 구하려면? 샤플리 값은 다음 순서로 기여도를 평가: 가능한 모든 순열을 고려 각 순열에서 A, B, C가 언제 팀에 합류했는지 그 선수가 팀에 들어오면서 얼마나 점수가 늘었는지 확인 -\u0026gt; 각 선수의 이 평균 기여도를 “샤플리 값”이라고 함. 머신러닝 모델에서:\n각 feature가 플레이어 역할을 함 모델의 예측값이 팀이 얻은 점수이고 SHAP은 \u0026ldquo;이 예측값이 나오는 데, 각 feature가 얼마나 기여했는가?\u0026ldquo;를 계산. 예를 들어 모델이 어떤 환자의 사망 확률을 80%라고 예측했을 때 나이: +15% 흡연 여부: +10% 혈압: +5% 기본값: 50% 총합: 50% + 15 + 10 + 5 = 80% 즉 SHAP은 예측값을 base value + 각 feature의 기여도로 분해해준다. 수학적 계산 과정:\n3개의 feature (A, B, C)에서 가능한 feature 조합: {}, {A}, {B}, {C}, {A,B}, {A,C}, {B,C}, {A,B,C} SHAP은 모든 조합에 대해, 해당 feature가 들어갔을 때와 안 들어갔을 때 예측값 차이를 계산 이를 평균하여 기여도(샤플리 값)로 설정한다. 단점: 조합 수가 2^M이라서 feature 수가 많으면 계산량 폭발 5. RF의 feature importance와의 차이? # 항목 Random Forest Feature Importance SHAP 기반 개념 모델 구조 기반 (gini 감소 등) 게임 이론 기반 (샤플리 값) 설명 방식 전체 모델 수준 (global) 전체 + 개별 샘플 수준 (global + local) 음/양 구분 없음 (0 이상, 크기만 제공) 있음 (양수: 예측 ↑, 음수: 예측 ↓) 상호작용 고려 부분적으로만 고려 일부 고려 가능 (특정 SHAP variant) 정확성 대략적인 영향도 수학적으로 보장된 기여도 단점 bias 있음 (범주 수 많은 변수 선호 등) 느릴 수 있음, 계산 비용 높음 Random Forest의 Feature Importance\n작동 방식 RF는 다수의 결정트리를 만들고 각 트리에서 어떤 feature를 쪼갤 때 예측 성능이 얼마나 좋아졌는지(ex: Gini impurity 감소량)를 기록. 여러 트리에서 해당 feature가 얼마나 자주, 얼마나 크게 성능 향상에 기여했는지를 평균하여 importance로 계산 단점 범주 수가 많은 feature가 유리 (더 잘 쪼갤 확률 높음) 상호작용 고려 부족 왜 중요했는지 설명 불가 개별 샘플 설명 불가 SHAP의 Feature Importance\nSHAP은 다음을 제공: 각 feature가 개별 예측값에 얼마나 영향을 줬는지. 양/음 포함. 모든 샘플에 대해 계산한 후 평균을 내면, global feature importance가 됨. 왜 중요했는지 샘플별로 추적 가능 먼소린지 이해 안돼서.. 직관적 예시.\nRandom Forest의 Feature Importance는 누가 결정 과정에 자주 참여했는지 본다, 마치 회의에서 많이 말한 사람을 중요한 사람이라고 보는 것과 같음. SHAP의 Feature Importance는 누가 실제로 의사결정 결과에 영향을 줬는지 본다, 마치 회의에서 실제로 투표를 바꿔놓은 사람을 중요한 사람으로 보는 것과 같음. 즉 RF는 참여 횟수, SHAP은 결과에 기여한 정도를 보는 거예요. 모델 예시\n모델이 환자의 사망 확률을 예측할때 환자 입력: 나이 80세 / 체온 39도 / 혈압 100 / 흡연 여부 Yes Random Forest는? 100개의 트리에서 나이로 70번 쪼갬 / 체온으로 10번 쪼갬 / 혈압으로 15번 쪼갬 / 흡연 여부로 5번 쪼갬 그래서 나이가 제일 중요하다고 판단 (근데 ‘나이’가 예측값에 얼마나 영향을 줬는지는 모름) SHAP은? 이 환자의 예측값은 0.80 (기본값은 0.50) 기여도: 나이 +0.20 / 체온: +0.10 / 흡연: +0.08 / 혈압: -0.08 합치면: 0.50 + 0.20 + 0.10 + 0.08 - 0.08 = 0.80 즉 SHAP은 예측값이 왜 0.80이 되었는지 명확하게 설명. 하나의 feature라도 값이 높을 때 어떤 경우엔 예측을 ↑ 어떤 경우엔 예측을 ↓ 시킬 수 있다 그리고 이 복잡한 관계를 시각적으로 한 번에 보여주는 것이 바로 SHAP의 summary plot.\n\u0026lsquo;체온\u0026rsquo; feature로 예시.\n체온 ───🔵🔵🔵🔴🔴🔴🔴🔴🔵🔵──▶ (SHAP 값: 음수~양수) 🔴: 체온이 높은 샘플들\n오른쪽(+)에 위치한 🔴: 체온이 높아서 예측값(사망 확률)이 증가 왼쪽(-)에 위치한 🔴: 체온이 높지만 예측값은 오히려 감소 🔵: 체온이 낮은 샘플들 오른쪽(+)에 위치한 🔵: 체온이 낮지만 예측값은 증가 왼쪽(-)에 위치한 🔵: 체온이 낮고 예측값도 낮음 샘플마다 기여도와 방향이 다른 이유\nSHAP은 모든 feature를 \u0026ldquo;다른 feature와 함께 썼을 때\u0026quot;의 영향력을 따진다. 그래서 \u0026ldquo;조건부 기여도\u0026quot;라고도 함. 즉 체온이 높아도 젊은 환자라면 사망 확률이 낮을 수 있고 체온이 낮아도 기저질환이 심한 환자라면 사망 확률이 높을 수 있다 이런 복잡한 상호작용을 반영하다보니 같은 feature라도 샘플마다 기여 방향이 다를 수 있다. 6. 결론 # 같은 feature라도, 샘플마다 다른 상황(context)이기 때문에, 그 feature의 예측에 대한 기여 방향(↑ 또는 ↓)이 달라질 수 있다.\n예를 들어\n나이 = 70인 사람이라도 다른 feature(혈압, 체온, 기저질환 등)에 따라 어떤 샘플에선 사망 확률 ↑에 기여 (양의 SHAP 값) 어떤 샘플에선 사망 확률 ↓에 기여 (음의 SHAP 값) 그래서 SHAP summary plot에서 같은 feature의 🔴와 🔵 점들이 좌우로 흩어져 있다. 정리하면?\nSHAP은 \u0026ldquo;같은 feature\u0026quot;가 \u0026ldquo;다양한 맥락에서 어떻게 작용하는가\u0026quot;를 보여주는 도구이다. 7. 예시 코드 # import pandas as pd import numpy as np import pickle import joblib import shap import matplotlib.pyplot as plt import seaborn as sns #Load rf model with open(\u0026#39;/model/rf_model.pkl\u0026#39;,\u0026#39;rb\u0026#39;) as f: rf_model = joblib.load(f) #Load dataset with open(\u0026#39;/preprocessing/processed_data.pickle\u0026#39;,\u0026#39;rb\u0026#39;) as f: preproc_data = pickle.load(f) cytokine_df = preproc_data[\u0026#39;cytokine_data\u0026#39;] patient_meta = preproc_data[\u0026#39;metadata\u0026#39;] patient_info = preproc_data[\u0026#39;clinical\u0026#39;] # Get feature importances importances = rf_model.feature_importances_ feature_names = cytokine_df.columns feature_importances = pd.DataFrame({\u0026#39;feature\u0026#39;: feature_names, \u0026#39;importance\u0026#39;: importances}) # Sort the feature importances in descending order and select the top 20 top_20_features = feature_importances.sort_values(by=\u0026#39;importance\u0026#39;, ascending=False).head(20) # Plot the top 20 feature importances plt.figure(figsize=(6, 10)) sns.barplot(x=\u0026#39;importance\u0026#39;, y=\u0026#39;feature\u0026#39;, data=top_20_features) plt.show() RF 내부의 feature importance를 시각화\n어떤 사이토카인이 모델에서 자주 쓰였는지(중요한지)를 보여줌 이 값은 SHAP처럼 \u0026ldquo;예측에 얼마나 기여했는가\u0026quot;를 나타내지 않고, 단순히 \u0026ldquo;쪼개는 데 많이 쓰였는가\u0026rdquo; 기준. tree_explainer = shap.TreeExplainer(rf_model) ## TreeExplainer shap_values = tree_explainer.shap_values(cytokine_df) ## SHAP Value 이진 분류 모델(Random Forest, XGBoost 등)에 shap.TreeExplainer를 적용하면\n이 shap_values는 리스트 2개로 구성: shap_values[0]: 클래스 0 (음성 클래스)에 대한 SHAP 값 shap_values[1]: 클래스 1 (양성 클래스)에 대한 SHAP 값 fig = plt.figure(figsize=(8,8)) fig.set_facecolor(\u0026#39;white\u0026#39;) ax = fig.add_subplot() #Plot SHAP as sever probability shap.summary_plot(shap_values[1], cytokine_df, cmap=\u0026#39;bwr\u0026#39;, show=False, plot_type=\u0026#39;dot\u0026#39;) ax.set_xlabel(\u0026#39;SHAP Value\u0026#39;) ax.set_title(\u0026#39;SHAP Dot Plot\u0026#39;, fontsize=20) plt.show() shap_values[1]: 이진 분류에서 양성 클래스에 대한 SHAP 값\nsummary plot: 각 feature가 예측에 미친 영향(양/음, 세기)을 샘플별로 시각화 (빨강=값 큼, 파랑=값 작음)\nshap_df = pd.DataFrame(shap_values[1],columns = cytokine_df.columns) shap_df.index = cytokine_df.index shap_df import umap.umap_ as umap import pandas as pd import matplotlib.pyplot as plt import seaborn as sns reducer = umap.UMAP() embedding = reducer.fit_transform(shap_df) import matplotlib.pyplot as plt # Extract UMAP coordinates and labels umap_x = embedding[:, 0] umap_y = embedding[:, 1] # Create scatter plot plt.figure(figsize=(10, 8)) scatter = plt.scatter(umap_x, umap_y, cmap=\u0026#34;bwr\u0026#34;, s=50, alpha=0.7, edgecolors=\u0026#34;w\u0026#34;, linewidth=0.5) from sklearn.cluster import DBSCAN # Initialize DBSCAN dbscan = DBSCAN(eps=0.8, min_samples=3) # partial data is too small to set min_sample=20. # Fit to UMAP data and get cluster labels clusters = dbscan.fit_predict(embedding) embedding, clusters (array([[16.714314 , -2.0475426], [17.279623 , -2.4140635], [16.705837 , -3.002305 ], [17.19955 , -1.342096 ], [17.838465 , -2.021136 ], [18.537838 , -1.5079662], [21.44188 , -2.1259143], [21.123413 , -3.075382 ], [20.373632 , -3.0233152], [21.83852 , -2.899527 ], [20.435349 , -2.2629123]], dtype=float32), array([ 0, 0, -1, -1, 0, -1, -1, 1, 1, 1, 1])) plt.figure(figsize=(10, 6)) unique_clusters = np.unique(clusters) for cluster in unique_clusters: idx = clusters == cluster plt.scatter(embedding[idx, 0], embedding[idx, 1], label=f\u0026#39;Cluster {cluster}\u0026#39;) plt.title(\u0026#39;Scatter Plot of UMAP Colored by Cluster\u0026#39;) plt.xlabel(\u0026#39;UMAP_1\u0026#39;) plt.ylabel(\u0026#39;UMAP_2\u0026#39;) plt.legend() plt.grid(True) plt.show() "},{"id":33,"href":"/docs/study/etc/etc3/","title":"#3 Random Forest","section":"기타","content":" #3 Random Forest # #2025-06-26\n1. Random Forest의 분류와 회귀 # 랜덤 포레스트(Random Forest)는\nRandomForestClassifier: 분류용 RandomForestRegressor: 회귀용 이다. 분류와 회귀의 핵심 차이는\n분류는 각 leaf node에 속한 클래스의 비율을 기반으로 확률 예측 회귀는 leaf node에 있는 target 값들의 평균을 예측값으로 사용 랜덤 포레스트의 트리 구조(= 리프 분기 방식)는 분류나 회귀나 똑같고\n단지 리프 노드에 어떤 데이터 형식이 들어가느냐에 따라 분류이면 라벨 비율(확률 분포) 회귀이면 값의 평균으로 예측을 내놓는다 2. 트리 기반 모델과 클러스터링의 차이 # 랜덤 포레스트(혹은 결정 트리)의 리프 분기 방식은 \u0026lsquo;거리 기반\u0026rsquo;이 아님\n대신, 목표 변수(y)를 가장 잘 구분할 수 있도록 feature를 기준으로 데이터 공간을 분할한다. 분기 기준\n분류 문제\nGini 불순도, Entropy(정보이득) 등을 기준으로 분기를 통해 클래스가 더 순수하게 나뉘도록 자름 회귀 문제\nMSE (Mean Squared Error) 또는 MAE (평균 절댓값 오차) 감소가 큰 방향으로 분기 분기의 본질은 분기를 할 때 두 점 사이의 거리를 따지지 않음. 대신 \u0026ldquo;어떤 feature에서 자르면, y가 더 잘 나눠지냐\u0026quot;만을 고려함.\n궁극적으로 차이점\n항목 결정 트리 / 랜덤 포레스트 계층적 클러스터링 학습 방식 지도 학습 (y 필요) 비지도 학습 (y 없음) 분기 기준 y를 잘 나누는 feature 기준 입력 간 거리 기준 분할 구조 트리 구조 (특정 feature 기준 분할) 덴드로그램 구조 (거리 기반 병합/분할) 목적 예측 성능 향상 그룹 내 유사성 확보 거리 개념 사용 안 함 핵심 기준 학습 방식이\nRF는 지도 학습으로 y필요, 클러스터링은 비지도 학습으로 y 불필요 분기 기준이 RF는 y를 잘 나누는 feature 기준, 클러스터링은 입력 간 거리 기준 목적이 RF는 얘측 성능 향상, 클러스터링은 그룹 내 유사성 확보. 3. 트리 기반 모델과 클러스터링의 차이 (2) # 랜덤 포레스트(RF)의 분기 조건이 리프 내 순도(클래스의 동질성)를 높이는 거라면, 클러스터링의 목적(그룹 내 유사성 확보)과 본질적으로 같은 거 아닌가?\n핵심 차이 1: 무엇을 기준으로 유사하다고 보는지.\n결정트리 (RF)는 \u0026ldquo;예측값 y가 비슷하면 유사하다\u0026quot;고 생각함. 즉, 입력 X가 다르더라도 y가 비슷하면 같은 노드로 분기 클러스터링은 \u0026ldquo;입력 값 X가 비슷하면 유사하다\u0026quot;고 생각함 즉 y는 고려하지 않음 예를 들어 두 환자의 면역 프로파일이 완전히 달라도 둘 다 사망(y=1)이라면, RF는 둘을 같은 리프에 보낼 수 있다. 반대로 클러스터링은 면역 프로파일이 다르면 y와 무관하게 다른 그룹으로 나눈다. 핵심 차이 2: 지도 vs 비지도\nRF는 정답(y)이 있는 지도학습이고 클러스터링은 y 없이 입력 X의 분포만으로 구조를 파악 즉 클러스터링은 \u0026ldquo;데이터 간 관계\u0026quot;에 집중, RF는 \u0026ldquo;데이터와 정답 간 관계\u0026quot;에 집중 예시\nFeature X1, X2로 된 점 100개 Class 0/1 이 섞여 있음 Random Forest: 어떤 feature (예: X1 \u0026lt; 5)로 나눴더니 클래스 0/1이 잘 나뉜다 -\u0026gt; 분기 수행. 이 과정에서 X 간의 거리나 모양은 고려 안 한다. 클러스터링: X1, X2 기준으로 거리상 가까운 점들끼리 묶음. 클래스(y) 정보는 전혀 고려하지 않는다. 비슷해보이는 이유\n결정트리는 리프 내 클래스가 비슷해지도록 데이터를 쪼개다 보니 결국 리프 안의 X 값들도 어느 정도 비슷해지는 경향이 발생. 이 때문에 시각적으로 보면 \u0026ldquo;트리가 일종의 분할 기반 클러스터링\u0026quot;처럼 보이기도 함 특히, y 자체가 X의 분포에 강하게 의존할 경우에는 트리 분기 ≈ 거리 기반 분할처럼 보인다. 하지만 유사성이 목표인지 수단인지가 다름:\n클러스터링은\t목표 자체 결정 트리 / RF는 예측을 위한 수단. 둘 다 \u0026ldquo;비슷한 것들끼리 묶는다\u0026quot;는 점에서 결과적으로 유사한 구조를 만들 수 있지만 클러스터링은 유사성 자체가 목적 결정 트리는 예측을 위한 수단으로 유사한 샘플을 묶을 뿐. "},{"id":34,"href":"/docs/hobby/favorite/favorite4/","title":"byemypie 뮤땅이 폰케이스","section":"🌸","content":" byemypie 뮤땅이 폰케이스 # #2025-06-26\n바이마이파이에서 신상 케이스중에 기여운게 있길래, 꽤괜 하고 보고있었는데 즉시 구매할 정도는 아니었다.\n근데\u0026hellip;\n후기샷 보고싶어서 인스타에 뮤땅이 쳤다가\n실물 냥이인 뮤땅이 사진들을 봐버렸고 ㅠㅠ 이제 얘가 너무기여워져버려서 저항없이 구매를 하였다\u0026hellip;ㅎㅎㅎ\n너무 기엽고 상세페이지에 이 사진들 넣었으면 요상품 5배는 더 팔렸을거라고 생각한다. ㅡㅡ\n에어팟 케이스까진 살생각 없었는데 이것마저 눈독들이고 있다.\n그리고 다른 버전이나 다른상품도 더 나왔으면 좋겠고 갠적으로 맥북 스티커 나오면 진짜 좋을거같다 \u0026hellip; 회색냥이라 실버 맥북에 너무너무 잘어울릴듯.\n#구매링크\nhttps://m.byemypie.kr/product/%EB%AE%A4%EB%95%85%EC%9D%B4/432/category/54/display/1/\n"},{"id":35,"href":"/docs/study/career/career2/","title":"SK AX 면접 준비","section":"취업","content":" SK AX 면접 준비 # #2025-06-26\n1. SK 면접질문 모음 # #1 https://community.linkareer.com/employment_data/2376979\nSK AX 관련해서 관심있게 본 뉴스 있는가?\n스트레스 해소법\n상관의 부당한 업무 지시에 대한 대처\n**야근이나 주말 근무도 할 수 있는가?\n**직무에 지원한 이유?\n지원자께서는 AI 서비스개발 직무는 무엇이라고 생각하십니까?\n지원자의 지금까지 가장 힘들었던 경험은 무엇입니까?\n마지막으로 반드시 하고 싶은 말은 무엇입니까?\n*자소서 각 항목에 대해 꼬리 질문. 물어본 질문에 대한 정답성 발언을 두괄식으로 제시하면서 부연 설명으로 구체적인 본인의 진솔된 일화나 예시를 언급해줘야 면접관들이 정답만이 아닌 진심을 말하는 것으로 인식한다.\n*가장 힘들었던 경험을 물어보는데 최대한 진솔되게 답변하는 것이 높은 점수를 받는 방법.\n#2 https://community.linkareer.com/mento_sk/2334776\n*꼬리 질문은 전형적인 SK형 압박 질문. 주로 자기소개서에서 면접관이 확인하고 싶은 / 검증하고 싶은 부분에 대해서 지속적으로 확인 질문이 들어온다. 따라서 본인이 자기소개서에 작성한 프로젝트/실적에 대해서 정확하게 설명하는 연습을 진행하는 것을 추천.\n#3 https://jasoseol.com/blog/post/sk%ED%95%98%EC%9D%B4%EB%8B%89%EC%8A%A4-%EB%A9%B4%EC%A0%91-%EC%A7%88%EB%AC%B8-%EB%A6%AC%EC%8A%A4%ED%8A%B8-%ED%95%A9%EA%B2%A9-%ED%9B%84%EA%B8%B0-%EB%AA%A8%EC%9D%8C/\n남은 당신을 어떤 사람으로 평가하나요?\n그 연구 분야를 선택한 이유가 무엇인가요?\n#4 https://www.jobkorea.co.kr/starter/review/view?C_Idx=42\u0026Crr_Year=\u0026Half_Year_Type_Code=0\u0026Ctgr_Code=5\u0026FavorCo_Stat=0\u0026G_ID=0\u0026Page=1\u0026itv_Qstn_Type_Code=3\n해당 직무에 언제부터 관심을 가지게 된 건가요?\n최근에 해당 직무 관련하여 관심 있게 읽은 기사나 논문이 있나요?\n학교를 다니면서 스트레스를 받을 때는 언제이고, 그럴 때 어떻게 했나요?\n장점으로 문제해결력, 소통 얘기하셨는데 구체적인 사례는?\n본인은 리드하는 타입인가요?\n실패했던 사례에 대해 말해보세요.\n왜 남들보다 졸업이 늦어졌는지 말해보세요.\n자신의 강점이 무엇이라고 생각하시는지?\n**협업에서 가장 중요한 것은?\n본인의 별명이 뭔가요?\n지원한 직무에서 이루고 싶은 목표는 무엇인가요?\n최근 진취적으로 한 일이 무엇인가요?\n엔트로피에 대해 설명해보세요.\n본인의 생활신조는 무엇인가요?\n일 많이하고 돈 많이주는 회사와 본인의 가치를 알아봐주고 본인이 하고 싶은 일을 하게 해주는 회사 중 어디를 선택할건가?\n인생의 목표?\n살면서 힘들었던 경험이 있는가? 어떻게 극복하였는가?\n2. 면접후기 # https://www.jobkorea.co.kr/starter/review/view?C_Idx=42\u0026Half_Year_Type_Code=0\u0026Ctgr_Code=3\u0026FavorCo_Stat=0\u0026G_ID=0\u0026Page=1\n"},{"id":36,"href":"/docs/study/etc/etc1/","title":"#1 DBSCAN","section":"기타","content":" #1 DBSCAN # #2025-06-25\n개념 # DBSCAN은 밀도 기반 클러스터링 알고리즘으로\n데이터가 밀집된 영역을 클러스터로 인식하고 밀도가 낮은 영역은 노이즈(이상치)로 간주하는 방법. KMeans와 달리, 군집 수를 미리 정하지 않아도 되며,\n비선형 구조나 잡음이 있는 데이터에서 잘 작동한다. 파라미터와 핵심 용어 # 주요 파라미터는 2개\neps: 반지름 거리. 한 점에서 eps 거리 내에 있는 점들을 \u0026ldquo;이웃\u0026quot;이라고 판단. min_samples: core point로 인정되기 위해 필요한 최소 이웃 수 핵심 용어는 3개\nCore Point (중심점): eps 거리 내에 min_samples 이상 이웃이 있는 점 Border Point (경계점): core point의 eps 거리 내에 있으나, 자기 자신은 core point가 아닌 점 Noise Point (잡음점): 어떤 core point의 eps 안에도 포함되지 않는 점 장점과 단점 # 장점 4개\n자동 군집수 결정 이상치 탐지 가능 복잡한 클러스터 형태 탐지 비지도 학습 단점 3개\neps 값 설정이 민감함 밀도가 다른 클러스터는 잘 분리 못함 (밀도 기준이 하나뿐이라 불균형 분포에 약함) 고차원 데이터에선 거리 개념이 희석되므로 차원 축소(t-SNE, PCA 등) 필요. Q\u0026amp;A # Q1) DBSCAN은 몇차원에서 제일 효율적인가?\nA1)\n2(~3)차원에서 가장 효율적.\n거리 개념이 명확하고 시각화 가능 시각화 가능 -\u0026gt; 시각화 통해 군집 구조 확인 가능 -\u0026gt; eps 직관적으로 조정 가능 4~10차원에서 점점 어려워짐.\n거리 분포가 평평해지고, core point 조건을 충족시키기 어려움 유클리드 거리 기반 eps 조정이 매우 민감해짐 차원 축소(PCA, t-SNE, UMAP) 후 사용 추천 10차원 이상\n거리 희소성(dimensionality curse): 모든 점 간 거리가 비슷해져 밀도 기반 판별이 어려워짐 eps와 min_samples 조합이 성능에 큰 영향을 주며, 조정이 어렵고 불안정함 고차원에선 DBSCAN보다 HDBSCAN, Spectral Clustering, 또는 Spherical KMeans 등을 고려 / 또는 차원 축소를 선행한 후 DBSCAN 사용 Q2) 파라미터 선택법?\nA2)\n이론적 기준으로 min_samples=2*d를 적용해서 min_samples 후보값을 정함 k = min_samples-1로 설정하여 k-distance plot을 그림 elbow point을 찾아 eps를 결정 다양한 min_samples로 그래프를 여러 번 그려보고 -\u0026gt; 가장 뚜렷한 elbow point을 주는 min_samples를 선택 성능 평가 # DBSCAN은 비지도 학습 알고리즘이기 때문에, 성능 평가에 있어서 supervised 방식과는 다른 접근이 필요\n내부 평가 지표\nSilhouette Score 각 점이 속한 클러스터 내부 응집도와, 가장 가까운 다른 클러스터와의 거리 차이를 비교 -1 ~ 1 (1: 잘 클러스터됨, 0: 경계에 있음) Davies-Bouldin Index 클러스터 간 간격이 멀고, 내부 응집도가 높을수록 좋은 값 값이 작을수록 우수 Calinski-Harabasz Index 클러스터 간 분산 / 클러스터 내 분산 비율 값이 클수록 좋은 클러스터링 외부 평가 지표 (만약 정답 레이블이 있다면 다음 지표들도 사용 가능)\nAdjusted Rand Index (ARI): 무작위 군집과 비교하여 클러스터 일치 정도 확인 (1에 가까울수록 좋음) Normalized Mutual Information (NMI): 군집 정보가 얼마나 label과 유사한지 확인 Fowlkes–Mallows index (FMI): TP 기준 군집 일치 정도 시각화 기반 평가\n2D나 t-SNE로 클러스터링 결과 시각화해서, 클러스터 모양, 분리 정도 노이즈의 위치 분포 군집 수가 과도하지 않은지 등을 확인 1D 데이터에서는 사용 불가 파이썬 구현 - DBSCAN # import math def euclidean_distance(p1, p2): return math.sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2))) def region_query(data, point_idx, eps): neighbors = [] for idx, point in enumerate(data): if euclidean_distance(data[point_idx], point) \u0026lt;= eps: neighbors.append(idx) return neighbors def expand_cluster(data, labels, point_idx, neighbors, cluster_id, eps, min_samples): labels[point_idx] = cluster_id i = 0 while i \u0026lt; len(neighbors): n_idx = neighbors[i] if labels[n_idx] == -1: # noise → now becomes part of a cluster labels[n_idx] = cluster_id elif labels[n_idx] == 0: labels[n_idx] = cluster_id n_neighbors = region_query(data, n_idx, eps) if len(n_neighbors) \u0026gt;= min_samples: neighbors += n_neighbors i += 1 def dbscan(data, eps, min_samples): labels = [0] * len(data) # 0 = unvisited, -1 = noise, ≥1 = cluster id cluster_id = 0 for idx in range(len(data)): if labels[idx] != 0: continue # 이미 방문한 점 neighbors = region_query(data, idx, eps) if len(neighbors) \u0026lt; min_samples: labels[idx] = -1 # noise else: cluster_id += 1 expand_cluster(data, labels, idx, neighbors, cluster_id, eps, min_samples) return labels 파이썬 구현 - k distance plot # import numpy as np import matplotlib.pyplot as plt def euclidean_distance(p1, p2): return np.sqrt(np.sum((p1 - p2) ** 2)) def compute_k_distances(X, k): \u0026#34;\u0026#34;\u0026#34; 각 포인트에 대해 k번째 최근접 이웃까지의 거리 계산 Parameters: - X: (n_samples, n_features) ndarray - k: 이웃의 수 (k = min_samples - 1) Returns: - k_distances: 각 포인트의 k번째 최근접 이웃 거리 리스트 \u0026#34;\u0026#34;\u0026#34; n_samples = len(X) k_distances = [] for i in range(n_samples): distances = [] for j in range(n_samples): if i != j: dist = euclidean_distance(X[i], X[j]) distances.append(dist) distances.sort() k_distances.append(distances[k - 1]) # k번째 작은 거리 return np.sort(k_distances) def plot_k_distance_manual(X, k): \u0026#34;\u0026#34;\u0026#34; sklearn 없이 k-distance plot 그리기 Parameters: - X: (n_samples, n_features) ndarray - k: int, 이웃 수 (= min_samples - 1) \u0026#34;\u0026#34;\u0026#34; k_distances = compute_k_distances(X, k) plt.figure(figsize=(8, 4)) plt.plot(k_distances) plt.ylabel(f\u0026#34;{k}-th nearest neighbor distance\u0026#34;) plt.xlabel(\u0026#34;Points sorted by distance\u0026#34;) plt.title(f\u0026#34;Manual k-distance plot (k={k})\u0026#34;) plt.grid(True) plt.show() 파이썬 구현 - silhouette score # import numpy as np def silhouette_score_manual(X, labels): \u0026#34;\u0026#34;\u0026#34; Silhouette Score를 직접 계산하는 함수 Parameters: - X: (n_samples, n_features) ndarray - labels: (n_samples,) 클러스터 ID, 노이즈는 제외되어 있어야 함 (-1 제거 필수) Returns: - 평균 Silhouette Score (float) \u0026#34;\u0026#34;\u0026#34; unique_labels = set(labels) if len(unique_labels) \u0026lt;= 1: raise ValueError(\u0026#34;클러스터가 1개 이하입니다. Silhouette Score를 계산할 수 없습니다.\u0026#34;) n_samples = len(X) silhouette_values = [] for i in range(n_samples): own_cluster = labels[i] same_cluster_indices = [j for j in range(n_samples) if labels[j] == own_cluster and j != i] # a(i): 같은 클러스터 내 평균 거리 if same_cluster_indices: a = np.mean([np.linalg.norm(X[i] - X[j]) for j in same_cluster_indices]) else: a = 0 # 고립된 점 # b(i): 가장 가까운 다른 클러스터와의 평균 거리 b = float(\u0026#39;inf\u0026#39;) for other_cluster in unique_labels: if other_cluster == own_cluster: continue other_indices = [j for j in range(n_samples) if labels[j] == other_cluster] if other_indices: b_dist = np.mean([np.linalg.norm(X[i] - X[j]) for j in other_indices]) b = min(b, b_dist) # s(i): silhouette score for point i if max(a, b) == 0: s = 0 else: s = (b - a) / max(a, b) silhouette_values.append(s) return np.mean(silhouette_values) 전체 파이프라인 실행 # # 1. 데이터 생성 X, _ = make_moons(n_samples=300, noise=0.05, random_state=0) # 2. k-distance plot min_samples = 5 plot_k_distance_manual(X, k=min_samples - 1) # 3. 클러스터링 (여기선 elbow 보고 eps=0.125 정도 선택) eps = 0.15 labels = dbscan(X, eps=eps, min_samples=min_samples) # 4. 클러스터 시각화 plt.figure(figsize=(6, 5)) plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\u0026#39;tab10\u0026#39;, s=10) plt.title(f\u0026#34;DBSCAN Clustering (eps={eps}, min_samples={min_samples})\u0026#34;) plt.xlabel(\u0026#34;X1\u0026#34;) plt.ylabel(\u0026#34;X2\u0026#34;) plt.grid(True) plt.show() # 5. Silhouette Score 계산 score = silhouette_score_manual(X, labels) print(f\u0026#34;\\nSilhouette Score: {score:.4f}\u0026#34;) Silhouette Score: 0.3327 "},{"id":37,"href":"/docs/hobby/favorite/favorite3/","title":"강지영과 저스틴 민의 속마음ㅣ고나리자 EP.66","section":"🌸","content":" 강지영과 저스틴 민의 속마음ㅣ고나리자 EP.66 # #2025-06-25\n이장면이 좋은포인트: 1) 영어할줄아는 사람만 모여서 연합을 맺고 싶지 않았다는것 2) 강지영이 듣고 바로이해함 3) 영어를 못하는사람들이 자기한테 다가오는게 부담이었을거라고 자연스럽게 생각하는 성품\n저런 성품을 가진 사람이 될수없어도 저런 성품인 사람의 편에 서는 사람인것만 유지하자 그를 위해서 능력을 키우자 ㅎㅎ\n#출처\n강지영과 저스틴 민의 속마음ㅣ고나리자 EP.66 https://www.youtube.com/watch?v=DxpBONR8znE\n"},{"id":38,"href":"/docs/hobby/book/book40/","title":"충족감","section":"글","content":" 충족감 # #2025-06-25\n#1\n\u0026lsquo;화내지 않는 것\u0026rsquo;을 무기로 사용해 자기 안에 숨어 있는 \u0026lsquo;화\u0026rsquo;를 이기세요.\n\u0026lsquo;긍정적인 마음\u0026rsquo;을 무기로 사용해 자기 안에 숨어 있는 \u0026lsquo;부정적인 마음을 이기세요.\n\u0026lsquo;나누어 주는 것\u0026rsquo;을 무기로 사용해 자기 안에 숨어 있는 \u0026lsquo;쩨쩨함\u0026rsquo;을 이기세요.\n\u0026lsquo;사실만 말하는 것\u0026rsquo;을 무기로 사용해 자기 안에 숨어 있는 \u0026lsquo;거짓말쟁이\u0026rsquo;를 이기세요.\n-법구경 223\n#2\n\u0026lsquo;저 사람보다 뛰어나다\u0026rsquo; 혹은 \u0026lsquo;이전의 나보다 낫다\u0026rsquo;는 말로 누군가와 지금의 자신을 비교하지 마세요. \u0026lsquo;저 사람보다 못하다\u0026rsquo; 혹은 \u0026lsquo;이전의 내게 미치치 못한다\u0026rsquo;는 말로 누군가와 지금의 자신을 비교하지 마세요. \u0026lsquo;저 사람과 같다\u0026rsquo; 혹은 \u0026lsquo;이전의 나와 같다\u0026rsquo;는 말로도 누군가와 지금의 자신을 비교하지 마세요.\n자존심이 걸린 질문을 받더라도 자신에 대한 우월감이나 열등감을 느끼는 대신 자신을 지나치게 의식하는 것에서 벗어나 냉정하게 답하세요.\n-경집 918\n#3\n\u0026lsquo;무승부다\u0026rsquo; \u0026lsquo;내가 훨씬 낫다\u0026rsquo; \u0026lsquo;내가 못하다\u0026rsquo;\n이 세 가지 종류의 사고방식에 지배당하면, 당신은 상대를 꺾고 싶어 어떻게든 트집을 잡게 됩니다. 예컨대 \u0026lsquo;당신이 방해하는 바람에 업무가 엉망이 됐다\u0026rsquo;고 억지를 부려서라도 알량한 자존심을 지키고 싶어집니다. 그러고 나면 서로 기분이 나빠지는 것은 당연하지요.\n\u0026lsquo;무승부\u0026rsquo;, \u0026lsquo;승리\u0026rsquo;, \u0026lsquo;패배\u0026rsquo; 같은 건 무시한 채 조금도 신경 쓰지 않는다면 건방진 태도도, 언쟁도 말끔히 사라지고 평화가 찾아올 것입니다.\n-경집 841\n#4\n당신 손에 주어진 게 아무리 하찮은 것이라 해도 거기서 행복을 찾아낸다면 \u0026lsquo;만족을 아는\u0026rsquo; 충족감으로 인해 마음은 깨끗하게 정화됩니다. 그 맑은 마음의 파동은 눈에 보이지 않는 더 높은 차원의 생명들을 기쁘게 하고 끌어당길 것입니다.\n-법구경 366\n#5\n당신이라는 존재는, 과거에 당신이 생각하고 느낀 내용 하나하나가 마음에 쌓이고 섞인 결과물입니다. 당신은 그 마음의 조각보로써 지금 여기에 있습니다.\n당신이 나쁜 생각을 한다면 나쁜 업의 에너지가 마음에 각인되고, 그만큼 당신은 나쁜 쪽으로 바뀝니다. 당신이 따스한 생각을 한다면 긍정적인 업의 에너지가 마음에 각인되고, 그만큼 따스한 당신으로 변화합니다. 이렇게 인간은 마음에 쌓인 생각대로 조금씩 달라집니다. 모든 것은 그 위에서 생겨나고, 그로 인해 만들어집니다.\n고로 부정적인 마음으로 불쾌한 이야기를 하거나 부정적인 마음에 의해 불쾌한 행동을 하게 되면, 그것은 반드시 자신에게 되돌아옵니다.\n온화하고 긍정적인 마음으로 이야기하거나 행동하면, 그것은 편안함으로 반드시 자신에게 되돌아옵니다. 마치 당신의 뒤로 그림자가 반드시 따라 걷듯이 말입니다.\n-법구경 1,2\n#6\n자신의 행동, 말, 생각에 의해 마음에 새겨지는 선한 업의 에너지를 가벼이 보고 어차피 좋은 일을 해도 그 과보는 내게 돌아오지 않으니 아무래도 상관없다며 내팽개치지 마세요.\n보는 사람이 없어도 다음 사람을 위해 공중화장실 변기에 묻은 오물을 닦아내는 그 한 방울의 선한 마음이 쌓여 이윽고 물병을 가득 채웁니다.\n선한 업의 긍정적인 에너지가 마음 속 물병에 똑똑 떨여저 조금씩 쌓이고 마침내 기분 좋은 과보를 불러옵니다.\n-법구경 122\n#7 몸에 밴 기품이 감돈다\n마음의 예절이라는 것은 단시간에 몸에 배는 것이 아닙니다. 오랜 시간에 걸쳐 차분히 마음의 예절을 몸에 익히고 그로 인해 험담이나 자랑이나 난잡한 행동을 하지 않을 때 자연스럽게 당신 주변에는 기품이 감돕니다.\n-경집 261\n# #출처\n책 초역 부처의 말\n"},{"id":39,"href":"/docs/study/tech/tech35/","title":"#5 결과 검증: 계통 결정 돌연변이와 연관성","section":"생물정보학","content":" #5 결과 검증: 계통 결정 돌연변이와 연관성 # #2025-06-24\n1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(\u0026#34;/data/home/ysh980101/2407/Mutclust\u0026#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(\u0026#34;/data/home/ysh980101/2506/mutclust\u0026#34;) from Bin.sc import * os.chdir(\u0026#34;/data/home/ysh980101/2506/mutclust\u0026#34;) 2. Load data # lineage_info_dir = \u0026#39;/data/home/ysh980101/2411/data/mutation_info\u0026#39; covid_annotation = \u0026#34;/data/home/ysh980101/2404/Data/covid_annotation.tsv\u0026#34; sig_hotspots = \u0026#34;result/sig_hotspots.csv\u0026#34; lineage_info = make_lineage_info(lineage_info_dir) hotspot_lineage = make_hotspot_lineage(lineage_info, sig_hotspots_path, covid_annotation) hotspot_lineage plot_hotspot_lineage(hotspot_lineage) outdir = \u0026#34;result/\u0026#34; hotspot_lineage.to_csv(f\u0026#34;{outdir}Supplementary_table_1.csv\u0026#34;, index=False) 만든건 저장.\n"},{"id":40,"href":"/docs/study/tech/tech34/","title":"#6 알고리즘 성능 평가 - k dist plot","section":"생물정보학","content":" #6 알고리즘 성능 평가 - k dist plot # #2025-06-24\n1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(\u0026#34;/data/home/ysh980101/2407/Mutclust\u0026#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(\u0026#34;/data/home/ysh980101/2506/mutclust\u0026#34;) from Bin.sc import * os.chdir(\u0026#34;/data/home/ysh980101/2506/mutclust\u0026#34;) 2. Load data # indir = \u0026#39;result/\u0026#39; resdir = \u0026#39;result/GISAID_test1/\u0026#39; with open(f\u0026#34;{indir}GISAID_total.pickle\u0026#34;, \u0026#34;rb\u0026#34;) as f: Input_df = pickle.load(f) hotspots = pd.read_csv(f\u0026#34;{resdir}clusters_test1.txt\u0026#34;, sep=\u0026#39;\\t\u0026#39;) sig_hotspots = pd.read_csv(f\u0026#34;{indir}sig_hotspots.csv\u0026#34;) 3. K-dist plot # kdist_plot(Input_df, hotspots, sig_hotspots, k=5) "},{"id":41,"href":"/docs/study/tech/tech32/","title":"#1 입력 데이터 생성","section":"생물정보학","content":" #1 입력 데이터 생성 # #2025-06-23\n1. Load package # %load_ext autoreload %autoreload 2 import sys import pandas as pd import numpy as np import os import pickle import ast sys.path.append(\u0026#39;/data3/projects/2025_Antibiotics/YSH/bin\u0026#39;) from sc import * os.chdir(\u0026#39;/data3/projects/2025_Antibiotics/YSH\u0026#39;) 2. Check data # datadir = \u0026#39;/data3/projects/2025_Antibiotics/PreprocessedData/TimecourseData\u0026#39; outdir = \u0026#39;res\u0026#39; pids =[d for d in os.listdir(datadir) if os.path.isdir(os.path.join(datadir, d))] len(pids) 4589 4589명 환자의 의료 데이터.\ncur_pid = pids[0] sev = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/SeverityScore.csv\u0026#34;) lab = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/Laboratory_processed.csv\u0026#34;) med = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/Medication.csv\u0026#34;) print(cur_pid) print(len(sev.columns.tolist()), sev.columns.tolist()) print(len(lab.columns.tolist()), lab.columns.tolist()) print(med) 74374 6 [\u0026#39;Date\u0026#39;, \u0026#39;NEWS\u0026#39;, \u0026#39;WHO\u0026#39;, \u0026#39;SOFA\u0026#39;, \u0026#39;PBS\u0026#39;, \u0026#39;qPitt\u0026#39;] 23 [\u0026#39;Date\u0026#39;, \u0026#39;ALT (U/L)\u0026#39;, \u0026#39;AST (U/L)\u0026#39;, \u0026#39;BUN (mg/dL)\u0026#39;, \u0026#39;Creatinine (mg/dL)\u0026#39;, \u0026#39;D-Dimer (ug/mL )\u0026#39;, \u0026#39;Ferritin (ng/mL)\u0026#39;, \u0026#39;HCO3 (mmol/L)\u0026#39;, \u0026#39;Hemoglobin (g/dL)\u0026#39;, \u0026#39;LDH (U/L)\u0026#39;, \u0026#39;Lymphocytes (%)\u0026#39;, \u0026#39;MDRD eGFR (mL/min/BSA)\u0026#39;, \u0026#39;Neutrophils (%)\u0026#39;, \u0026#39;O2 saturation (%)\u0026#39;, \u0026#39;PCO2 (mmHg)\u0026#39;, \u0026#39;PO2 (mmHg)\u0026#39;, \u0026#39;Platelet count (10^3/uL)\u0026#39;, \u0026#39;Potassium (mmol/L)\u0026#39;, \u0026#39;Sodium (mmol/L)\u0026#39;, \u0026#39;WBC count (10^3/uL)\u0026#39;, \u0026#39;hs-CRP (mg/dL)\u0026#39;, \u0026#39;pH ()\u0026#39;, \u0026#39;total CO2, calculated (mmol/L)\u0026#39;] Date antimicrobials antimicrobials_dose antimicrobials_2 \\ 0 2020-10-30 Trizele 500.0mg/2 Cefotaxime 1 2020-10-31 Trizele 500.0mg/3 Cefotaxime 2 2020-11-01 Pospenem 1.0g/1 Pospenem 3 2020-11-02 Pospenem 1.0g/1 Meropen 4 2020-11-03 Vanco Kit 1.0g/1 Meropen 5 2020-11-04 Vanco Kit 1.0g/1 Meropen 6 2020-11-05 NaN NaN NaN antimicrobials_2_dose antimicrobials_3 antimicrobials_3_dose 0 2.0mg/2 NaN NaN 1 2.0mg/3 NaN NaN 2 1.0g/2 NaN NaN 3 500.0mg/2 Vanco Kit 1.0g/1 4 500.0mg/3 NaN NaN 5 500.0mg/3 NaN NaN 6 NaN NaN NaN 환자 \u0026lsquo;74374\u0026rsquo;를 확인해보면\nSeverityScore는 날짜별 5개의 중증도 점수 Laboratory는 22개 임상 정보 Medication은 날짜별 투여 항생제 및 투여용량 정보이다. 3. Merge data # input_dict = make_input(datadir, pids) len(list(input_dict.keys())) 4516 Severity, Laboratory, Medication 정보가 모두 있는 환자(4516명)만 사용해서\n의료 데이터 딕셔너리 input_dict를 생성했다 input_dict = add_strain_info(input_dict) print(cur_pid) print(input_dict[cur_pid]) Date NEWS WHO SOFA PBS qPitt ALT (U/L) AST (U/L) BUN (mg/dL) \\ 0 2020-10-30 4 5 0 0 0 43.0 79.0 16.9 1 2020-10-31 4 5 1 0 0 71.0 149.0 21.5 2 2020-11-01 12 5 5 1 2 83.0 149.0 30.8 3 2020-11-02 9 5 6 1 2 83.0 149.0 19.2 4 2020-11-03 12 5 5 1 1 83.0 149.0 20.2 5 2020-11-04 8 5 6 2 1 83.0 149.0 22.5 6 2020-11-05 9 5 7 4 2 83.0 149.0 22.5 Creatinine (mg/dL) ... Platelet count (10^3/uL) Potassium (mmol/L) \\ 0 0.68 ... 395.0 3.6 1 1.22 ... 340.0 3.0 2 1.42 ... 272.0 4.2 3 0.93 ... 83.0 4.7 4 0.77 ... 61.0 4.7 5 0.84 ... 67.0 5.2 6 0.84 ... 67.0 5.2 total CO2, calculated (mmol/L) med_cnt med_list \\ 0 18.3 2 Trizele;Cefotaxime 1 18.3 2 Trizele;Cefotaxime 2 15.7 2 Pospenem;Pospenem_2 3 15.7 3 Pospenem;Meropen;Vanco Kit 4 31.0 2 Vanco Kit;Meropen 5 31.0 2 Vanco Kit;Meropen 6 31.0 0 strain 0 [] 1 [] 2 [] 3 [Enterobacter cloacae ssp cloacae] 4 [Enterobacter cloacae ssp cloacae] 5 [Enterobacter cloacae ssp cloacae] 6 [Enterobacter cloacae ssp cloacae] [7 rows x 31 columns] 환자별 균주 정보를 넣어주고\n환자 \u0026lsquo;74374\u0026rsquo;를 확인해보면 날짜별 중증도(5), 임상 정보(22), 항생제 정보(2), 균주 리스트(1)까지 총 30개 feature가 통합 정리된 딕셔너리가 생성됐다. with open(f\u0026#34;{outdir}/Input.pkl\u0026#34;, \u0026#39;wb\u0026#39;) as f: pickle.dump(input_dict, f) 만든건 저장하기.\n4. Sequence 생성 # indir = \u0026#39;res\u0026#39; with open(f\u0026#34;{indir}/Input.pkl\u0026#34;, \u0026#39;rb\u0026#39;) as f: input_dict = pickle.load(f) pids = input_dict.keys().tolist() cur_pid = pids[0] print(cur_pid) print(input_dict[cur_pid]) Date NEWS WHO SOFA PBS qPitt ALT (U/L) AST (U/L) BUN (mg/dL) \\ 0 2020-10-30 4 5 0 0 0 43.0 79.0 16.9 1 2020-10-31 4 5 1 0 0 71.0 149.0 21.5 2 2020-11-01 12 5 5 1 2 83.0 149.0 30.8 3 2020-11-02 9 5 6 1 2 83.0 149.0 19.2 4 2020-11-03 12 5 5 1 1 83.0 149.0 20.2 5 2020-11-04 8 5 6 2 1 83.0 149.0 22.5 6 2020-11-05 9 5 7 4 2 83.0 149.0 22.5 Creatinine (mg/dL) ... Platelet count (10^3/uL) Potassium (mmol/L) \\ 0 0.68 ... 395.0 3.6 1 1.22 ... 340.0 3.0 2 1.42 ... 272.0 4.2 3 0.93 ... 83.0 4.7 4 0.77 ... 61.0 4.7 5 0.84 ... 67.0 5.2 6 0.84 ... 67.0 5.2 total CO2, calculated (mmol/L) med_cnt med_list \\ 0 18.3 2 Trizele;Cefotaxime 1 18.3 2 Trizele;Cefotaxime 2 15.7 2 Pospenem;Pospenem_2 3 15.7 3 Pospenem;Meropen;Vanco Kit 4 31.0 2 Vanco Kit;Meropen 5 31.0 2 Vanco Kit;Meropen 6 31.0 0 strain 0 [] 1 [] 2 [] 3 [Enterobacter cloacae ssp cloacae] 4 [Enterobacter cloacae ssp cloacae] 5 [Enterobacter cloacae ssp cloacae] 6 [Enterobacter cloacae ssp cloacae] [7 rows x 31 columns] 환자별 임상 정보는 최소 5 / 최대 1202일 길이의 데이터인데\n이를 항생제 투여일 기준 D-3~D+6만 남겨서 길이 10의 sequence로 만들어준다. indir = \u0026#39;res\u0026#39; outdir = \u0026#39;data/res_dict\u0026#39; make_sequence(med, indir, outdir) res_list = os.listdir(outdir) print(len(res_list)) 169 169개의 input sequence가 생성되었고\noutdir에 저장되었다. with open(f\u0026#39;{outdir}/Dexamethasone.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f: dexamethasone = pickle.load(f) cur_ids = cur_ids = list(dexamethasone.keys()) print(len(cur_ids)) print(cur_ids[0]) print(dexamethasone[cur_ids[0]]) 783 543865_0 Date NEWS med_cnt strain 4 2017-08-18 4 0 [Staphylococcus epidermidis] 5 2017-08-19 4 0 [Staphylococcus epidermidis] 6 2017-08-20 4 0 [Staphylococcus epidermidis] 7 2017-08-21 4 1 [Staphylococcus epidermidis] 8 2017-08-22 3 1 [Staphylococcus epidermidis] 9 2017-08-23 4 1 [Staphylococcus epidermidis] 10 2017-08-24 4 1 [Pseudomonas aeruginosa] 11 2017-08-25 7 1 [Pseudomonas aeruginosa] 12 2017-08-26 4 1 [Pseudomonas aeruginosa] 13 2017-08-27 4 1 [Pseudomonas aeruginosa] 항생제 \u0026lsquo;dexamethasone\u0026rsquo;에 대해 생성된 sequence를 확인해보면\n783개 sequence가 생성되었고 환자 543865의 첫번째 시퀀스 \u0026lsquo;543865_0\u0026rsquo;를 확인해보면 D-3~D+6인 것을 확인 가능하다! "},{"id":42,"href":"/docs/study/tech/tech33/","title":"#3 모델 구축","section":"생물정보학","content":" #3 모델 구축 # #2025-06-23\n1. Load package # import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import copy from pathlib import Path import warnings import lightning.pytorch as pl from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor from lightning.pytorch.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) import pytorch_forecasting import torch import pytorch_lightning as pl print(\u0026#34;PyTorch Forecasting:\u0026#34;, pytorch_forecasting.__version__) print(\u0026#34;PyTorch:\u0026#34;, torch.__version__) print(\u0026#34;PyTorch Lightning:\u0026#34;, pl.__version__) PyTorch Forecasting: 0.10.2 PyTorch: 1.13.1+cu117 PyTorch Lightning: 1.6.5 pytorch 및 관련 패키지 버전.\n2. Load data # os.chdir(\u0026#39;/data3/projects/2025_Antibiotics/YSH/\u0026#39;) meddir = \u0026#39;res\u0026#39; seqdir = \u0026#39;data/final_dict\u0026#39; with open(f\u0026#34;{meddir}/all_meds.txt\u0026#34;, \u0026#39;r\u0026#39;) as f: all_meds = [line.strip().replace(\u0026#34;/\u0026#34;, \u0026#34;_\u0026#34;) for line in f if line.strip()] all_df = [] for med in tqdm(all_meds): try: with open(f\u0026#34;{seqdir}/{med}.pkl\u0026#34;, \u0026#39;rb\u0026#39;) as f: sequences = pickle.load(f) except Exception as e: print(f\u0026#34;[ERROR] {med}: {e}\u0026#34;) continue for pid, df in sequences.items(): df = df.copy() df[\u0026#34;pid\u0026#34;] = pid df[\u0026#34;med\u0026#34;] = med df[\u0026#34;time_idx\u0026#34;] = range(len(df)) all_df.append(df) total_sequences = pd.concat(all_df).reset_index(drop=True) 100%|██████████| 169/169 [00:14\u0026lt;00:00, 11.86it/s] "},{"id":43,"href":"/docs/study/tech/algo4/","title":"#5 타겟 넘버","section":"생물정보학","content":" #5 타겟 넘버 # #2025-06-22\n1. 문제 # #문제 설명\nn개의 음이 아닌 정수들이 있습니다. 이 정수들을 순서를 바꾸지 않고 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다.\n-1+1+1+1+1 = 3\n+1-1+1+1+1 = 3\n+1+1-1+1+1 = 3\n+1+1+1-1+1 = 3\n+1+1+1+1-1 = 3\n사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요.\n#제한사항\n주어지는 숫자의 개수는 2개 이상 20개 이하입니다. 각 숫자는 1 이상 50 이하인 자연수입니다. 타겟 넘버는 1 이상 1000 이하인 자연수입니다. #입출력 예\nnumbers target return [1, 1, 1, 1, 1] 3 5 [4, 1, 2, 1] 4 2 #입출력 예 설명\n입출력 예 #1\n문제 예시와 같습니다.\n입출력 예 #2\n+4+1-2+1 = 4 +4-1+2-1 = 4\n총 2가지 방법이 있으므로, 2를 return 합니다. 2. 정답 # def solution(numbers, target): count = 0 def dfs(index, total): nonlocal count if index == len(numbers): # 모든 숫자 다 썼을 때 if total == target: count += 1 return dfs(index + 1, total + numbers[index]) dfs(index + 1, total - numbers[index]) dfs(0, 0) return count "},{"id":44,"href":"/docs/hobby/favorite/favorite2/","title":"Peder Elias - When I´m Still Getting Over You","section":"🌸","content":" Peder Elias - When I´m Still Getting Over You # #2025-06-21\nhttps://www.youtube.com/watch?v=DB2bsf3xkNk 한동안 내 컬러링이었던 곡 ㅎㅎ\n# #Lyrics\nI wish that I had stayed inside\nAnd not got that coffee at 8:05\n\u0026lsquo;Cause if I did, then I wouldn\u0026rsquo;t have seen you\nThere with someone, that I\u0026rsquo;ve never known\nAnd looking and know you look beautiful\nDon\u0026rsquo;t wanna care, but I still do\n… \u0026lsquo;Cause you were my first, you were my last\nYou were my future, but now you\u0026rsquo;re my past\nAnd I just can\u0026rsquo;t let go\n… I don\u0026rsquo;t wanna see you two laughing at a party\nDoing things that we used to do\nWhile I\u0026rsquo;m here with somebody doing nothing\nBut wishing that she was you\nIt\u0026rsquo;s not that I\u0026rsquo;m hoping, you\u0026rsquo;re not happy\nBaby, I do, but\nI\u0026rsquo;m not ready to see you at parties with somebody new\nWhen I\u0026rsquo;m still getting over you\n… I\u0026rsquo;m staring at a photograph\nI\u0026rsquo;d lie if I said I don\u0026rsquo;t want you back\nNow you and I\u0026rsquo;ve become nothing but strangers (oh no)\nI\u0026rsquo;m with someone \u0026lsquo;cause I don\u0026rsquo;t want\nTo show you it hurts me like hell, you\u0026rsquo;re gone\nDon\u0026rsquo;t wanna care but I still do\n… \u0026lsquo;Cause you were my first, you were my last\nYou were my future, but now you\u0026rsquo;re my past\nAnd I just can\u0026rsquo;t let go\n… I don\u0026rsquo;t wanna see you two laughing at a party\nDoing things that we used to do\nWhile I\u0026rsquo;m here with somebody doing nothing\nBut wishing that she was you\nIt\u0026rsquo;s not that I\u0026rsquo;m hoping, you\u0026rsquo;re not happy\nBaby I do, but\nI\u0026rsquo;m not ready to see you at parties with somebody new\nWhen I\u0026rsquo;m still getting over you\nWhen I\u0026rsquo;m still getting over you\n… I don\u0026rsquo;t wanna see you two laughing at a party\nDoing things that we used to do\nWhile I\u0026rsquo;m here with somebody doing nothing\nBut wishing that she was you\nIt\u0026rsquo;s not that I\u0026rsquo;m hoping, you\u0026rsquo;re not happy\nBaby I do, but\nI\u0026rsquo;m not ready to see you at parties with somebody new\nWhen I\u0026rsquo;m still getting over you\n# #내맘속 공식영상\nhttps://www.youtube.com/watch?v=UdDjlhYhawk 뮤비보다 더 뮤비같은 영상 예전에 때잉에서 좋은노래쇼핑 종종했었는데 ㅎㅎ\n"},{"id":45,"href":"/docs/hobby/favorite/favorite1/","title":"김하린 harin 데블스플랜2 비하인드 | Q\u0026A","section":"🌸","content":" 김하린 harin 데블스플랜2 비하인드 | Q\u0026amp;A # #2025-06-21\n#1\n제가 정말 후회하는 타입이 아니거든요 근데 돌이켜봤을때 이게 내 최선이었나? 생각해보니까 아닌 것 같은거에요 내가 그때 그런 선택을 했으면 달라졌을까 하는 생각들이 (자막: 하는 if문들이) 머릿속에서 떠나질 않아서 새벽내내 울다가 도저히 그냥은 못자겠다 하고 술도 못먹는데 매운닭발 하나 시켜서 레드와인 한병 통째로 들이붓고 기절해서 겨우 잤습니다\n그러고 나서도 한동안은 좀 폐인 상태였던 것 같아요 그만큼 아쉬움이 많이 남았던 거죠 이제는 괜찮습니다 시간이 약이긴 하더라고요\n#2\n그것보다 사람이 계속 많이 붙어있고 시끄럽고 INTJ로서 합숙 자체가 실시간으로 hp가 닳는 그런 느낌이었거든요 그게 힘들었고 정신이 없어서 이게 앉아서 차분하게 우리 이야기하자가 안돼요\u0026hellip; 내가 좋은 전략을 생각해도 다들 사회에서 자기주장이 강한 분들이시다 보니 다 자기할말만 하고있단 말이에요 그래서 이게 말하다가 지쳐요 일단 나 없는 사이 이상한 전략 짜고 이상한 이야기 할까봐 화장실도 못가고 좀 그런게 많이 힘들었죠 이게 잠시만 한눈팔아도 딴소리 하고있기 때문에 오감을 예민하게 곤두세워야 됩니다\n#3\n서바이벌 또 나갈 생각 있나요? 이건 딱 반반이라고 말씀드릴수 있을것같아요 타임루프물인거죠 영화나 애니에서 내가 너무 소중하게 여기는 사람이 죽어서 그 미래를 바꾸고자 과거로 계속 돌아가고 하잖아요 다 아는 미래라고 생각했는데 계속 새로운 변수가 나타나서 또 죽고 같은 과거를 반복하고 돌아갈수록 좌절과 상실감만 커지고 멘탈이 무너지고 그런거 많이 나오잖아요 제가 서바이벌을 한번 겪어봤으니 한번더 해보고싶다 잘할수있는데 하는 생각도 들고 또 실패하면 상실감도 두배가 되지 않을까 그런 생각? 섭외가 오면 미래의 나에게 결정을 맡기겠습니다 똑같은 이름이지만 1-2년 뒤엔 다른 사람이 돼있을것 같아서요\n#4\n최애 애니메이션 3개 골라주세요. 이것도 너무 어려운 질문인데 전 애니메이션은 종합예술이라고 생각하거든요 작화 사운드 스토리 다 합해서 강철의 연금술사 주술회전 극장판이랑 회옥/옥절 마법소녀 마도카 마기카 꼽겠습니다 제가 인생에서 제일 힘들 때 삶의 철학을 생각하게 해준 작품들이라 골랐고요 그리고 타임루프물 워낙 좋아해서 슈타인즈 게이트랑 미래일기도 재밌게 봤습니다 근데 이런것들만 보니까 제가 사랑을 너무 무겁게 생각하게 된것같아요 막 이사람을 위해서 내가 몇번을 죽고 과거로 돌아가고 모든 희생을 바쳐야 진짜 사랑이 아닌가? 좀 그런 이상한 가치관을 갖게 된것 같기도 해요\n#5\n저는 뭐하나에 꽂히면 그거만 한달내내 먹다가 질려서 영영 안먹고 그러거든요 얘를들면 갑자기 프렌치토스트가 땡긴다 하면 프토로 유명한 맛집을 다 가봐요 근데 같은곳 두번은 잘 안가고 도장깨기를 다하고 마음속에 순위를 매겨요 그런 덕후기질이 뼛속까지 있습니다 이렇게 프토마스터가 되고 나면 나는 이제 마스터니까 미련이 사라져서 다시는 안먹고 그런\u0026hellip;거죠\n#6\n싸패는 아닙니다만 어떤 지점에서 그렇게 느끼시는지는 알아요 저는 이렇게 훈련이 오랜 시간 됐잖아요 어떤 일이 생겨도 매주 시험은 봐야되고 당직은 서야되고 그래서 예를들면 남자친구랑 크게 싸우고 헤어졌다든지해도 아 내가 컨트롤할수 있는 영역 밖의 일이 일어난거니까 다른 사람의 감정이나 행동은 내가 영향을 줄수있는게 아니니까 그러면 내가 바꿀수있는게 없다면 지금부터 그만 슬퍼해야지 하면 저도 사람이니까 100%는 안돼도 한 7-80%는 조절이 돼요 그래서 멘탈관리법이나 스트레스 해소법을 물어보시면 저는 그냥 아 지금부터 내가 스트레스 안받는다고 생각해야지 하고 굳게 결심하면 그게 어느정도 돼요 근데 이렇게 항상 급해서 억지로 감정을 다 막아두면 1년에 두세번은 좀 크게 공허함이 파도처럼 밀려오고 아 나 왜살지 싶은 순간들이 있거든요 저는 인복이 좋아서 다행히 그때 친구들이 막 분위기 좋은 와인바도 데려가주고 같이 먓있는거먹고 하면서 그런 순간들을 이겨냈던것 같아요 그래서 제가 좋아하는 것들을 sns 통해서 많이 말하려고 해요 내가 이걸 해보니까 좋더라 인생이 살만하더라 하는걸 나누고싶은 마음이 있어요\n#7\n싫어하는사람의 특징. 저랑 결이 안맞는 사람이라면 지나치게 낙관적이거나 비관적인사람? 저는 그냥 있는 현실을 그대로 받아들이고 내가 컨트롤할수없는건 신경은 쓰되 너무 걱정하기보단 내가 할수있는일을 열심히 하는게 좋아요 저도 막 엄청 큰 일이 닥치면 막 욕하고 화내면서 할건 다 하거든요 그래서 좀 화가 많아요 화가\n#8\n저는 특별히 이상형은 없는데 그 사람의 전체적인 분위기를 보는것 같아요 웃기긴 한데 제가 동물적 감각이 좀 좋거든요 느낌이 좋은사람? 반대로 뭔가 싸하면 아무리 남들이 좋은사람이라 하고 겉보기에 블링블링한 사람이어도 제가 도망가요\n#9\n내가 이렇게 뭔가 잘해보고 싶어서 열심히 살고 재밌는 일도 막 새롭게 해보고싶은데 너희는 어때? 하고 세상한테 좀 물어보고 싶었던것 같아요 그 질문에 많은분들이 공감해주시고 화답해주시니까 정말 기뻤기때문에 나오길 잘한거같아요 앞으로 운동하고 책읽으면서 건강도 챙기고 이번 기회를 통해 저도 앞만보고 달리다가 스스로를 돌아볼수 있어서 정말 감사했고 소중한 경험이었고 많이 배웠습니다\n# #링크\nhttps://www.youtube.com/watch?v=zcWa-m6QUT8\u0026t=379s "},{"id":46,"href":"/docs/hobby/daily/daily16/","title":"비오는날의 카페 페이스포포","section":"일상","content":" 비오는날의 카페 페이스포포 # #2025-06-21\n팍팍한 일상이지만 오랜만에 브런치먹으러 왔다..!\n오늘 시킨 메뉴는 루꼴라 잠봉뵈르 / 루꼴라 쉬림프 타르틴 / 페스츄리소세지 푀이테 / 플레인 사워도우!!\n여기는 모든메뉴가 맛이 중~상이어서 역시 맛있었다 ㅎㅎ 그래두 젤 마싯었던건 루꼴라 잠봉뵈르(이유: 원래 조아하는 스타일이라서..)였구 엄마아빠는 페스츄리소세지 마싯다고했는데 난그냥 무난했음\n루꼴라 쉬림프 타르틴은 저번에도 시켰던건데 이메뉴는 실패가 없다. ㅎㅎ\n비오니깐 뭔가 더 이쁜것같은 내부\n요 케이크 넘 기엽다 ㅋㅋ\n"},{"id":47,"href":"/docs/study/tech/tech30/","title":"#2 중요도 지표 계산","section":"생물정보학","content":" #2 중요도 지표 계산 # #2025-06-20\n1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(\u0026#34;/data/home/ysh980101/2407/Mutclust\u0026#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load GISAID data # indir = \u0026#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/\u0026#34; Refseq = getNucleotideRefSeq() GISAID_Freq = pd.read_csv(f\u0026#39;{indir}gisaid_freq_all.csv\u0026#39;, index_col=0) GISAID_meta = get_GISAID_meta() print(GISAID_Freq) A C G T R Y S W K M B D H V N 1 10612 390 415 785 11 1 3 4 24 2 1 2 0 0 219995 2 287 502 218 12942 3 31 14 4 61 0 1 2 1 0 218179 3 166 461 348 18168 1 12 29 10 15 1 0 1 1 0 213032 4 19398 267 502 972 12 5 1 33 37 6 1 1 0 1 211009 5 24962 281 334 699 6 21 6 17 15 10 5 1 1 1 205886 ... ... ... ... ... .. .. .. .. .. .. .. .. .. .. ... 29899 41707 36 38 100 1 0 2 5 0 3 0 0 1 0 190351 29900 40483 30 25 99 8 1 0 2 1 4 0 1 0 0 191590 29901 39258 25 19 22 1 0 0 4 1 1 0 0 0 0 192913 29902 38015 23 22 19 1 0 0 5 0 1 0 0 0 0 194158 29903 34729 18 32 99 0 3 0 4 0 3 0 0 1 0 197355 [29903 rows x 15 columns] 3. Calculate H-score # def calculate_hscore(Refseq, Freq, N): freq_df = Freq[[\u0026#39;A\u0026#39;,\u0026#39;T\u0026#39;,\u0026#39;G\u0026#39;,\u0026#39;C\u0026#39;]].copy() for i,row in enumerate(Refseq): freq_df.iloc[i][row] = 0 per_df = freq_df.apply(lambda row: row/row.sum(), axis=1) per_df = per_df.fillna(0) ent_df = per_df.apply(lambda row: entropy(row, base=2), axis = 1) ent_df = ent_df.fillna(0) count_df = freq_df.apply(lambda row: row.sum(), axis=1) ratio_df = freq_df.apply(lambda row: row.sum()/N, axis=1) hscore_df = np.log2(ratio_df*ent_df*100+1) Input_df = pd.concat([count_df, ratio_df, ent_df, hscore_df], axis=1, keys=[FREQ, PER, ENT, HSCORE]) Input_df = Input_df.reset_index() Input_df = Input_df.rename(columns={\u0026#39;index\u0026#39;: POS}) return Input_df N = len(GISAID_meta) Input_df = calculate_hscore(Refseq, GISAID_Freq, N) print(Input_df) Position Frequency Percentage Entropy H-score 0 1 1590 0.007088 1.505823 1.047783 1 2 1007 0.004489 1.494709 0.740711 2 3 975 0.004347 1.476319 0.715176 3 4 1741 0.007761 1.401635 1.062019 4 5 1314 0.005858 1.462576 0.892773 ... ... ... ... ... ... 29898 29899 174 0.000776 1.408897 0.149631 29899 29900 154 0.000687 1.295297 0.122905 29900 29901 66 0.000294 1.575992 0.065393 29901 29902 64 0.000285 1.580312 0.063624 29902 29903 149 0.000664 1.236853 0.113909 [29903 rows x 5 columns] 4. Save # Input_df.to_pickle(f\u0026#34;{indir}GISAID_total.pickle\u0026#34;) "},{"id":48,"href":"/docs/study/tech/tech29/","title":"#3 밀도 기반 클러스터링","section":"생물정보학","content":" #3 밀도 기반 클러스터링 # #2025-06-20\n1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(\u0026#34;/data/home/ysh980101/2407/Mutclust\u0026#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Find CCMs # i = 1 tag = f\u0026#34;test{i}\u0026#34; input_path = \u0026#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.pickle\u0026#34; outdir = f\u0026#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/\u0026#34; Path(outdir).mkdir(parents=True, exist_ok=True) info = set_env(input = input_path, output = outdir) Input_df = readPickle(input_path) init(Input_df, info) mutInfo, ccms = get_candidate_core_mutations(Input_df, info, tag, i) --- Configurations --- Input data: \u0026#39;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.pickle\u0026#39; (29903, 5) Output dir: \u0026#39;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_test1/\u0026#39; Parameters: Min Eps=5 Max Eps=1000 Min per_sum=0.0 Eps scaling factor=10.0 Expansion diminishing factor=3 Min cluster length=10 ---------------------- Searching candidate core mutations... 1990 CCMs found. sample_ccm = ccms[0] mutInfo[sample_ccm] {\u0026#39;index\u0026#39;: 11, \u0026#39;Position\u0026#39;: 277, \u0026#39;Frequency\u0026#39;: 86, \u0026#39;Percentage\u0026#39;: 0.00038338430264178534, \u0026#39;Entropy\u0026#39;: 0.6078847228873923, \u0026#39;H-score\u0026#39;: 0.03323669788067187, \u0026#39;length\u0026#39;: 12, \u0026#39;freq_sum\u0026#39;: 1476, \u0026#39;freq_avr\u0026#39;: 123.0, \u0026#39;per_sum\u0026#39;: 0.0065799445430148274, \u0026#39;per_avr\u0026#39;: 0.0005483287119179023, \u0026#39;ent_sum\u0026#39;: 6.254087818941727, \u0026#39;ent_avr\u0026#39;: 0.5211739849118106, \u0026#39;H-score_sum\u0026#39;: 0.15877807556629392, \u0026#39;H-score_avr\u0026#39;: 0.01323150629719116, \u0026#39;eps_scaler\u0026#39;: 1, \u0026#39;left_distance\u0026#39;: 5, \u0026#39;right_distance\u0026#39;: 5, \u0026#39;l_pos\u0026#39;: 272, \u0026#39;r_pos\u0026#39;: 282, \u0026#39;mut_n\u0026#39;: 11} 3. Perform clustering # hotspots = dynaclust(mutInfo, ccms, info, tag, i) Performing dynamic clustering... 1990 clusters found Merging clusters... Merged clusters: 477 print(hotspots) left_position right_position length \\ 0 272 290 19 1 332 347 16 2 358 392 35 3 433 448 16 4 482 495 14 .. ... ... ... 472 29568 29577 10 473 29581 29599 19 474 29613 29633 21 475 29640 29651 12 476 29654 29671 18 mut_positions 0 272,273,274,275,277,278,279,280,281,282,283,28... 1 332,334,335,336,337,338,341,343,344,345,346,347 2 358,360,361,362,363,364,365,366,367,368,369,37... 3 433,435,436,437,438,439,440,441,442,443,444,44... 4 482,483,485,487,488,489,490,491,493,495 .. ... 472 29568,29570,29571,29572,29573,29574,29575,29577 473 29581,29583,29584,29585,29586,29587,29588,2958... 474 29613,29615,29616,29617,29618,29619,29620,2962... 475 29640,29641,29643,29645,29647,29648,29649,2965... 476 29654,29655,29656,29657,29659,29660,29661,2966... [477 rows x 4 columns] "},{"id":49,"href":"/docs/study/tech/tech31/","title":"#4 결과 검증: 임상 결과와의 연관성","section":"생물정보학","content":" #4 결과 검증: 임상 결과와의 연관성 # #2025-06-20\n1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(\u0026#34;/data/home/ysh980101/2407/Mutclust\u0026#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load COVID19 data # i = 1 tag = f\u0026#34;test{i}\u0026#34; resdir = f\u0026#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/\u0026#34; covid19_dir = \u0026#34;/data3/projects/2020_MUTCLUST/Data/Projects/COVID19/Sequence/Preprocessed/Nucleotide/Mutationinfo\u0026#34; meta_path = \u0026#34;/data/home/ysh980101/2506/data/meta.csv\u0026#34; hotspots = pd.read_csv(f\u0026#34;{resdir}clusters_{tag}.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;) metaData = pd.read_csv(meta_path, index_col=0) mutInfo = make_mutInfo_covid19(covid19_dir) mutSignature = make_mutSignature(mutInfo, hotspots, metaData) print(mutSignature) COV-CCO-001 COV-CCO-002 COV-CCO-003 COV-CCO-004 COV-CCO-006 \\ c0 0 0 0 0 0 c1 0 0 0 0 0 c2 0 0 0 0 0 c3 0 0 0 0 0 c4 0 0 0 0 0 ... ... ... ... ... ... c472 0 1 0 0 0 c473 0 0 0 0 0 c474 0 0 0 0 0 c475 0 0 0 0 0 c476 0 0 0 0 0 COV-CCO-008 COV-CCO-009 COV-CCO-010 COV-CCO-011 COV-CCO-013 ... \\ c0 0 0 0 0 0 ... c1 0 0 0 0 0 ... c2 0 0 0 0 0 ... c3 0 0 0 0 0 ... c4 0 0 0 0 0 ... ... ... ... ... ... ... ... c472 0 0 0 0 0 ... c473 0 0 0 0 0 ... c474 0 0 0 0 0 ... c475 0 0 0 0 0 ... c476 0 0 0 0 0 ... [477 rows x 387 columns] 3. Select severity related hotspots # sig_hotspots, significance = select_sig_hotspots(mutSignature, metaData, hotspots) significance Hotspot p-value FDR Significant 0 c22 1.882327e-07 4.489349e-06 True 1 c90 1.158366e-03 2.051443e-02 True 2 c118 9.750940e-15 1.162800e-12 True 3 c123 8.587634e-14 6.827169e-12 True 4 c124 1.051981e-03 2.007179e-02 True 5 c198 2.827480e-10 1.123923e-08 True 6 c239 5.739929e-16 2.737946e-13 True 7 c258 1.489502e-08 4.301825e-07 True 8 c292 6.617715e-07 1.372457e-05 True 9 c298 1.205966e-04 2.396858e-03 True 10 c309 2.746212e-08 7.277461e-07 True 11 c315 7.603734e-08 1.908937e-06 True 12 c319 5.323421e-07 1.154215e-05 True 13 c334 4.989612e-10 1.830804e-08 True 14 c337 8.625002e-12 4.114126e-10 True 15 c350 4.178970e-07 9.492232e-06 True 16 c364 9.750940e-15 1.162800e-12 True 17 c385 4.112387e-13 2.802298e-11 True 18 c390 1.161194e-03 2.051443e-02 True 19 c412 5.946573e-12 3.151684e-10 True 20 c429 3.073511e-09 9.773764e-08 True 21 c431 7.755493e-14 6.827169e-12 True 22 c438 1.929048e-09 6.572544e-08 True 23 c442 5.644954e-13 3.365804e-11 True 24 c444 1.362927e-15 3.250582e-13 True 25 c455 1.928137e-03 3.171454e-02 True 26 c460 1.723333e-03 2.935821e-02 True 27 c462 1.533145e-08 4.301825e-07 True 28 c468 1.169796e-11 5.072662e-10 True outdir = \u0026#34;result/\u0026#34; sig_hotspots.to_csv(f\u0026#34;{outdir}sig_hotspots.csv\u0026#34;, index=False) 만든건 저장하기.\n"},{"id":50,"href":"/docs/study/luck/luck16/","title":"6월 18-20일","section":"﹂#","content":" 6월 18-20일 # #2025-06-20\n오늘은먼가또!!!!! 불안이 몰려오는데 그럼 할일을했냐?하면 리비전작업은 가시화된 결과물이 없고 인적성공부를 안했고 코테는 오전에좀공부했지만 많이는못했다. 인성검사공부는 오늘시작했는데 막막하고어렵다. 운동도안갔다. 그럼 할일을안해놨으니 우울한게 당연함!! 심지어 이번주는 잠도 좀 부족했다.\n우울하지 않으려면 아무리 바빠도 인적성/코테 공부를 11시-12시에는 꼭 하자. (오전 집중시간애 리비전을 안하는게 불안하니깐 무리해서 오전에 할필요는없음) 그리고 8시-9시 운동은 꼭 가자.\n생각을넘많이하지말구 주변에 건강하게 의지하면서 할일을 하자\n"},{"id":51,"href":"/docs/study/tech/tech28/","title":"#5 Revision","section":"생물정보학","content":" #5 Revision # #2025-06-19\nReviewer 1 - Comment 1 # \u0026ldquo;In the introduction section, the authors note that most computational methods focus on the frequency of mutation occurrences rather than mutation diversity. This point should be more thoroughly discussed, with a clear explanation of the advantages and potential insights offered by analyzing mutation diversity.\u0026rdquo;\n“서론에서 저자들은 대부분의 계산 방법들이 돌연변이 발생 빈도에 집중하고 있으며, 돌연변이 다양성(mutation diversity)을 간과한다고 언급하였습니다. 돌연변이 다양성을 분석하는 것의 장점과 잠재적인 통찰에 대해 보다 명확하게 논의해 주시기 바랍니다.”\n저희 접근법의 이론적 근거, 생물학적 맥락, 선행 연구들과의 연계성, 그리고 기존 빈도 기반 방법과의 비교를 바탕으로 자세히 설명드립니다.\n[돌연변이 빈도 기반 접근법의 한계]\n기존의 대부분의 계산 기반 돌연변이 핫스팟 탐지 방법은, 특정 유전체 위치에서의 돌연변이 빈도(frequency)에 기반해 관심 영역을 정의해왔습니다. 이 접근은 특정 변이가 여러 샘플에서 반복적으로 관찰될수록 기능적으로 중요할 가능성이 높다는 전제하에 설계되어 있습니다. 예를 들어, 암 유전체에서의 driver mutation 탐지에 유용한 방법입니다. [a]\n하지만 이 방식은 다음과 같은 한계점을 지니고 있습니다:\n희귀하지만 다양한 변이들을 간과: 하나의 위치에서 다양한 돌연변이들이 각각 낮은 빈도로 존재할 수 있으며, 이는 바이러스의 적응, 면역 회피, 재조합 등을 반영합니다. 빈도 기반 필터링은 이러한 패턴을 쉽게 놓칩니다. 계통군 영향: 변이 빈도는 특정 계통(lineage)에 따라 좌우되는 경향이 있습니다. 빈도 기반 방법은 계통 정의 변이(lineage-defining mutation)를 기능적 중요성과 혼동할 수 있습니다. 따라서, 단순 빈도 분석만으로는 바이러스 유전체 내의 진화적/기능적 동역학을 충분히 포착하기 어렵습니다.\n[돌연변이 다양성 분석 - 선행 연구 사례]\n돌연변이 다양성(mutation diversity)은 일반적으로 샤논 엔트로피(Shannon entropy) 혹은 유사한 지표를 통해 정량화되며, 특정 위치에서 서로 다른 염기들의 분포와 균등성을 반영합니다. 엔트로피가 높은 위치는 다양한 염기가 발생하고 있다는 것을 나타내며, 이는 기능적 유연성 또는 양성 선택이 작용하는 지점을 의미할 수 있습니다.\n엔트로피 기반 접근법을 활용해 바이러스 유전체를 돌연변이 다양성 측면에서 분석한 선행 연구 사례는 다음과 같습니다:\nRouchka et al. (2024): HIV, HCV, SARS-CoV-2를 비교 분석하여, 엔트로피가 높은 위치가 면역 회피 및 기능적 도메인과 일치함을 보고하였습니다. Singh et al. (2022, PLoS Pathogens): 델타 → 오미크론 전환기 동안 스파이크 단백질의 엔트로피가 급격히 증가하며, 전염성과 면역 회피 성질 변화가 동반됨을 확인하였습니다. Kim et al. (2025, Nat Commun): 백신 접종 이후 스파이크 영역의 엔트로피가 감소했지만, 회피 부위에서는 높은 다양성이 유지됨을 보였습니다. Veeravalli et al. (2023): 샤논 엔트로피 및 Hellinger 거리 기반 분석으로 시간에 따른 구성적 다양성 변화를 추적하였습니다. 정리하면, 엔트로피가 높은 위치는 에피톱, 표면 노출 부위, 면역 회피 부위와 겹치며, 잠재적 변이 발생지 조기 탐지에도 활용 가능함이 확인되었습니다. 단순 빈도 분석에 비해 계통 유래 효과에 덜 민감하면서 특정 위치에서의 진화 경향을 유연성 측면에서 반영하기 때문에 면역적으로 중요한 유연한 영역 탐지에 활용될 수 있습니다.\n477개가 entropy만, frequency만 썻을대 어떻게 뽑히는지 개수가 너무 많을거같음\nReviewer 1 - Comment 4 # \u0026ldquo;The full name and definition of the H-score should be provided, along with a clear explanation of why it was chosen for this study.\u0026rdquo;\n“H-score의 전체 이름과 정의를 제공하고, 본 연구에서 이를 선택한 이유를 명확히 설명해 주세요.”\nH-score는 유전체 내 특정 위치의 돌연변이 중요도를 정량화하기 위한 복합 지표로, 아래 두 요소의 곱으로 계산됩니다: [1] 해당 위치에서 돌연변이가 발생할 확률 (mutation frequency) [2] 돌연변이가 발생한 경우의 돌연변이 엔트로피 (mutation entropy).\n[기존 접근의 한계]\n저희는 빈도 기반 단독 접근과 엔트로피 단독 접근의 한계를 극복하기 위해 H-score를 도입하였습니다.\n빈도만 사용하는 접근의 한계는 다음과 같습니다:\n다양하지만 희귀한 변이들을 놓침: 빈도는 낮지만 다양한 변이들이 기능적으로 중요할 수 있으나, 빈도 필터에 의해 배제됩니다. 계통 편향에 취약: 특정 변이가 자주 나타나는 것은 기능보다는 계통적 이유일 수 있습니다. 샤논 엔트로피만 사용하는 접근의 한계는 다음과 같습니다:\n돌연변이 자체의 빈도를 반영하지 않음: 특정 변이 위치가 매우 다양한 경우라도, 전체 샘플의 1%에만 존재한다면 (즉 선택률이 낮다면) 중요성이 높다고 판단하기 어렵습니다. [H-score의 균형적 특성]\n저희는 돌연변이의 발생 빈도와 다양성을 동시에 반영할 수 있는 H-score를 제안합니다. 이는 특정 위치에서 돌연변이가 발생했을 때의 조건부 엔트로피를 계산하고, 그 위치의 돌연변이 빈도와 곱하여 계산합니다.\n이러한 이중 초점 방식은 다음을 가능하게 합니다:\n빈도와 다양성의 통합적 반영: H-score는 자주 변이되며 동시에 다양한 방식으로 변이된 위치를 강조합니다. 이는 선택 압력의 주요 feature를 보존(기능적 중요성)과 다양성(유연성) 두 측면으로 보고, 이를 동시에 반영하는 진정한 핫스팟을 식별합니다. 엔트로피는 돌연변이된 염기만 고려: 보존된 참조 염기는 제외되므로, 변이 분포의 다양성 즉 유연성을 좀 더 강조합니다. [개념적 및 생물학적 해석력]\nH-score는 선택 압력을 정량화한 돌연변이 중요도 지표입니다. 선택 압력은 frequency로 나타난 보존적 특성과 entropy로 나타난 유연성으로 정량화되었습니다. 다시 말해, 양성 선택이 적용된 자주 변이되며 다양한 방식으로 변이되는 위치는 양성 선택에 기여한 \u0026lsquo;중요도\u0026rsquo;에 따른 결과라는 원리를 반영합니다.\n타 연구에 미루어 보면, 양성 선택에 기여한 \u0026lsquo;중요도\u0026rsquo;는 다음과 같은 의미를 가질 수 있습니다:\n면역 회피 가능성 백신 저항성 병원성 변화 환자 예후 변화 strain이\nReviewer 1 - Comment 5 # \u0026ldquo;The full name of MutClust should be included, and the rationale for selecting it over other clustering methods should be elaborated.\u0026rdquo;\n“MutClust의 전체 이름을 명시하고, 다른 클러스터링 방법들보다 이를 선택한 이유를 구체적으로 설명해 주세요.”\n[MutClust의 전체 이름]\nMutClust는 선형 바이러스 유전체 상의 돌연변이 핫스팟을 탐지하기 위해 저희가 직접 개발한 밀도 및 다양성 인식 클러스터링 알고리즘입니다. 기존 클러스터링 기법이 단순한 공간적 거리나 빈도에만 의존하는 반면, MutClust는 생물학적 중요도(H-score)를 반영하고, 동적 밀도 조정 및 감쇠 전략을 통해 선택압에 따른 돌연변이 패턴을 정교하게 포착할 수 있습니다.\n[일반적인 클러스터링 방법의 한계]\n기존의 K-means, 계층적 클러스터링 등의 일반적 알고리즘은 바이러스 유전체에서의 돌연변이 핫스팟 탐지에 직접 적용하기엔 여러 한계를 지닙니다. 이는 다음과 같은 바이러스 유전체의 생물학적 특성 때문입니다.\n불규칙한 클러스터 형태와 크기: 핫스팟의 길이와 밀도는 다양합니다. 클러스터 수 미지정: 생물학적으로 의미 있는 돌연변이 군집 수는 사전에 알 수 없습니다. 노이즈 존재: 무의미한 돌연변이 등이 유의미한 돌연변이 식별 분석을 방해합니다. [밀도 기반 접근을 수행한 타 연구]\n이에 밀도 기반 클러스터링 알고리즘인 DBSCAN이 돌연변이 핫스팟 분석에 효과적으로 활용된 바 있습니다.\nIdentifying recurrent mutations in cancer reveals widespread lineage diversity and mutational specificity: DBSCAN을 사용하여 프로모터나 스플라이스 부위와 같은 기능적 요소와 겹치는 의미 있는 돌연변이 영역(SMR, significantly mutated regions)을 식별하였습니다. 각 암 유형에서 SMR에 포함된 mutation을 가진 환자군을 구분하여 분석하였고 특정 SMR를 가진 환자군이 유의미하게 나쁜 예후 또는 표현형적 특징 차이를 보이는 경우 확인, 일부 SMR는 암 발생 경로가 알려진 유전자 경로(예: p53 signaling, PI3K/AKT 경로)와 연관되어 있었습니다. 이는 탐지된 hotspot이 임상적 표현형, 예후, 치료 반응 등과도 연계됨을 줍니다. Unsupervised clustering analysis of SARS-Cov-2 population structure reveals six major subtypes at early stage across the world: t-SNE와 DBSCAN을 결합하여 SARS-CoV-2 변이를 클러스터링하고, 초기 아형 구조 및 계통 확산 패턴을 규명하였습니다. Extended methods for spatial cell classification with DBSCAN-CellX: DBSCAN을 커스텀한 DBSCAN-CellX 알고리즘을 개발하였습니다. Local adaptive ε \u0026amp; minPts 설정으로 세포 밀도를 기반으로 위치별 ε 조정해서 세포가 희박한 위치는 더 넓게, 밀집된 위치는 좁게 탐색하였고 Core / Edge / Noise 3분류를 수행해서 기존의 이분법(core/noise)에서 벗어나 edge 세포를 따로 구분하여 생물학적으로 중요한 경계 특성을 반영하여 클러스터링하였습니다. Core / Edge / Noise 비율 분석 결과 고밀도 배양 세포에서 core 세포가 중심에 몰리는 edge cell 비율이 나타남을 확인하여 DBSCAN‑CellX가 구조를 잘 반영하고 있음을 확인하였고 다양한 세포주에 적용 결과 각 세포주마다 밀도, 분포 양상이 다름에도 불구, 클러스터 형태 재현이 잘 동작함을 확인하였습니다. [특수 목적 알고리즘의 필요성]\n하지만 DBSCAN의 다음과 같은 특성에 따라 특정 데이터에서는 그대로 적용하기에 부적합합니다.\nGlobal ε, MinPts 고정: 돌연변이는 전체 데이터(유전체) 상에서 특정 영역에 집중되어 나타납니다. 비균일한 데이터 분포 상황에서 위치마다 동일한 파라미터를 적용하면 저밀도 영역은 클러스터 누락 고밀도 영역은 클러스터 과도 확장이 발생할 수 있습니다. Edge의 무조건적 통합: DBSCAN은 데이터 포인트를 Core, Edge, Noise로 분류하며 Edge를 따로 취급하지 않고 Core에 같은 클러스터로 소속시킵니다. 노이즈가 많은 돌연변이 데이터의 특성상 Edge의 통합 여부를 결정할 척도, 즉 중요도 지표가 존재한다면 생물학적 중요도를 반영한 동적 밀도 조정 및 감쇠 전략을 통해 선택압에 따른 돌연변이 패턴을 정교하게 포착 가능할 것입니다. 데이터 포인트 별 중요도 가중치 반영 불가: Core 선택애는 데이터 간 거리 즉 밀도만 고려됩니다. 중요도 지표와 밀도를 모두 고려하여 Core를 선택하고 클러스터를 생성한다면 밀도 기반으로 중요한 클러스터를 포착할 때 생물학적 중요도를 반영 가능할 것입니다. 특수 목적 알고리즘의 필요성에 따라 MutClust는 다음과 같은 요건을 충족하기 위해 설계되었습니다:\n중요도 기반 Local ε 설정으로 클러스터 형성에 돌연변이의 density와 중요도를 모두 반영 기존의 edge 처리 방식에서 벗어나, density와 중요도를 반영하여 cluster에 edge 포함 유무를 판단(하여 경계를 보정)하는 알고리즘을 도입하여 potential edge의 중요도와 데이터의 density를 모두 반영하여 클러스터 크기 즉 경계 설정을 커스텀 가능하게함 (diminishing factor) 이에 따라 기존 DBSCAN을 기반으로 하되, 바이러스 유전체 분석에 맞춰 구조를 확장한 MutClust를 개발하였습니다.\nCCM non CCM 사이 중요도 판단? ccm으로 선별된 애들이 리니지 결정일 확률이 높지 않나 생각이 든다. sars cov 2 돌연변이 db 중에 annotation된 애들이 잇으면\ndbscan과 비교 해보기. 중요한 핫스팟을 못찾는다.\nReviewer 2 - Comment 1 # “The results of this study hold significant value but are buried under technical redundancy. Condensing the manuscript and focusing only on the key contributions will enhance clarity and appeal to a broader audience.”\n“이 연구의 결과는 상당한 가치를 지니고 있으나, 과도한 기술적 설명으로 인해 그 가치가 묻혀 있습니다. 원고를 간결하게 다듬고 핵심 기여에 집중한다면 명확성이 향상되어 더 폭넓은 독자층에 어필할 수 있을 것입니다.”\n저희는 본 연구의 가치가 방법론적 참신성과 그 함의에 있으며, 과도한 기술적 세부 사항이 이러한 기여를 흐릴 수 있다는 우려에 동의합니다.\n본 연구의 핵심 기여는 MutClust의 개발입니다. 이는 DBSCAN 기반 밀도 클러스터링 알고리즘을 유전체 돌연변이의 생물학적 특성에 맞게 적응시킨 새로운 알고리즘입니다. 이 커스터마이징은 기존 DBSCAN의 다음과 같은 한계를 극복하기 위한 것입니다: [1] 고정된 밀도 파라미터 사용으로 지역별 돌연변이 중요도 변화에 민감하지 않음 [2] 단순 빈도 기준 필터링으로 낮은 빈도의 기능적으로 중요한 돌연변이 탐지가 어려움 [3] 클러스터 경계가 자동으로 결정되어 생물학적 신호를 반영한 커스텀 불가\n이에 다음 특성을 반영하여 설계되었으며: [1] 중요도 가중 클러스터링 [2] 지역 적응형 파라미터 [3] 감쇠 계수 기반 경계 제어\n이러한 개선을 통해 MutClust는 단순히 빈도 높은 클러스터뿐 아니라 기능적으로 중요한 돌연변이 군집을 보다 세밀하게 포착하였으며 생물학적/통계적 유의성을 검증하는 여러 결과를 통해 그 타당성을 입증하였습니다.\n[방법론 검증 – 6가지 평가로 구조화]\n이전에는 사용된 기술 방법론에 따라 생물학적 해석 및 검증이 흩어져 있었으나 현재는 MutClust의 유효성을 다음 여섯 가지 평가 기준에 따라 명확히 정리하였습니다:\n기존 기능적 돌연변이와의 중복: MutClust는 SARS-CoV-2 스파이크 단백질 내의 기능적으로 특성화된 10개 돌연변이 중 9개를 성공적으로 재탐지하였으며, 이는 본 알고리즘이 핵심 기능적 위치를 정확히 포착함을 시사합니다. 계통학적 분석: 일부 핫스팟은 계통 정의 돌연변이와 중첩되었지만, 일부는 기존 계통 기반 분석으로는 포착되지 않았던 새로운 기능 기반 군집으로 확인되어, MutClust가 계통학적 접근의 한계를 보완함을 보여줍니다. 통계적 유의성 (부트스트랩 기반 검증): 무작위 기대 분포에 기반한 부트스트랩 분석을 통해, 탐지된 클러스터는 통계적으로 유의하게 무작위성에서 벗어남이 입증되었습니다. 임상 결과와의 연관성: COVID-19 환자들을 핫스팟 돌연변이 개수 기준으로 계층화한 결과, 특정 핫스팟 돌연변이 수가 많은 환자일수록 COVID-19 중증도가 높았습니다. 이들 바이러스는 NK 세포 기능 변동에 영향을 주었으며, 이는 환자 NK 세포 수용체 교란과 동반되었습니다. 중요도 점수 방법 비교: Shannon 엔트로피와 mutation entropy를 비교하여 핫스팟 우선순위를 평가한 결과, mutation entropy를 포함했을 때 중증 연관 핫스팟이 일관되게 상위에 랭크되어, H-중요도 설계의 타당성을 입증하였습니다. 타 바이러스 적용 가능성: MutClust를 인플루엔자 유전체에 적용한 결과, 다른 돌연변이율과 분포 특성에도 불구하고 의미 있는 핫스팟이 식별되었고, 기능적으로 알려진 돌연변이도 일부 재탐지되어 알고리즘의 범용성이 확인되었습니다. 핫스팟이 생물학적으로 어떤 중요성을 띠는가? 왜 mutation hotspot을 찾고싶은가? 에피톱에 변이가 생겨서 그로 인한 immune evasion 즉 mhc tcr 에피톱 binding affinity가 변화함에 따라 면역 기작이 달라진다. 그 기작에 영향을 주는 mutation hotspot을 찾음.\n[원고 수정]\n본 연구의 핵심인 방법론적 기여가 강조되도록 결과 섹션을 위의 6가지 생물학적 검증 중심 구조로 재구성하여 각 결과의 의미와 근거가 명확히 드러나도록 수정하였습니다.\n또한 본문의 Introduction을 핵심 기여에 집중하도록 아래와 같이 수정하였습니다.\n바이러스 돌연변이의 진화적 동역학과 임상적 영향을 이해하기 위해서는 기능적으로 중요한 돌연변이 핫스팟의 식별이 필요하다. 초기 연구들은 주로 돌연변이 빈도에 초점을 맞췄지만, 최근 연구들은 생물학적으로 의미 있는 변이를 중립적 혹은 승객 돌연변이와 구분하기 위해 엔트로피로 측정되는 돌연변이 다양성의 중요성을 강조하고 있다. 샤논 지수를 사용하여 계산되는 돌연변이 엔트로피는 단순히 돌연변이의 존재 여부뿐만 아니라, 다양한 개체군에서의 변이 패턴을 반영한다. 엔트로피가 높은 돌연변이 지점은 선택 압력을 받거나 면역 회피와 관련된 부위일 가능성이 높으며, 특히 SARS-CoV-2와 같은 빠르게 진화하는 바이러스에서 그러하다. 예를 들어, 엔트로피 기반 분석은 SARS-CoV-2 스파이크 단백질 내 면역원성 다양성을 강조하거나, 바이러스 적응과 관련된 돌연변이 시그니처를 식별하거나, 집단 면역 환경에서의 회피 변이의 시간적 추적에 사용되어 왔다. 그러나 엔트로피는 유용함에도 불구하고, 돌연변이의 공간적 조직이나 유전체 상에서의 클러스터링 구조에 대한 정보를 제공하지 못한다는 한계가 있다.\n이러한 한계를 보완하기 위해, DBSCAN과 같은 밀도 기반 클러스터링 알고리즘이 도입되었으며, 이는 비정규적이고 불균일한 공간 분포를 가진 돌연변이 핫스팟을 탐지하는 데 강력한 도구로 부상했다. 기존의 슬라이딩 윈도우나 고정된 구간 기반 접근 방식과 달리, DBSCAN은 임의 형태의 클러스터를 탐지할 수 있고, 이상치(잡음 돌연변이)를 처리하며, 사전에 클러스터 수를 지정할 필요가 없기 때문에, 돌연변이가 고르게 분포하지 않고 생물학적으로 관련된 신호가 희소할 수 있는 바이러스 유전체 분석에 특히 적합하다. 실제로, 이전의 여러 연구에서는 DBSCAN을 활용하여 암 유전체 및 바이러스 데이터에서 공간적 돌연변이 클러스터를 성공적으로 탐지한 바 있다. 예를 들어, 한 주요 범암종 연구에서는 밀도 기반 방법을 이용해 비암호화 DNA의 조절 영역에 풍부한 의미 있는 돌연변이 영역(SMRs)을 식별하였다. 바이러스학 분야에서는 DBSCAN이 차원 축소 기법(t-SNE)과 함께 사용되어 SARS-CoV-2 변이체를 전 세계 아형으로 분리함으로써, 기존의 계통 분류만으로는 설명되지 않는 지리적 및 계통학적 구조를 조명하였다. 또한, 공간 전사체 연구에서는 DBSCAN이 지역 세포 밀도 변화를 반영할 수 있도록 맞춤형으로 조정되어, 적절한 튜닝을 통해 다양한 생물학 데이터에 유연하게 적용될 수 있음을 보여주었다.\n그럼에도 불구하고, 고전적 DBSCAN은 바이러스 돌연변이 데이터에 적용될 때 몇 가지 중요한 한계를 가진다. 첫째, 돌연변이의 중요성(예: 엔트로피나 임상 연관성)을 클러스터링 과정에 통합할 수 있는 메커니즘이 없다. 둘째, 글로벌 ε 및 MinPts 값에 의존하기 때문에 지역별 밀도 이질성에 민감하여, 생물학적으로 구별되어야 할 영역을 과도하게 분할하거나 병합할 수 있다. 셋째, 전통적인 DBSCAN은 경계점(border point)을 단순한 클러스터 확장의 일부로 간주하기 때문에, 에피토프 회피 영역이나 조절 경계와 같은 기능적으로 중요한 주변부 돌연변이의 역할을 간과할 수 있다.\n이러한 문제를 해결하기 위해 우리는 SARS-CoV-2 유전체 상에서 돌연변이 핫스팟을 탐지하도록 설계된 완전 맞춤형 밀도 기반 클러스터링 알고리즘인 MutClust를 제안한다. MutClust는 DBSCAN의 기본 틀을 바탕으로 세 가지 주요 혁신을 통해 확장되었다. 첫째, ε와 MinPts를 지역별 돌연변이 밀도 및 중요도(H-score, 즉 돌연변이 빈도와 엔트로피의 함수)를 기준으로 조정하는 지역 적응형 파라미터 체계를 도입하였다. 둘째, 클러스터 확장을 거리와 지역 엔트로피 신호에 따라 감쇠시키는 경계 인식 확장 알고리즘(diminishing factor)을 구현함으로써, 수동적 컷오프 없이 클러스터 경계를 세밀하게 조절할 수 있도록 하였다. 셋째, 경계점의 생물학적 중요성을 평가하여 선택적으로 통합하는 기능을 추가하여, 단순히 이웃이라는 이유만으로 클러스터에 포함시키지 않도록 하였다.\n이러한 수정은 단순히 돌연변이 빈도가 높은 영역이 아니라, 다양성과 기능적 중요성이 교차하는 영역, 즉 생물학적으로 중요한 돌연변이 핫스팟을 탐지하는 데 초점을 맞춘 생물학적 목적의 문제를 해결하기 위해 고안되었다. MutClust를 SARS-CoV-2 유전체 22만 건 이상에 적용한 결과, 바이러스 전반에서 총 477개의 돌연변이 핫스팟을 식별하였으며, 이 중 28개는 COVID-19의 임상적 중증도와 강한 연관성을 보였다. 이들 핫스팟은 알려진 에피토프나 구조적 영역과의 기능적 중첩, NK 세포 반응의 이상과의 연관성, 계통수 상의 클레이드 일치성 등 여러 근거를 통해 검증되었다. 또한, MutClust는 인플루엔자 유전체 등 다른 바이러스 종에서도 견고한 성능을 보여, 바이러스 유전체 전반에 걸쳐 활용 가능한 잠재력을 지녔다.\n요약하자면, 본 연구는 (i) 돌연변이 엔트로피와 밀도 기반 클러스터링의 개념적 통합을 통해 핫스팟을 탐지하고, (ii) 바이러스 유전체 분석 요구에 맞춘 유연하고 생물학적으로 해석 가능한 DBSCAN 확장 알고리즘을 제시하며, (iii) 임상, 통계, 기능적 검증을 통해 이를 평가한 계산 프레임워크를 제공한다. 이 결과들은 MutClust가 바이러스 진화 분석 및 기능 유전체학에 있어 유용한 도구임을 강조한다.\n[a] Identifying recurrent mutations in cancer reveals widespread lineage diversity and mutational specificity\nReviewer 2 – Comment 4 # \u0026ldquo;Network propagation/DEG suggest NK receptor imbalance but lack causal evidence linking mutations to HLA affinity changes. Moreover, the model in Fig. 5 seems to lack direct evidence from patient samples.\u0026rdquo;\n\u0026ldquo;네트워크 전파 및 차등 발현 유전자 분석은 NK 수용체 불균형을 시사하지만, 해당 돌연변이들이 HLA 결합 친화도 변화를 유발한다는 인과적 증거는 부족합니다. 또한 Figure 5에 제시된 모델은 환자 샘플로부터의 직접적인 실험적 증거가 부족합니다.\u0026rdquo;\nNK 수용체 불균형 해석과 Figure 5에서 제시한 모델에 직접적인 분자적 증거가 부족하다는 지적에 대해 감사드립니다. 저희 역시 SARS-CoV-2 돌연변이와 선천 면역 조절 장애 간의 인과관계를 현재 데이터만으로 확정할 수 없음을 명확히 인지하고 있습니다. 그럼에도 불구하고, 본 연구에서 제시한 결과는 면역학적 맥락에 기반한 의미 있는 가설 생성 증거를 제공하며, 향후 탐색적 연구로 이어질 수 있는 충분한 생물학적 가능성을 시사한다고 생각합니다.\n[본 연구의 목적: ‘중요도’ 기반 핫스팟 발굴]\n본 연구의 궁극적 목표는 기능적으로 중요한 돌연변이 핫스팟을 탐지할 수 있는 해석 가능한 클러스터링 알고리즘(MutClust)을 개발하는 것이었습니다. 여기서 돌연변이의 중요성은 \u0026lsquo;선택 압력\u0026rsquo;을 정량화한 중요도 수치로 나타냈습니다. 타 바이러스 면역 연구에 미루어 보면, 양성 선택에 기여한 \u0026lsquo;중요도\u0026rsquo;는 다음과 같은 의미를 가질 수 있습니다: [1] 면역 회피 가능성 [2] 백신 저항성 [3] 전파력 변화 [4] 환자 예후 변화\n기존 연구들이 특정 형질(예: 항생제 내성, 사망률 등)을 명확히 정의하고 시작하는 반면, 저희는 [1] 비지도 클러스터링 기반의 돌연변이 후보 탐색 [2] 사후 생물학적 검증이라는 경로를 따랐습니다. 사전 정의된 라벨에 의존하지 않기 때문에, 기존의 라벨 (계통 정의 유무, 전파력 증가/면역 회피 등의 기능 확인 유무 등)에서 비교적 자유롭게, 의미 있는 생물학적 변이를 감지할 수 있는 구조입니다.\n중요도 및 밀도 기반 클러스터링 알고리즘인 MutClust로 중요한 돌연변이 핫스팟 후보 477개를 식별하였고, 특정 핫스팟이 COVID-19 예후가 나쁜 환자에서 돌연변이가 높았으며 전사체 수준에서 NK 신호 이상과의 일관된 연관성을 보였습니다.\n[Figure 5 모델과 한계]\n저희는 Figure 5가 환자 단위의 기능적 검증(HLA 결합 실험, 펩타이드 가공 확인, NK 세포 살상력 측정 등)에 기반하지 않았다는 점에 동의합니다. 이 도식은 돌연변이 burden, 환자군 층화, 사이토카인 불균형, 유전자 네트워크 전파 등에서 관측된 상관관계 기반 증거를 요약한 개념 모델이며, 기전적 경로로 해석되어서는 안 됩니다.\n이를 명확히 하기 위해, 원고 본문과 그림 캡션 모두에서 Figure 5는 가설 기반 요약 모델임을 분명히 명시하겠습니다. 또한, 현재 데이터의 한계와 향후 기전 검증 연구의 필요성을 분리된 단락으로 서술하겠습니다.\n[돌연변이와 NK 수용체 신호 간 연관성]\n직접적인 인과관계를 입증하지는 못했지만, 다음과 같은 면역학적으로 타당한 정황 근거들이 관측되었습니다:\nc315 핫스팟은 스파이크 단백질 내 HLA 제시 펩타이드 영역에 위치하며, CD8+ 및 CD4+ T 세포 에피토프와 겹칩니다. 해당 위치의 돌연변이는 NetMHCpan 4.1 예측에 따르면 HLA 앵커 잔기 패턴을 변경시킬 수 있으며, HLA 결합력에 영향을 줄 가능성이 있습니다 (현재는 계산 기반 추정). c442 핫스팟은 ORF3a 영역에 위치하며, 과거 연구에 따르면 ORF3a는 NLRP3 인플라마좀 활성화 및 숙주 세포 스트레스 반응 조절에 관여합니다. 비록 HLA 에피토프는 아니지만, 펩타이드 처리 및 구조 변화로 면역 인식에 영향을 줄 수 있습니다. 전사체 DEG 기반 네트워크 전파 분석에서는 활성 수용체 및 억제 수용체의 일관된 발현 증가가 확인되었으며, 이는 선천면역 활성 상태가 변이 보유 환자에서 달라졌을 가능성을 시사합니다. 이는 분자적 기전을 확정하는 것은 아니지만, 게놈-전사체 수준에서 면역 이상 특징을 형성하는 데 충분한 근거입니다.\n[시스템 면역학에서 형질과 돌연변이 간의 연결 접근법]\n시스템 면역학 연구에서는 형질-연관 유전적 특징을 발굴하는 데 있어 일반적으로 다음 둘 중 하나의 방법론을 따릅니다: [1] 데이터 기반 탐색과 간접적 검증 [2] 중요 특징 선택 후 실험적 검증.\n저희 연구는 비지도 클러스터링을 통한 탐색 후 간접 검증에 해당하며, Mutclust를 통해 탐색한 중요한 후보 핫스팟에 대한 실험적 검증은 수행되지 않고 간접적으로만 검증하였으며 이후 수행된 검증이 명확한 기능적 연결고리를 보이지 않을 수 있음은 인지하고 있습니다. 그러나 변수가 많은 생물학 데이터의 특성상 기전적으로 타당한 가설이라 하더라도 완전한 증거를 찾기는 어렵습니다. 그럼에도 불구하고 엄격한 가설 설정과 통계적 검증을 기반으로 설계된 면역 기전 후보는 또다른 가설 생성을 위한 출발점으로 기능할 수 있습니다. 예를 들어 c315, c442 등 일부 핫스팟이 기존 계통 정의 변이 또는 기능이 알려진 변이와 겹치지 않음은, 오히려 이전까지 알려지지 않은 면역 상호작용 부위일 가능성을 제시합니다.\n[본 연구의 공헌과 향후 연구 방향]\n저희는 본 연구가 다음 두 측면에서 기여한다고 판단합니다:\n방법론적 공헌: 밀도 기반 클러스터링 알고리즘이 밀도와 다양성(엔트로피)를 모두 반영하여 생물학적으로 의미 있는 돌연변이 핫스팟을 탐지할 수 있음을 입증 생물학적 통찰 제시: 일부 핫스팟은 기존 변이와 겹치고, 일부는 신규 변이입니다. 이들에 대한 면역학 기반 해석은 실험 면역학 및 생물정보학 연구자들에게 유용한 가설을 제시할 수 있습니다. 이에 다음과 같은 향후 연구 방향을 설정할 수 있습니다:\nHLA 결합력 및 NK 세포 활성 실험 기반 검증 해당 핫스팟과 질병 경과 간 임상 연관성 분석 MutClust의 기타 바이러스 및 암 유전체에의 적용 및 일반화 가능성 평가 [결론]\n정리하자면, 저희도 돌연변이와 NK 수용체 활성 간의 직접적인 인과 연결 고리는 아직 입증되지 않았음을 인정합니다. 이에 원고 본문과 그림 캡션 모두에서 Figure 5는 가설 기반 요약 모델임을 분명히 명시하겠습니다.\n그러나 본 연구의 접근은 면역학적으로 타당하고, 통계적으로 견고하며, 해석 가능성을 갖춘 프레임워크입니다. 특정 핫스팟에서 돌연변이 burden이 높은 환자들에서 NK 경로의 이상 조절이 나타났다는 관찰은 충분히 후속 연구의 동기가 되며, SARS-CoV-2의 숙주 면역 상호작용에 대한 이해를 확장시키는 데 기여한다고 생각합니다. 원고에서는 이러한 한계를 명확히 밝히고, 본 결과가 제시하는 연구의 공헌과 향후 연구 방향 강조하도록 수정하겠습니다.\n"},{"id":52,"href":"/docs/study/career/career1/","title":"SK AX 인성검사 준비","section":"취업","content":" SK AX 인성검사 준비 # #2025-06-19\n1. 서류 전형 # 서류가 적부심사였는지.. 붙어버려서 생애첫 인성검사 준비를 하게되었다\n2. SKCT 인성검사 # SK 인재상은 패기, 실행력, 협업, 윤리의식이구\n이 블로그 글을 보니깐 솔직하게 하되 인재상에 맞추는것두 어느정도 필요하다는거같아서 잡플래닛에서 유료 검사랑 이런저런 무료 검사도 해보겟지만 일단 챗지피티에 돌려서 예시문제 100개 뽑음. 노션에 문제만 붙여넣기 해서 좀 외운담에 실제 인성검사연습을 해볼려고한다. SKCT 인성검사 이론 정리 및 예시 문항\nSKCT 모의인성검사 360개 만든거\n3. 잡플랫 PTS 대기업 인성검사 합격예측솔루션 # https://pts.jobplat.co.kr/Jobplat.Product/DetailView/3920?scate=M210\n모의인성검사 해봤는데\n너무 구라를 많이 쳤나..\n차별화 포인트로 \u0026lsquo;도전\u0026rsquo; \u0026lsquo;정직\u0026rsquo; \u0026lsquo;조직생활\u0026rsquo;만 가져가고 리더십 감정조절 이타심 이쪽은 버리는게 낫나? ㅜㅜ 어렵군\n"},{"id":53,"href":"/docs/study/tech/tech24/","title":"#3 Density based clustering 알고리즘 - DBSCAN","section":"생물정보학","content":" #3 Density based clustering 알고리즘 - DBSCAN # #2025-06-18\n1. 유전체 돌연변이 연구와 DBSCAN # 유전체 돌연변이 연구에서 DBSCAN 또는 유사한 density-based clustering을 통해 군집을 식별하는 여러 연구가 있다.\nDBSCAN이 사용된 유전체 돌연변이 연구들은 서로 다른 바이러스나 유전체 영역을 분석했지만\n사용한 데이터에는 공통 특성이 있다. 변이의 비정규적 분포 (non-uniform): 돌연변이는 일정 위치에 집중되는 hotspot 현상을 보인다. ex) spike 단백질 특정 영역에 몰림. 클러스터 수 미정: 몇 개의 변이 집단(hotspot)이 존재하는지 사전 지식이 없다. 군집의 불규칙한 모양과 크기: hotspot의 길이, 모양(밀도, 거리)이 다양하다. 노이즈 존재: 무작위적 돌연변이, 측정 오류 등으로 인해 의미 없는 변이들(outlier)가 섞여 있다. 2. 왜 density-based clustering이 적절한가? # DBSCAN의 기본 아이디어는\n같은 군집 내에서는 점들이 서로 가깝고 군집 사이에는 점 간 거리가 멀다. 또한 일정 밀도 이하의 점은 군집이 아닌 노이즈로 간주한다. 이는 다음과 같이 효과를 발휘한다\n전체 데이터로부터 군집을 형성할 때 군집의 분포에 대한 사전 가정 없음 군집의 분포가 비정규적, 일정 위치에 집중되는 경향이 있을 때 효과적 군집 수 지정 불필요 몇 개의 변이 집단(hotspot)이 존재하는지 사전 지식이 없을 때 효과적 불규칙한 모양의 클러스터 탐지 가능 군집의 길이, 모양이 다양할 때 효과적 밀도 차를 이용해 클러스터 경계 형성(노이즈 제거) 무작위적 돌연변이, 측정 오류 등으로 인해 의미 없는 변이들(outlier)을 제거할 때에 밀도를 반영하고 싶을 때 효과적 3. DBSCAN의 핫스팟 검출 성능 판단 # 기능적 영역(Functional Domains)과의 중복률 분석\n단백질 도메인 (Pfam), splice site, promoter region, 5′/3′ UTR, transcription factor binding site 등 기능적 유전체 요소들과 유의하게 겹쳤는지 통계적으로 평가 결과, 무작위 구간 대비 기능 영역과의 중첩 빈도가 유의하게 높음 (p-value \u0026lt; 1e-5 수준) 즉, 이 SMR들이 우연히 모인 hotspot이 아니라 기능적으로 중요한 유전체 위치에 집중된 것임을 입증함 [1] 타겟 유전자와의 중복도 분석\n타겟 유전자가 암 드라이버 유전자였고 전체 SMR 중 약 30~50%가 Cancer Gene Census (CGC)에 포함된 드라이버 유전자에 존재하거나 overlap됨 일부 SMR는 기존 driver mutation 메서드(MutSigCV 등)로는 탐지되지 않았지만 DBSCAN 기반 SMR에 포함되어 있었고 Cancer Gene Census (CGC)에 포함된 드라이버 유전자였다 이는 SMR가 단순한 변이 밀집 영역이 아니라 ‘암을 유발할 수 있는’ 기능적 hotspot일 가능성이 높다는 것을 보여줌 [1] 타 feature와의 연관성 분석\n각 암 유형에서 SMR에 포함된 mutation을 가진 환자군을 구분하여 분석 (feature: 암 유형, 예후) 특정 SMR를 가진 환자군이 유의미하게 나쁜 예후 또는 표현형적 특징 차이를 보이는 경우 확인 일부 SMR는 암 발생 경로가 알려진 유전자 경로(예: p53 signaling, PI3K/AKT 경로)와 연관되어 있었음 이는 탐지된 hotspot이 임상적 표현형, 예후, 치료 반응 등과도 연계됨을 보여줌 [1] 각 클러스터가 특정 대륙에 치우쳐 있는지 혹은 전 지리권에 고르게 분포하는지 분석 (feature: 대륙, 분포 양상) 오세아니아는: 모든 cluster가 거의 균등하게 존재 = 다양한 variant 존재 유럽은: 특정 cluster가 강하게 집중됨 → 지역 유행 strain 반영 아시아, 아프리카는: 일부 cluster 집중 및 몇 개 variant에 편중 즉 DBSCAN이 생성한 클러스터가 단순 벡터 유사도 기반 grouping이 아니라 지역 유행성과 유전적 계통 차이를 반영한 군집임을 보여줌 clustering 결과가 epidemiological insight 제공 가능성 확인 [2] 각 클러스터의 대표 서열을 계통수 상에 표시했을 때, 같은 클러스터 내 서열들이 계통수에서도 인접하는지 아닌지 확인 (feature: 계통수와 matching 유무) 같은 클러스터 내 서열들이 계통수에서도 인접함 즉 클러스터 내 유전적 근접성과 진화적 근접성이 일치함을 확인 [2] 통계적 유의성 분석\n각 SMR가 우연히 형성된 것이 아니라는 것을 입증하기 위해 Monte Carlo 시뮬레이션 기반의 p-value 산출 FDR (False Discovery Rate) ≤ 0.05 기준으로 유의한 SMR만 남김 이로써 “우연히 밀집된 변이 집합”이 아닌, 통계적으로 신뢰할 수 있는 hotspot임을 검증 [1] 타 method와 정량적 지표 기반 비교\nK-means, Hierarchical, BIRCH과 Intra-cluster genetic distance(각 클러스터 내 서열 간 pairwise distance 평균) 비교하였고 hierarchical/BIRCH보다 응집도 높음을 확인 noise 제거와 경계 구분에서 더 우수함을 확인 [2] 딥러닝 기반 clustering (Deep Embedded Clustering, DEC)과 Intra-cluster 거리 및 Silhouette score, SSE (Sum of Squared Error), BIC를 비교하였고 딥러닝보다 응집도는 약간 낮았으나 Silhouette score를 봣을때 클러스터 간 이질성(inter-cluster separation)이 뛰어남 [2] 시각적 구조 평가\nt-SNE plot 시각화 결과 각 군집 간 경계 명확 (well-separated clusters) 군집 내 샘플들이 밀집도 유지 (dense core clusters) 일부 샘플은 Noise (DBSCAN의 outlier 분류)로 감지되어 클러스터 경계 바깥에 위치함 을 확인 즉 군집 수를 미리 지정하지 않아도 자동으로 cluster + noise를 구분할 수 있었음을 눈으로 확인 [2] 세포 밀도 기반으로 ε 및 minPts 조합을 자동 선택하는 성능 검증? 여러 ε–minPts 조합에 대해 실제 세포 이미지와 결과 클러스터 위치의 일치도를 눈으로 검토 클러스터 형상이 세포 덩어리와 실제 겹치는지의 시각적 일치를 통해 “올바른 클러스터”를 판단 [3] 4. DBSCAN의 커스텀과 성능 평가 및 후속 검증 # DBSCAN의 다음과 같은 단점에 따라 특정 데이터에서는 그대로 적용하기에 부적합\n군집 내 지역 밀도 차이 (일부 클러스터는 매우 조밀하고, 다른 클러스터는 상대적으로 희박)가 있는 데이터\nGlobal ε, MinPts 고정이어서 세포 배양 밀도(density)가 이미지 내 위치마다 다른데 동일한 파라미터를 적용하면 저밀도 영역은 클러스터 누락 고밀도 영역은 클러스터 과도 확장 발생함 [3] Edge 구분 불가능이어서 세포를 core vs. noise로만 나누며 경계(edge)에 있는 세포를 명확히 분류하지 못함 [3] 중요도를 반영하여 edge를 선택적으로 통합하는 등, 경계 즉 클러스터 size를 robust하게 커스텀하지 못함 [4] 클러스터 수 불확정성 때문에 실험 조건, 이미지 품질, 배양 상태에 따라 군집 수가 크게 달라져 비교·해석이 어려움 [3] 데이터 별 중요도 가중치 반영 불가 여서 클러스터 형성에 돌연변이의 density만 반영되어 중요도를 반영하지 못함 [4] DBSCAN‑CellX는 다음과 같은 구조적 커스터마이징을 통해 위 문제를 해결:\nLocal adaptive ε \u0026amp; minPts 설정으로 세포 밀도를 기반으로 위치별 ε 조정해서 세포가 희박한 위치는 더 넓게, 밀집된 위치는 좁게 탐색함 Core / Edge / Noise 3분류를 수행해서 기존의 이분법(core/noise)에서 벗어나 edge 세포를 따로 구분하여 생물학적으로 중요한 경계 특성 반영 자동 파라미터 튜닝 알고리즘 탑재해서 사용자가 설정할 필요 없이, 이미지에서 local density를 추정하여 적절한 파라미터 추론함 [3] Mutclust는 다음과 같은 커스터마이징:\n중요도 기반 Local adaptive ε \u0026amp; minPts 설정으로 클러스터 형성에 돌연변이의 density와 중요도를 모두 반영 기존의 edge 처리 방식(core에 edge를 포함시킴)에서 벗어나, density와 중요도를 반영하여 cluster에 edge 포함 유무를 판단(하여 경계를 보정)하는 알고리즘을 도입하여 potential edge의 중요도와 데이터의 density를 모두 반영하여 클러스터 크기 즉 경계 설정을 커스텀 가능하게함 (diminishing factor) [4] cf) DBSCAN의 작동 방식\nDBSCAN은 세 데이터를 다음 세 부류로 분류: Core point: 반경 ε 내에 minPts 이상 이웃이 있는 점 (밀집 지역의 중심) Border point: Core의 이웃이지만 minPts 조건은 안 됨 (경계에 있는 점) Noise point: 어떤 클러스터에도 속하지 않음 (외부 이상치) 이때 Border point를 따로 취급하지 않고, 그냥 Core와 같은 클러스터에 소속시킴 DBSCAN‑CellX의 클러스터 성능 검증\n시각적 유효성 평가: 여러 ε, minPts 설정에 따라 클러스터링 결과를 실제 세포 이미지에 중첩하여 시각적으로 비교 Core / Edge / Noise 비율 분석 edge cell 비율이 높은지 낮은지를 확인, 높은 경우는 core 세포가 중심에 몰리는 구조로 해석되고 고밀도 배양 세포에서 이와 같이 나오는것을 확인. ex) 고밀도 배양: edge 20%, core 70%, noise 10% / 저밀도 배양: edge 40%, core 30%, noise 30% 이를 통해 DBSCAN‑CellX가 구조를 잘 반영하고 있음을 확인 다양한 세포주(cell lines)에 적용 여러 세포주 (e.g., T84, HeLa, H2B-turquoise 등)에 적용하여 범용성 평가 각 세포주마다 밀도, 분포 양상이 다름에도 불구하고 자동 파라미터 탐색 경계 세포 구분 클러스터 형태 재현이 잘 작동함 [3] Mutclust의 클러스터 성능 검증\n기능이 알려진 중요한 돌연변이와 중복률 분석 spike 단백질의 기능이 알려진 중요한 돌연변이 10개 중 9개를 잘 포함하고 있음. 타 feature와의 연관성 분석 (feature: covid-19 예후) covid-19에서 hotspot에 포함된 Mutation 개수에 따라 환자를 구분하여 분석 특정 hotspot의 mutation이 높은 환자군이 유의미하게 나쁜 예후를 보이는 것을 확인 해당 환자군과 환자군 유래 바이러스 분석 결과 NK 수용체의 교란이 확인 이는 탐지된 hotspot이 임상적 예후, 선천 면역 반응 등과도 연계됨을 보여줌 타 feature와의 연관성 분석 (feature: 계통수) 각 hotspot이 계통수 결정 돌연변이를 포함하고 있는지를 분석 어떤 hotspot은 계통수 결정 돌연변이를 포함, 어떤 핫스팟은 미포함 어떤 hotspot은 적은 계통수와 연관, 어떤 hotspot은 다양한 계통수와 연관 즉 식별한 클러스터가 단순 유전적 계통 차이를 기반으로 하는 군집을 포함함과 동시에 유전적 계통 차이 외에도 밝혀지지 않은 중요한 feature를 반영한 군집임을 보여줌 이와 같은 군집들은 단순 유전적 계통 차이를 기반으로 한 연구에서는 밝히기 어려움 통계적 유의성 분석 각 핫스팟이 우연히 형성된 것이 아니라는 것을 입증하기 위해 bootstrap으로 통계 분석 수행 naive method와 정량적 지표 기반 비교 중요도 지표 계산 시 mutation entropy를 도입한 경우와 shannon entropy를 도입한 경우를 계산하고 계산된 중요도를 확인 나쁜 예후를 보인 환자에서 mutation이 높았던 중요 핫스팟 2개의 중요도가 mutation entropy를 도입한 경우에서 훨씬 상위권에 위치 해당 핫스팟 식별에 해당 방법론이 유용하게 작용함을 확인 시각적 평가: elbow plot dbscan의 파라미터 결정 방식에서 사용되는 elbow plot을 해당 방법론에 적절하게 수정(기존: k번째로 가까운 데이터포인트까지의 거리로 plotting/수정: k번째로 가까우면서 클러스터 형성 조건인 h-score 평균, Minpts, h-score 합 등을 만족하는 클러스터가 형성될때까지의 거리로 plotting) \u0026lsquo;중요한\u0026rsquo; 477개 핫스팟이 elbow 아래에 위치함을 확인 \u0026lsquo;중증도와 연관된\u0026rsquo; 28개 핫스팟이 elbow 아래에 위치하면서도 그중 낮은 위치(좋음)에 위치함을 확인 파라미터 설정이 \u0026lsquo;중요한\u0026rsquo; 핫스팟을 잘 식별하도록 설정됨을 확인 범용성 평가: 다른 virus에 적용 Influenza genome에 적용하여 범용성 평가 데이터 수, 돌연변이의 밀도, 분포 양상이 다름에도 불구하고 클러스터가 잘 포착되었으며 기능이 알려진 중요한 돌연변이와 중복 또한 일부 확인됨. Reference # [1] Identifying recurrent mutations in cancer reveals widespread lineage diversity and mutational specificity [2] Unsupervised clustering analysis of SARS-Cov-2 population structure reveals six major subtypes at early stage across the world [3] Extended methods for spatial cell classification with DBSCAN-CellX [4] Our research\n"},{"id":54,"href":"/docs/study/tech/tech25/","title":"#4 Clustering 알고리즘의 parametric test","section":"생물정보학","content":" #4 Clustering 알고리즘의 parametric test # #2025-06-18\n정답 label이 없는 unsupervised learning인 clustering은 supervised learning과 달리 정확도, AUC curve 등으로 성능 평가 불가\n정량적 평가 지표로는:\nIntra-cluster genetic distance (클러스터 내 유전 거리): 작을수록 내부 군집 응집도가 좋음 Silhouette score, SSE, BIC 등의 지표 사 그 외 방법으로는:\n방향성이 같은 또는 같지 않아야 하는 비교 feature를 선택하고 비교 ex) 계통학적 구조가 지리적 패턴과 일치함 t‑SNE 시각화 등 시각적 확인 t‑SNE로 축소된 2D scatter plot 위에 DBSCAN으로 얻은 cluster를 색상별로 표시해서 군집 간의 명확한 경계, 군집 내 응집성이 시각적으로 확인 "},{"id":55,"href":"/docs/study/luck/luck15/","title":"6월 18일","section":"﹂#","content":" 6월 18일 # #2025-06-18\n오늘할일\n인적성 1회 코테 1문제 리비전작업 "},{"id":56,"href":"/docs/study/tech/tech22/","title":"#1 Entropy 기반 mutation 분석","section":"생물정보학","content":" #1 Entropy 기반 mutation 분석 # #2025-06-17\n1. 단백질 서열과 샤넌 엔트로피 # 샤논 엔트로피는 단백질 서열 상 특정 위치에 다양한 아미노산이 얼마나 골고루 존재하는지를 나타내는 지표이다.\n어떤 위치에 여러 아미노산이 비슷한 비율로 존재한다면 엔트로피가 높고, 하나의 아미노산이 압도적으로 우세하다면 엔트로피가 낮다. 전통적인 샤논 엔트로피에 대한 해석은 논코딩 영역의 식별이다.\n염기의 돌연변이에 따른 아미노산의 결실 및 변동은 개체에 대부분은 부정적인 영향을 줌으로써 돌연변이를 가진 개체가 태어날 수 없게 할 확률이 높기 때문이다. 하지만 일부 돌연변이는 개체의 발생에 영향을 주지 않으며, 극히 일부는 \u0026lsquo;살아남는 데 그치지 않고\u0026rsquo; 각기 다양한 방식으로 개체의 생존력을 높임으로써 \u0026lsquo;진화\u0026rsquo;의 원인이 되기도 한다. [1] 이에 샤넌 엔트로피가 높은 위치,\n즉 \u0026lsquo;살아남은 염기 다양성\u0026rsquo;에 대한 분석은 흥미로운 결과를 낼 수 있다. 샤넌 엔트로피가 높다는 것은\n바이러스가 해당 아미노산 자리에 대해 여러 가능한 \u0026lsquo;돌연변이 조합\u0026rsquo;을 실험하고 있다는 신호로 해석될 수 있다. 돌연변이는 바이러스의 적응력(adaptability), 면역 회피 능력(immune evasion), 치료 저항성(antiviral resistance) 등에 영향을 줄 수 있고 돌연변이가 다양할수록 특정 돌연변이가 선택받아 우세해지는 기반(pool)이 되기 때문에 숙주의 방어(면역, 백신, 치료제)를 회피할 확률이 높아진다. 2. 엔트로피 기반의 돌연변이 연구 # 샤넌 엔트로피를 기반으로\n돌연변이의 다양성을 정량화하고 높은 엔트로피를 보인 위치를 식별 및 분석하는 여러 연구들이 수행되었다. 이는 다양한 에피톱에 대응하는 백신 설계, 저항성 돌연변이 출현 시점 예측을 통한 항바이러스제 개발, 전파 가능성 높은 변이 조기 감지를 통한 공중보건 대응 전략 수립 등에 사용할 수 있다. (높은 엔트로피 영역에 대한) 분석 결과\n면역 회피 영역에서 높은 entropy, 필수 보존 유전자에서 낮은 entropy가 관측됨에 따라 고 entropy 영역은 선택 압력, 면역 회피 및 진화 적응이 일어난 자리로 해석되었다. [2] VOCs의 VOCs의 주요 변이 자리(N501Y, E484K 등)에서 높은 entropy가 관측되었다. [3] 오미크론 출현기에 spike protein 부위 entropy가 급등함이 확인됨에 따라 VOCs의 출현 시기와 Entropy 급등 시기 사이 연관성이 있으며 시점별 entropy 변화는 전파력 및 면역 회피 변이의 확산 시점을 포착할 수 있다고 해석되었다. [4] 백신 접종 후 spike 영역 다양성 감소가 확인되었고 escape 부위는 지속적으로 다양성을 유지함이 확인됨에 따라 백신 접종 전후와 entropy 증감 사이 연관성이 있으며 백신이 일부 epitope의 다양성을 억제하였다고 해석되었다. 면역 회피 부위와 다양성 유지 사이 연관성이 있음도 확인되었다. [5] Shannon Entropy + Hellinger Distance 조합으로 구성적 돌연변이 분포 변화를 분석하였고 VOC 출현 시점에서 구성적 다양성 변화가 급증함을 확인함으로써 VOCs의 출현 시기와 ‘구성적 다양성 지표’ 급등 시기 사이 연관성이 있으며 엔트로피 기반으로 한 해당 지표로 다양한 돌연변이가 새롭게 출현한 시점을 수치적으로 정량화 가능하다고 해석하였다. [6] 하지만 synonymous/nonsynonymous 돌연변이를 분리해서 분석한 연구에서는 Shannon entropy 단독으론 기능적 영향 해석이 어렵고 분석을 위해서는 구조-기능 정보가 필요함을 확인하였다. [7] 우리 연구는?\nMutation diversity focused entropy + Frequency 조합으로 다양하게 발생한 돌연변이 영역을 식별하였고 면역 회피 능력, 강화된 전파능, 중화 항체 기능 감소, 인간 세포 침입능 강화 등의 기능을 가진 여러 돌연변이를 포착했으며 중증 outcome을 보인 환자 그룹에서 돌연변이 빈도가 높음이 확인됨에 따라 해당 영역과 중증 outcome 사이 연관성이 있다고 해석 후 여러 임상 데이터로 이를 검증하였다. Reference # [1] Information theory in molecular biology [2] Information-Theoretic Approaches for the Analysis of Genetic Diversity in Viral Populations [3] Mutational landscape of SARS-CoV-2 reveals three mutually exclusive clusters of mutations [4] Temporal entropy of SARS-CoV-2 genome reflects adaptive evolution during Delta to Omicron transition [5] Impact of vaccination on genetic diversity of SARS-CoV-2 spike protein [6] Entropy and Hellinger distance highlight mutational shifts in SARS-CoV-2 evolution [7] Codon-level entropy analysis differentiates synonymous and functional mutation hotspots\n"},{"id":57,"href":"/docs/study/tech/tech20/","title":"#1 입력 데이터 생성","section":"생물정보학","content":" #1 입력 데이터 생성 # #2025-06-17\nLoad package # %load_ext autoreload %autoreload 2 import sys import os sys.path.append(\u0026#39;/data3/projects/2025_Antibiotics/YSH/bin\u0026#39;) from sc import * os.chdir(\u0026#39;/data3/projects/2025_Antibiotics/YSH\u0026#39;) Check data # raw_path = \u0026#39;/data3/projects/2025_Antibiotics/YSH/res/sev_dict_filtered.pkl\u0026#39; with open(raw_path, \u0026#39;rb\u0026#39;) as f: raw_data = pickle.load(f) keys = list(raw_data.keys()) print(len(keys)) print(keys[0], \u0026#39;\\n\u0026#39;, raw_data[keys[0]]) 4515 74374 Date NEWS med_cnt med_list \\ 0 2020-10-30 4 2 Trizele;Cefotaxime 1 2020-10-31 4 2 Trizele;Cefotaxime 2 2020-11-01 12 2 Pospenem;Pospenem_2 3 2020-11-02 9 3 Pospenem;Meropen;Vanco Kit 4 2020-11-03 12 2 Vanco Kit;Meropen 5 2020-11-04 8 2 Vanco Kit;Meropen 6 2020-11-05 9 0 strain 0 [] 1 [] 2 [] 3 [Enterobacter cloacae ssp cloacae] 4 [Enterobacter cloacae ssp cloacae] 5 [Enterobacter cloacae ssp cloacae] 6 [Enterobacter cloacae ssp cloacae] 4515명 환자 데이터이고\n첫번째 환자 \u0026lsquo;74374\u0026rsquo;의 데이터를 확인해보면 날짜, NEWS 중증도 점수, 항생제 투여 횟수, 항생제 투여 종류, 균주 정보가 있다.\nindir = \u0026#39;res\u0026#39; with open(f\u0026#34;{indir}/all_meds.txt\u0026#34;, \u0026#39;r\u0026#39;) as f: all_meds = [line.strip() for line in f if line.strip()] all_meds = [s.replace(\u0026#34;/\u0026#34;, \u0026#34;_\u0026#34;) for s in all_meds] print(len(all_meds)) print(all_meds) 169 [\u0026#39;Sevatrim\u0026#39;, \u0026#39;Nystatin syrup\u0026#39;, \u0026#39;Fungizone\u0026#39;, \u0026#39;Vancozin\u0026#39;, \u0026#39;Gavir\u0026#39;, \u0026#39;Regkirona\u0026#39;, \u0026#39;Omnicef Granule_g\u0026#39;, \u0026#39;Pyrazinamide\u0026#39;, \u0026#39;Cotrim\u0026#39;, \u0026#39;Ubacsin\u0026#39;, \u0026#39;Netilmicin\u0026#39;, \u0026#39;Cycin\u0026#39;, \u0026#39;Amoxicle\u0026#39;, \u0026#39;Vancomycin HCl\u0026#39;, \u0026#39;Anycef\u0026#39;, \u0026#39;Valcyte\u0026#39;, \u0026#39;Septrin tab\u0026#39;, \u0026#39;Imicil Kit\u0026#39;, \u0026#39;Rifampin\u0026#39;, \u0026#39;Enped\u0026#39;, \u0026#39;Meropen\u0026#39;, \u0026#39;Valvirus\u0026#39;, \u0026#39;Azitops\u0026#39;, \u0026#39;Viramid\u0026#39;, \u0026#39;Cymevene\u0026#39;, \u0026#39;Flumarin\u0026#39;, \u0026#39;Yuhanzid\u0026#39;, \u0026#39;Foxolin\u0026#39;, \u0026#39;Vgavir\u0026#39;, \u0026#39;Suprax\u0026#39;, \u0026#39;Vivir\u0026#39;, \u0026#39;Cefetat\u0026#39;, \u0026#39;Pospenem\u0026#39;, \u0026#39;Minocin\u0026#39;, \u0026#39;Ceftazidime\u0026#39;, \u0026#39;Banan dry syrup\u0026#39;, \u0026#39;Vivaquine\u0026#39;, \u0026#39;Rifodex\u0026#39;, \u0026#39;Duricef\u0026#39;, \u0026#39;Tygacil\u0026#39;, \u0026#39;Amocla duo tab\u0026#39;, \u0026#39;Famvics\u0026#39;, \u0026#39;Baraclude\u0026#39;, \u0026#39;Veklury\u0026#39;, \u0026#39;Taurolin\u0026#39;, \u0026#39;Diflucan POS\u0026#39;, \u0026#39;Rulid\u0026#39;, \u0026#39;Klaricid Dry syrup\u0026#39;, \u0026#39;Teracycline\u0026#39;, \u0026#39;Closerin\u0026#39;, \u0026#39;Zithromax Dry Syrup\u0026#39;, \u0026#39;Tapocin\u0026#39;, \u0026#39;Zinperazone\u0026#39;, \u0026#39;Amoxapen\u0026#39;, \u0026#39;Prevymis\u0026#39;, \u0026#39;Trison Kit\u0026#39;, \u0026#39;Aclova\u0026#39;, \u0026#39;Doxycyclin\u0026#39;, \u0026#39;Cefazedone\u0026#39;, \u0026#39;Finipenem\u0026#39;, \u0026#39;Cefazolin\u0026#39;, \u0026#39;Epocelin\u0026#39;, \u0026#39;Ceftezole\u0026#39;, \u0026#39;Ciprobay\u0026#39;, \u0026#39;Adefovir\u0026#39;, \u0026#39;Tamiflu\u0026#39;, \u0026#39;Zavicefta\u0026#39;, \u0026#39;Nafcillin Sodium\u0026#39;, \u0026#39;Bearcef\u0026#39;, \u0026#39;Linoped\u0026#39;, \u0026#39;Prepenem\u0026#39;, \u0026#39;Roxithromycin\u0026#39;, \u0026#39;Cravit\u0026#39;, \u0026#39;Invanz\u0026#39;, \u0026#39;Tobra\u0026#39;, \u0026#39;Zeffix\u0026#39;, \u0026#39;Sporanox cap\u0026#39;, \u0026#39;Ampibactam\u0026#39;, \u0026#39;Levoplus\u0026#39;, \u0026#39;Itra\u0026#39;, \u0026#39;Cravit tab\u0026#39;, \u0026#39;Viread\u0026#39;, \u0026#39;Zithromax\u0026#39;, \u0026#39;Penbrex\u0026#39;, \u0026#39;Paxlovid\u0026#39;, \u0026#39;Myambutol\u0026#39;, \u0026#39;Daptocin\u0026#39;, \u0026#39;Finibax\u0026#39;, \u0026#39;Zyvox\u0026#39;, \u0026#39;Omnicef\u0026#39;, \u0026#39;Colis\u0026#39;, \u0026#39;Amoxicillin\u0026#39;, \u0026#39;Flasinyl\u0026#39;, \u0026#39;Pentamidine Isethionate\u0026#39;, \u0026#39;Kaletra\u0026#39;, \u0026#39;Adikan\u0026#39;, \u0026#39;Maxipime\u0026#39;, \u0026#39;Amikacin\u0026#39;, \u0026#39;Triaxone\u0026#39;, \u0026#39;Ceradolan\u0026#39;, \u0026#39;Moveloxin\u0026#39;, \u0026#39;Meiact\u0026#39;, \u0026#39;Hanmiflu solution\u0026#39;, \u0026#39;Avelox\u0026#39;, \u0026#39;Acillin\u0026#39;, \u0026#39;Entecabell ODT\u0026#39;, \u0026#39;Fullgram\u0026#39;, \u0026#39;Ceftil\u0026#39;, \u0026#39;Augmentin\u0026#39;, \u0026#39;Remdesivir\u0026#39;, \u0026#39;Lagevrio\u0026#39;, \u0026#39;Lamiffix\u0026#39;, \u0026#39;Ambisome\u0026#39;, \u0026#39;Monodoxy-M\u0026#39;, \u0026#39;Unasyn\u0026#39;, \u0026#39;Prezcobix\u0026#39;, \u0026#39;Ceftriaxone\u0026#39;, \u0026#39;Noxafil tab\u0026#39;, \u0026#39;Tiroxin\u0026#39;, \u0026#39;Rukasyn\u0026#39;, \u0026#39;Amikin\u0026#39;, \u0026#39;Prothionamide\u0026#39;, \u0026#39;Gentamicin\u0026#39;, \u0026#39;Hanomycin\u0026#39;, \u0026#39;Monurol\u0026#39;, \u0026#39;Mezactam\u0026#39;, \u0026#39;Plunazol\u0026#39;, \u0026#39;Cancidas\u0026#39;, \u0026#39;Citopcin\u0026#39;, \u0026#39;Claric\u0026#39;, \u0026#39;Isepacin\u0026#39;, \u0026#39;Oxiklorine\u0026#39;, \u0026#39;Nitrofurantoin\u0026#39;, \u0026#39;Combicin\u0026#39;, \u0026#39;Mycamine\u0026#39;, \u0026#39;Amocla Duo Syrup\u0026#39;, \u0026#39;Distocide\u0026#39;, \u0026#39;Rulid D\u0026#39;, \u0026#39;Meicelin\u0026#39;, \u0026#39;Tazocin\u0026#39;, \u0026#39;Vfend\u0026#39;, \u0026#39;Zerbaxa\u0026#39;, \u0026#39;Akocin\u0026#39;, \u0026#39;Yamatetan\u0026#39;, \u0026#39;Oneflu\u0026#39;, \u0026#39;Sebivo\u0026#39;, \u0026#39;Enteone\u0026#39;, \u0026#39;Trizele\u0026#39;, \u0026#39;Gomcephin\u0026#39;, \u0026#39;Amocla\u0026#39;, \u0026#39;Banan Dry Syrup\u0026#39;, \u0026#39;Synagis\u0026#39;, \u0026#39;Isepamicin\u0026#39;, \u0026#39;Famvir\u0026#39;, \u0026#39;Dexamethasone Inj\u0026#39;, \u0026#39;Sporanox Oral Solution\u0026#39;, \u0026#39;Pamoxin Dry Syrup\u0026#39;, \u0026#39;Vanco Kit\u0026#39;, \u0026#39;Factive\u0026#39;, \u0026#39;Cefotaxime\u0026#39;, \u0026#39;Casfun\u0026#39;, \u0026#39;Banan\u0026#39;, \u0026#39;Tubes\u0026#39;, \u0026#39;Eraxis\u0026#39;, \u0026#39;Ubacillin\u0026#39;, \u0026#39;Dexamethasone\u0026#39;, \u0026#39;Normix\u0026#39;, \u0026#39;Peramiflu\u0026#39;, \u0026#39;Vemlidy\u0026#39;] 항생제 종류는 169종이고\n각 항생제에 따라 NEWS sequence를 생성해서 input data를 만들 예정이다.\nMake sequence # indir = \u0026#39;res\u0026#39; outdir = \u0026#39;data/sev_dict\u0026#39; for med in all_meds: print(med) sev_dict = make_sev_dict(med, indir, outdir) 항생제별로 sequence를 분리해서 위의 raw_data와 동일한 형식의 딕셔너리 169개를 outdir에 생성했다.\n이제 생성한 sequence의 길이를 10으로 맞출건데,\n항생제 투여 시점 기준으로\n투여 전 3일부터 투여 후 7일(D-3 ~ D+6) 10일짜리 NEWS sequence를 만들어줄 예정이다. #1 indir = \u0026#39;data/sev_dict\u0026#39; outdir = \u0026#39;data/timecourse\u0026#39; for med in all_meds: timecourse = make_timecourse(indir, outdir, med) #2 indir = \u0026#39;data/timecourse\u0026#39; outdir = \u0026#39;data/sev_idx\u0026#39; for med in all_meds: sev_idx = make_sev_idx(indir, outdir, med) #3 indir = \u0026#39;data/sev_dict\u0026#39; indexdir = \u0026#39;data/sev_idx\u0026#39; outdir = \u0026#39;data/res_dict\u0026#39; for med in all_meds: res_dict = make_res_dict(indir, indexdir, outdir, med) 각 항생제에 따라 10 day sequence를 생성해서 outdir에 저장했다.\ncur_path = \u0026#39;data/res_dict\u0026#39; cur_med = \u0026#39;Dexamethasone\u0026#39; with open(f\u0026#34;{cur_path}/{cur_med}\u0026#34;, \u0026#39;rb\u0026#39;) as f: res_dict = pickle.load(f) cur_keys = list(res_dict.keys()) print(len(cur_keys)) print(res_dict[cur_keys[0]]) 783 Date NEWS med_cnt strain 4 2017-08-18 4 0 [Staphylococcus epidermidis] 5 2017-08-19 4 0 [Staphylococcus epidermidis] 6 2017-08-20 4 0 [Staphylococcus epidermidis] 7 2017-08-21 4 1 [Staphylococcus epidermidis] 8 2017-08-22 3 1 [Staphylococcus epidermidis] 9 2017-08-23 4 1 [Staphylococcus epidermidis] 10 2017-08-24 4 1 [Pseudomonas aeruginosa] 11 2017-08-25 7 1 [Pseudomonas aeruginosa] 12 2017-08-26 4 1 [Pseudomonas aeruginosa] 13 2017-08-27 4 1 [Pseudomonas aeruginosa] 항생제 \u0026lsquo;Dexamethasone\u0026rsquo;에서 생성된 sequence를 확인해보면\n783개 sequence가 생성되었고 투여일(21일) 기준으로 투여전 3일, 투여후 7일로 잘 생성된것을 확인 가능하다! "},{"id":58,"href":"/docs/study/tech/tech27/","title":"#1 입력 데이터 생성","section":"생물정보학","content":" #1 입력 데이터 생성 # #2025-06-18\n1. Load package # import pandas as pd import numpy as np import os import pickle import ast os.chdir(\u0026#39;/data3/projects/2025_Antibiotics/YSH/\u0026#39;) 2. Load raw data # datadir = \u0026#39;/data3/projects/2025_Antibiotics/PreprocessedData/TimecourseData\u0026#39; outdir = \u0026#39;res\u0026#39; pids =[d for d in os.listdir(datadir) if os.path.isdir(os.path.join(datadir, d))] len(pids) 4589 datadir에 4589명 환자의 의료 데이터가 존재한다.\ncur_pid = pids[0] sev = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/SeverityScore.csv\u0026#34;) lab = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/Laboratory_processed.csv\u0026#34;) med = pd.read_csv(f\u0026#34;{datadir}/{cur_pid}/Medication.csv\u0026#34;) print(cur_pid) print(len(sev.columns.tolist()), sev.columns.tolist()) print(len(lab.columns.tolist()), lab.columns.tolist()) print(med) 74374 6, [\u0026#39;Date\u0026#39;, \u0026#39;NEWS\u0026#39;, \u0026#39;WHO\u0026#39;, \u0026#39;SOFA\u0026#39;, \u0026#39;PBS\u0026#39;, \u0026#39;qPitt\u0026#39;] 23, [\u0026#39;Date\u0026#39;, \u0026#39;ALT (U/L)\u0026#39;, \u0026#39;AST (U/L)\u0026#39;, \u0026#39;BUN (mg/dL)\u0026#39;, \u0026#39;Creatinine (mg/dL)\u0026#39;, \u0026#39;D-Dimer (ug/mL )\u0026#39;, \u0026#39;Ferritin (ng/mL)\u0026#39;, \u0026#39;HCO3 (mmol/L)\u0026#39;, \u0026#39;Hemoglobin (g/dL)\u0026#39;, \u0026#39;LDH (U/L)\u0026#39;, \u0026#39;Lymphocytes (%)\u0026#39;, \u0026#39;MDRD eGFR (mL/min/BSA)\u0026#39;, \u0026#39;Neutrophils (%)\u0026#39;, \u0026#39;O2 saturation (%)\u0026#39;, \u0026#39;PCO2 (mmHg)\u0026#39;, \u0026#39;PO2 (mmHg)\u0026#39;, \u0026#39;Platelet count (10^3/uL)\u0026#39;, \u0026#39;Potassium (mmol/L)\u0026#39;, \u0026#39;Sodium (mmol/L)\u0026#39;, \u0026#39;WBC count (10^3/uL)\u0026#39;, \u0026#39;hs-CRP (mg/dL)\u0026#39;, \u0026#39;pH ()\u0026#39;, \u0026#39;total CO2, calculated (mmol/L)\u0026#39;] Date antimicrobials antimicrobials_dose antimicrobials_2 \\ 0 2020-10-30 Trizele 500.0mg/2 Cefotaxime 1 2020-10-31 Trizele 500.0mg/3 Cefotaxime 2 2020-11-01 Pospenem 1.0g/1 Pospenem 3 2020-11-02 Pospenem 1.0g/1 Meropen 4 2020-11-03 Vanco Kit 1.0g/1 Meropen 5 2020-11-04 Vanco Kit 1.0g/1 Meropen 6 2020-11-05 NaN NaN NaN antimicrobials_2_dose antimicrobials_3 antimicrobials_3_dose 0 2.0mg/2 NaN NaN 1 2.0mg/3 NaN NaN 2 1.0g/2 NaN NaN 3 500.0mg/2 Vanco Kit 1.0g/1 4 500.0mg/3 NaN NaN 5 500.0mg/3 NaN NaN 6 NaN NaN NaN 환자 \u0026lsquo;74374\u0026rsquo;의 데이터를 예시로 확인해보면\n중증도 정보 sev에는 date별로 5개의 중증도 점수가 있고, 임상 정보 lab에는 date별로 22개의 임상 데이터가 있다. 항생제 정보 med에는 date 별로 투여한 항생제 종류와 투여 용량 데이터가 있다. 작업 계획은 # sev_dict = {} for pid in pids: try: # 1. SeverityScore 불러오기 sev = pd.read_csv(f\u0026#34;{datadir}/{pid}/SeverityScore.csv\u0026#34;) # 2. Laboratory 데이터 불러오기 및 병합 lab = pd.read_csv(f\u0026#34;{datadir}/{pid}/Laboratory_processed.csv\u0026#34;) sev = pd.merge(sev, lab, on=\u0026#39;Date\u0026#39;, how=\u0026#39;left\u0026#39;) # \u0026#39;Date\u0026#39; 기준 오른쪽에 lab 붙이기 # 3. Medication 불러오기 med = pd.read_csv(f\u0026#34;{datadir}/{pid}/Medication.csv\u0026#34;) med_filtered = med[med[\u0026#39;Date\u0026#39;].isin(sev[\u0026#39;Date\u0026#39;])] med_filtered = med_filtered.loc[:, ~med_filtered.columns.str.endswith(\u0026#39;_dose\u0026#39;)] # 4. Medication 관련 열 추가 sev[\u0026#39;med_cnt\u0026#39;] = 0 sev[\u0026#39;med_list\u0026#39;] = \u0026#34;\u0026#34; # 5. 날짜별로 약물 정보 병합 for _, row in med_filtered.iterrows(): cur_date = row[\u0026#39;Date\u0026#39;] cur_meds_raw = row.iloc[1:] cur_meds_clean = cur_meds_raw.dropna().tolist() med_freq = {} cur_meds = [] for med in cur_meds_clean: if med not in med_freq: med_freq[med] = 1 cur_meds.append(med) else: med_freq[med] += 1 cur_meds.append(f\u0026#34;{med}_{med_freq[med]}\u0026#34;) cur_med_cnt = len(cur_meds) cur_med_str = \u0026#34;;\u0026#34;.join(cur_meds) sev_idx = sev[sev[\u0026#39;Date\u0026#39;] == cur_date].index if len(sev_idx) \u0026gt; 0: sev.loc[sev_idx, \u0026#39;med_cnt\u0026#39;] = cur_med_cnt sev.loc[sev_idx, \u0026#39;med_list\u0026#39;] = cur_med_str zero_cnt = (sev[\u0026#39;med_cnt\u0026#39;] == 0).sum() except FileNotFoundError: sev[\u0026#39;med_cnt\u0026#39;] = \u0026#34;\u0026#34; sev[\u0026#39;med_list\u0026#39;] = \u0026#34;\u0026#34; zero_cnt = \u0026#34;N/A (no med file)\u0026#34; if sev.shape[0] == zero_cnt: print(pid) if zero_cnt == \u0026#34;N/A (no med file)\u0026#34;: print(pid, zero_cnt) sev_dict[pid] = sev "},{"id":59,"href":"/docs/study/tech/tech23/","title":"#2 Mutation hotspot 발굴 알고리즘","section":"생물정보학","content":" #2 Mutation hotspot 발굴 알고리즘 # #2025-06-17\n돌연변이는 무작위로 발생하지만\n실제 분포를 확인해보면 그렇지 않다. 엄연히 군집을 형성하고 있으며 이는 해당 돌연변이의 \u0026lsquo;생존\u0026rsquo;에 관여한 외부 요인의 존재를 시사한다. 논문 \u0026ldquo;Computational methods for detecting cancer hotspots\u0026quot;는\n암에서 반복적으로 관찰되는 돌연변이 즉 핫스팟(hotspot)을 식별하기 위한 계산적 방법 40여개를 리뷰하였는데 암에서 Hotspot mutation은 여러 환자에서 동일한 위치에 반복적으로 나타나는 돌연변이로써 우연히 발생할 가능성이 낮기 때문에 기능적 역할을 할 가능성이 높다고 간주됨에 따라 무의미한 hotspot을 거르고 중요한 hotspot 식별을 위한 여러 알고리즘이 고안되었다. [1] 우리 데이터는 암 유전체가 아닌 바이러스 유전체이지만 돌연변이 빈도가 높은 위치를 hotspot mutation으로 보는 시각이 동일하며 우연히 발생할 가능성이 낮기 때문에 기능적 역할을 할 가능성이 높다 \u0026laquo; 라는 가정 또한 일치하므로 중요한 hotspot 식별을 위해 고안된 해당 알고리즘들은 우리 알고리즘과 비교 대상으로 적절하다. "},{"id":60,"href":"/docs/study/tech/algo2/","title":"#2 베스트앨범","section":"생물정보학","content":" #2 베스트앨범 # #2025-06-17\n문제 # #문제 설명\n스트리밍 사이트에서 장르 별로 가장 많이 재생된 노래를 두 개씩 모아 베스트 앨범을 출시하려 합니다. 노래는 고유 번호로 구분하며, 노래를 수록하는 기준은 다음과 같습니다.\n속한 노래가 많이 재생된 장르를 먼저 수록합니다. 장르 내에서 많이 재생된 노래를 먼저 수록합니다. 장르 내에서 재생 횟수가 같은 노래 중에서는 고유 번호가 낮은 노래를 먼저 수록합니다. 노래의 장르를 나타내는 문자열 배열 genres와 노래별 재생 횟수를 나타내는 정수 배열 plays가 주어질 때, 베스트 앨범에 들어갈 노래의 고유 번호를 순서대로 return 하도록 solution 함수를 완성하세요.\n#제한사항\ngenres[i]는 고유번호가 i인 노래의 장르입니다. plays[i]는 고유번호가 i인 노래가 재생된 횟수입니다. genres와 plays의 길이는 같으며, 이는 1 이상 10,000 이하입니다. 장르 종류는 100개 미만입니다. 장르에 속한 곡이 하나라면, 하나의 곡만 선택합니다. 모든 장르는 재생된 횟수가 다릅니다. #입출력 예\ngenres plays return [\u0026ldquo;classic\u0026rdquo;, \u0026ldquo;pop\u0026rdquo;, \u0026ldquo;classic\u0026rdquo;, \u0026ldquo;classic\u0026rdquo;, \u0026ldquo;pop\u0026rdquo;] [500, 600, 150, 800, 2500] [4, 1, 3, 0] #입출력 예 설명\nclassic 장르는 1,450회 재생되었으며, classic 노래는 다음과 같습니다.\n고유 번호 3: 800회 재생 고유 번호 0: 500회 재생 고유 번호 2: 150회 재생 pop 장르는 3,100회 재생되었으며, pop 노래는 다음과 같습니다.\n고유 번호 4: 2,500회 재생 고유 번호 1: 600회 재생 따라서 pop 장르의 [4, 1]번 노래를 먼저, classic 장르의 [3, 0]번 노래를 그다음에 수록합니다.\n장르 별로 가장 많이 재생된 노래를 최대 두 개까지 모아 베스트 앨범을 출시하므로 2번 노래는 수록되지 않습니다. #정답\ndef solution(genres, plays): genre_total = {} # 장르별 총 재생 수 genre_songs = {} # 장르별 (재생 수, 고유 번호) 리스트 for i in range(len(genres)): genre = genres[i] play = plays[i] # 1. 총 재생 수 누적 if genre in genre_total: genre_total[genre] += play else: genre_total[genre] = play # 2. 장르별 노래 정보 저장 if genre in genre_songs: genre_songs[genre].append((i, play)) else: genre_songs[genre] = [(i, play)] # 3. 장르를 총 재생 수 기준으로 정렬 sorted_genres = sorted(genre_total.items(), key=lambda x: x[1], reverse=True) result = [] for genre, _ in sorted_genres: # 4. 각 장르 내에서 노래를 재생 수 기준 내림차순, 고유번호 오름차순 정렬 songs = sorted(genre_songs[genre], key=lambda x: (-x[1], x[0])) # 5. 최대 두 개까지 수록 for song in songs[:2]: result.append(song[0]) return result 풀이 # #단계별로보기\n장르별 재생수 딕셔너리 장르별 곡 딕셔너리 (인덱스와 재생수) 장르별 재생수 딕셔너리를 내림차순 정렬 장르별 곡 딕셔너리를 곡 재생수 내림차순, 고유번호 오름차순 정렬 2개까지 result에 수록 #1\nfor i in range(len(genres)): genre = genres[i] play = plays[i] if genre in genre_total: genre_total[genre] += play else: genre_total[genre] = play 장르별 재생수 딕셔너리 genre_total를 채움 결과: genre_total = {\u0026#39;classic\u0026#39;: _, \u0026#39;pop\u0026#39;: _} #2\nif genre in genre_songs: genre_songs[genre].append((i, play)) else: genre_songs[genre] = [(i, play)] 장르별 곡 딕셔너리 genre_songs를 채움 결과: genre_songs = {\u0026#39;classic\u0026#39;: [(0, _), (1, _), (2, _)], \u0026#39;pop\u0026#39;: [(3, _), (4, _)]} #3\nsorted_genres = sorted(genre_total.items(), key=lambda x: x[1], reverse=True) genre_total를 내림차순 정렬 #4\nfor genre, _ in sorted_genres: songs = sorted(genre_songs[genre], key=lambda x: (-x[1], x[0])) genre_songs를 곡 재생수 내림차순, 고유번호 오름차순 정렬 #5\nfor song in songs[:2]: result.append(song[0]) 2개까지 result에 수록 "},{"id":61,"href":"/docs/study/tech/tech21/","title":"#2 입력 feature 생성","section":"생물정보학","content":" #2 입력 feature 생성 # #2025-06-17\n1. Load package # %load_ext autoreload %autoreload 2 import sys import os import pickle import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages import pandas as pd sys.path.append(\u0026#39;/data3/projects/2025_Antibiotics/YSH/bin\u0026#39;) from sc import * os.chdir(\u0026#39;/data3/projects/2025_Antibiotics/YSH\u0026#39;) 2. Previous # seqdir = \u0026#39;data/res_dict\u0026#39; seq_list = os.listdir(seqdir) print(len(seq_list)) 169 항생제 169종에 대해서 size 10 sequence를 생성했었는데\n모델 입력 feature로 다음을 제외하는대신 antibiotics 리스트 strain 리스트 저 2개 feature를 반영하는 새로운 feature를 2개 생성하려고 한다: 현재 antibiotics가 현재 strain 환자의 NEWS를 감소시킨 이력이 있는지? (binary: 0/1) 현재 antibiotics가 NEWS를 감소시키는데 소요 기간은? (범주형: short/mid/long) 3. Create feature1 # 먼저 feature1을 생성하기 위해\n투여 후 NEWS가 감소한 sequence를 남기고 keep된 sequence의 균주-항생제 pair를 얻는데 이때 \u0026lsquo;투여 후 NEWS의 감소\u0026rsquo;는? 투여 전날(D-1) NEWS 수치와 투여 후 7일(D+0~D+6)를 봣을때 투여 후 최고치가 투여 전날보다 낮으면 NEWS가 감소한 것으로 보았다. med_dir = \u0026#39;res\u0026#39; seqdir = \u0026#39;data/res_dict\u0026#39; outdir = \u0026#39;res/feature1\u0026#39; with open(f\u0026#34;{med_dir}/all_meds.txt\u0026#34;, \u0026#39;r\u0026#39;) as f: all_meds = [line.strip() for line in f if line.strip()] all_meds = [s.replace(\u0026#34;/\u0026#34;, \u0026#34;_\u0026#34;) for s in all_meds] for med in all_meds: print(med) cur_path = f\u0026#39;{seqdir}/{med}.pkl\u0026#39; with open(cur_path, \u0026#39;rb\u0026#39;) as f: res_dict = pickle.load(f) feature1_list = [] for pid, df in res_dict.items(): news_bf = df.iloc[2][\u0026#39;NEWS\u0026#39;] # 3번째 행 (0-indexed) news_af = df.iloc[3:][\u0026#39;NEWS\u0026#39;].max() # 4번째 행부터 마지막까지 중 최댓값 if news_af \u0026lt; news_bf: # \u0026#34;작은\u0026#34; 경우만 (같은 건 포함하지 않음) feature1_list.append(pid) print(len(feature1_list)) filtered_res_dict = {pid: res_dict[pid] for pid in feature1_list if pid in res_dict} with open(f\u0026#34;{outdir}/{med}.pkl\u0026#34;, \u0026#39;wb\u0026#39;) as f: pickle.dump(filtered_res_dict, f) Dexamethasone에 대해 selected sequence를 시각화한것을 보면\n투여일(점선) 이후의 NEWS 수치들이 투여전날보다 낮은 것만 잘 선택된것을 확인 가능하다! strain_dic = {} for med in all_meds: cur_path = f\u0026#39;{outdir}/{med}.pkl\u0026#39; with open(cur_path, \u0026#39;rb\u0026#39;) as f: filtered_res_dict = pickle.load(f) for pid, df in filtered_res_dict.items(): if len(df) \u0026lt; 3: continue # 3번째 행이 없는 경우 skip try: cur_strain = df.iloc[2][\u0026#39;strain\u0026#39;] if isinstance(cur_strain, list): strains = cur_strain else: strains = [cur_strain] except Exception as e: continue for strain in strains: if strain in strain_dic: strain_dic[strain].append(med) else: strain_dic[strain] = [med] for strain in strain_dic: strain_dic[strain] = list(set(strain_dic[strain])) keep된 sequence의 균주-항생제 pair를 얻을 때는\n각 항생제에 대해 selected sequence의 투여 전날(D-1) 균주(들)에 해당 항생제 매핑 하는 방식으로 수행했다. strains = list(strain_dic.keys()) print(len(strains)) print(strains[0], \u0026#39;\\n\u0026#39;, strain_dic[strains[0]]) 98 Pseudomonas aeruginosa [\u0026#39;Imicil Kit\u0026#39;, \u0026#39;Prepenem\u0026#39;, \u0026#39;Sevatrim\u0026#39;, \u0026#39;Meropen\u0026#39;, \u0026#39;Finibax\u0026#39;, \u0026#39;Ciprobay\u0026#39;, \u0026#39;Hanomycin\u0026#39;, \u0026#39;Citopcin\u0026#39;, \u0026#39;Tazocin\u0026#39;, \u0026#39;Dexamethasone\u0026#39;, \u0026#39;Pospenem\u0026#39;, \u0026#39;Tygacil\u0026#39;, \u0026#39;Pentamidine Isethionate\u0026#39;, \u0026#39;Lagevrio\u0026#39;, \u0026#39;Plunazol\u0026#39;, \u0026#39;Vancomycin HCl\u0026#39;] 균주별 효과 항생제 딕셔너리 strain_dic를 확인해보면\n98개 균주에 대해 효능을 보인(것으로 추정되는) 항생제 목록이 제대로 생성돼있다! ourdir = \u0026#39;res\u0026#39; with open(f\u0026#34;{outdir}/Feature1.pkl\u0026#34;, \u0026#39;wb\u0026#39;) as f: pickle.dump(strain_dic, f) 만든건 저장하기.\n4. Create feature2 # feature2는 솔직히 좀 애매한데 로직을 짜보면\n일단 투여 후 NEWS가 감소한 sequence를 모두 모으고 \u0026lsquo;일정 수준\u0026rsquo;이하로 감소하는데 소요된 시간을 봐서 (ex. 3이하는 moderate니까 3까지 도달하는데 소요된 날짜) 상위 30%/하위30%/나머지 \u0026laquo; 이런 식으로 가려고 했으나? sequence의 선택 기준이 \u0026lsquo;투여 전날 news\u0026rsquo;로써 sequence마다 기준이 달랐기때문에 y축 즉 news 범위가 다 달라서 절대적인 값으로 설정하기 어려울거같다. (ex. 투여 전날 최고치가 3보다 낮을 수도 있음. 또는 투여후 3 아래로 안떨어지는 날이 있을수도있음) 그래서 상대적인 값으로 볼까 했는데? 기준을 \u0026lsquo;절반 이하로 떨어지기\u0026rsquo;로 잡는다고 치면 news가 전날 12 -\u0026gt; 8로 감소 전날 3 -\u0026gt; 1.5로 감소 인 경우 1은 좋은 데이터지만 non selected 되어 라벨링되지않고 2는 별로인 데이터지만 selected 되어 라벨링되게된다. 결론: feature2는 일단 보류하기.\n"},{"id":62,"href":"/docs/study/tech/algo7/","title":"#3 네트워크","section":"생물정보학","content":" #3 네트워크 # #2025-06-17\n문제 # #문제 설명\n네트워크란 컴퓨터 상호 간에 정보를 교환할 수 있도록 연결된 형태를 의미합니다. 예를 들어, 컴퓨터 A와 컴퓨터 B가 직접적으로 연결되어있고, 컴퓨터 B와 컴퓨터 C가 직접적으로 연결되어 있을 때 컴퓨터 A와 컴퓨터 C도 간접적으로 연결되어 정보를 교환할 수 있습니다. 따라서 컴퓨터 A, B, C는 모두 같은 네트워크 상에 있다고 할 수 있습니다.\n컴퓨터의 개수 n, 연결에 대한 정보가 담긴 2차원 배열 computers가 매개변수로 주어질 때, 네트워크의 개수를 return 하도록 solution 함수를 작성하시오.\n#제한사항\n컴퓨터의 개수 n은 1 이상 200 이하인 자연수입니다. 각 컴퓨터는 0부터 n-1인 정수로 표현합니다. i번 컴퓨터와 j번 컴퓨터가 연결되어 있으면 computers[i][j]를 1로 표현합니다. computer[i][i]는 항상 1입니다. #입출력 예\nn computers return 3 [[1, 1, 0], [1, 1, 0], [0, 0, 1]] 2 3 [[1, 1, 0], [1, 1, 1], [0, 1, 1]] 1 #입출력 예 설명\n예제 #1\n아래와 같이 2개의 네트워크가 있습니다.\n예제 #2\n아래와 같이 1개의 네트워크가 있습니다.\n#정답\ndef solution(n, computers): visited = [False] * n def dfs(node): visited[node] = True for next_node in range(n): if computers[node][next_node] == 1 and not visited[next_node]: dfs(next_node) network_count = 0 for i in range(n): if not visited[i]: dfs(i) network_count += 1 return network_count 풀이 # #단계별로보기\nvisited array 만들고 dfs 함수 만들고 돌린다 ##1\nvisited = [False] * n visited = [False, False, False, False, False] ##2\ndef dfs(node): visited[node] = True for next_node in range(n): if computers[node][next_node] == 1 and not visited[next_node]: dfs(next_node) 현재 idx에서 탐색법: visited[idx] = True 하고 -\u0026gt; for next_node in range(n): 연결이고 미방문이면 다음탐색 (-\u0026gt; computers[idx][next_node] == 1 이고 not visited[next_node]이면 dfs(next_node)) ##3\nfor i in range(n): if not visited[i]: dfs(i) network_count += 1 전체 idx에서 탐색: -\u0026gt; for i in range(n): 미방문이면 dfs(i), 끝나면 cnt+=1 "},{"id":63,"href":"/docs/study/tech/tech26/","title":"#4 모델 학습","section":"생물정보학","content":" #4 모델 학습 # #2025-06-18\n1. Load package # import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import copy from pathlib import Path import warnings import lightning.pytorch as pl from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor from lightning.pytorch.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) 2. Check version # import pytorch_forecasting import torch import pytorch_lightning as pl print(\u0026#34;PyTorch Forecasting:\u0026#34;, pytorch_forecasting.__version__) print(\u0026#34;PyTorch:\u0026#34;, torch.__version__) print(\u0026#34;PyTorch Lightning:\u0026#34;, pl.__version__) PyTorch Forecasting: 0.10.1 PyTorch: 1.13.1+cu117 PyTorch Lightning: 1.6.5 3. Load data # "},{"id":64,"href":"/docs/study/tech/algo3/","title":"#4 완전범죄","section":"생물정보학","content":" #4 완전범죄 # #2025-06-19\n1. 문제 # #문제 설명\nA도둑과 B도둑이 팀을 이루어 모든 물건을 훔치려고 합니다. 단, 각 도둑이 물건을 훔칠 때 남기는 흔적이 누적되면 경찰에 붙잡히기 때문에, 두 도둑 중 누구도 경찰에 붙잡히지 않도록 흔적을 최소화해야 합니다.\n물건을 훔칠 때 조건은 아래와 같습니다.\n물건 i를 훔칠 때, A도둑이 훔치면 info[i][0]개의 A에 대한 흔적을 남깁니다. B도둑이 훔치면 info[i][1]개의 B에 대한 흔적을 남깁니다. 각 물건에 대해 A도둑과 B도둑이 남기는 흔적의 개수는 1 이상 3 이하입니다. 경찰에 붙잡히는 조건은 아래와 같습니다.\nA도둑은 자신이 남긴 흔적의 누적 개수가 n개 이상이면 경찰에 붙잡힙니다. B도둑은 자신이 남긴 흔적의 누적 개수가 m개 이상이면 경찰에 붙잡힙니다. 각 물건을 훔칠 때 생기는 흔적에 대한 정보를 담은 2차원 정수 배열 info, A도둑이 경찰에 붙잡히는 최소 흔적 개수를 나타내는 정수 n, B도둑이 경찰에 붙잡히는 최소 흔적 개수를 나타내는 정수 m이 매개변수로 주어집니다. 두 도둑 모두 경찰에 붙잡히지 않도록 모든 물건을 훔쳤을 때, A도둑이 남긴 흔적의 누적 개수의 최솟값을 return 하도록 solution 함수를 완성해 주세요. 만약 어떠한 방법으로도 두 도둑 모두 경찰에 붙잡히지 않게 할 수 없다면 -1을 return해 주세요.\n#제한사항\n1 ≤ info의 길이 ≤ 40 info[i]는 물건 i를 훔칠 때 생기는 흔적의 개수를 나타내며, [A에 대한 흔적 개수, B에 대한 흔적 개수]의 형태입니다. 1 ≤ 흔적 개수 ≤ 3 1 ≤ n ≤ 120 1 ≤ m ≤ 120 #입출력 예\ninfo n m result [[1, 2], [2, 3], [2, 1]] 4 4 2 [[1, 2], [2, 3], [2, 1]] 1 7 0 [[3, 3], [3, 3]] 7 1 6 [[3, 3], [3, 3]] 6 1 -1 2. 정답 # def solution(info, n, m): import heapq N = len(info) visited = [[False] * m for _ in range(n)] heap = [] heapq.heappush(heap, (0, 0, 0)) # (a_sum, idx, b_sum) while heap: a_sum, idx, b_sum = heapq.heappop(heap) if idx == N: return a_sum # 최소 a_sum에 도달한 경로 a_cost, b_cost = info[idx] # A가 훔치는 경우 next_a = a_sum + a_cost if next_a \u0026lt; n and not visited[next_a][b_sum]: visited[next_a][b_sum] = True heapq.heappush(heap, (next_a, idx + 1, b_sum)) # B가 훔치는 경우 next_b = b_sum + b_cost if next_b \u0026lt; m and not visited[a_sum][next_b]: visited[a_sum][next_b] = True heapq.heappush(heap, (a_sum, idx + 1, next_b)) return -1 3. 단계별로 보기 # visited array 만들고 heap 만들고 (A누적흔적, 몇번째물건인지, B누적흔적) 넣고 돌리기 시작 (idx=N이면 끝) A가 훔치면 (next_a, idx + 1, b_sum) 넣고 B가 훔치면 (a_sum, idx + 1, next_b) 넣고. (0, 0, 0) 보고 암것도 안넣은거면 A도 B도 못훔친거니까 -1 반환 (아래 내용은 틀림\u0026hellip; 어쩐지 이해안대더라 ㅂㄷㅂㄷ)\n#정답\ndef solution(info, n, m): from collections import deque N = len(info) visited = [[False] * m for _ in range(n)] visited[0][0] = True queue = deque() queue.append((0, 0, 0)) # (i번째 물건까지 처리, A의 누적 흔적, B의 누적 흔적) while queue: idx, a_sum, b_sum = queue.popleft() if idx == N: return a_sum # 모든 물건을 훔쳤고, A의 흔적 누적 최소값 리턴 a_cost, b_cost = info[idx] # A가 훔치는 경우 if a_sum + a_cost \u0026lt; n and not visited[a_sum + a_cost][b_sum]: visited[a_sum + a_cost][b_sum] = True queue.append((idx + 1, a_sum + a_cost, b_sum)) # B가 훔치는 경우 if b_sum + b_cost \u0026lt; m and not visited[a_sum][b_sum + b_cost]: visited[a_sum][b_sum + b_cost] = True queue.append((idx + 1, a_sum, b_sum + b_cost)) return -1 # 모든 경우 탐색했는데도 실패한 경우 #단계별로보기\nvisited array 만들고 queue 만들고 (몇번째물건인지, A누적흔적, B누적흔적) 넣고 돌리기 시작 (idx=N이면 끝) A가 훔치면 (idx+1, a_sum+a_cost, b_cost) 넣고 B가 훔치면 (idx+1, a_cost, b_sum+b_cost) 넣고. 안넣은거면 A도 B도 못훔친거니까 -1 반환 ##1\nvisited = [[False] * m for _ in range(n)] visited = [[False, False, False, False], [False, False, False, False], [False, False, False, False], [False, False, False, False]] 시작은 visited = [[True, False, False, False], [False, False, False, False], [False, False, False, False], [False, False, False, False]] ##2\nqueue = deque() ##3\nidx, a_sum, b_sum = queue.popleft() idx=0, a_sum=0, b_sum=0 ##4\nif idx == N: return a_sum # 모든 물건을 훔쳤고, A의 흔적 누적 최소값 리턴 ##5\n# A가 훔치는 경우 if a_sum + a_cost \u0026lt; n and not visited[a_sum + a_cost][b_sum]: visited[a_sum + a_cost][b_sum] = True queue.append((idx + 1, a_sum + a_cost, b_sum)) # B가 훔치는 경우 if b_sum + b_cost \u0026lt; m and not visited[a_sum][b_sum + b_cost]: visited[a_sum][b_sum + b_cost] = True queue.append((idx + 1, a_sum, b_sum + b_cost)) a_sum + a_cost \u0026lt; n and not visited[a_sum + a_cost][b_sum] 에서 not visited가 아니라 visited이면 A가 안훔치냐? ##예시\n(A 도둑의 누적 흔적 수(a_sum)가 최소가 되도록 보장하는 로직은 어디지?)\ninfo = [[1, 3], [2, 1], [3, 1]] n = 10 # A가 잡히는 기준 m = 4 # B가 잡히는 기준 \u0026lt;초기 상태\u0026gt; BFS Queue: [(0, 0, 0)] → 아직 아무것도 훔치지 않음 visited[0][0] = True \u0026lt;1단계: idx=0, a_sum=0, b_sum=0\u0026gt; 가능한 선택: A가 1번 훔침 → (1, 1, 0) → visited[1][0] = True B가 1번 훔침 → (1, 0, 3) → visited[0][3] = True → Queue: [(1, 1, 0), (1, 0, 3)] \u0026lt;2단계: idx=1, a_sum=1, b_sum=0\u0026gt; A가 1,2번 훔침 → (2, 3, 0) → visited[3][0] = True A가 1번 B가 2번 훔침 → (2, 1, 1) → visited[1][1] = True → Queue: [(1, 0, 3), (2, 3, 0), (2, 1, 1)] \u0026lt;2-2단계: idx=1, a_sum=0, b_sum=3\u0026gt; A가 2번 B가 1번 훔침 → (2, 2, 3) → visited[2][3] = True B가 1,2번 훔침 → (2, 0, 4) → (b_sum=4 ≥ m=4) → 불가 → Queue: [(2, 3, 0), (2, 1, 1), (2, 2, 3)] \u0026lt;3단계: idx=2, a_sum=3, b_sum=0\u0026gt; A가 1,2,3번 훔침 → (3, 6, 0) → visited[6][0] = True A가 1,2번 B가 3번 훔침 → (3, 3, 1) → visited[3][1] = True → Queue: [(2, 1, 1), (2, 2, 3), (3, 6, 0), (3, 3, 1)] \u0026lt;3-2단계: idx=2, a_sum=1, b_sum=1 A가 1,3번 B가 2번 훔침 → (3, 4, 1) → visited[4][1] = True A가 1번 B가 2,3번 훔침 → (3, 1, 2) → visited[1][2] = True → Queue: [(2, 2, 3), (3, 6, 0), (3, 3, 1), (3, 4, 1), (3, 1, 2)] ← 여기서 idx==3 도달 → 정답은 A \u0026lt;결론 - A 도둑의 누적 흔적 수(a_sum)가 최소가 되도록 보장하는 로직은 어디인가?\u0026gt; Queue: [(2, 2, 3), (3, 6, 0), (3, 3, 1), (3, 4, 1), (3, 1, 2)] 하고 다음 단계가 마지막 idx=2 데이터 를 pop으로 queue에서 제거하는것. - queue에서 첫번째 값이 idx=3인거중에 A 흔적이 최소인 값인가? - #A가 훔침 이 #B가 훔침보다 빨리 수행되는데 - 어떻게 A 도둑의 누적 흔적 수(a_sum)가 최소가 되도록 보장하는 로직이 됨?? "},{"id":65,"href":"/docs/study/luck/luck13/","title":"6월 17일","section":"﹂#","content":" 6월 17일 # #2025-06-17\n오늘할일\n인적성 1회 코테 1문제 리비전작업 컴활강의듣기 1. 인적성 # 풀이문항\n언어 - 4/4, 3/4 추리 - 5/5, 4/5 추리 - 5/5, 5/5 공간지각 - 1/4, 1/1 총평\n오늘 kt 모고를 풀었는데 공간지각빼고는 풀만햇다 공간지각은 규칙찾긴햇는데 넘늦게찾음\u0026hellip; 담에다시풀어보기. 2. 코테 # 문제: 연결 요소의 개수 구하기 https://school.programmers.co.kr/learn/courses/30/lessons/43162\n정답\ndef solution(n, computers): visited = [False] * n def dfs(node): visited[node] = True for next_node in range(n): if computers[node][next_node] == 1 and not visited[next_node]: dfs(next_node) network_count = 0 for i in range(n): if not visited[i]: dfs(i) network_count += 1 return network_count 정답풀이\nn = 3 computers = [[1, 1, 0], [1, 1, 0], [0, 0, 1]]\treturn = 2 1. visited = [False] * n 2. 현재 idx에서 탐색법: visited[idx] = True 하고 -\u0026gt; for next_node in range(n): 연결이고 미방문이면 다음탐색 (-\u0026gt; computers[idx][next_node] == 1 이고 not visited[next_node]이면 dfs(next_node)) 3. 전체 탐색 for i in range(n): 미방문이면 dfs(i), 끝나면 cnt+=1 (내풀이)\nn = 3 computers = [[1, 1, 0], [1, 1, 0], [0, 0, 1]]\treturn = 2 1. visited = [0, 0, 0] 2. idx=0: 방문안함 cnt+=1 visited = [1, 0, 0] connected = [1, 1, 0] -\u0026gt; 1인 인덱스는 1 3. visited = [1, 1, 0] connected = [1, 1, 0] -\u0026gt; 1인 인덱스 없음 -\u0026gt; 끝 4. idx=1: 방문함 pass 5. idx=2: 방문안함 cnt+=1 visited = [1, 1, 1] connected = [0, 0, 1] -\u0026gt; 1인 인덱스 없음 -\u0026gt; 끝 "},{"id":66,"href":"/docs/study/tech/tech17/","title":"Project 1: Density based local clustering algorithm","section":"생물정보학","content":" Project 1: Density based local clustering algorithm # 개념 # #1 Entropy 기반 mutation 분석\n#2 Mutation hotspot 발굴 알고리즘\n#3 Density based clustering 알고리즘 - DBSCAN\n#4 Clustering 알고리즘의 parametric test\n연구 # #0 연구 설계\n#1 GISAID 데이터베이스 - sequence 다운로드\n#2 중요도 지표 계산\n#3 밀도 기반 클러스터링\n﹂슈도코드\n#4 결과 검증\n﹂임상 결과와의 연관성\n﹂계통 결정 돌연변이와 연관성\n﹂HLA-peptide interaction affected by mutation of c315 and c442\n#5 알고리즘 성능 평가\n﹂k dist plot\n﹂중요도 지표 평가\n﹂DBSCAN 비교\n#6 Revision\n"},{"id":67,"href":"/docs/study/tech/tech18/","title":"Project 2: Shap based RF model","section":"생물정보학","content":" Project 2: Shap based RF model # 개념 # 연구 # #0 연구 설계\n"},{"id":68,"href":"/docs/study/tech/tech19/","title":"Project 3: Antibiotics TFT model","section":"생물정보학","content":" Project 3: Antibiotics TFT model # 개념 # 연구 # #0 연구 설계\n#1 입력 데이터 생성\n#2 입력 feature 생성\n#3 모델 구축\n🗒️ # #TFT 연구 방향\n"},{"id":69,"href":"/docs/study/tech/algo1/","title":"#1 완주하지 못한 선수","section":"생물정보학","content":" #1 완주하지 못한 선수 # #2025-06-16\n#문제 설명\n수많은 마라톤 선수들이 마라톤에 참여하였습니다. 단 한 명의 선수를 제외하고는 모든 선수가 마라톤을 완주하였습니다.\n마라톤에 참여한 선수들의 이름이 담긴 배열 participant와 완주한 선수들의 이름이 담긴 배열 completion이 주어질 때, 완주하지 못한 선수의 이름을 return 하도록 solution 함수를 작성해주세요.\n#제한사항\n마라톤 경기에 참여한 선수의 수는 1명 이상 100,000명 이하입니다. completion의 길이는 participant의 길이보다 1 작습니다. 참가자의 이름은 1개 이상 20개 이하의 알파벳 소문자로 이루어져 있습니다. 참가자 중에는 동명이인이 있을 수 있습니다. #입출력 예\nparticipant completion return [\u0026ldquo;leo\u0026rdquo;, \u0026ldquo;kiki\u0026rdquo;, \u0026ldquo;eden\u0026rdquo;] [\u0026ldquo;eden\u0026rdquo;, \u0026ldquo;kiki\u0026rdquo;] \u0026ldquo;leo\u0026rdquo; [\u0026ldquo;marina\u0026rdquo;, \u0026ldquo;josipa\u0026rdquo;, \u0026ldquo;nikola\u0026rdquo;, \u0026ldquo;vinko\u0026rdquo;, \u0026ldquo;filipa\u0026rdquo;] [\u0026ldquo;josipa\u0026rdquo;, \u0026ldquo;filipa\u0026rdquo;, \u0026ldquo;marina\u0026rdquo;, \u0026ldquo;nikola\u0026rdquo;] \u0026ldquo;vinko\u0026rdquo; [\u0026ldquo;mislav\u0026rdquo;, \u0026ldquo;stanko\u0026rdquo;, \u0026ldquo;mislav\u0026rdquo;, \u0026ldquo;ana\u0026rdquo;] [\u0026ldquo;stanko\u0026rdquo;, \u0026ldquo;ana\u0026rdquo;, \u0026ldquo;mislav\u0026rdquo;] \u0026ldquo;mislav\u0026rdquo; #입출력 예 설명\n예제 #1\n\u0026ldquo;leo\u0026quot;는 참여자 명단에는 있지만, 완주자 명단에는 없기 때문에 완주하지 못했습니다.\n예제 #2\n\u0026ldquo;vinko\u0026quot;는 참여자 명단에는 있지만, 완주자 명단에는 없기 때문에 완주하지 못했습니다.\n예제 #3\n\u0026ldquo;mislav\u0026quot;는 참여자 명단에는 두 명이 있지만, 완주자 명단에는 한 명밖에 없기 때문에 한명은 완주하지 못했습니다.\n#정답\nfrom collections import Counter def solution(participant, completion): answer = Counter(participant) - Counter(completion) return list(answer.keys())[0] "},{"id":70,"href":"/docs/study/luck/luck12/","title":"6월 16일","section":"﹂#","content":" 6월 16일 # #2025-06-16\n오늘할일\n인적성 1회 코테 1문제 리비전작업 #인적성\n##풀이문항\n언어 - 14/15, 12/14 수리 - 11/15, 5/11 추리 - 14/15, 10/14 공간지각 - 6/10, 3/6\n##총평\n첫날보단 익숙해진듯.. 수리 넘급하게풀지말자\n#코테\n문제: 지게차와 크레인 https://school.programmers.co.kr/learn/courses/30/lessons/388353\n##입출력 예\nstorage = [\u0026#34;AZWQY\u0026#34;, \u0026#34;CAABX\u0026#34;, \u0026#34;BBDDA\u0026#34;, \u0026#34;ACACA\u0026#34;]\trequests = [\u0026#34;A\u0026#34;, \u0026#34;BB\u0026#34;, \u0026#34;A\u0026#34;] result = 11 ##풀이\nmyarray = [[False, False, False, False, False], [False, False, False, False, False], [False, False, False, False, False], [False, False, False, False, False]] myarray = [[True, True, True, True, True], [True, False, False, False, True], [True, False, False, False, True], [True, True, True, True, True]] storage_array = [A Z W Q Y C A A B X B B D D A A C A C A] rows = 4 columns = 5 left = rows*columns = 20 오늘한일\n오늘은 인적성 코테 리비전 하고 6시좀 넘어서 퇴근하구 8시반쯤 운동갔다가 10시에 독서실 왔다. 이번주에 리비전도해야되고 토요일에 컴활도 쳐야하니까 내일부터 금요일까지 인적성은 수리/공간만 풀기. 오늘푼 코테 나름끄적였지만 감이안와서 우울햇는데 챗지피티에쳣는데 얘도틀리길래 기분이나아졋다(ㅋㅋ) 구글에 쳣는데 풀이가 별로없어서 복잡하지만 이사람 코드로 공부하기로햇다 "},{"id":71,"href":"/docs/study/luck/luck14/","title":"취준기록","section":"﹂#","content":" 취준기록 # 06-20 ⋯ 6월 18-20일\n06-17 ⋯ 6월 17일\n06-16 ⋯ 6월 16일\n06-15 ⋯ 6월 15일\n06-14 ⋯ 6월 14일\n06-11 ⋯ 6월 11일\n06-10 ⋯ 6월 10일\n06-09 ⋯ 6월 9일\n06-08 ⋯ 6월 8일 (+스트레스 받을 이유가 없는이유)\n06-07 ⋯ 6월 7일\n06-06 ⋯ 6월 6일\n06-05 ⋯ 6월 5일 (특이점:외부에쫌많이 흔들림)\n"},{"id":72,"href":"/docs/study/luck/luck10/","title":"6월 15일","section":"﹂#","content":" 6월 15일 # #2025-06-15\n오늘할일\n인적성 1회 - 언어: p.66-80 / 수리: p.153-166 / 추리: p.234-241 / 공간지각: p.322-329 코테 1문제 리비전작업 #인적성\n##풀이문항\n언어 - 15/15, 12/15\n수리 - 7/15, 3/7\n추리 - 12/15, 8/12\n공간지각 - 6/10, 3/6\n##간단리뷰\n언어 - 11번: 기반식 고인돌이랑 개석식 고인돌 비교해야대는데 탁자식이랑 기반식을 비교하는 실수를함 / 16번: 근거를 안찾고 느낌으로 배열했는데 다시읽어보니까 \u0026ldquo;여러 학자\u0026quot;를 받는단어가없으니 (가)가 1번이 안됨 근데 빨리읽어서 그거까지 안봣다 / 20번: (나)랑 (라)는좀 헷갈릴만한듯 다시풀기\n수리 - 3,4번: 이건걍쉬웟는데 그마저도 암산을잘못함 / 7번: 항생제 판매량 보랬는데 전체를봄 1:30만에 풀수준이엇는데,, / 16번: 397.7 -\u0026gt; 439.9가 112% 증가냐는 문제였는데 400 -\u0026gt; 440으로 어림하니까 12%길래 넘어갓는데 10.6%여서 틀린선지엿음 너무어림하면안댄다 그리고 다시풀긴햇는데 3분걸림\n추리 - 1,2번: 다시보니 암산해놓음 손으로쓰자 / 8번: 공식안쓰고 밴다이어그램으로그냥했는데 실수나오는거보니 공식쓰는게 낫나?? / 10번: 찬성팀 2명인걸 안읽음\n공간지각 - 1번: 3번틀린거같앗는데 다시보니아님.. 1번이틀렷다. / 3번: 어이없음 이걸어케품 / 7번: 갑자기접는방향이 헷갈림 담에다시봐야할듯\n##총평\n언어 2:00, 수리 2:00, 추리 2:00, 공간지각 3:00으로 제한두고 했는데 수리랑 공간지각은 각각 3:00, 4:00으로 제한두는게 나을듯 특히 자료해석은 정말 혼돈의 카오스였다,,\n그리고 연습장에풀고 책에 체크만했는데 체크도안하는게 조을거같음\n#코테\n문제: 완전범죄 https://school.programmers.co.kr/learn/courses/30/lessons/389480\n##입출력 예\ninfo = [[1, 2], [2, 3], [2, 1]] n = 4 m = 4 result = 2 ##정답\ndef solution(info, n, m): from collections import deque N = len(info) visited = [[False] * m for _ in range(n)] visited[0][0] = True queue = deque() queue.append((0, 0, 0)) # (i번째 물건까지 처리, A의 누적 흔적, B의 누적 흔적) while queue: idx, a_sum, b_sum = queue.popleft() if idx == N: return a_sum # 모든 물건을 훔쳤고, A의 흔적 누적 최소값 리턴 a_cost, b_cost = info[idx] # A가 훔치는 경우 if a_sum + a_cost \u0026lt; n and not visited[a_sum + a_cost][b_sum]: visited[a_sum + a_cost][b_sum] = True queue.append((idx + 1, a_sum + a_cost, b_sum)) # B가 훔치는 경우 if b_sum + b_cost \u0026lt; m and not visited[a_sum][b_sum + b_cost]: visited[a_sum][b_sum + b_cost] = True queue.append((idx + 1, a_sum, b_sum + b_cost)) return -1 # 모든 경우 탐색했는데도 실패한 경우 ##틀린이유\n1행: A 1개, B 2개 3개 다 가능하다고 가정. [[1, 2], [2, 3], [2, 1]] -\u0026gt; [[2, 1], [1, 2], [2, 3]] (b가작은순으로 sorting) N = len(info) = 3 0-\u0026gt; m-1 = 3 (o) 1-\u0026gt; 3-2 = 1 (o) 2 -\u0026gt; 1-3\u0026lt;0이므로 안함. n-2 = 2 (o) 답: 2 에서 3개 다 가능하다고 가정\u0026laquo; 이 아니라 3개 다 훔치는게 문제였음 3개가안되면 2개만 훔치는게아니라 그냥 -1 하고 끝내면됨\ndef solution(info, n, m): N = len(info) for _ in range(N): cur_m, cur_n, cur_N = m, n, len(info) A_flag, B_flag = False, False for i in range(cur_N): info.sort(key=lambda x : x[1]) cur_a = info[i][0] cur_b = info[i][1] while not A_flag: while not B_flag: if cur_m \u0026gt; cur_b: cur_m -= cur_b else: B_flag = True #B가잡힘 break if cur_n \u0026gt; cur_a: cur_n -= cur_a else: A_flag = True #A가잡힘 break if A_flag and B_flag: #둘다 잡힘 info.pop(-1) else: #A나 B가 안잡힘 break if A_flag and B_flag: return -1 else: return cur_n 오늘은 \u0026gt; 오전에 인적성이랑 코테 풀고 오후에 리비전 조금하구 집와서 저녁으로 치킨먹고 운동갔다가 씻고 밤에 코테랑 인적성 공책정리를했다.\n"},{"id":73,"href":"/docs/study/luck/luck11/","title":"원서정리","section":"﹂#","content":" 원서정리 # 4.20 삼양라운드스퀘어\n﹂직무: OMICS AI Engineer (연구원)\n4.21 한국산업기술기획평가원\n﹂직무: 일반직 \u0026gt; R\u0026amp;D 기획평가 \u0026gt; 바이오생명\n5.20 네이버클라우드\n﹂직무: 환자향 진료기록 생성 모델 개발 (체험형 인턴)\n﹂원서확인 https://recruit.navercorp.com/my/aplcnt.do#none\n6.11 SK AX\n﹂직무: 채용연계형 AI 서비스 개발 과정 (채용연계형 인턴)\n﹂정보 http://linkareer.com/activity/245743\n﹂전형: 지원접수 \u0026gt; 서류 검토 \u0026gt; SKCT(인성검사) 응시 및 AI 면접 \u0026gt; 면접 \u0026gt; 결과 안내\n﹂SKALA 1기 후기 / SK 그룹 인적성 SKCT TIP\n﹂SK AX 인성검사 준비\n﹂SK AX 면접 준비\n"},{"id":74,"href":"/docs/study/luck/luck8/","title":"6월 14일","section":"﹂#","content":" 6월 14일 # #2025-06-14\n오늘한일\n코테 공책 정리 SK AX 원서 운동 코테 3개 #코테\n문제: 같은 숫자는 싫어 https://school.programmers.co.kr/learn/courses/30/lessons/12906\n##입출력 예\narr = [1,1,3,3,0,1,1] answer = [1,3,0,1] ##정답\ndef solution(arr): answer = [arr[0]] for i in arr: if i != answer[-1]: answer.append(i) return answer 문제: 기능개발 https://school.programmers.co.kr/learn/courses/30/lessons/42586\n##입출력 예\nprogresses = [95, 90, 99, 99, 80, 99]\tspeeds = [1, 1, 1, 1, 1, 1]\treturn = [1, 3, 2] ##정답\nn=1 -\u0026gt; [96, 91, 100, 100, 81, 100] n=5 -\u0026gt; [100, 95, 100, 100, 85, 100] -\u0026gt; 배포 -\u0026gt; [100, 95, 100, 100, 85, 100].pop(0) -\u0026gt; [95, 100, 100, 85, 100] n=10 -\u0026gt; [100, 100, 100, 90, 100] -\u0026gt; 배포 -\u0026gt; [90, 100] n=20 -\u0026gt; [100, 100] -\u0026gt; 배포 -\u0026gt; [] def solution(progresses, speeds): answer = [] while progresses: for i in range(len(progresses)): progresses[i] += speeds[i] count = 0 while progresses and progresses[0] \u0026gt;= 100: progresses.pop(0) speeds.pop(0) count += 1 if count \u0026gt; 0: answer.append(count) return answer 문제: 올바른 괄호 https://school.programmers.co.kr/learn/courses/30/lessons/12909\n##입출력 예\ns = \u0026#34;(())()\u0026#34; answer = true ##정답\ndef solution(s): count = 0 for char in s: if char == \u0026#39;(\u0026#39;: count += 1 else: # char == \u0026#39;)\u0026#39; count -= 1 if count \u0026lt; 0: return False return count == 0 #기업정보\n[AI/IT·Digital] 미래에셋증권 2025 상반기 채용연계형 인턴(신입사원) 모집 https://jasoseol.com/recruit/96221\n우대사항: x\n[바이오의료연구센터-의료분야 국책과제 보조] 한국산업기술시험원 2025년 2분기 2차수 한국산업기술시험원 위촉 계약직(행정,연구직) 공개모집 https://jasoseol.com/recruit/96072\nNCS 없고 면접만 있어서 내보면 좋을듯\n오늘은 미뤄왔던 \u0026lsquo;수분크림이랑 썬크림사기\u0026rsquo;를했는데 썬크림 1+1이라길래 아무생각없이 2통 집었는데 1통에 2개입이어서 4개산사람이돼버림 그리고 SK AX 신경쓰기싫어서 그냥안낼려다가 갑자기 내고싶어져서 1시간만에 대충써서 내버렸다 그래서 리비전작업을 못했다 내일하지뭐\u0026hellip;\n"},{"id":75,"href":"/docs/study/luck/luck9/","title":"시험일정정리","section":"﹂#","content":" 시험정리 # 6.21(토) 컴퓨터활용능력\n접수내역 확인 https://license.korcham.net/mp/jubsuList.do 해야하는것: 접수증 출력, 수험표 출력 6.28(토) AICE\n신청내역 확인 https://aice.study/mypage/receipt 16:00-17:30 / 온라인 / 해야하는것: 사전점검 7.19(토) SAS\n응시환경 설정 https://wsr.pearsonvue.com/testtaker/checkout/OrderSummaryPage/SAS?conversationId=1496932 9:30-12:00 / 온라인 / 신분증 지참 8.8(토) SQLD\n신청 7.21 "},{"id":76,"href":"/docs/study/luck/luck7/","title":"6월 11일","section":"﹂#","content":" 6월 11일 # #2025-06-11\n오늘 할일\n미팅 ppt 만들기 epitope 자료보내기 4:00에 미팅하기 컴활공부 멀로할지 찾기 (21일 시험..) "},{"id":77,"href":"/docs/study/luck/luck6/","title":"6월 10일","section":"﹂#","content":" 6월 10일 # #2025-06-10\n오늘 한일\n내일 미팅 ppt 만들기 오늘은 어제 운동을 안가서그런지 몸도부은거같구 쳐지는날이었당 저녁에 운동가긴했는데 하기싫어서 15분하고 도망나옴. ㅋㅋ\n"},{"id":78,"href":"/docs/study/luck/luck5/","title":"6월 9일","section":"﹂#","content":" 6월 9일 # #2025-06-09\n5:45-6:15 코테\n6:15-6:45 SAS, AICA 시험 확인\n#코딩테스트\n문제: 가장 큰 수 https://school.programmers.co.kr/learn/courses/30/lessons/42746\n##입출력 예\nnumbers = [3, 30, 34, 5, 9] return = \u0026#39;9534330\u0026#39; ##정답\nnumbers.sort() -\u0026gt; [3, 5, 9, 30, 34] numbers = [str(i] for i in numbers] -\u0026gt; numbers.sort() = [3, 30, 34, 5, 9] -\u0026gt; 원하는 모양 = [30, 3, 34, 5, 9]인데 길이가 3자리 이하임을 이용하기. numbers = [i*3 for i in numbers -\u0026gt; [333, 555, 999, 303030, 343434] numbers.sort() = [303030, 333, 343434, 555, 999] answer = 9534330 def solution(numbers): numbers = list(map(str, numbers)) numbers.sort(key=lambda x: x*3, reverse=True) return str(int(\u0026#39;\u0026#39;.join(numbers))) 문제: H-index https://school.programmers.co.kr/learn/courses/30/lessons/42747\n##입출력 예\ncitations = [3, 0, 6, 1, 5]\treturn = 3 ##정답\ndef solution(citations): citations.sort(reverse=True) for idx, c in enumerate(citations): if idx + 1 \u0026gt; c: return idx return len(citations) 오늘한일\n코테 2개 항생제 작업 "},{"id":79,"href":"/docs/study/luck/luck4/","title":"6월 8일 (+스트레스 받을 이유가 없는이유)","section":"﹂#","content":" 6월 8일 (+스트레스 받을 이유가 없는이유) # #2025-06-08\n10:10-10:40 코테\n10:40-11:00 공기업 서칭\n#코테\n문제: 베스트앨범 https://school.programmers.co.kr/learn/courses/30/lessons/42579\n##입출력 예\ngenres = [\u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;] plays = [500, 600, 150, 800, 2500] return = [4, 1, 3, 0] ##정답\ndef solution(genres, plays): genre_total = {} # 장르별 총 재생 수 genre_songs = {} # 장르별 (고유 번호, 재생 수) 리스트 for i in range(len(genres)): genre = genres[i] play = plays[i] # 1. 총 재생 수 누적 if genre in genre_total: genre_total[genre] += play else: genre_total[genre] = play # 2. 장르별 노래 정보 저장 if genre in genre_songs: genre_songs[genre].append((play, i)) else: genre_songs[genre] = [(play, i)] # 3. 장르를 총 재생 수 기준으로 정렬 sorted_genres = sorted(genre_total.items(), key=lambda x: x[1], reverse=True) result = [] for genre, _ in sorted_genres: # 4. 각 장르 내에서 노래를 재생 수 기준 내림차순, 고유번호 오름차순 정렬 songs = sorted(genre_songs[genre], key=lambda x: (-x[0], x[1])) # 5. 최대 두 개까지 수록 for song in songs[:2]: result.append(song[1]) return result ##개념\ni) genre_total = {\u0026#34;classic\u0026#34;: 1450, \u0026#34;pop\u0026#34;: 3100} -\u0026gt; genre_total.items() = [(\u0026#34;classic\u0026#34;, 1450), (\u0026#34;pop\u0026#34;, 3100)] ii) sorted(genre_total.items(), key=lambda x: x[1], reverse=True) -\u0026gt; [(\u0026#34;pop\u0026#34;, 3100), (\u0026#34;classic\u0026#34;, 1450)] -\u0026gt; x[1]: 딕셔너리의 value(재생 수)를 기준으로 정렬 -\u0026gt; reverse=True: 내림차순 정렬 즉 이거랑 같음.\ndef solution(genres, plays): genre_total = {} # 장르별 총 재생 수 genre_songs = {} # 장르별 (재생 수, 고유 번호) 리스트 for i in range(len(genres)): genre = genres[i] play = plays[i] # 1. 총 재생 수 누적 if genre in genre_total: genre_total[genre] += play else: genre_total[genre] = play # 2. 장르별 노래 정보 저장 if genre in genre_songs: genre_songs[genre].append((i, play)) else: genre_songs[genre] = [(i, play)] # 3. 장르를 총 재생 수 기준으로 정렬 sorted_genres = sorted(genre_total.items(), key=lambda x: x[1], reverse=True) result = [] for genre, _ in sorted_genres: # 4. 각 장르 내에서 노래를 재생 수 기준 내림차순, 고유번호 오름차순 정렬 songs = sorted(genre_songs[genre], key=lambda x: (-x[1], x[0])) # 5. 최대 두 개까지 수록 for song in songs[:2]: result.append(song[0]) #고유번호 return result #공기업서칭\n링크 - https://www.ncs.go.kr/blind/bl04/RecrtNotifDetail.do?recrtNo=20250221095938\n요약 - SAS랑 AICE Associate를 따면 도움됨\n##25.2.20 국민건강보험공단_2025년도 제1차 전문인력 채용 - AI\n##25.2.20 국민건강보험공단_2025년도 제1차 전문인력 채용 - 보건의료통계연구\n오늘은 경주갓다가 집에와서 그냥쉴려다가 불안지수가 올라가서 9시에 스카에 왓는데 10시인지금까지 멍때리다가 1시간 남았다\u0026hellip; 갑자기 네이버 체험형인턴 낸거 왜연락없지??싶어서 들어갔는데 아직안나온게맞았음 말도안해주고 떨어뜨리는데에 넘마니데여서 또떨어진줄알았네\n아무튼 6-8월까지는 포폴강화를 메인으로 하고 9월부터 졸논쓰면서 본격지원을 할거고 그전까지 포폴강화를 제대로해야 원하는직무를 손에넣을수있을텐데 하는 부담에(실제로 내역량밖이맞기도함) 자꾸 멘탈나가는중인데 사실 내가 손에넣지못하는그림도 충분히 그려지긴한다 즉 일어나면 끝장인 그런일은 아닌것이다. 그저그런회사에 갈거긴한데 6-8월이 비니까 비는김에 심심하니까 한다고 생각하자. 어차피그저그런데에 갈거니까 마음편하게먹자.\n내일할일\n인적성 강의 ppt 프린트\n"},{"id":80,"href":"/docs/hobby/daily/daily15/","title":"여름경주🍡🌿","section":"일상","content":" 여름경주🍡🌿 # #2025-06-08\n주말이순삭됐지만 재밌었다 ㅎㅎ\n행운/대박/합격 이런것만 보면 저항없이 사는 사람 ㅠ\n"},{"id":81,"href":"/docs/study/luck/luck3/","title":"6월 7일","section":"﹂#","content":" 6월 7일 # #2025-06-07\n#백신연구 진행상황\n##1\n목적이 개인의 면역 환경을 대표하고 면역 반응을 예측하는 feature를 찾는 것이라고 할때.\n개인의 면역 환경을 대표하는 feature를 찾는 것은 쉽다. 단일 시점 clustering을 하고 군집의 feature를 찾으면 됨. 다만 이중에 \u0026lsquo;면역 반응\u0026rsquo;을 예측하는데 유용한 feature를 골라내는게 어렵다. \u0026lsquo;면역 반응\u0026rsquo;을 type1 2 3등으로 정의하는게 필요하고 그 반응을 대표하는 feature를 찾는게 필요하다. 목적이 \u0026lsquo;면역 반응\u0026rsquo;이 비슷한 환자를 군집화하는것일때.\n\u0026lsquo;면역 반응\u0026rsquo;을 유전자 발현량 패턴이라고 정의하면 (단일 시점 clustering과) spherical kmeans는 편향이 최소화된 비지도학습 방법이지만 feature 선택이 어렵다. 어떤 feature가 비슷해야 면역 반응이 비슷한것인가?\n정답 feature(gene set) X가 있다고 가정했을때 이 feature를 맞히기는 어렵다. 근데 공동 발현 유전자가 많으면 면역 반응이 비슷할것이다. 공동 발현 유전자가 많은 애들의 특징이, 비슷한 면역 반응을 보이는 애들의 특징이 되지 않을까? 목적\n면역 반응이 비슷한 환자를 분류해내는 feature를 찾기. 가정\n공동 발현 유전자가 많으면 면역 반응이 비슷할것이다. 방향\n면역 반응이 비슷한 환자를 분류해내는 feature를 한번에 찾기 어려우니까, 면역 반응이 비슷한 3 type을 찾아서 걔네의 특징을 feature로 쓰자. 3type의 특징을 잘 나타내면서 그중 개인의 면역 환경을 잘 대표하는 gene set을 골라내기. 즉 3 type의 특성 중 t0으로 각 그룹을 분류 가능한 애들을 찾기. 또는 1차의 효과로(t2-t0) 분류 가능한 애들을 찾기. 결과는?\n개인의 면역 반응을 정의 개인의 면역 반응을 대표하는 feature를 찾음 그중 개인의 면역 환경 정의도 할수있는 feature를 고름 최종 feature인 \u0026lsquo;개인의 면역 환경\u0026rsquo;을 상요해서 면역 반응 유형을 예측 가능. 참고하면 좋을 논문 추가\nDictionary of immune responses to cytokines at single-cell resolution Single-cell immune aging clocks reveal inter-individual heterogeneity during infection and vaccination "},{"id":82,"href":"/docs/study/luck/luck2/","title":"6월 6일","section":"﹂#","content":" 6월 6일 # #2025-06-06\n9:30-10:00 코테 10:00-10:30: 인적성\n#코딩테스트\n문제: 폰켓몬 https://school.programmers.co.kr/learn/courses/30/lessons/1845\n##입출력 예\nnums = [3,1,2,3] result = 2 ##정답\nnums = [3,1,2,3] -\u0026gt; list(set(nums)) = [3,1,2] N/2 = 2인데 X길이가 2보다 길면 2, 2보다 짧으면 리스트 길이가 정답. def solution(nums): nums_list = list(set(nums)) N = len(nums) answer = min(N/2, len(nums_list)) return answer 문제: 전화번호 목록\n##입출력 예\nphone_book = [\u0026#34;119\u0026#34;, \u0026#34;97674223\u0026#34;, \u0026#34;1195524421\u0026#34;] return = false ##개념\nphone_book = [\u0026#34;119\u0026#34;, \u0026#34;97674223\u0026#34;, \u0026#34;1195524421\u0026#34;] -\u0026gt; phone_book.sort() = [\u0026#34;119\u0026#34;, \u0026#34;1195524421\u0026#34;, \u0026#34;97674223\u0026#34;] \u0026#34;1195524421\u0026#34;.startwith(\u0026#34;119\u0026#34;)이면 false ##정답\ndef solution(phone_book): phone_book.sort() for i in range(len(phone_book)-1): if phone_book[i+1].startswith(phone_book[i]): return False return True 문제: 의상 https://school.programmers.co.kr/learn/courses/30/lessons/42578\n##입출력 예\nclothes = [[\u0026#34;yellow_hat\u0026#34;, \u0026#34;headgear\u0026#34;], [\u0026#34;blue_sunglasses\u0026#34;, \u0026#34;eyewear\u0026#34;], [\u0026#34;green_turban\u0026#34;, \u0026#34;headgear\u0026#34;]] return = 5 ##정답\nclothes_dict = {\u0026#34;headgear\u0026#34;:2, \u0026#34;eyewear\u0026#34;:1} return = 3*2-1 = 5 def solution(clothes): clothes_dict = {} for cloth in clothes: if cloth[1] in clothes_dict.keys(): clothes_dict[cloth[1]] += 1 else: clothes_dict[cloth[1]] = 1 answer = 1 for key, value in clothes_dict.items(): answer = answer*(value+1) return answer - 1 "},{"id":83,"href":"/docs/study/luck/luck1/","title":"6월 5일 (특이점:외부에쫌많이 흔들림)","section":"﹂#","content":" 6월 5일 (특이점:외부에쫌많이 흔들림) # #2025-06-05\n#코딩테스트\n문제: 완주하지 못한 선수 https://school.programmers.co.kr/learn/courses/30/lessons/42576?language=python3\n##입출력 예\nparticipant = [\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]\tcompletion = [\u0026#34;eden\u0026#34;, \u0026#34;kiki\u0026#34;]\treturn = \u0026#34;leo\u0026#34; ##개념\nCounter([\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) -\u0026gt; {\u0026#39;leo\u0026#39;:1, \u0026#39;kiki\u0026#39;:1, \u0026#39;eden\u0026#39;:1} Counter([\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) - Counter([\u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) -\u0026gt; {\u0026#39;leo\u0026#39;:1} (key별로 value를 빼서 0이나 음수되면 제거) ##정답\ni) Counter(participant) -\u0026gt; {\u0026#39;leo\u0026#39;:1, \u0026#39;kiki\u0026#39;:1, \u0026#39;eden\u0026#39;:1} ii) Counter(participant) - Counter(completion) -\u0026gt; {\u0026#39;leo\u0026#39;:1} iii) 답은? 위를 X로 봣을때 list(X.keys())[0] from collections import Counter def solution(participant, completion): answer = Counter(participant) - Counter(completion) return list(answer.keys())[0] 오늘한일\n빅분기, sqld, 정처기, 컴활 시험일정 확인 항생제 작업 코테-인적성-NCS 1회 오늘회고\n오늘은 잠을 2시간밖에못잤는데 무력하기까지해서 잠을 덜잔 핑계로 시간을 버려도되는날이라고 생각해버리려했는데 아무리생각해도 이건 퇴마의문제가아니라 내일이되면 현실이 더 악화되는것 아닌가??라는걸 깨달아버리고 다급하게 스카에 와서 계획을다잡았다 노력만 하지말고 계획을 이행하기. 고통은 최소화하기. "},{"id":84,"href":"/docs/study/tech/tech15/","title":"ADsP 45회","section":"생물정보학","content":" ADsP 45회 응시결과 # #2025-06-05\nㅎㅎ 붙었다!!\n"},{"id":85,"href":"/docs/study/tech/tech16/","title":"빅데이터분석기사 / 정보처리기사 / SQLD 시험일정","section":"생물정보학","content":" 빅데이터분석기사 / 정보처리기사 / SQLD 시험일정 # #2025-06-05\n#빅데이터분석기사(필기-9.6)\n#정보처리기사(필기-9.10)\n#SQLD(8.23)\n"},{"id":86,"href":"/docs/hobby/book/book39/","title":"불행에 대한 수비력","section":"글","content":" 불행에 대한 수비력 # #2025-06-02\n#1\n해소되지 않은 기분은 성격이 된다. 작은 짜증으로 시작된 기분은 일상에 대한 분노로 이어지고 속속들이 헤쳐 모여 결국 더러운 성격으로 완성된다. 어떤 성격으로 살고 싶은지는 빼곡히 적은 새해 다짐이 아니라 일상을 어떻게 다루는지에 달려 있었다.\n#2\n사람의 진짜 우아함은 무너졌을 때 드러난다고 한다.\n윗사람에게 깨진 날 후배를 대하는 태도나 안 좋은 일이 넘친 날 웃응며 인사할 줄 아는 여유에서 우린 그 사람의 깊이를 느낄 수 있다. 그러니까 우아함이란 다시 말해 이렇게 정의할 수 있을 것이다.\n마음이 두 조각 난 날에도 평소처럼 인사하고 웃고 공들여 사과할 수 있는 태도.\n한때는 이런 생각을 한 적도 있었다. 아니 요즘처럼 내 감정 참는 게 손해인 시대에 저런 고리타분한 태도가 필요하긴 해? 나만 손해잖아. 근데, 필요하더라. 무너진 날조차 우아함을 유지하는 나를 보며 남뿐만 아니라 나 자신도 생각하게 되기 때문이다.\n\u0026ldquo;나는 이런 것에 무너지지 않아.\u0026rdquo; 우아함이란 결국 나를 위한 태도였다.\n마음이 지옥 같은 날, 모든게 실패한 것 같은 날일수록 보다 공들여 웃고 감사하고 인사하자. 나를 위해서. 내 마음을 지키기 위해서. 그 작은 태도가 어떤 말보다 강력한 신호가 되어줄 테니.\n#3\n현명한 사람일수록 함부로 불행해지지 않는다.\n현명함이란 행복의 양을 늘리는 것보다 불행의 양을 줄이는 데 더 많이 쓰인다. 일단 한번 불행으로 물든 마음은 어떤 행복으로도 쉽게 퇴치되지 않기 때문이다.\n월급날이어도 승진을 해도. 아니 원하는 모든 목표를 다 이뤄내도 가족이 아프면 절대 행복해질 수 없듯. 불행은 행복에 비해 너무 강하고, 구체적이다. 행복이 상상이라면 불행은 일상인 것이다. 어른이 될수록 불행에 대한 수비력이 더 중요해지는 이유다.\n‘내 인생이 진짜로 그렇게 불행해?’ 30분이고 한 시간이고, 아니 몇 날 며칠이고 홀로 답을 적는다. 그러다 보면 대체로 답이 간단해진다. 내 인생은 생각만큼 불행하지 않고, 생각보다 행복하다.\n# #출처\n책 어른의 행복은 조용하다\n"},{"id":87,"href":"/docs/hobby/daily/daily14/","title":"힘들때만 책보는 가짜독서인,,","section":"일상","content":" 힘들때만 책보는 가짜독서인,, # #2025-06-02\n아니!! 응원하던 커플이 진짜커플이댓더라고\n좋은기억이라고 해도되나? 좋은일이 없었지만 진심이남은 공간이된 핸즈커피\n요즘좋아하는유튜버!! 못참고 인스타까지달려가 정주행완료 ㅎㅋ\nㅋㅋㅋ\n우울할때만 책찾고 우울할때만 일기쓰고 ㅋㅋ 이가짜독서인에 가짜블로거야!!!!\n"},{"id":88,"href":"/docs/study/tech/tech14/","title":"TFT 연구 방향","section":"생물정보학","content":" TFT 연구 방향 # #2025-05-31\n#1\n##사용하고자 하는 데이터는?\nfeature\nClinical feature (17, float): Creatinine, Hemoglobin, LDH, Lymphocytes, Neutrophils, Platelet count, WBC count, hs-CRP, D-Dimer, BDTEMP, BREATH, DBP, SBP, PULSE, SPO2, O2_APPLY Antibiotics feature (2, str) Treatment (list, str): 투여한 항생제, 결측값일수도있고 2개 이상일수도 있음 Strain (str): 환자가 감염된 균주, 1개 NEWS (int): 중증도 Code (int/str): 환자 등록번호 time-series\n10개 시점 (항생제 투여 기준 D-3 ~ D+6) ##TFT input 형식은?\nObserved (18): Creatinine, Hemoglobin, LDH, Lymphocytes, Neutrophils, Platelet count, WBC count, hs-CRP, D-Dimer, BDTEMP, BREATH, DBP, SBP, PULSE, SPO2, O2_APPLY / Strain Known (1): Treatment Static (1): Code Target: NEWS (목적: 항생제 투여에 따른 NEWS 예측)\n##문제점1\n\u0026lsquo;Treatment\u0026rsquo; 즉 리스트를 feature로 넣으려면 one hot encoding 해야함 one hot encoding 하면? \u0026lsquo;Treatment\u0026rsquo; feature의 차원이 너무 많아짐 항생제가 100종류 이상이라서 ##solution1\n\u0026lsquo;Treatment\u0026rsquo; feature를 항생제 리스트 대신 존재 유무 (0,1)로 변경 \u0026lsquo;Strain\u0026rsquo; feature도 항생제랑 관련된 feature이므로 우선 제거 ##수정된 input\nObserved (17): Creatinine, Hemoglobin, LDH, Lymphocytes, Neutrophils, Platelet count, WBC count, hs-CRP, D-Dimer, BDTEMP, BREATH, DBP, SBP, PULSE, SPO2, O2_APPLY Known (1): Treatment Static (1): Code Target: NEWS ##데이터 단순화 과정에서 무시된 내용 (문제점2)\n항생제의 균주 특이성 항생제는 투여 1일만에 NEWS를 낮출 수도 있고 2일 이상 소요될 수도 있음 중복 투여된 항생제가 서로 영향을 줬을 가능성 ##solution2\n항생제별 균주 특이성 feature 추가\n원래 데이터에서 항생제 별로 Sequence를 찾고 투여 후 NEWS가 감소하는 Sequence를 식별 (K means등 clustering 기법을 써도되고 단순히 감소폭을 봐도 되고) Sequence의 투여 첫날 기준 항생제-균주 pair를 찾기 Paired_antibiotics feature 추가 항생제별 NEWS를 낮추는데 소요시간 feature 추가\n항생제 투여 후 NEWS가 일정 수준까지 낮아지는데 소요되는 일수에 따라 유형 A, B, C로 구분 Response_time feature로 추가 ##solution1,2의 효과\n\u0026lsquo;항생제 종류\u0026rsquo;와 \u0026lsquo;균주\u0026rsquo;를 제거한 대신에 \u0026lsquo;항생제 종류\u0026rsquo;가 갖는 아래 특성만 (중요하다고 가정하고) 반영시킴 특정 균주 감염된 경우에 NEWS를 일정 수준 감소시킨 이력이 있는지 (0,1) 모든 투여 경우에서 NEWS를 일정 수준 감소시키는데 걸리는 시간이 느린편인지 빠른편인지 (A, B, C) #2\n##solution1,2에서 생각할수있는 이슈 사항\n추후 항생제 시뮬레이션을 할때도 자체적으로 annotation한 Paired_antibiotics 및 Response_time feature를 넣어줘야 할것인데 우리 데이터상에 적은 antibiotics나 strain인 경우 과대적합일 수도 있고 우리 데이터셋이 없는 antibiotics나 strain에 대해서는 적용하기 어렵다는 문제가 있음 known feature인 Treatment가 모든 sequence에서 투여 이전에 0인데 이게 TFT 알고리즘에서 불리하게 작용하는 점은 없을까? Encoder와 Decoder에 다른 feature가 들어가도 괜찮던데 이걸 최대로 이용할 방법은 없을까? ##solution1,2를 사용한 결과 모델의 의의\n17개 임상 feature와 항생제 투여유무 feature에 추가적으로 항생제-균주가 NEWS를 낮추는지 여부, 항생제 효과 소요시간을 반영했을때 NEWS 예측 성능이 올라갔다. 이는 항생제 항목 자체를 넣어주는 원-핫 인코딩을 썻을때보다는 dimension 축소 효과로 인해 예측 성능이 높아진거고 항생제-균주가 NEWS를 낮추는지 여부, 항생제 효과 소요시간을 반영하지 않았을때보다는 항생제의 2가지 특성을 반영했다는 이유로 인해 예측 성능이 높아진 것이며 이런 모델을 통해 6종 항생제 투여로 시뮬레이션 해본 결과 최적의 항생제 탐색에 사용 가능할거같다. 항생제 자체가 갖는 다차원 특성을 medical insight를 토대로 2개로 줄인것에 의의가 있다. ##생각\n이대로 진행해도 괜찮지만 뭔가 일찍부터 카테고리를 나눠서 수행하는것보다 항생제별로 다 결과를 뽑은 담에 결과를 토대로 역으로 그 카테고리가 나오게 하는게 이쁠거같음\n#3\n##이슈사항정리\nQ1) 항생제 feature가 D+0 이전에는 결측인 경우가 문제되지 않을까?\nA1) TFT의 known feature는 미래 예측을 위한 입력이며, 과거 구간에서는 비어있어도 문제가 되지 않는다고함. Encoder는 과거 임상 수치와 항생제 미투여 상태(0)만 보고 학습하고, Decoder는 항생제 시나리오가 주어졌을 때 그 조건하에 예측을 수행하는것은 TFT 구조 설계상 허용되는 일반적인 상황.\nQ2) 항생제 종류가 너무 많은 경우(100종 이상) 직접 one-hot or embedding 사용하면?\nA2) 100개 one-hot 인코딩 시 차원이 너무 크고 sparse하여 과적합 유발. Embedding도 너무 많아지면 학습 어렵고 특히 데이터 적으면 성능 저하될수있음.\nQ3) multi-hot 임베딩 하면?\nA3) 100종 항생제라고 치면 100개 binary feature로 넣어주는건데 구조가 단순하고 해석이 쉽지만 feature 수 많고 sparse하고 상호작용 표현 어려움\nQ4) 항생제 군 분류 후 군 정보 feature 쓰면?\nA4) beta-lactam계, macrolide계 등으로 10~15종으로 분류한 feature를 넣어주는건데 feature 수 줄고 효과 해석도 나쁘지않음 다른 항생제 종류 쓴 데이터에 시뮬레이션 하기도 괜찮을듯 근데 일반적으로 나누는 분류법이라서 일반적인 결과가 나와버릴수도\nQ5) 항생제 임베딩해서 균주, 반응시간과의 상호작용 반영된 latent vector 학습\nA5) feature를 가내수공업으로 넣어주는게 아니라 항생제 효과 요약 벡터를 생성하는건데 균주와의 관계, 반응소요시간 등에서 내가 놓칠수있는 부분을 캐치해서 넣어줄수있음. 예를들어 나는 샘플을 보고 NEWS를 3.0 이하로 떨어뜨린 경우가 많은 pair에 해당하면 \u0026ldquo;효과적\u0026rdquo; 아니면 \u0026ldquo;알수없음\u0026quot;으로 생각하는 알고리즘인데 딥러닝 돌리면 샘플을 보고 \u0026ldquo;~~~\u0026rdquo; 하니까 임베딩공간상 이 위치, 이 샘플은 \u0026ldquo;~~~\u0026rdquo; 하니까 임베딩공간상 다른 위치 이렇게 할당하는거고 \u0026ldquo;\u0026ldquo;에 NEWS를 3.0 이하로 떨어뜨린 경우가 많은지에 대한 비중이 큰지 작은지 없는지는 모르지만 어떤 weight가 줘진상태든 간에 데이터 상 내가생각한 저 기준보다 더 중요한 특성이 있으니가 weight를 덜 줬겠지 라고 생각하는것임. 이 방법은 설명력이 낮을 수 있다.\nQ6) 위 연구는 TFT를 적용한 항생제 연구로서 항생제 투여에 따른 NEWS 예측에 중요한 feature와 그렇지 않은 feature를 자동으로 weight 조절해서 학습하는게 포인트임. 근데 항생제 종류에 따라 중요한 feature가 다를 수도 있지 않나? 이걸 반영하지 않고 도출한 \u0026lsquo;중요한 feature 목록\u0026rsquo;은 그냥 \u0026ldquo;항생제\u0026quot;라는 x로 \u0026ldquo;NEWS\u0026quot;라는 y를 예측할때 일반적으로 이런 feature가 중요하다 선에서 그침. 모조리 넣고 항생제마다 결과를 봣을때 이런이런 feature가 비슷하다고 나온 애들은 확인해보니 이런 공통 특성을 갖더라 이런식으로 카테고리화는 마무리 단계에 들어가야하지 않나 싶음.\nA6) 조건 분기 Decoder를 적용하는 방법이 있는데 더 찾아봐야함\nQ7) 중복 투여에 따른 영향을 고려 안해도되나..\nA7) 아래 추천받은 방법을 일단 수행해보고 결정하기.\n목적: 본 연구는 \u0026ldquo;항생제 종류에 따라 NEWS score를 예측\u0026quot;하는 문제를 해결하고자 한다. 이를 위해 기존 Temporal Fusion Transformer(TFT) 구조에 다음 네 가지 기능을 통합한 모델을 제안한다:\nMulti-path 구조 항생제-균주 상호작용 임베딩 조건부 시나리오 입력 항생제 효과 지연 시간 반영 기존 TFT 구조 요약\nEncoder: 과거 시계열(임상 수치 등) 정보를 인코딩 Decoder: 미래 시점 예측 (known feature 사용) GRN + Attention: 중요 변수 선택 및 장기 의존성 반영 기존 TFT의 한계 (본 연구 기준)\n항생제 효과 구분 불가: 항생제 정보를 모델에 제대로 반영하지 못함 균주와 항생제 상호작용 무시\t특정 항생제가 어떤 균주에 효과적인지 파악 불가 약물 반응 지연 미반영\t투여 즉시 효과가 나타난다고 가정함 조건부 시나리오 예측 불가\t같은 환자라도 항생제를 바꾸었을 때의 결과 비교 불가 데이터 부족 문제\t항생제별로 모델을 나누면 데이터가 부족하고 과적합 발생 가능 제안하는 개선 TFT: Multi-path TFT with Antibiotic × Strain Interaction\n[1] Multi-path 구조: 항생제 종류에 따라 Decoder 경로 또는 Attention 흐름을 다르게 설정/Decoder 입력에 항생제 조건을 명시하여 조건부 예측 가능/같은 환자에 대해 항생제 시나리오를 바꿔 결과 비교 가능 [2] 항생제 × 균주 상호작용 임베딩: 항생제 임베딩과 균주 임베딩 간의 상호작용을 모델링 (concat, bilinear 등)/항균력 차이를 자동 학습할 수 있어 특정 조합의 효과를 반영 가능 [3] 조건부 Gated Layer: 항생제와 균주 정보에 따라 경로 가중치를 다르게 부여/특정 조합에 따라 예측 흐름을 다르게 조정 가능 [4] 효과 지연 반영: 항생제마다 효과가 나타나는 시간 차이를 가중치 또는 마스크 형태로 반영/예: Vancomycin은 1일 후, Piperacillin은 2일 후 효과가 나타나는 지연 구조를 학습\n최종 구조 개요\nStatic Encoder: 항생제 종류, 감염 균주 등 고정 정보 인코딩 Encoder (Observed features): 시계열 임상 수치 및 항생제 투여 여부 등 Decoder (Known future inputs): 미래 시점의 항생제 종류 및 투여 계획 Conditional Gating Layer: 항생제와 균주 정보를 입력으로 받아 예측 경로 가중치 조절 Output: 조건에 따른 NEWS score 예측 기대 효과\n항생제 반응 차이 반영: 항생제-균주 조합에 따른 실제 임상 반응 예측 가능 시나리오 기반 예측: 항생제 변경 시 예후 변화를 시뮬레이션 가능 데이터 손실 방지: 항생제별로 데이터를 분할하지 않아 데이터 효율성 유지 상호작용 내재화: 항생제-균주 관계를 잠재 공간에서 학습 가능 반응 지연 반영: 실제 약물 효과 발생 시점을 반영해 예측 정확도 향상 -\u0026gt; 항생제나 균주에 따라 중요한 feature가 다를 수도 있고 delay 효과가 다를 수도 있음을 반영 가능 (맞나?)\n"},{"id":89,"href":"/docs/study/tech/tech13/","title":"TFT PyTorch Forecasting - Stallion 튜토리얼 #2","section":"생물정보학","content":" TFT PyTorch Forecasting - Stallion 튜토리얼 #2 # #2025-05-29\n#version check\n예제 코드에 맞는 패키지 버전\nCUDA: 11.7 PyTorch: 1.13.1+cu117 PyTorch Lightning: 1.9.0 PyTorch Forecasting: 0.10.3 PyTorch Forecasting 0.10.3 선택 이유: 최신 버전은 아래 코드랑 호환 안됨\nTuner().lr_find() -\u0026gt; 학습률 탐색, lightning\u0026gt;=2.x에서는 내부 콜백 구조 변경됨 trainer.checkpoint_callback.best_model_path -\u0026gt; 베스트 모델 로드, trainer.checkpoint_callback 속성 제거됨 optimizer=\u0026quot;ranger\u0026quot; -\u0026gt; Ranger 옵티마이저 지정, 제거됨 plot_prediction, plot_interpretation, plot_dependency -\u0026gt; 시각화 함수, 제거되거나 구조 변경됨 optimize_hyperparameters -\u0026gt; Optuna 기반 튜닝, deprecated 또는 작동 오류 여기에 맞게 설치해주기.\nconda create -n tftspace python=3.9 -y pip install pip==23.3.1 #pip다운그레이드 #pip uninstall torch -y pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html pip install torchmetrics==0.10.3 # torch\u0026lt;2.0 호환 pip install pytorch-lightning==1.6.5 pip install pytorch-forecasting==0.10.3 pip install ranger-adabelief #optimizer=\u0026#34;ranger\u0026#34; 수행에 필요함 python 스크립트 상에서 확인하기\npip install ipykernel python -m ipykernel install --user --name tftspace --display-name \u0026#34;tftspace\u0026#34; #커널등록 #!pip install \u0026#34;numpy\u0026lt;2.0\u0026#34; import torch import torchvision import torchaudio import torchmetrics import pytorch_lightning import pytorch_forecasting print(\u0026#34;torch:\u0026#34;, torch.__version__) print(\u0026#34;torchvision:\u0026#34;, torchvision.__version__) print(\u0026#34;torchaudio:\u0026#34;, torchaudio.__version__) print(\u0026#34;torchmetrics:\u0026#34;, torchmetrics.__version__) print(\u0026#34;pytorch_lightning:\u0026#34;, pytorch_lightning.__version__) print(\u0026#34;pytorch_forecasting:\u0026#34;, pytorch_forecasting.__version__) ) torch: 1.13.1+cu117 torchvision: 0.14.1+cu117 torchaudio: 0.13.1+cu117 torchmetrics: 0.10.3 pytorch_lightning: 1.6.5 pytorch_forecasting: 0.10.3 제대로 설치됨!!\n#load package\nimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # avoid printing out absolute paths import copy from pathlib import Path import warnings #import lightning.pytorch as pl #from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor #from lightning.pytorch.loggers import TensorBoardLogger import pytorch_lightning as pl from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor from pytorch_lightning.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) lightning import 할때\n주석 처리된게 원래 스크립트이고 pytorch-lightning\u0026gt;=2.0에서 동작한다고 함 스크립트 중에 pytorch-lightning\u0026lt;2.0에서만 동작하는 함수가 있어서 \u0026gt;=2.0로는 설치할수없음 그래서 수정함. from pytorch_forecasting.data.examples import get_stallion_data data = get_stallion_data() 근데 버전을 낮춰서그런지 get_stallion_data()가 안먹어서 그냥 원래대로 버전을 맞추고 코드를 수정하는쪽으로 해야댈거같다.\n"},{"id":90,"href":"/docs/study/tech/tech11/","title":"bashrc 스크립트","section":"생물정보학","content":" bashrc 스크립트 # #2025-05-28\n#local\n1 #alias cobi2=\u0026#39;ssh -p 5290 ysh980101@155.230.28.211\u0026#39; 2 alias cobi2=\u0026#34;ssh -p 3160 ysh980101@155.230.110.91\u0026#34; 3 alias cobi3=\u0026#34;ssh -p 7777 ysh980101@155.230.110.92\u0026#34; 4 alias cobi4=\u0026#34;ssh -p 4712 ysh980101@155.230.110.93\u0026#34; 5 # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; 6 # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! 7 __conda_setup=\u0026#34;$(\u0026#39;/opt/anaconda3/bin/conda\u0026#39; \u0026#39;shell.bash\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null )\u0026#34; 8 if [ $? -eq 0 ]; then 9 eval \u0026#34;$__conda_setup\u0026#34; 10 else 11 if [ -f \u0026#34;/opt/anaconda3/etc/profile.d/conda.sh\u0026#34; ]; then 12 . \u0026#34;/opt/anaconda3/etc/profile.d/conda.sh\u0026#34; 13 else 14 export PATH=\u0026#34;/opt/anaconda3/bin:$PATH\u0026#34; 15 fi 16 fi 17 unset __conda_setup 18 # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; 19 20 21 22 source /opt/homebrew/opt/chruby/share/chruby/chruby.sh 23 source /opt/homebrew/opt/chruby/share/chruby/auto.sh 24 chruby ruby-3.1.3 #cobi2\n1 # ~/.bashrc: executed by bash(1) for non-login shells. 2 # see /usr/share/doc/bash/examples/startup-files (in the package bash-doc) 3 # for examples 4 5 # If not running interactively, don\u0026#39;t do anything 6 case $- in 7 *i*) ;; 8 *) return;; 9 esac 10 11 # don\u0026#39;t put duplicate lines or lines starting with space in the history. 12 # See bash(1) for more options 13 HISTCONTROL=ignoreboth 14 15 # append to the history file, don\u0026#39;t overwrite it 16 shopt -s histappend 17 18 # for setting history length see HISTSIZE and HISTFILESIZE in bash(1) 19 HISTSIZE=1000 20 HISTFILESIZE=2000 21 22 # check the window size after each command and, if necessary, 23 # update the values of LINES and COLUMNS. 24 shopt -s checkwinsize 25 26 # If set, the pattern \u0026#34;**\u0026#34; used in a pathname expansion context will 27 # match all files and zero or more directories and subdirectories. 28 #shopt -s globstar 29 30 # make less more friendly for non-text input files, see lesspipe(1) 31 [ -x /usr/bin/lesspipe ] \u0026amp;\u0026amp; eval \u0026#34;$(SHELL=/bin/sh lesspipe)\u0026#34; 32 33 # set variable identifying the chroot you work in (used in the prompt below) 34 if [ -z \u0026#34;${debian_chroot:-}\u0026#34; ] \u0026amp;\u0026amp; [ -r /etc/debian_chroot ]; then 35 debian_chroot=$(cat /etc/debian_chroot) 36 fi 37 38 # set a fancy prompt (non-color, unless we know we \u0026#34;want\u0026#34; color) 39 case \u0026#34;$TERM\u0026#34; in 40 xterm-color|*-256color) color_prompt=yes;; 41 esac 42 43 # uncomment for a colored prompt, if the terminal has the capability; turned 44 # off by default to not distract the user: the focus in a terminal window 45 # should be on the output of commands, not on the prompt 46 #force_color_prompt=yes 47 48 if [ -n \u0026#34;$force_color_prompt\u0026#34; ]; then 49 if [ -x /usr/bin/tput ] \u0026amp;\u0026amp; tput setaf 1 \u0026gt;\u0026amp;/dev/null; then 50 # We have color support; assume it\u0026#39;s compliant with Ecma-48 51 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such 52 # a case would tend to support setf rather than setaf.) 53 color_prompt=yes 54 else 55 color_prompt= 56 fi 57 fi 58 59 if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then 60 PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; 61 else 62 PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ \u0026#39; 63 fi 64 unset color_prompt force_color_prompt 65 66 # If this is an xterm set the title to user@host:dir 67 case \u0026#34;$TERM\u0026#34; in 68 xterm*|rxvt*) 69 PS1=\u0026#34;\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\u0026#34; 70 ;; 71 *) 72 ;; 73 esac 74 75 # enable color support of ls and also add handy aliases 76 if [ -x /usr/bin/dircolors ]; then 77 test -r ~/.dircolors \u0026amp;\u0026amp; eval \u0026#34;$(dircolors -b ~/.dircolors)\u0026#34; || eval \u0026#34;$(dircolors -b)\u0026#34; 78 alias ls=\u0026#39;ls --color=auto\u0026#39; 79 #alias dir=\u0026#39;dir --color=auto\u0026#39; 80 #alias vdir=\u0026#39;vdir --color=auto\u0026#39; 81 82 alias grep=\u0026#39;grep --color=auto\u0026#39; 83 alias fgrep=\u0026#39;fgrep --color=auto\u0026#39; 84 alias egrep=\u0026#39;egrep --color=auto\u0026#39; 85 fi 86 87 # colored GCC warnings and errors 88 #export GCC_COLORS=\u0026#39;error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01\u0026#39; 89 90 # some more ls aliases 91 alias ll=\u0026#39;ls -alF\u0026#39; 92 alias la=\u0026#39;ls -A\u0026#39; 93 alias l=\u0026#39;ls -CF\u0026#39; 94 95 # Add an \u0026#34;alert\u0026#34; alias for long running commands. Use like so: 96 # sleep 10; alert 97 alias alert=\u0026#39;notify-send --urgency=low -i \u0026#34;$([ $? = 0 ] \u0026amp;\u0026amp; echo terminal || echo error)\u0026#34; \u0026#34;$(history|tail -n1|sed -e \u0026#39;\\\u0026#39;\u0026#39;s/^\\s*[0-9]\\+\\s*//;s/[;\u0026amp;|]\\s*alert$//\u0026#39;\\\u0026#39;\u0026#39;)\u0026#34;\u0026#39; 98 99 # Alias definitions. 100 # You may want to put all your additions into a separate file like 101 # ~/.bash_aliases, instead of adding them here directly. 102 # See /usr/share/doc/bash-doc/examples in the bash-doc package. 103 104 if [ -f ~/.bash_aliases ]; then 105 . ~/.bash_aliases 106 fi 107 108 # enable programmable completion features (you don\u0026#39;t need to enable 109 # this, if it\u0026#39;s already enabled in /etc/bash.bashrc and /etc/profile 110 # sources /etc/bash.bashrc). 111 if ! shopt -oq posix; then 112 if [ -f /usr/share/bash-completion/bash_completion ]; then 113 . /usr/share/bash-completion/bash_completion 114 elif [ -f /etc/bash_completion ]; then 115 . /etc/bash_completion 116 fi 117 fi 118 119 export LANG=ko_KR.UTF-8 120 121 PS1=\u0026#39;[\\u@\\h \\W]\\n $ \u0026#39; 122 123 EDITOR=vim; export EDITOR 124 125 # User specific aliases and functions 126 alias rm=\u0026#39;rm -i\u0026#39; 127 alias cp=\u0026#39;cp -i\u0026#39; 128 alias mv=\u0026#39;mv -i\u0026#39; 129 alias l.=\u0026#39;ls -dl .[a-zA-Z]*\u0026#39; 130 alias ll=\u0026#39;ls -lht --color=tty\u0026#39; 131 alias ls=\u0026#39;ls -F --color=auto --show-control-char\u0026#39; 132 alias grep=\u0026#39;grep --color=auto\u0026#39; 133 alias vi=\u0026#39;vim\u0026#39; 134 alias sudo=\u0026#39;sudo \u0026#39; 135 136 alias tophatpy=\u0026#39;/usr/local/src/tophat-2.0.13/src/tophat.py\u0026#39; 137 138 KALLISTO=/data1/packages/kallisto 139 BOWTIE2=/data1/packages/bowtie2 140 BWA=/data1/packages/bwa/bin 141 BISMARK=/data3/workshop/2023_methylation_analysis/tool/Bismark-0.22.3/ 142 TOPHAT=/usr/local/src/tophat-2.0.13/src/ 143 SRA=/data/home/ysh980101/2310/sratoolkit/sratoolkit.3.0.7-centos_linux64/bin 144 145 TOOLS=$TOOLS:$BOWTIE2:$BISMARK:$TOPHAT:$KALLISTO:$BWA:$SRA 146 PATH=$PATH:$TOOLS 147 148 export PATH 149 150 151 # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; 152 # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! 153 __conda_setup=\u0026#34;$(\u0026#39;/data1/home/ysh980101/miniconda3/bin/conda\u0026#39; \u0026#39;shell.bash\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; 154 if [ $? -eq 0 ]; then 155 eval \u0026#34;$__conda_setup\u0026#34; 156 else 157 if [ -f \u0026#34;/data1/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; ]; then 158 . \u0026#34;/data1/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; 159 else 160 export PATH=\u0026#34;/data1/home/ysh980101/miniconda3/bin:$PATH\u0026#34; 161 fi 162 fi 163 unset __conda_setup 164 # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; 165 166 alias jupyter=\u0026#39;nohup jupyter lab --config .jupyter/jupyter_lab_config.py \u0026amp; \u0026gt;/dev/null\u0026#39; 167 168 169 170 171 172 173 PATH=\u0026#34;/data1/home/ysh980101/perl5/bin${PATH:+:${PATH}}\u0026#34;; export PATH; 174 PERL5LIB=\u0026#34;/data1/home/ysh980101/perl5/lib/perl5${PERL5LIB:+:${PERL5LIB}}\u0026#34;; export PERL5LIB; 175 PERL_LOCAL_LIB_ROOT=\u0026#34;/data1/home/ysh980101/perl5${PERL_LOCAL_LIB_ROOT:+:${PERL_LOCAL_LIB_ROOT}}\u0026#34;; export PERL_LOCAL_LIB_ROOT; 176 PERL_MB_OPT=\u0026#34;--install_base \\\u0026#34;/data1/home/ysh980101/perl5\\\u0026#34;\u0026#34;; export PERL_MB_OPT; 177 PERL_MM_OPT=\u0026#34;INSTALL_BASE=/data1/home/ysh980101/perl5\u0026#34;; export PERL_MM_OPT; #cobi3\n# ~/.bashrc: executed by bash(1) for non-login shells. # see /usr/share/doc/bash/examples/startup-files (in the package bash-doc) # for examples # If not running interactively, don\u0026#39;t do anything case $- in *i*) ;; *) return;; esac # don\u0026#39;t put duplicate lines or lines starting with space in the history. # See bash(1) for more options HISTCONTROL=ignoreboth # append to the history file, don\u0026#39;t overwrite it shopt -s histappend # for setting history length see HISTSIZE and HISTFILESIZE in bash(1) HISTSIZE=1000 HISTFILESIZE=2000 # check the window size after each command and, if necessary, # update the values of LINES and COLUMNS. shopt -s checkwinsize # If set, the pattern \u0026#34;**\u0026#34; used in a pathname expansion context will # match all files and zero or more directories and subdirectories. #shopt -s globstar # make less more friendly for non-text input files, see lesspipe(1) [ -x /usr/bin/lesspipe ] \u0026amp;\u0026amp; eval \u0026#34;$(SHELL=/bin/sh lesspipe)\u0026#34; # set variable identifying the chroot you work in (used in the prompt below) if [ -z \u0026#34;${debian_chroot:-}\u0026#34; ] \u0026amp;\u0026amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi # set a fancy prompt (non-color, unless we know we \u0026#34;want\u0026#34; color) case \u0026#34;$TERM\u0026#34; in xterm-color|*-256color) color_prompt=yes;; esac # uncomment for a colored prompt, if the terminal has the capability; turned # off by default to not distract the user: the focus in a terminal window # should be on the output of commands, not on the prompt #force_color_prompt=yes if [ -n \u0026#34;$force_color_prompt\u0026#34; ]; then if [ -x /usr/bin/tput ] \u0026amp;\u0026amp; tput setaf 1 \u0026gt;\u0026amp;/dev/null; then # We have color support; assume it\u0026#39;s compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fi fi if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ \u0026#39; fi unset color_prompt force_color_prompt # If this is an xterm set the title to user@host:dir case \u0026#34;$TERM\u0026#34; in xterm*|rxvt*) PS1=\u0026#34;\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\u0026#34; ;; *) ;; esac # enable color support of ls and also add handy aliases if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors \u0026amp;\u0026amp; eval \u0026#34;$(dircolors -b ~/.dircolors)\u0026#34; || eval \u0026#34;$(dircolors -b)\u0026#34; alias ls=\u0026#39;ls --color=auto\u0026#39; #alias dir=\u0026#39;dir --color=auto\u0026#39; #alias vdir=\u0026#39;vdir --color=auto\u0026#39; alias grep=\u0026#39;grep --color=auto\u0026#39; alias fgrep=\u0026#39;fgrep --color=auto\u0026#39; alias egrep=\u0026#39;egrep --color=auto\u0026#39; fi # colored GCC warnings and errors #export GCC_COLORS=\u0026#39;error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01\u0026#39; # some more ls aliases alias ll=\u0026#39;ls -alF\u0026#39; alias la=\u0026#39;ls -A\u0026#39; alias l=\u0026#39;ls -CF\u0026#39; # Add an \u0026#34;alert\u0026#34; alias for long running commands. Use like so: # sleep 10; alert alias alert=\u0026#39;notify-send --urgency=low -i \u0026#34;$([ $? = 0 ] \u0026amp;\u0026amp; echo terminal || echo error)\u0026#34; \u0026#34;$(history|tail -n1|sed -e \u0026#39;\\\u0026#39;\u0026#39;s/^\\s*[0-9]\\+\\s*//;s/[;\u0026amp;|]\\s*alert$//\u0026#39;\\\u0026#39;\u0026#39;)\u0026#34;\u0026#39; # Alias definitions. # You may want to put all your additions into a separate file like # ~/.bash_aliases, instead of adding them here directly. # See /usr/share/doc/bash-doc/examples in the bash-doc package. if [ -f ~/.bash_aliases ]; then . ~/.bash_aliases fi # enable programmable completion features (you don\u0026#39;t need to enable # this, if it\u0026#39;s already enabled in /etc/bash.bashrc and /etc/profile # sources /etc/bash.bashrc). if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(\u0026#39;/home/ysh980101/miniconda3/bin/conda\u0026#39; \u0026#39;shell.bash\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; else export PATH=\u0026#34;/home/ysh980101/miniconda3/bin:$PATH\u0026#34; fi fi unset __conda_setup # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; #cobi4\n# ~/.bashrc: executed by bash(1) for non-login shells. # see /usr/share/doc/bash/examples/startup-files (in the package bash-doc) # for examples # If not running interactively, don\u0026#39;t do anything case $- in *i*) ;; *) return;; esac # don\u0026#39;t put duplicate lines or lines starting with space in the history. # See bash(1) for more options HISTCONTROL=ignoreboth # append to the history file, don\u0026#39;t overwrite it shopt -s histappend # for setting history length see HISTSIZE and HISTFILESIZE in bash(1) HISTSIZE=1000 HISTFILESIZE=2000 # check the window size after each command and, if necessary, # update the values of LINES and COLUMNS. shopt -s checkwinsize # If set, the pattern \u0026#34;**\u0026#34; used in a pathname expansion context will # match all files and zero or more directories and subdirectories. #shopt -s globstar # make less more friendly for non-text input files, see lesspipe(1) [ -x /usr/bin/lesspipe ] \u0026amp;\u0026amp; eval \u0026#34;$(SHELL=/bin/sh lesspipe)\u0026#34; # set variable identifying the chroot you work in (used in the prompt below) if [ -z \u0026#34;${debian_chroot:-}\u0026#34; ] \u0026amp;\u0026amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi # set a fancy prompt (non-color, unless we know we \u0026#34;want\u0026#34; color) case \u0026#34;$TERM\u0026#34; in xterm-color|*-256color) color_prompt=yes;; esac # uncomment for a colored prompt, if the terminal has the capability; turned # off by default to not distract the user: the focus in a terminal window # should be on the output of commands, not on the prompt #force_color_prompt=yes if [ -n \u0026#34;$force_color_prompt\u0026#34; ]; then if [ -x /usr/bin/tput ] \u0026amp;\u0026amp; tput setaf 1 \u0026gt;\u0026amp;/dev/null; then # We have color support; assume it\u0026#39;s compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fi fi if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ \u0026#39; fi unset color_prompt force_color_prompt # If this is an xterm set the title to user@host:dir case \u0026#34;$TERM\u0026#34; in xterm*|rxvt*) PS1=\u0026#34;\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\u0026#34; ;; *) ;; esac # enable color support of ls and also add handy aliases if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors \u0026amp;\u0026amp; eval \u0026#34;$(dircolors -b ~/.dircolors)\u0026#34; || eval \u0026#34;$(dircolors -b)\u0026#34; alias ls=\u0026#39;ls --color=auto\u0026#39; #alias dir=\u0026#39;dir --color=auto\u0026#39; #alias vdir=\u0026#39;vdir --color=auto\u0026#39; alias grep=\u0026#39;grep --color=auto\u0026#39; alias fgrep=\u0026#39;fgrep --color=auto\u0026#39; alias egrep=\u0026#39;egrep --color=auto\u0026#39; fi # colored GCC warnings and errors #export GCC_COLORS=\u0026#39;error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01\u0026#39; # some more ls aliases alias ll=\u0026#39;ls -alF\u0026#39; alias la=\u0026#39;ls -A\u0026#39; alias l=\u0026#39;ls -CF\u0026#39; # Add an \u0026#34;alert\u0026#34; alias for long running commands. Use like so: # sleep 10; alert alias alert=\u0026#39;notify-send --urgency=low -i \u0026#34;$([ $? = 0 ] \u0026amp;\u0026amp; echo terminal || echo error)\u0026#34; \u0026#34;$(history|tail -n1|sed -e \u0026#39;\\\u0026#39;\u0026#39;s/^\\s*[0-9]\\+\\s*//;s/[;\u0026amp;|]\\s*alert$//\u0026#39;\\\u0026#39;\u0026#39;)\u0026#34;\u0026#39; # Alias definitions. # You may want to put all your additions into a separate file like # ~/.bash_aliases, instead of adding them here directly. # See /usr/share/doc/bash-doc/examples in the bash-doc package. if [ -f ~/.bash_aliases ]; then . ~/.bash_aliases fi # enable programmable completion features (you don\u0026#39;t need to enable # this, if it\u0026#39;s already enabled in /etc/bash.bashrc and /etc/profile # sources /etc/bash.bashrc). if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi # \u0026gt;\u0026gt;\u0026gt; user specific aliases and functions # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(\u0026#39;/home/ysh980101/miniconda3/bin/conda\u0026#39; \u0026#39;shell.bash\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/home/ysh980101/miniconda3/etc/profile.d/conda.sh\u0026#34; else export PATH=\u0026#34;/home/ysh980101/miniconda3/bin:$PATH\u0026#34; fi fi unset __conda_setup # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; "},{"id":91,"href":"/docs/study/tech/tech12/","title":"TFT PyTorch Forecasting - Stallion 튜토리얼","section":"생물정보학","content":" TFT PyTorch Forecasting - Stallion 튜토리얼 # #2025-05-28\n#introduction\n데이터셋: Kaggle - Stallion 데이터셋 목적: Temporal Fusion Transformer(TFT)를 활용하여 음료 판매량을 예측 #install\n$ nvidia-smi Wed May 28 14:00:07 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 545.23.08 Driver Version: 545.23.08 CUDA Version: 12.3 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA RTX A6000 Off | 00000000:3B:00.0 Off | Off | | 30% 59C P2 204W / 300W | 8339MiB / 49140MiB | 95% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA RTX A6000 Off | 00000000:5E:00.0 Off | Off | | 30% 60C P2 213W / 300W | 6897MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA RTX A6000 Off | 00000000:B1:00.0 Off | Off | | 30% 60C P2 203W / 300W | 6799MiB / 49140MiB | 94% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 3 NVIDIA RTX A6000 Off | 00000000:D9:00.0 Off | Off | | 32% 63C P2 212W / 300W | 6885MiB / 49140MiB | 96% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 0 N/A N/A 20199 C ...dg/miniconda3/envs/woodg/bin/python 664MiB | | 0 N/A N/A 860801 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1201205 C ...u1098/anaconda3/envs/dna/bin/python 6198MiB | | 0 N/A N/A 1216286 C ...jyj/miniconda3/envs/TiCC/bin/python 338MiB | | 0 N/A N/A 1225349 C python 782MiB | | 1 N/A N/A 1201206 C ...u1098/anaconda3/envs/dna/bin/python 6104MiB | | 1 N/A N/A 1224607 C python 782MiB | | 2 N/A N/A 1201207 C ...u1098/anaconda3/envs/dna/bin/python 6006MiB | | 2 N/A N/A 1224848 C python 782MiB | | 3 N/A N/A 1201208 C ...u1098/anaconda3/envs/dna/bin/python 6092MiB | | 3 N/A N/A 1225121 C python 782MiB | +---------------------------------------------------------------------------------------+ NVIDIA 드라이버 버전: 545.23.08\nCUDA 버전: 12.3\nPyTorch 및 관련 패키지를 설치할 때 CUDA 12.3을 지원하는 버전으로 맞춰야 GPU 사용이 가능.\nCUDA 12.3을 그대로 쓰는 경우 PyTorch GPU 버전과의 호환성이 낮거나 불안정할 수 있어 CUDA 11.7로 설치해준다 $ pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 -f https://download.pytorch.org/whl/torch_stable.html Looking in links: https://download.pytorch.org/whl/torch_stable.html Collecting torch==1.13.1+cu117 Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp37-cp37m-linux_x86_64.whl (1801.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 GB 1.5 MB/s eta 0:00:00 Collecting torchvision==0.14.1+cu117 Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp37-cp37m-linux_x86_64.whl (24.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/24.3 MB 42.5 MB/s eta 0:00:00 Collecting torchaudio==0.13.1 Downloading https://download.pytorch.org/whl/rocm5.2/torchaudio-0.13.1%2Brocm5.2-cp37-cp37m-linux_x86_64.whl (3.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 60.0 MB/s eta 0:00:00 Requirement already satisfied: typing-extensions in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from torch==1.13.1+cu117) (4.7.1) Requirement already satisfied: numpy in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from torchvision==0.14.1+cu117) (1.21.6) Requirement already satisfied: requests in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from torchvision==0.14.1+cu117) (2.31.0) Requirement already satisfied: pillow!=8.3.*,\u0026gt;=5.3.0 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from torchvision==0.14.1+cu117) (9.5.0) Requirement already satisfied: charset-normalizer\u0026lt;4,\u0026gt;=2 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from requests-\u0026gt;torchvision==0.14.1+cu117) (3.3.2) Requirement already satisfied: certifi\u0026gt;=2017.4.17 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from requests-\u0026gt;torchvision==0.14.1+cu117) (2022.12.7) Requirement already satisfied: urllib3\u0026lt;3,\u0026gt;=1.21.1 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from requests-\u0026gt;torchvision==0.14.1+cu117) (1.26.20) Requirement already satisfied: idna\u0026lt;4,\u0026gt;=2.5 in /home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages (from requests-\u0026gt;torchvision==0.14.1+cu117) (3.7) Installing collected packages: torch, torchvision, torchaudio Attempting uninstall: torch Found existing installation: torch 1.13.1 Uninstalling torch-1.13.1: Successfully uninstalled torch-1.13.1 Successfully installed torch-1.13.1+cu117 torchaudio-0.13.1+rocm5.2 torchvision-0.14.1+cu117 정상 설치 여부 확인\npython -c \u0026#34;import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \u0026#39;No GPU\u0026#39;)\u0026#34; 1.13.1+cu117 True NVIDIA RTX A6000 문제없이 설치되었다!\n#load package\n$ pip install lightning $ pip install pytorch-forecasting $ pip install pyarrow import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # avoid printing out absolute paths import copy from pathlib import Path import warnings import lightning.pytorch as pl from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor from lightning.pytorch.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) #load data\nfrom pytorch_forecasting.data.examples import get_stallion_data data = get_stallion_data() # add time index data[\u0026#34;time_idx\u0026#34;] = data[\u0026#34;date\u0026#34;].dt.year * 12 + data[\u0026#34;date\u0026#34;].dt.month data[\u0026#34;time_idx\u0026#34;] -= data[\u0026#34;time_idx\u0026#34;].min() # add additional features data[\u0026#34;month\u0026#34;] = data.date.dt.month.astype(str).astype( \u0026#34;category\u0026#34; ) # categories have be strings data[\u0026#34;log_volume\u0026#34;] = np.log(data.volume + 1e-8) data[\u0026#34;avg_volume_by_sku\u0026#34;] = data.groupby( [\u0026#34;time_idx\u0026#34;, \u0026#34;sku\u0026#34;], observed=True ).volume.transform(\u0026#34;mean\u0026#34;) data[\u0026#34;avg_volume_by_agency\u0026#34;] = data.groupby( [\u0026#34;time_idx\u0026#34;, \u0026#34;agency\u0026#34;], observed=True ).volume.transform(\u0026#34;mean\u0026#34;) # we want to encode special days as one variable and thus need to first reverse one-hot encoding special_days = [ \u0026#34;easter_day\u0026#34;, \u0026#34;good_friday\u0026#34;, \u0026#34;new_year\u0026#34;, \u0026#34;christmas\u0026#34;, \u0026#34;labor_day\u0026#34;, \u0026#34;independence_day\u0026#34;, \u0026#34;revolution_day_memorial\u0026#34;, \u0026#34;regional_games\u0026#34;, \u0026#34;fifa_u_17_world_cup\u0026#34;, \u0026#34;football_gold_cup\u0026#34;, \u0026#34;beer_capital\u0026#34;, \u0026#34;music_fest\u0026#34;, ] data[special_days] = ( data[special_days].apply(lambda x: x.map({0: \u0026#34;-\u0026#34;, 1: x.name})).astype(\u0026#34;category\u0026#34;) ) data.sample(10, random_state=521) data.describe() #Create dataset and dataloaders\nmax_prediction_length = 6 max_encoder_length = 24 training_cutoff = data[\u0026#34;time_idx\u0026#34;].max() - max_prediction_length training = TimeSeriesDataSet( data[lambda x: x.time_idx \u0026lt;= training_cutoff], time_idx=\u0026#34;time_idx\u0026#34;, target=\u0026#34;volume\u0026#34;, group_ids=[\u0026#34;agency\u0026#34;, \u0026#34;sku\u0026#34;], min_encoder_length=max_encoder_length // 2, # keep encoder length long (as it is in the validation set) max_encoder_length=max_encoder_length, min_prediction_length=1, max_prediction_length=max_prediction_length, static_categoricals=[\u0026#34;agency\u0026#34;, \u0026#34;sku\u0026#34;], static_reals=[\u0026#34;avg_population_2017\u0026#34;, \u0026#34;avg_yearly_household_income_2017\u0026#34;], time_varying_known_categoricals=[\u0026#34;special_days\u0026#34;, \u0026#34;month\u0026#34;], variable_groups={ \u0026#34;special_days\u0026#34;: special_days }, # group of categorical variables can be treated as one variable time_varying_known_reals=[\u0026#34;time_idx\u0026#34;, \u0026#34;price_regular\u0026#34;, \u0026#34;discount_in_percent\u0026#34;], time_varying_unknown_categoricals=[], time_varying_unknown_reals=[ \u0026#34;volume\u0026#34;, \u0026#34;log_volume\u0026#34;, \u0026#34;industry_volume\u0026#34;, \u0026#34;soda_volume\u0026#34;, \u0026#34;avg_max_temp\u0026#34;, \u0026#34;avg_volume_by_agency\u0026#34;, \u0026#34;avg_volume_by_sku\u0026#34;, ], target_normalizer=GroupNormalizer( groups=[\u0026#34;agency\u0026#34;, \u0026#34;sku\u0026#34;], transformation=\u0026#34;softplus\u0026#34; ), # use softplus and normalize by group add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True, ) # create validation set (predict=True) which means to predict the last max_prediction_length points in time # for each series validation = TimeSeriesDataSet.from_dataset( training, data, predict=True, stop_randomization=True ) # create dataloaders for model batch_size = 128 # set this between 32 to 128 train_dataloader = training.to_dataloader( train=True, batch_size=batch_size, num_workers=0 ) val_dataloader = validation.to_dataloader( train=False, batch_size=batch_size * 10, num_workers=0 ) #Create baseline model\n# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history baseline_predictions = Baseline().predict(val_dataloader, return_y=True) MAE()(baseline_predictions.output, baseline_predictions.y) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /tmp/ipykernel_1239848/2174382858.py in \u0026lt;module\u0026gt; 1 # calculate baseline mean absolute error, i.e. predict next value as the last available value from the history ----\u0026gt; 2 baseline_predictions = Baseline().predict(val_dataloader, return_y=True) 3 MAE()(baseline_predictions.output, baseline_predictions.y) ~/miniconda3/envs/workspace/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in predict(self, data, mode, return_index, return_decoder_lengths, batch_size, num_workers, fast_dev_run, show_progress_bar, return_x, mode_kwargs, **kwargs) 1157 1158 # make prediction -\u0026gt; 1159 out = self(x, **kwargs) # raw output is dictionary 1160 1161 lengths = x[\u0026#34;decoder_lengths\u0026#34;] ~/miniconda3/envs/workspace/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1193 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1194 return forward_call(*input, **kwargs) 1195 # Do not call functions when jit is used 1196 full_backward_hooks, non_full_backward_hooks = [], [] TypeError: forward() got an unexpected keyword argument \u0026#39;return_y\u0026#39; 왜인지 모르겠지만 baseline_predictions = Baseline().predict(val_dataloader, return_y=True)에서 return_y라는 인자는 안받는다고함. return_y=True를 빼고 Baseline().predict(val_dataloader)만 사용하면 예측값(prediction)만 반환되고 실제값(y)은 반환되지 않음 return_y=True 결과를 얻으려면 즉 MAE를 계산하려면 직접 val_dataloader에서 y 값을 꺼내도록 코드 수정 from pytorch_forecasting.metrics import MAE from pytorch_forecasting.models.baseline import Baseline baseline_model = Baseline() y_pred = baseline_model.predict(val_dataloader) y_true = torch.cat([batch[0][\u0026#34;decoder_target\u0026#34;] for batch in val_dataloader]) mae = MAE()(y_pred, y_true) mae tensor(293.0088) #Train the Temporal Fusion Transformer\n##Find optimal learning rate\n# configure network and trainer pl.seed_everything(42) trainer = pl.Trainer( accelerator=\u0026#34;cpu\u0026#34;, # clipping gradients is a hyperparameter and important to prevent divergance # of the gradient for recurrent neural networks gradient_clip_val=0.1, ) tft = TemporalFusionTransformer.from_dataset( training, # not meaningful for finding the learning rate but otherwise very important learning_rate=0.03, hidden_size=8, # most important hyperparameter apart from learning rate # number of attention heads. Set to up to 4 for large datasets attention_head_size=1, dropout=0.1, # between 0.1 and 0.3 are good values hidden_continuous_size=8, # set to \u0026lt;= hidden_size loss=QuantileLoss(), optimizer=\u0026#34;ranger\u0026#34;, # reduce learning rate if no improvement in validation loss after x epochs # reduce_on_plateau_patience=1000, ) print(f\u0026#34;Number of parameters in network: {tft.size() / 1e3:.1f}k\u0026#34;) Global seed set to 42 GPU available: True (cuda), used: False TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs Number of parameters in network: 13.5k # find optimal learning rate from lightning.pytorch.tuner import Tuner res = Tuner(trainer).lr_find( tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, max_lr=10.0, min_lr=1e-6, ) print(f\u0026#34;suggested learning rate: {res.suggestion()}\u0026#34;) fig = res.plot(show=True, suggest=True) fig.show() --------------------------------------------------------------------------- ImportError Traceback (most recent call last) /tmp/ipykernel_1239848/4268711780.py in \u0026lt;module\u0026gt; 1 # find optimal learning rate ----\u0026gt; 2 from lightning.pytorch.tuner import Tuner 3 4 res = Tuner(trainer).lr_find( 5 tft, ImportError: cannot import name \u0026#39;Tuner\u0026#39; from \u0026#39;lightning.pytorch.tuner\u0026#39; (/home/ysh980101/miniconda3/envs/workspace/lib/python3.7/site-packages/lightning/pytorch/tuner/__init__.py) pip show lightning으로 확인 결과 lightning 버전이 1.9.5이고 lightning.pytorch 패키지 구조가 도입되기 전 버전이어서 오류가 났다 import를 수정해주고 import한거에 맞춰서 코드도 수정 lr_finder 종료 후 자동 복원, 학습률 자동 업데이트 \u0026laquo;를 충족하도록 수정했다. # find optimal learning rate from pytorch_lightning import Trainer from pytorch_lightning.tuner.tuning import Tuner tuner = Tuner(trainer) lr_finder = tuner.lr_find( tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-6, max_lr=10.0, update_attr=False ) 왜인지 모르겟는데 수정한 코드에서는 \u0026lsquo;ranger\u0026rsquo;가 안먹어서, 학습률 선택은 다른 optimizer로 하고 선택한 학습률을 ranger optimizer 쓰는 원래 모델에 적용시키는 아래 코드를 챗지피티가 추천해줬다 from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters from pytorch_forecasting import TemporalFusionTransformer # Ranger 대신 Adam 사용 (lr 찾기 전용) tft_tmp = TemporalFusionTransformer.from_dataset( training, learning_rate=0.03, hidden_size=8, attention_head_size=1, dropout=0.1, hidden_continuous_size=8, loss=QuantileLoss(), optimizer=\u0026#34;adam\u0026#34;, ) # lr_find() tuner = Tuner(trainer) lr_finder = tuner.lr_find( tft_tmp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-6, max_lr=10.0, ) suggested_lr = lr_finder.suggestion() print(f\u0026#34;Suggested LR: {suggested_lr}\u0026#34;) fig = lr_finder.plot(suggest=True) fig.show() Suggested LR: 0.0019498445997580445 tft.hparams.learning_rate = suggested_lr #ranger 옵티마이저를 사용하는 원래 tft 모델에 이 학습률을 적용 근데 이게 맞나.. 싶어서 버전 맞춰서 다시해볼예정.\n코드: https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html\n"},{"id":92,"href":"/docs/study/bioinformatics/bi22/","title":"[NAVER Cloud] 환자향 진료기록 생성 모델 개발 (체험형 인턴)","section":"생물정보학","content":" [NAVER Cloud] 환자향 진료기록 생성 모델 개발 (체험형 인턴) # 링크 - https://recruit.navercloudcorp.com/rcrt/view.do?annoId=30003399\u0026lang=ko\n"},{"id":93,"href":"/docs/study/bioinformatics/bi21/","title":"한국사능력검정시험 시험일정","section":"생물정보학","content":" 한국사능력검정시험 시험일정 # 정보 - https://www.historyexam.go.kr/pageLink.do?link=examSchedule\u0026netfunnel_key=B074990245F42DF6F192C5CF3EDF850DA87F7088B5EC6B2A5509EF323965FA6EEFA3B493E69B1EC65461E3F8696674B62CB7178428F55B3E8FF8E7D02A753B4362CAC618E726254B9B86061931358E535040FF6CCA34DB8A028CF47044B6F18A234B1EEDF2C1E725FD8CB4420BEBC394362C312C302C30\n"},{"id":94,"href":"/docs/hobby/daily/daily11/","title":"초여름 부산˚‧｡🐋","section":"일상","content":" 초여름 부산˚‧｡🐋 # #2025-05-20\n간단하게 일상으로만 쓸려다가,,, 넘좋앗어서 그냥 따로 뺐다 ㅎㅎㅎ\n다대포 할매집!! 문어삼합 / 냄비라면 / 올빚베리막걸리 시켯는데 다맛있었당 특히 딸기막걸리는 집에오니깐 또생각나서 사올걸 후회해따\n넘이뻤던바닷가\n길이사왓는데 넘맛있어서 막퍼먹은 케이크 ㅋㅋ 멜트인멜로우 검색해보니까 다른디저트도 다마싯을거같아서 또사먹을듯\n생레몬하이볼 첨먹어봣는데 주점에서파는하이볼 같음\n다음날\u0026hellip; 지수집엔 마싯는게 많다 일리커피가 네스프레소보다 훨배 맛있는거같음 다크원두인데 쓴맛이없고 향이 엄청 좋다\n부산현대미술관 구경가서 소화시켜줌 전시란 어렵다 .. 저 망원경안엔 무려 구슬 4-5개가 들어있었는데 뭘전하고싶은건지모르겟어서 혼란스러웟음 ㅋㅋ\n각자의취향대로 픽한 전시굿즈 ㅎㅎ\n메인전시가 10개의눈(시각장애인) 초록 전율(생태) 두개엿는데, 생태 전시는 넷플릭스 다큐멘터리 내지는 네셔널지오그래픽 느낌나서 그나마 재밋게봐가지구 엽서가튼거 있으면 살랬는데 없어서 그냥 왔다\n굿즈샵에서 이쁜반팔티 발견해서 즉흥으로맞춰입음 ㅎㅎㅎ\n그리고\u0026hellip; 역시 즉흥으로 검색한 디베르소에스프레소바 라는 카페를갓다가 웨이팅이 걸려서 또 즉흥으로 들어온 동일건물 1층 브런치카페 프라한명지 인데 안이 넘 이뻤다!! 윤식당 발리편 가게 느낌났는데 북유럽감성 브런치집이라구함\n알감자\u0026amp;비프 칠리 보울 / 크림 버섯 파스타 / 파라한 프렌치 토스트 시켰는데 다맛있었당 1등은 프렌치토스트!!\n다먹구 디베르소에스프레소바 다시올라갓는데 자리없다구 3-4층 에어비앤비 자리 주심\u0026hellip;ㅋㅋ 안에 너무 넓고 예쁘고 음악이좋고 소파가 안락햇다\n에스프레소바답게 커피도 진짜 맛있었음\n더오래잇고싶엇는데 (아무도눈치안줫지만) 적당히있다나와야댈거같아서 한 두시간??알차게 즐기고 나왔당 에어비앤비 주중12만 주말18만인가 그랬는데 뷰도이쁘구 갈만한거같다!\n바다 최고..\n최근에 adsp / 한국산업기술기획평가원 필기 / sqld 사이에서 정신없었는데 말그대로 힐링(치료)받구옴 취업해서 다음엔 더맘편히 풍족하게 놀러나오고싶다는 생각이 들었당\n"},{"id":95,"href":"/docs/hobby/daily/daily9/","title":"13일의화요일","section":"일상","content":" 13일의화요일 # #2025-05-13\n넘이쁜 유튜버 찾아따\n갑자기쇼핑도해줬고\n아침엔또행복을 수혈해줬다\n운동을하면 저녁시간이잘간다 밥챙겨먹고 운동가고 잠잘자고 남은시간에는 유튜브보고 해야할일을하고..\n일상이 유지돼서 내마음이단단하면 쫌덜흔들릴수있으니깐 뇌에힘을주자!!!! 너무욕심내지말고 너무포기하지말자 그리구잘잊자\n"},{"id":96,"href":"/docs/hobby/daily/daily8/","title":"열시미충전하는 연휴","section":"일상","content":" 열시미충전하는 연휴 # #2025-05-06\n"},{"id":97,"href":"/docs/hobby/daily/daily7/","title":"스페이스임원","section":"일상","content":" 스페이스임원 # #2025-04-28\n전체적으로 초록초록한 분위기가 넘 예뻤던 스페이스임원!!\n브런치 종류가 많았는데 쉬림프 감자 타르틴 / 샥슈카 / 스페이스 치아바타 샌드위치를 시켰다.\n셋다 마싰었지만 치아바타 샌드위치가 내스탈이었다 ㅎㅎㅎ 쉬림프 감자 타르틴은 엄마가 맛있다고 했는데 평소에 감자 사라다 st 그렇게 좋아하지 않는데두 내 입에도 괜찮았당\n샥슈카는 일반적인 라구소스맛 브런치들에 비해 고기맛이랑 짠맛이 적게 나고 토마토맛이 많이 나서 맛있게 먹었다!\n두명이서 오면 2층 테라스 자리에 앉아도 좋을것같음. 나오면서 트러플 에그 갈레트랑 라구 오픈 샌드위치를 다음에 먹을 메뉴로 찜해뒀다 ㅎㅎ\n"},{"id":98,"href":"/docs/study/tech/tech7/","title":"DBeaver 환경설정, SELECT문","section":"생물정보학","content":" DBeaver 환경설정, SELECT문 # 목록 # 2025-04-22 ⋯ 섹션 1. SQLD 시험 개요, 강의 소개, 실습 환경 설정\n2025-04-22 ⋯ 섹션 3. SELECT 문\n섹션 1. SQLD 시험 개요, 강의 소개, 실습 환경 설정 # 1. 실습환경 설정 # DBeaver Community 설치 https://dbeaver.io/download/](https://dbeaver.io/download/) Wallet 다운로드 JDBC Driver 다운로드 https://www.oracle.com/kr/database/technologies/appdev/jdbc-downloads.html $ pwd /Users/yshmbid/oracle $ ls Wallet_SQLD\tojdbc8-full Wallet_SQLD.zip\tojdbc8-full.tar.gz /Users/yshmbid/oracle 위치에 잘 넣어줬다\nDBeaver 열기\nJDBC URL Template, Username, Password 입력 Driver setting \u0026gt; Libraries \u0026gt; /Users/yshmbid/oracle/ojdbc8-full 넣어줌 Test Connection했을때 아래처럼 뜨면 정상!\n2. 데이터세트 소개 # 부서,사원 데이터셋\n축구 데이터셋\nstadium은 여러개의 team 데이터셋을 가질수있다. (삼지창)\nstadium에는 team이 없는 경우도 있다. (optional) team은 반드시 경기장이 있어야하고 하나만 가질수있다. team은 여러명의 player를 가질수있고 0명의 player를 가져도된다(흰색원). player는 반드시 team을 하나 가져야한다. stadium은 경기가 여러개일수있고 경기가 없을수도 있다. schedule은 경기장을 하나만 가질수있다.\nschedule에 stadium_id랑 sche_date는 여러개의키를 쓰는 복합키이다. 불러오는법\nSELECT * FROM sqld.emp; SELECT * FROM sqld.dept; SELECT * FROM sqld.stadium; 섹션 3. SELECT 문 # 1. 스키마 # SELECT * FROM emp; sqld.를 앞에 안붙이려면 스키마를 SQLD로 바꿔줘야한다. 2. 실습 # #1\n-- *는 모든 열 선택 SELECT * FROM emp; -- 조회하려는 컬럼을 콤마(,)로 구분해서 가져오기 SELECT empno, ename, job, deptno FROM emp; #2\nSELECT ALL job FROM emp; SELECT DISTINCT job FROM emp; 두 쿼리의 차이? ALL은 중복 포함, DISTINCT하면 중복 제거. SELECT DISTINCT deptno, job FROM emp ORDER BY 1, 2; DEPTNO와 JOB의 모든 조합을 가져온다 #3\n-- ALIAS 부여하기 -- AS 키워드로 컬럼에 별명 부여 SELECT empno AS 사원번호, ename AS 이름, deptno AS 부서번호, job AS 업무 FROM emp; 출력되는 컬럼 label이 변경. AS 생략가능 띄어쓰기, 특수문자 안됨, 대소문자 구별안됨 #4\n-- 산술 연산자, 수학의 사칙연산 -- NUMBER와 DATE에 적용 가능 -- 연산자 우선순위 -- 1. () -- 2. *, / -- 3. +, - SELECT sal, sal*0.3, 100+300, -- 모든 행에 같은 값 sal-deptno FROM emp; -- NULL과의 산술 연산은 항상 NULL을 반환 SELECT sal+comm, sal+NULL, sal-NULL, sal*NULL, sal/NULL FROM emp; #5\n합성 연산자는 문자열 결합에 사용. /* * 합성 연산자 * 오라클 ||, SQL Server + */ -- KING의 직책은 PRESIDENT이며 연봉은 5000이다. SELECT ename || \u0026#39;의 직책은 \u0026#39; || job || \u0026#39;이며 연봉은 \u0026#39; || sal ||\u0026#39;이다.\u0026#39; FROM emp; -- CONCAT 2개 문자열 합성 SELECT CONCAT(\u0026#39;연봉\u0026#39;, sal) -- CONCAT(\u0026#39;연봉\u0026#39;, \u0026#39; \u0026#39;, sal) -- 오류. Oracle CONCA은 인자 2개만 받음 FROM emp; CONCAT도 문자열을 합성하는 함수인데 2개만 가능하다. 강의 출처 https://www.inflearn.com/course/sqld-%EC%99%84%EC%84%B1-2%EA%B3%BC%EB%AA%A9\n"},{"id":99,"href":"/docs/study/tech/tech9/","title":"문자형/숫자형/날짜형/기타 함수","section":"생물정보학","content":" 문자형/숫자형/날짜형/기타 함수 # 목록 # 2025-04-22 ⋯ 섹션 4. 함수\n섹션 4. 함수 # 1. 문자형 함수 # #1\nSELECT lower(\u0026#39;SQL Expert\u0026#39;), upper(\u0026#39;SQL Expert\u0026#39;), ascii(\u0026#39;A\u0026#39;), chr(65), concat(\u0026#39;SQL\u0026#39;, \u0026#39; Expert\u0026#39;), -- 2개까지만 length(\u0026#39;SQL Expert\u0026#39;) FROM dual; 각각 이렇게 나온다. oracle은 concat 2개까지만, sql server는 3개도 됨. oracle은 length이고 sql은 len이다. SELECT ltrim(\u0026#39;xxxYYYZZxYZ\u0026#39;, \u0026#39;x\u0026#39;), ltrim(\u0026#39;xxxYYYZZxYZ\u0026#39;, \u0026#39;xY\u0026#39;), ltrim(\u0026#39;xxxYYYZZxYZ\u0026#39;, \u0026#39;xYZ\u0026#39;), ltrim(\u0026#39; xxxx\u0026#39;), rtrim(\u0026#39;xxxYYYZZxYZ\u0026#39;, \u0026#39;ZY\u0026#39;), rtrim(\u0026#39;xxxx \u0026#39;), trim(\u0026#39; xxxx \u0026#39;) FROM dual; ltrim: 왼쪽부터 검사해서 \u0026lsquo;x\u0026rsquo; 제거된 문자열을 반환\n\u0026lsquo;xY\u0026rsquo; 넣어주면 \u0026lsquo;x\u0026rsquo;와 \u0026lsquo;Y\u0026rsquo;가 모두 제거 아무것도 안넣어주면 공백 제거 sql 서버에서는?\nltrim이 두번째 인자를 받지않는다. LTRIM과 RTRIM이 오직 공백만 제거 SELECT trim(\u0026#39;x\u0026#39; FROM \u0026#39;xxxxYYYzzYZxxxx\u0026#39;), trim(BOTH \u0026#39;x\u0026#39; FROM \u0026#39;xxxxYYYzzYZxxxx\u0026#39;), trim(LEADING \u0026#39;x\u0026#39; FROM \u0026#39;xxxxYYYzzYZxxxx\u0026#39;), trim(TRAILING \u0026#39;x\u0026#39; FROM \u0026#39;xxxxYYYzzYZxxxx\u0026#39;) FROM dual; trim은 ltrim rtrim과 달리 문자열 하나밖에 못넣어준다. trim('x' FROM 'xxxxYYYzzYZxxxx')은 사실 앞에 BOTH가 생략된거랑 같다. leading trailing은 ltrim rtrim과 사실상 같다. #2\nSELECT REPLACE(\u0026#39;sql expert\u0026#39;,\u0026#39;ert\u0026#39;, \u0026#39;tre\u0026#39;) FROM dual; REPLACE: 문자열을 찾아서 치환해줘서 sql exptre가 나옴 빈문자열을 넣으면 그냥 삭제해준다. SELECT substr(\u0026#39;Gangneung Wonju\u0026#39;, 8, 4) FROM dual; substr: 8번째부터 4개 출력해줘서 g Wo가 나옴 sql에서는 이름이 substring이다. oracle은 substr('Gangneung Wonju', 8)도 되는데 sql은 세번째인자 생략하면 안된다. SELECT left(\u0026#39;Gangneung Wonju\u0026#39;, 8) right(\u0026#39;Gangneung Wonju\u0026#39;, 8) FROM dual; sql에서만 되고 left는 1번째부터 8번째까지 출력해줘서 Gangneun가 나오고 right는 ng Wonju 나옴 oracle에서 right와 동일한결과는 substr('Gangneung Wonju', -8) oracle에서 substr('Gangneung Wonju', -5, 2) 하면 Wo 나옴 2. 숫자형 함수 # #1\nSELECT mod(7,3), sign(-3), ABS(-15), ceil(38.123), floor(38.888), round(38.525, 2) FROM dual mod는 나머지, sign은 부호 sql은 mod(7,3)대신 7%3으로쓴다. ceil 대신 ceiling 쓰고 floor은 그대로 쓴다. round(38.525, 2)는 소수점 둘째자리까지 반올림한다. oracle은 round(38.525)와 같이 두번째 인자 생략 가능하다. round(38.525, -1)하면 십의자리까지 반올림해서 40이 된다 #2\nSELECT trunc(38.888, 2), round(38.888, 2), ceil(38.888), floor(38.888) FROM dual floor, ceil은 두번째 인자 못받는데 trunc는 받는다 trunc는 버림이고 round는 반올림이다. sql은 trunc 없고 round(38.525, 2)해주면 38.89가, round(38.525, 2, 1) 해주면 버림으로 38.88이 나온다 SELECT power(2, 4), exp(2), sqrt(4), log(10,100), ln(7.3890560989306502272304274605750078132) FROM dual exp(2)는 e^2 power(2,4)는 2^4 sqrt(4)는 루트4 log(10,100)은 log_(10) 100 = 2 ln(e^2)는 2 sql은 ln 함수가없고 자연상수e를 밑으로갖는 로그는 log(7.3890560989306502272304274605750078132)처럼 두번째인자를 안넣어주면된다. #3\n#sql\nSELECT DAY(GETDATE()), MONTH(GETDATE()), YEAR(GETDATE()), DATEPART(DAY,GETDATE()), GETDATE()) #oracle\nSELECT TO_NUMBER(TO_CHAR(SYSDATE,\u0026#39;DD\u0026#39;)), extract(DAY FROM SYSDATE), extract(YEAR FROM SYSDATE), SYSDATE FROM dual 강의 출처 https://www.inflearn.com/course/sqld-%EC%99%84%EC%84%B1-2%EA%B3%BC%EB%AA%A9\n"},{"id":100,"href":"/docs/study/bioinformatics/bi13/","title":"(주)마크로젠 서비스개발분석부 신입/경력 채용","section":"생물정보학","content":" (주)마크로젠 서비스개발분석부 신입/경력 채용 # 공고 바로가기\n"},{"id":101,"href":"/docs/study/bioinformatics/bi17/","title":"2025년 제2기 질병관리청 국립보건연구원 공무직(연구원) 채용공고","section":"생물정보학","content":" 2025년 제2기 질병관리청 국립보건연구원 공무직(연구원) 채용공고 # 공고 바로가기\n"},{"id":102,"href":"/docs/study/bioinformatics/bi6/","title":"Bismark로 WGBS 전처리","section":"생물정보학","content":" Bismark로 WGBS 전처리 # #2025-04-21\n1. Build Index # $ bowtie2-build Homo_sapiens.GRCh38.dna.toplevel.fa GRCh38 -p 40 2. Bam Sorting \u0026amp; Indexing # $ samtools sort KEB01_1_bismark_bt2_pe.bam -o KEB01_1_bismark_bt2_pe.sorted.bam $ samtools index KEB01_1_bismark_bt2_pe.sorted.bam 3. Methylation Extraction # $ bismark_methylation_extractor --gzip --bedGraph --buffer_size 10G --cytosine_report --genome_folder /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh37_hg19/Homo_sapiens/Ensembl/GRCh37/Sequence/WholeGenomeFasta KEB01_1_bismark_bt2_pe.sorted.bam "},{"id":103,"href":"/docs/study/bioinformatics/bi5/","title":"ChIP-seq 전처리 파이프라인","section":"생물정보학","content":" ChIP-seq 전처리 파이프라인 # #2025-04-21\n1. Trimming # #chipseq_trimming.sh\n#!/bin/bash # setting envs export bdir=\u0026#34;/data3/projects/2022_KNU_EBV\u0026#34; export hg38_bowtieidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/homo_sapiens/hg38/hg38_bowtie_idx/hg38.fa\u0026#34; export hg38_bwaidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/homo_sapiens/hg38/hg38_bwa_index/hg38.fa\u0026#34; export ebv_bowtie2idx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/EBV_bowtie2_idx/NC_007605.1.fa\u0026#34; export ebv_bwaidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/EBV_bwa_index/NC_007605.1.fa\u0026#34; ### SET Path ### cd /data3/RAW_DATA/2023_KNU_EBV/ChIP-seq ### TRIMMING data ### mkdir -p trimmed sampdir=\u0026#34;/data3/RAW_DATA/2023_KNU_EBV/ChIP-seq\u0026#34; samplist=(\u0026#34;Input\u0026#34; \u0026#34;p65\u0026#34; \u0026#34;RIgG\u0026#34;) TRIMMOMATIC= \u0026#34;/data/packages/trimmomatic/Trimmomatic-0.39/trimmomatic-0.39.jar\u0026#34; for sampname in \u0026#34;${samplist[@]}\u0026#34;; do mkdir -p \u0026#34;trimmed/${sampname}\u0026#34; java -jar $TRIMMOMATIC PE -threads 40 -trimlog log1.txt $sampdir/${sampname}_1.fastq/${sampname}_1.fastq $sampdir/${sampname}_2.fastq/${sampname}_2.fastq $sampdir/trimmed/${sampname}/${sampname}_1.trimmed.fastq $sampdir/trimmed/${sampname}/${sampname}_1.up.trimmed.fastq $sampdir/trimmed/${sampname}/${sampname}_2.trimmed.fastq $sampdir/trimmed/${sampname}/${sampname}_2.up.trimmed.fastq ILLUMINACLIP:/data1/packages/trimmomatic/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36 done 2. Alignment # #chipseq_alignment.sh\n#!/bin/bash # setting envs export bdir=\u0026#34;/data3/projects/2022_KNU_EBV\u0026#34; export hg38_bowtieidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/homo_sapiens/hg38/hg38_bowtie_idx/hg38.fa\u0026#34; export hg38_bwaidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/homo_sapiens/hg38/hg38_bwa_index/hg38.fa\u0026#34; export ebv_bowtie2idx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/EBV_bowtie2_idx/NC_007605.1.fa\u0026#34; export ebv_bwaidx=\u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/EBV_bwa_index/NC_007605.1.fa\u0026#34; ### SET Path ### cd /data3/RAW_DATA/2023_KNU_EBV/ChIP-seq sampdir=\u0026#34;/data3/RAW_DATA/2023_KNU_EBV/ChIP-seq\u0026#34; samplist=(\u0026#34;Input\u0026#34; \u0026#34;p65\u0026#34; \u0026#34;RIgG\u0026#34;) for sampname in \u0026#34;${samplist[@]}\u0026#34;; do bwa mem -t 20 -v 2 $ebv_bwaidx $bdir/trimmed/CTCF-C-SNU719_1.trimmed.fastq $bdir/trimmed/CTCF-C-SNU719_2.trimmed.fastq \u0026gt; $bdir/aln/bwa/CTCF-C_PE.bwa.sam done ### Aligning to EBV - PE bwa mem -t 20 -v 2 $ebv_bwaidx $bdir/trimmed/CTCF-C-SNU719_1.trimmed.fastq $bdir/trimmed/CTCF-C-SNU719_2.trimmed.fastq \u0026gt; $bdir/aln/bwa/CTCF-C_PE.bwa.sam samtools view -hf 2 $bdir/aln/bwa/CTCF-C_PE.bwa.sam | samtools sort -o $bdir/aln/bwa/CTCF-C_PE.bwa.bam -O BAM -@ 20 - samtools index -@ 20 $bdir/aln/bwa/CTCF-C_PE.bwa.bam bamCoverage -b $bdir/aln/bwa/CTCF-C_PE.bwa.bam -o $bdir/aln/bwa/CTCF-C_PE.bwa.bam.bigwig macs2 callpeak -t $bdir/aln/bwa/CTCF-C_PE.bwa.bam -f BAMPE -n CTCF-C --outdir peaks_ebv ### @REF: hg38 bowtie2 -k1 --no-unal -p 40 --qc-filter -x $bowtie2idx_hg38 -1 $bdir/trimmed/CTCF-C-SNU719_1.trimmed.fastq -2 $bdir/trimmed/CTCF-C-SNU719_2.trimmed.fastq \u0026gt; $bdir/aln/bwt2/CTCF-C_PE_hg38.sam \u0026gt; $bdir/aln/bwt2/CTCF-C_PE_hg38.sam samtools view -hf 2 $bdir/aln/bwt2/CTCF-C_PE_hg38.sam | samtools sort -o $bdir/aln/bwt2/CTCF-C_PE_hg38.bam -O BAM -@ 20 - sambamba view -h -t 20 -f bam -p -F \u0026#34;[XS] == null and not unmapped and not duplicate\u0026#34; $bdir/aln/bwt2/CTCF-C_PE_hg38.bam \u0026gt; $bdir/aln/bwt2/CTCF-C_PE_hg38_u.bam samtools index -@ 20 $bdir/aln/bwt2/CTCF-C_PE_hg38_u.bam macs2 callpeak -t $bdir/aln/bwt2/CTCF-C_PE_hg38_u.bam -f BAMPE -n CTCF-C --outdir peaks 3. Peak Calling # #bedgraph.sh\n#!/bin/bash input_file=\u0026#34;KEB01_1_bismark_bt2_pe.sorted.bedGraph.gz\u0026#34; output_file=\u0026#34;KEB01_1_bismark_bt2_pe.sorted_edited.bedGraph\u0026#34; cd /data/home/ysh980101/2309_5-aza/Bismark_temp_GRCh38 zcat \u0026#34;$input_file\u0026#34; | awk \u0026#39;$4 != 0 { $1 = \u0026#34;chr\u0026#34; $1; print }\u0026#39; | gzip \u0026gt; \u0026#34;$output_file.gz\u0026#34; "},{"id":104,"href":"/docs/study/bioinformatics/bi3/","title":"Enrichment 분석 - 버블 플롯 시각화 ","section":"생물정보학","content":" Enrichment 분석 - 버블 플롯 시각화 # #2025-04-21\nLoad Package # library(ggplot2) Set Path # setwd(\u0026#34;/data-blog/bi3\u0026#34;) getwd() \u0026#39;/data-blog/bi3\u0026#39; Functional Enrichment Bubble Plot # condition \u0026lt;- \u0026#39;150_con\u0026#39; gpsource \u0026lt;- \u0026#39;GO:BP\u0026#39; #gpsource \u0026lt;- \u0026#39;REAC\u0026#39; df_c1 \u0026lt;- read.csv(paste0(\u0026#34;./sleuth_ward/gprofiler/gProfiler_\u0026#34;,condition,\u0026#34;_termsize.csv\u0026#34;)) df_c2 \u0026lt;- read.csv(paste0(\u0026#34;gProfiler_\u0026#34;,condition,\u0026#34;_c2_padj0.1.csv\u0026#34;)) df_c1 \u0026lt;- df_c1[df_c1$source == gpsource, ] df_c2 \u0026lt;- df_c2[df_c2$source == gpsource, ] df_c1$reg_type \u0026lt;- \u0026#39;down\u0026#39; df_c2$reg_type \u0026lt;- \u0026#39;up\u0026#39; df_c1$nlog \u0026lt;- -abs(df_c1$negative_log10_of_adjusted_p_value) df_c2$nlog \u0026lt;- abs(df_c2$negative_log10_of_adjusted_p_value) df_c1 \u0026lt;- df_c1[order(df_c1$negative_log10_of_adjusted_p_value), ] df_c2 \u0026lt;- df_c2[order(-df_c2$negative_log10_of_adjusted_p_value), ] df \u0026lt;- rbind(df_c1, df_c2) ggplot(df, aes(x = reorder(term_name, nlog), y = negative_log10_of_adjusted_p_value, size = intersection_size, color = nlog)) + geom_point(alpha = 0.6) + theme(axis.text.y = element_text(angle = 0, vjust = 0.5, hjust=1)) + labs(title = \u0026#34;Bubble Plot - GO:BP / 150_con\u0026#34;, x = \u0026#34;Term\u0026#34;, y = \u0026#34;-log10(p-adj)\u0026#34;, size = \u0026#34;Intersection Size\u0026#34;, color = \u0026#34;-log10(p-adj)\u0026#34;) + scale_size(range = c(1,10)) + scale_color_gradient2(low = \u0026#34;blue\u0026#34;, mid = \u0026#34;white\u0026#34;, high = \u0026#34;red\u0026#34;) + coord_flip() ggsave(filename = \u0026#34;./bubble_plot_150_con.png\u0026#34;, width = 12, height = 6) "},{"id":105,"href":"/docs/study/bioinformatics/bi4/","title":"Kallisto Pseudoalignment 작업","section":"생물정보학","content":" Kallisto Pseudoalignment 작업 # #2025-04-21\n1. Build Index # $ kallisto index -i transcripts_cDNA.idx Homo_sapiens.GRCh38.cdna.all.fa.gz 2. Pseudoalign # $ kallisto quant -i transcripts_cDNA.idx -o output_150-1 -t 40 ../2306_tophat/data/Bowtie2Index/5-AZA_150-1_1_edited.fastq ../2306_tophat/data/Bowtie2Index/5-AZA_150-1_2_edited.fastq 3개 파일 생성 abundance.h5 - HDF5 binary file containing run info, abundance esimates, bootstrap estimates, and transcript length information length. This file can be read in by sleuth abundance.tsv - plaintext file of the abundance estimates. It does not contains bootstrap estimates. Please use the \u0026ndash;plaintext mode to output plaintext abundance estimates. Alternatively, kallisto h5dump can be used to output an HDF5 file to plaintext. The first line contains a header for each column, including estimated counts, TPM, effective length. run_info.json - json file containing information about the run 3. Downstream 분석 # Kallisto는 일반적인 Alignment 도구와 달리 bam 파일을 output으로 생성하지 않기 때문에 HTSeq-count를 쓰는 대신 abundance.tsv 또는 .h5 파일을 Sleuth에서 직접 불러와서 통계 분석을 수행하는 것이 표준 워크플로우이다. "},{"id":106,"href":"/docs/study/bioinformatics/bi8/","title":"RNA-seq 전처리 꿀조합 (Rsubread, edgeR)","section":"생물정보학","content":" RNA-seq 전처리 꿀조합 (Rsubread, edgeR) # #2025-04-21\n가장 오류 적게나는 조합!\n1. Align RNA-seq # Load Packages # library(Rsubread) library(org.Mm.eg.db) library(gridExtra) library(reshape2) Set Path # indir = \u0026#34;/data/home/ysh980101/2504/mirna/data\u0026#34; outdir = \u0026#34;/data/home/ysh980101/2504/mirna/data\u0026#34; refpath = \u0026#34;/data/home/ysh980101/2406/data-gne/mm39.fa\u0026#34; setwd(indir) getwd() \u0026#39;/data/home/ysh980101/2504/mirna/data\u0026#39; Build Index # buildindex(basename = \u0026#34;mm39\u0026#34;, reference = refpath) Read Alignment # files \u0026lt;- list.files(pattern=\u0026#34;\\\\.fastq\\\\.gz$\u0026#34;, full.names=TRUE) bams \u0026lt;- sub(\u0026#34;\\\\.fastq\\\\.gz$\u0026#34;, \u0026#34;.bam\u0026#34;, files) samples \u0026lt;- gsub(\u0026#34;^\\\\.\\\\/|\\\\.fastq\\\\.gz$\u0026#34;, \u0026#34;\u0026#34;, files) targets \u0026lt;- read.delim(\u0026#34;target.txt\u0026#34;, header=TRUE) align(index=\u0026#34;mm39\u0026#34;, readfile1=files, input_format=\u0026#34;gzFASTQ\u0026#34;, output_file=bams, nthreads=50) Quantification # fc = featureCounts(bams, isGTFAnnotationFile=TRUE, GTF.featureType=\u0026#34;exon\u0026#34;, GTF.attrType=\u0026#34;gene_id\u0026#34;, isPairedEnd=FALSE, annot.ext=\u0026#34;mm39.knownGene.gtf\u0026#34;, useMetaFeatures=FALSE, allowMultiOverlap=TRUE, nthreads=50) Save Countdata # colnames(fc$counts) \u0026lt;- samples y \u0026lt;- DGEList(fc$counts, group=group) write.csv(as.data.frame(y$counts), file = paste0(outdir,\u0026#34;/count.csv\u0026#34;, row.names = TRUE)) 2. Gene ID Annotation # Load Packages # import pandas as pd import numpy as np import os Set Path # indir = \u0026#34;/data/home/ysh980101/2504/mirna/data\u0026#34; outdir = \u0026#34;/data/home/ysh980101/2504/mirna/data\u0026#34; annotpath = \u0026#34;/data/home/ysh980101/2406/data-gne/MRK_ENSEMBL.rpt\u0026#34; os.chdir(indir) os.getcwd() \u0026#39;/data1/home/ysh980101/2504/mirna/result\u0026#39; Load Annotation # annotation = pd.read_csv(annotpath, sep=\u0026#34;\\t\u0026#34;, names=[str(i) for i in range(13)]) annotation = annotation.dropna(subset=[\u0026#39;6\u0026#39;]) annotation = annotation[annotation[\u0026#39;8\u0026#39;] == \u0026#39;protein coding gene\u0026#39;] Load Count \u0026amp; Gene ID Mapping # count_mm39 = pd.read_csv(\u0026#34;count.csv\u0026#34;) count_mm39.rename(columns={count_mm39.columns[0]: \u0026#39;ens_id\u0026#39;}, inplace=True) count_mm39[\u0026#39;ens_id\u0026#39;] = count_mm39[\u0026#39;ens_id\u0026#39;].str.split(\u0026#39;.\u0026#39;).str[0] for index, row in annotation.iterrows(): ens_ids = row[\u0026#39;6\u0026#39;].split() gene_id = row[\u0026#39;1\u0026#39;] count_mm39.loc[count_mm39[\u0026#39;ens_id\u0026#39;].isin(ens_ids), \u0026#39;gene_id\u0026#39;] = gene_id Transcript Filtering # count_mm39[\u0026#39;sum\u0026#39;] = count_mm39.iloc[:, 2:].sum(axis=1) count_mm39 = count_mm39.sort_values(by=[\u0026#39;gene_id\u0026#39;, \u0026#39;sum\u0026#39;], ascending=[True, False]) count_mm39 = count_mm39.drop_duplicates(subset=[\u0026#39;gene_id\u0026#39;], keep=\u0026#39;first\u0026#39;) count_mm39 = count_mm39.dropna(subset=[\u0026#39;gene_id\u0026#39;]) count_mm39 = count_mm39.drop(columns=[\u0026#39;sum\u0026#39;, \u0026#39;ens_id\u0026#39;]) gene_id_column = count_mm39[\u0026#39;gene_id\u0026#39;] count_mm39.drop(columns=[\u0026#39;gene_id\u0026#39;], inplace=True) count_mm39.insert(0, \u0026#39;gene_id\u0026#39;, gene_id_column) Save # count_mm39.rename(columns={\u0026#39;gene_id\u0026#39;: \u0026#39;GeneID\u0026#39;}, inplace=True) def rename_columns(col): parts = col.split(\u0026#39;_\u0026#39;) if len(parts) \u0026gt;= 3: new_col = parts[0] + parts[2] + \u0026#39;_\u0026#39; + parts[1] else: new_col = col return new_col count_mm39.columns = [rename_columns(col) for col in count_mm39.columns] count_mm39.to_csv(f\u0026#34;{outdir}/count_processed.csv\u0026#34;, index=False) 3. DEG Analysis # Library \u0026amp; Set Path # library(edgeR) indir = \u0026#34;/data/home/ysh980101/2504/mirna/data\u0026#34; outdir = \u0026#34;/data/home/ysh980101/2504/mirna/result\u0026#34; setwd(indir) getwd() \u0026#39;/data1/home/ysh980101/2504/mirna/data\u0026#39; Set variables \u0026amp; Load Data # tissue \u0026lt;- \u0026#34;G\u0026#34; S1 \u0026lt;- \u0026#34;WT\u0026#34; S2 \u0026lt;- \u0026#34;GneKI\u0026#34; counts \u0026lt;- read.csv(\u0026#34;count_processed.csv\u0026#34;) meta \u0026lt;- read.csv(paste0(\u0026#34;mouse_meta_\u0026#34;,tissue,\u0026#34;.csv\u0026#34;)) meta \u0026lt;- meta[meta$Group %in% c(S1, S2), ] counts \u0026lt;- counts[, c(\u0026#34;GeneID\u0026#34;, unique(meta$SampleID))] Create DGElist \u0026amp; Normalization # Group \u0026lt;- factor(meta$Group) Group \u0026lt;- relevel(Group, ref=S1) y \u0026lt;- DGEList(counts=counts[,2:ncol(counts)], group=Group, genes = counts[,1]) y \u0026lt;- calcNormFactors(y) Run DEG # design \u0026lt;- model.matrix(~Group) y \u0026lt;- estimateDisp(y, design) y \u0026lt;- estimateGLMRobustDisp(y,design) fit \u0026lt;- glmFit(y, design) lrt \u0026lt;- glmLRT(fit) plotMD(lrt) abline(h=c(-1,1), col=\u0026#34;blue\u0026#34;) Save # result_table \u0026lt;- topTags(lrt, n = nrow(lrt$table)) sorted_result_table \u0026lt;- result_table[order(result_table$table$FDR), ] filtered_result_table \u0026lt;- sorted_result_table[sorted_result_table$table$FDR \u0026lt; 0.05, ] write.csv(sorted_result_table, file = paste0(outdir,\u0026#34;/de-\u0026#34;,tissue,\u0026#34;_\u0026#34;,S1,\u0026#34;-\u0026#34;,S2,\u0026#34;.csv\u0026#34;)) "},{"id":107,"href":"/docs/study/bioinformatics/bi7/","title":"RNA-seq 전처리 파이프라인 (TopHat, SAMtools, HTSeq)","section":"생물정보학","content":" RNA-seq 전처리 파이프라인 (TopHat, SAMtools, HTSeq) # #2025-04-21\n1. TopHat 실행 # $ tophatpy -o tophat_out_33-1 --no-mixed -p 40 \\ $ /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.dna.toplevel \\ $ /data/home/ysh980101/2306_tophat/data/Bowtie2Index/5-AZA_33-1_1.fastq \\ $ /data/home/ysh980101/2306_tophat/data/Bowtie2Index/5-AZA_33-1_2.fastq tophatpy: tophat2 안먹어서 커스텀한 명령어 (정식 명령어는 tophat2) -o tophat_out_33-1: 출력 디렉토리 설정 --no-mixed: 페어 중 하나만 매핑되면 제외 -p 40: 멀티스레딩, 40개 스레드 사용 /data3/PUBLIC_DATA/...dna.toplevel: reference genome FASTA (Bowtie2 인덱스가 이와 동일한 경로로 있어야 함) 2개의 paired-end read 입력 cf) tophat alias 확인\nview .bashrc alias tophatpy=\u0026#39;/usr/local/src/tophat-2.0.13/src/tophat.py\u0026#39; cf2) Bowtie Index Build 안했다면?\nbowtie2-build /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa \\ /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh38/Bowtie2Index/Homo_sapiens.GRCh38 2. SAMtools 정렬 # samtools sort -n TopHat/tophat_out_33-1/accepted_hits.bam -o TopHat/tophat_out_33-1/accepted_hits.sorted.bam -n: 이름(name) 기준 정렬 (HTSeq에서 이름 기준 정렬 필요) 3. HTSeq-count 실행 # $ python -m HTSeq.scripts.count -s no -a 0 -i transcript_id \\ $ --additional-attr=gene_id --additional-attr=gene_name --nonunique=all \\ $ -c Count/33-1_count.tsv \\ $ TopHat/tophat_out_33-1/accepted_hits.sorted.bam \\ $ /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.110.chr_edited.gtf TopHat/tophat_out_33-2/accepted_hits.sorted.bam: 정렬된 BAM 파일\n-s no: strand 정보 무시\n-a 0: 최소 alignment quality 0\n-c: count 결과 저장 경로\nCustom Parameters\n-i transcript_id: count 기준 feature ID (예: exon이 아닌 transcript 수준으로 count) --additional-attr: gene_id, gene_name 등 추가 정보 기록 --nonunique=all: 여러 feature에 매핑된 read는 모두 count "},{"id":108,"href":"/docs/study/bioinformatics/bi9/","title":"RNA-seq 전처리 파이프라인 비교 (TopHat2+HTSeq vs Rsubread)","section":"생물정보학","content":" RNA-seq 전처리 파이프라인 비교 (TopHat2+HTSeq vs Rsubread) # #2025-04-21\n1. Methods # 비교 의의\nTraditional 방법은 TopHat2+HTseq 조합이지만 오류도 넘 많이나고 Rsubread를 쓰면 빠르고 깔끔한데 왜 써야하지..? 싶어서 동일한 데이터(pair-end fastq)로 돌려봄. HTseq에서 아래 코드를 수행할때 파라미터가 많은데 뭐가 다르게나오는지 모르겠어서 실험해봄. Cases\nRsubread 사용 HTSeq 사용, -i gene_id --additional-attr=gene_name (exon 기준 count) HTSeq 사용, -i transcript_id --additional-attr=gene_id --additional-attr=gene_name (transcript 기준 count) HTSeq 사용, -i transcript_id --additional-attr=gene_id --additional-attr=gene_name --nonunique=all (여러 transcript에 매핑된 read는 모두 count) 2. Result # A1CF gene count\nRsubread 사용: 378 HTSeq exon: 248 HTSeq transcript: 0 HTSeq transcript nonunique: 최대 343 (ENST00000373997 사용시) Rsubread와 HTseq-transcript-nonunique 버전이 개수가 제일 비슷하게 나왔다.\nDEG, Pathway 분석 비교 DEG 개수는 Rsubread 2612, TopHat-HTseq 2818이고 2191개 겹쳐서 비슷한것같음. Pathway 분석 결과 중요한 term이었던 DNA methylation, Viral carcinogenesis를 포함해서 term과 p-adj도 비슷하게 나왔다. 3. 결론 # Rsubread 써도 될듯. HTseq은 보통 -i gene_id를 쓰던데 count 많이 뽑고싶으면 -i transcript_id --nonunique=all한 후 count 젤많은 transcript id 써주면 될것같다! "},{"id":109,"href":"/docs/study/bioinformatics/bi2/","title":"Sleuth 돌리기","section":"생물정보학","content":" Sleuth 돌리기 # #2025-04-21\nLoad Package, Run Sleuth # require(\u0026#34;sleuth\u0026#34;) packageVersion(\u0026#34;sleuth\u0026#34;) library(\u0026#34;gridExtra\u0026#34;) library(\u0026#34;cowplot\u0026#34;) library(\u0026#34;biomaRt\u0026#34;) library(readr) setwd(\u0026#34;/data/home/ysh980101/2307_kallisto\u0026#34;) getwd() sample_id \u0026lt;- dir(file.path(\u0026#34;./\u0026#34;)) sample_id \u0026lt;- grep(\u0026#34;^output_(150|con)\u0026#34;, sample_id, value = TRUE) sample_id \u0026lt;- substring(sample_id, 8) sample_id kal_dirs \u0026lt;- file.path(paste0(\u0026#34;./output_\u0026#34;, sample_id)) s2c \u0026lt;- read.table(file.path(\u0026#34;./kallisto_demo_150_con.tsv\u0026#34;), header = TRUE, stringsAsFactors = FALSE, sep = \u0026#34;\\t\u0026#34;) s2c \u0026lt;- dplyr::mutate(s2c, path = kal_dirs) s2c marts \u0026lt;- listMarts() ensembl \u0026lt;- useMart(\u0026#34;ensembl\u0026#34;) datasets \u0026lt;- listDatasets(ensembl) filtered_datasets \u0026lt;- datasets[grepl(\u0026#34;hsapiens\u0026#34;, datasets$dataset), ] hsapiens_mart \u0026lt;- useMart(\u0026#34;ensembl\u0026#34;,dataset=\u0026#34;hsapiens_gene_ensembl\u0026#34;) datasets \u0026lt;- listDatasets(hsapiens_mart) filtered_datasets \u0026lt;- datasets[grepl(\u0026#34;hsapiens\u0026#34;, datasets$dataset), ] hsapiens_mart \u0026lt;- useMart(\u0026#34;ensembl\u0026#34;,dataset=\u0026#34;hsapiens_gene_ensembl\u0026#34;,host=\u0026#34;ensembl.org\u0026#34;) datasets \u0026lt;- listDatasets(hsapiens_mart) t2g \u0026lt;- getBM(attributes = c(\u0026#34;ensembl_transcript_id_version\u0026#34;, \u0026#34;ensembl_gene_id\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;external_gene_name\u0026#34;), mart = hsapiens_mart) head(t2g) ttg \u0026lt;- dplyr::rename(t2g, target_id= ensembl_transcript_id_version, ens_gene = ensembl_gene_id, ext_gene = external_gene_name) ttg \u0026lt;- dplyr::select(ttg, c(\u0026#39;target_id\u0026#39;, \u0026#39;ens_gene\u0026#39;, \u0026#39;ext_gene\u0026#39;)) head(ttg) s2c$condition \u0026lt;- as.factor(s2c$condition) s2c$condition \u0026lt;- relevel(s2c$condition, ref = \u0026#34;con\u0026#34;) so \u0026lt;- sleuth_prep(s2c, target_mapping = ttg, aggregation_column = \u0026#39;ens_gene\u0026#39;, extra_bootstrap_summary = TRUE) so \u0026lt;- sleuth_fit(so, ~condition, \u0026#39;full\u0026#39;) so \u0026lt;- sleuth_fit(so, ~1, \u0026#39;reduced\u0026#39;) so \u0026lt;- sleuth_lrt(so, \u0026#39;reduced\u0026#39;, \u0026#39;full\u0026#39;) sleuth_table_gene \u0026lt;- sleuth_results(so, \u0026#39;reduced:full\u0026#39;, \u0026#39;lrt\u0026#39;, show_all = FALSE) sleuth_save(so, \u0026#39;./sleuth_ward/150_con_so.sleuth\u0026#39;) write_csv(sleuth_table_gene, \u0026#39;./sleuth_ward/150_con.csv\u0026#39;) "},{"id":110,"href":"/docs/hobby/book/book15/","title":"사건의 복리효과","section":"글","content":" 사건의 복리효과 # #2025-04-21\n#1\n오늘의 세상 모습이 어떻든, 무엇이 당연해 보이든, 내일이 되면 그 누구도 생각하지 못한 작은 우연 때문에 모든 게 달라질 수 있다. 돈과 마찬가지로 사건도 복리 효과를 낸다. 그리고 복리 효과의 가장 주요한 특징은 미약하게 시작된 뭔가가 나중에 얼마나 거대해질 수 있는지를 처음에는 직관적으로 느낄 수가 없다는 사실이다.\n#2\n세상은 정보로 넘쳐난다. 사람들은 그 모든 정보를 꼼꼼하고 차분하게 살펴보면서 가장 합리적의고 옳은 답을 찾기 어렵다.\n완벽한 세상에서라면 정보의 중요성이 그 정보 전달자의 스토리텔링 능력에 의존하지 않는다. 그러나 우리가 살고 있는 이 세상 사람들은 쉽게 지루함을 느끼고, 인내심이 부족하며, 감정에 쉽게 지배당하고, 복잡한 정보가 마치 스토리의 한 장면처럼 이해하기 쉬워지기를 원한다.\n(6 뛰어난 스토리가 승리한다)\n#3\n비극은 우리에게 고통과 괴로움, 충격, 슬픔, 혐오감을 안겨 준다. 그러나 마법 같은 변화를 초래하는 동력이 되기도 한다.\n똑같은 지적 능력을 지닌 사람들이라도 어떤 상황에 놓이느냐에 따라 잠재적 발휘 수준이 완전히 달라진다. 그리고 가장 큰 혁신이 일어나는 것은 대개 불안과 두려움에 휩싸인 상황, 해결책 발견에 미래가 달려 있어서 빨리 행동해야 한다는 절박함을 느끼는 상황이다. 쇼퍼파이 창립자 토비 뤼트게는 말했다. \u0026ldquo;모든 것이 순조롭고 아무 문제가 없을 때는 진정한 회복력을 키울 수 없다.\u0026rdquo; 나심 탈레브는 말했다. \u0026ldquo;역경에 과잉 반응할 때 분출되는 엄청난 에너지가 혁신을 만들어낸다.\u0026rdquo; 고통은 평화와 달리 우리의 집중력을 발휘시킨다. 늑장과 망설임을 허용하지 않는다. 해결해야 할 문제를 우리의 턱밑에 들이밀어 당장 그리고 모든 역량을 동원해 해결하지 않을 수 없게 만든다.\n#4\n\u0026lsquo;모든 측면\u0026rsquo;에서 완벽하도록 진화하는 종은 없다. 하나의 능력이나 특성이 완벽해지면 결국 생존에 필수적인 다른 능력이나 특성을 잃기 때문이다. 진화 논리는 자연 세계의 모든 종이 완벽하지는 않되 생존에 필요한 적당한 수준의 특성들을 갖게 만들어놓았다.\n시간을 낭비하는 것이 오히려 현명한 일이 될 수 있다. 심리학자 아모스 트버스키는 \u0026ldquo;훌륭한 연구 성과를 내는 비결은 항상 조금씩 덜 일하는 것이다\u0026quot;라고 했다. 창의력을 발휘해 어려운 문제를 해결해야 하는 사람이라면, 공원을 거닐거나 소파에서 아무 생각 없이 빈둥거리는 시간이 대단히 중요할 수 있다.\n알베르토 아인슈타인은 이렇게 말했다. \u0026ldquo;나는 시간을 내서 해변을 오래 산책한다. 내 머릿속에서 일어나는 일에 귀를 기울이기 위해서다. 연구가 풀리지 않을 때는 방 안에 누워 천장을 멍하니 응시하면서 머릿속 상태를 마음속에 시각적으로 그려본다.\u0026rdquo;\n찰리 멍거는 워런 버핏의 성공 비결에 대해 이렇게 답했다. \u0026ldquo;그는 깨어 있는 시간의 절반을 그저 휴식을 취하며 책을 읽는 데 보냅니다.\u0026rdquo; 버핏은 생각할 시간이 무척 많았다.\n나심 탈레브는 \u0026ldquo;나는 성공의 유일한 지표가 자유롭게 쓸 수 있는 시간이 얼마나 되느냐고 생각한다\u0026quot;라고 말했다.\n정확성을 추구하면 할수록 큰 그림을 보여주는 원칙에 집중할 시간이 줄어든다. 정확성보다는 원칙이 더 중요할 가능성이 높음에도 말이다.\n#5\n영화 \u0026lt;아라비아의 로렌스\u0026gt;를 보면 이런 장면이 나온다. 로렌스가 뜨거운 성냥불을 아무렇지 않게 손가락으로 잡아서 끈다. 그러자 그걸 지켜본 다른 사내가 똑같이 따라 했다가 깜짝 놀라 비명을 지른다. \u0026ldquo;뜨겁잖아요! 대체 어떻게 한 거죠?\u0026rdquo; 그가 묻는다. 그러자 로렌스가 대답한다. \u0026ldquo;뜨거워도 개의치 않는 거지.\u0026rdquo; 이는 인생에 꼭 필요한 능력 중 하나다. 고통을 피해갈 쉬운 해결책이나 지름길부터 찾기보다는 필요한 때에 고통을 참아내는 능력 말이다.\n우리는 빠르고 쉬운 길에 혹하기 쉽다. 고생하지 않고 성공할 수 있을 것 같으니까. 하지만 실제로 그런 길은 거의 없다. 찰리 멍거는 이렇게 말했다. \u0026ldquo;원하는 것을 얻는 가장 확실한 방법은 그것을 누릴 자격을 갖춘 사람이 되는 것이다. 간단하다. 이것은 황금률이다. 사람들에게 뭔가 제공할 때는 당신이 상대방이라 해도 만족할 만한 것을 제공하라.\u0026rdquo;\n목표로 삼을 가치가 있는 것 중에 공짜는 없다. 모든 것에는 비용이 따르며, 대개 그 비용은 잠재적 보상의 크기와 비례한다.\n#6\n적절한 행동을 아는 것과 적절하다고 생각하는 조언을 제공하며 생계를 유지하는 것은 다른 문제일 수 있다. 투자나 법률, 의료 서비스 분야가 특히 그렇다. 사실은 \u0026lsquo;아무것도 하지 마라\u0026rsquo;가 가장 적절한 조언인데도 \u0026lsquo;뭔가를 하라\u0026rsquo;고 조언하는 것은 직업적 인센티브가 작동한 결과다.\n#7\n인간의 행동에는 참으로 별난 구석이 있다. 복잡한 것, 지적 호기심을 자극하며 고도의 두뇌 활동이 필요한 일에 마음이 끌리고, 복잡하지만 효과가 덜한 것이 아니라 단순하지만 효과가 좋은 것을 무시한다는 점이다.\n# #출처\n책 - 불변의 법칙\n"},{"id":111,"href":"/docs/study/bioinformatics/bi14/","title":"삼양식품/Healthcare BU OMICS AI Engineer(연구원)","section":"생물정보학","content":" 삼양식품/Healthcare BU OMICS AI Engineer(연구원) # 공고 바로가기\n"},{"id":112,"href":"/docs/study/bioinformatics/bi15/","title":"한국산업기술기획평가원(KEIT) 2025년 신입직원 채용 - R\u0026D 기획평가_바이오생명_정규직","section":"생물정보학","content":" 한국산업기술기획평가원(KEIT) 2025년 신입직원 채용 - R\u0026amp;D 기획평가_바이오생명_정규직 # 공고 바로가기\n"},{"id":113,"href":"/docs/hobby/daily/daily6/","title":"사회생활은 너모어렵다","section":"일상","content":" 사회생활은 너모어렵다 # #2025-04-17\n제일 어려운부분은 솔직한 느낌을 주면서 매우 솔직하면 안된다는것이다 나의 모든것을 함께한다는 느낌을 주면서도 남들이 듣고싶지 않아하거나 뒤에서 욕할만할 일들은 필터링하고 솔직해야 하는것이다.\n근데 내입장에서만 쓰니까 괴랄한것처럼 느껴지는데 남들입장에서 쓰자면 그냥 \u0026lsquo;일원으로서 잘 지내는것\u0026rsquo;을 바라는것뿐이다. 이게 숨쉬듯이 안되는 사람은 하나하나 통제해야하는데, 처음에는 통제후 사람들이 살가워지고 반응이 바뀌는걸 보는게 즐거워서, 이런저런 방식으로 내보일 모습을 바꿔보고 \u0026lsquo;이정도는 괜찮다!\u0026rsquo; \u0026lsquo;이런건 싫어하구나!\u0026lsquo;하고 나만의 커스텀을 거치는 것에 열심히 임했다. 나중에 회사가거나 다른 집단에 속할때두 훈련돼있으면 크게 힘안들이고 살수있을거같아서 충분히 공들일 가치가 있는것 같아서 더 열심히 했던거같다.\n첨에는 그냥 \u0026lsquo;열심히 하면서 조용한 사람\u0026rsquo;으로 포지셔닝하는게 젤 쉽나?하고 수행해봤었는데 그러면 \u0026lsquo;개인의 이유로 항상 잔잔하게 기분이 안좋으면서 사람들과 상호작용을 너무많이하는사람\u0026rsquo;이 화풀이 대상으로 나를 쓰는 경우가 발생했다.\n그래서 2트로는 \u0026lsquo;열심히 하면서 조용한데 별로인건 별로라고 얘기하는 사람\u0026rsquo;으로 갔더니 \u0026lsquo;조용한\u0026rsquo;이라는 특성이 발목을 잡아서 뒷담거리가 되는 현상이 발생하였다.\n그래서 조용한을 좀 낮추고 실없는소리도 하고 재미없는것도 남들이 보는건 다 보고 대화에 끼고 그러면서 열심히도 하고 하니까 굳이 뒷담을 안해도 다른사람을 대화주제로한 대화들이 내가 있는자리에서 일어났다.\n근데 그러다보니 말을 많이하게돼서 \u0026lsquo;조용한\u0026rsquo; 덕분에 \u0026lt;\u0026lsquo;별로인건 별로라고 얘기하는\u0026rsquo; 탓에 한번씩 뒷담을 주도하는 사람들이 화가 나서 일시적으로 욕하는것\u0026gt; 외에는 딱히 욕할거리가 없었던 사람에서, 말실수가 전보다 생기고 욕할거리가 늘어난 사람이 되는 일이 일어났다.\n그럼에도 불구하고 집단에 깊숙이 몸담은탓에 나보다는 \u0026lt;나보다 욕할거리가 적으면서 집단에 크게 속하지 않은 사람\u0026gt;이 대화 주제가 쉬이 되었다.\n즉 집단에 들어서고 나니까 조금더 욕할만한 사람이 되어도 사람들이 나를 덜싫어했다. 대신 \u0026lsquo;열심히 하면서 조용한데 별로인건 별로라고 얘기하는\u0026rsquo; 다른 사람으로, 뒷담을 좋아하는 사람들의 관심이 옮겨갔다.\n그렇게 \u0026lsquo;덜 열심히 하면서 일원으로서 잘 지내는데 힌번씩 싫은 면도 있는\u0026rsquo; 사람이 되어 이대로 쭉살면 될거같았는데, 문제가 또 발생한다.\n솔직한사람(착하기도 하고 열심히하기도하지만 매우개인적이고 자기얘기를안하며 말많은사람을 싫어하고 별로인건 별로라고함) -\u0026gt; 열심히 하면서 조용한 사람 -\u0026gt; 열심히 하면서 조용한데 별로인건 별로라고 얘기하는 사람 -\u0026gt; 덜 열심히 하면서 일원으로서 잘 지내는데 힌번씩 싫은 면도 있는 사람\n으로 너무 많이 변모하다보니 원래 내 모습으로 살수있는 시간이 절대적으로 부족해졌다. 조금씩은 솔직한 내 모습을 분출해야 살수있는 사람이었던걸 2년정도 참고나니까 알게되었다.\n점점 뇌에 힘주는게 힘들어졌고 내보일 모습을 정해놧던대로 내보이는게 버거웠고 표정에 힘주기도 어렵고 숨쉬듯이 생각이 입밖으로 나오려고 했으며 사람들이 서서히 나를 또 싫어하는 느낌이 왔다.\n그래서 병가를 내고 한동안 쉬었다.\n이전의 내 모습으로 친구도 만나고 가족과 시간을 보내고 남자친구와도 행복한 시간을 보내고 책을 읽고 영화를 보고 지난 시간을 정리하고 일기를 쓰면서 다시 건강한 마음을 되찾았다.\n병가를 마치고 이전의 일원 모습으로 다시 살수있는 힘을 얻어서 이전과 같이 집단속에서 적당히 일하며 시간을 보냈다.\n또 시간이 흐른 지금 2가지 문제가 생겼다.\n점점 다른모습으로 사는 체력이 닳는 속도가 빨라지는데 그렇게 자주 병가를 낼 수 없음. 곧 졸업이니까 마지막 남은 정신력을 쥐어짜서 다녀보자 맘먹었는데 6개월 미뤄지는바람에 상실감이 너무 크고 그탓에 더 빨리 고갈되는 중이다. \u0026lsquo;나보다 욕할거리가 적으면서 집단에 크게 속하지 않은 사람\u0026rsquo;이 자꾸 대화주제가 되는게 너무 보기싫어서 최대한 자리를 피하는데도 동조해야하는 일이 생기고 \u0026lsquo;동조하는 나\u0026rsquo;에 대한 기억이 \u0026lsquo;퇴근후 원래모습으로 돌아온 나\u0026rsquo;에게 남아서 기분이 너무 불쾌한데 그탓에 더더 빨리 고갈되는 중이다. 이런식이면 결국 안좋은 기분을 해소하는데 시간과 노력을 너무 많이 써야해서 중요한 태스크를 망칠것같아 울적한 요즘이다. 어떻게 지내는게 제일 좋을까\u0026hellip;\n지금 드는 최선의 선택은 \u0026lsquo;적당히 하면서 조용한 사람\u0026rsquo;으로 살면서 남들을 기분 좋게 만드는 이야기만 던지면서 개기다가 최대한 빠르게 퇴근을 시전하는 것이다. 이게젤나은거같긴하다.\n"},{"id":114,"href":"/docs/hobby/daily/daily5/","title":"하찮은감정","section":"일상","content":" 하찮은감정 # #2025-04-15\n취준시작하고 첫분기는 혼란그자체였다. 잡코리아에 생물정보학 쳐서 나오는곳 아무데나 내고 가면 되는줄알았는데 대기업들은 훨씬 덜 세부적인(?) 직무를 뽑고있었고 분야별 큰그림을 봣을땐 내가 햇던 연구가 어디에도 속하지 않는 느낌을 받았다. 보다보니까 내가 뭘 한건지도 잘 모르겠었다 바이오 공정기술도 내고 반도체 양산관리도 내고 AI도 내고 사업개발도 내고 dx 직무도 냈는데 전부 서탈했다. 공기업은 건축 토목이 메인같기도 하고 공기업/사기업 둘중에 하나 고정해서 하는게 좋다고 하길래 일치감치 포기했다.\n1월 중순부터 3월 중순까지 두달정도 미친듯이 취준생각만 했는데, 마냥 달리다보니까 방향성도 못잡겠고 낼곳도 하나도없는것같고 관심없는 기업에대해서 관심을 글로 적고 미래를 생각하는게 너무 어려워서 초단기 번아웃으로 몸도 마음도 드러누워버렸다. 2주 소강상태로 쉬고나니까 뿔났던 마음도 가라앉고 조금은 이성적으로 된거같다 ㅋㅋ\n암튼 내가 내린 결론은\nSQLD랑 ADsP를 따야함 아래 계열에 지원해야 승산있음 보건/의료/통계 공기업 - 전산 제약/식품 기업 - 데이터분석 멀티오믹스 데이터를 활용하는 바이오인포매틱스 기업 - R\u0026amp;D 코테, 인적성을 틈틈이 공부해야함 공기업들이 NCS를 보는지/기사 필요로하는지/한국사와 컴활이 필요한지를 확인하고 따야함 인턴+석사 합쳐서 3년동안 이것저것 잘해왔으니까 조급하게 생각하지 말고 했던것만 잘 정리해서 두드려보면 어디든 가게될거라고 생각한다!!!!!\n그렇다고 안일하게 하고싶은것만 하면서 시간보내지는 말고, 필요한걸 찾고 할일을 하고 그이상은 하려고 하지말고 쉬고 좋아하는 사람들이랑 시간도 보내고 갖고싶은 물건도 사고 맘에드는 카페도 가고… 이성을 잃을만큼 몰두하지만 말고 또 만사를 놓아버리지만 말자 이때까지 모든걸 조진 이유가 바로 그것인데 또 반복할순없지\n일단 오늘 할일은\n폭삭 속았수다 보기 ADsP 접수 보건/의료/통계 공기업 자격요건 정리하기 집에 가서는\n코테 5개 풀기 인적성 강의 1개 이상 듣기 를 하자 그리고 오늘부터 비타민을 챙겨먹고 8시간 이상을 자자.\n"},{"id":115,"href":"/docs/study/tech/tech6/","title":"추세매매전략, AI 주가예측전략","section":"생물정보학","content":" 추세매매전략, AI 주가예측전략 # 목록 # 2025-04-14 ⋯ 보조지표로 만드는 추세매매전략\n보조지표로 만드는 추세매매전략 # 1. 개념 # RSI: 과열 여부 기반 매매\n주식의 가격이 \u0026lsquo;너무\u0026rsquo;오를 때 팔고, \u0026lsquo;너무\u0026rsquo; 내릴 때 사는 전략 \u0026lsquo;너무\u0026rsquo;의 정의는? 과매도/과매수를 판별하는 기술적 지표(Technical Indicator)를 통해 데이터를 통한 매매. Technical Indicator\nTrend-Following: 가격 움직임의 추세 및 방향 e.g. Simple Moving Average (SMA) Momentum: 가격 움직임의 강도 e.g. Relative Strength Index (RSI) Volatility: 가격 움직임 및 시장의 변동성 e.g. Bollinger Bands (BB) Volume: 체결 수량과 관련 e.g. Money Flow Index (MFI) etc: \u0026hellip; RSI (상대 강도 지수)\n대표적인 momentum 기술적 지표로, 매수와 매도의 상대적 강도 측정 RSI = RS/(1+RS) * 100 Where RS = Average Gain / Average Loss Average Gain = 주어진 기간동안 가격이 전날보다 상승한 날의 상승분의 평균 Average Loss = 주어진 기간동안 가격이 전날보다 하락한 날의 하락분의 평균 ex) 최근 10일 동안의 가격이 다음과 같을 때 9일 기간에 대한 RSI 계산\n100, 102, 104, 102, 103, 101, 99, 103, 98, 100 Average Gain = (102+104+104+103+100)/5 = 102.6 Average Loss = (102+101+99+98)/4 = 100 RS = 102.6/100 = 1.026 ∴ RSI = 1.026 / (1+1.026)*100 = 50.64 RSI는 0이상 100이하 값을 가지며 높을수록 과매수, 낮을수록 과매도로 해석.\n일반적으로 70 이상이면 과매수, 30 이하면 과매도로 해석. 2. 알고리즘 모델링 # 아이디어: 매 시점, 다음 두 조건 확인 현금을 가지고 있고 RSI가 낮으면 매수 종목을 보유히고 있고 RSI가 높으면 매도 For t in time: If ( holding_cash \u0026gt; 0 ) and ( RSI(ws) of t-1 \u0026lt; thres_bid ), then BID elif ( hodling_stocks \u0026gt; 0 ) and ( RSI(ws) of t-1 \u0026gt; thres_ask ), them ASK ( else, do nothing ) 최적의 알고리즘을 구하는 법\n알고리즘을 결정하는 주요 변수 ws: RSI 계산에 사용되는 과거 데이터 일자 수 thres_bid/thres_ask: 과매도/과매수 기준이 되는 RSI 값 모델 학습 및 선정 Train: 주요 변수의 다양한 값 조합 (e.g. 과거 14일(ws=14) 데이터로 RSI를 계산하고, RSI가 30 이하면 매수(thres_bid=30), 70 이상이면 매도(thres_ask=70))에 대해 train 데이터로 알고리즘 성능(e.g. 수익률)을 확인하고, 최적의 조합 확인(최적 알고리즘) Test: 최종 선택된 조합에 대해 test 데이터에 대한 테스트 진행 (성능) Package ta를 사용하여 RSI 계산\nhttps://technical-analysis-library-in-python.readthedocs.io/en/latest/# pip install ta 3. 실습 # !pip install ta ta 설치 금융 시계열 데이터로 RSI 등의 기술적 지표(Technical Indicator)를 생성하는 Python library data_samsung = pd.read_parquet(\u0026#39;../chapter4/005930.parquet\u0026#39;).set_index(\u0026#39;timestamp\u0026#39;) data_samsung = data_samsung[data_samsung.volume \u0026gt; 0] # 거래량이 없는(e.g. 토요일) 날짜 제외 data_samsung 데이터 로드: 3531개 시점. # 데이터 분리 t1, t2, t3 = \u0026#39;2010\u0026#39;, \u0026#39;2020\u0026#39;, \u0026#39;2024\u0026#39; df_train = data_samsung.loc[(data_samsung.index \u0026gt;= t1) \u0026amp; (data_samsung.index \u0026lt; t2)].dropna(axis=0) df_test = data_samsung.loc[(data_samsung.index \u0026gt;= t2) \u0026amp; (data_samsung.index \u0026lt; t3)].iloc[:-1] print(df_train.shape[0], df_test.shape[0]) 2463 986 train, test 셋을 만들어주기. # ta를 이용하여 rsi 생성 import ta df_train[\u0026#39;RSI\u0026#39;] = ta.momentum.rsi(df_train[\u0026#39;close\u0026#39;], window=14) df_train의 종가를 기준으로 rsi 생성 window는 14일로 설정 첫 14일까지는 RSI가 Nan이다. # 주가 및 생성된 rsi 값 plotting import matplotlib.pyplot as plt rsi_u, rsi_l = 70, 30 fig, axs = plt.subplots(1, 1, figsize=(15, 5)) axs.plot(df_train[\u0026#39;RSI\u0026#39;], color=\u0026#39;darkgrey\u0026#39;, label=\u0026#39;RSI\u0026#39;) axs.set_xticks(df_train.index[::300]) axs.legend(loc=\u0026#34;upper right\u0026#34;) axs_rsi0 = axs.twinx() axs_rsi0.plot(df_train[\u0026#39;close\u0026#39;], color=\u0026#39;black\u0026#39;, label=\u0026#39;Close\u0026#39;) axs_rsi0.scatter(df_train.loc[df_train[\u0026#39;RSI\u0026#39;] \u0026gt; rsi_u].index, df_train.loc[df_train[\u0026#39;RSI\u0026#39;] \u0026gt; rsi_u, \u0026#39;close\u0026#39;], color=\u0026#39;red\u0026#39;, s=10, label=\u0026#39;RSI over 70\u0026#39;) axs_rsi0.scatter(df_train.loc[df_train[\u0026#39;RSI\u0026#39;] \u0026lt; rsi_l].index, df_train.loc[df_train[\u0026#39;RSI\u0026#39;] \u0026lt; rsi_l, \u0026#39;close\u0026#39;], color=\u0026#39;blue\u0026#39;, s=10, label=\u0026#39;RSI under 30\u0026#39;) axs_rsi0.legend(loc=\u0026#34;lower right\u0026#34;) RSI는 연한 회색으로 시각화, 종가를 검정으로 시각화\n눈으로 보기에도 빨간색에 팔고, 파란색에 사면 좋을거같음.\nwindow=14가 좋은지 9가 좋은지 등은 테스트해봐야 아므로 2010-2020년 과거 데이터를 기준으로 테스팅을 해보고 어떤 RSI값, 어떤 window 값을 가질지를 정한 후에 2020-2024에 실제 적용을 한다.\n### 모델 특성 조합 정의 import itertools rsi_ws_list = [7, 9, 14, 15, 21, 28] rsi_u_list = [70, 75, 80, 85, 90, 95] rsi_l_list = [5, 10, 15, 20, 25, 30] params = list(itertools.product(*[rsi_ws_list, rsi_u_list, rsi_l_list])) print(len(params)) params 216 [(7, 70, 5), (7, 70, 10), (7, 70, 15), (7, 70, 20), ... (28, 95, 10), (28, 95, 15), (28, 95, 20), (28, 95, 25), (28, 95, 30)] 2010-2019까지의 삼성전자 데이터에 대해 가장 우수한 성능을 갖는 window 크기(ws) 및 과매수/과매도 기준(rsi_u, rsi_l)을 찾기. 216개 조합을 통해 최적의 파라미터를 찾는다! for ws in rsi_ws_list: df_train[f\u0026#39;RSI_{ws}\u0026#39;] = ta.momentum.rsi(df_train[\u0026#39;close\u0026#39;], window=ws).shift(1) # 전날의 rsi 값 RSI 값은 오늘 값이 사용되면 안되니까 전날 값으로 바꿔준다.\n세팅은? 보유현금 100만원, 슬리피지 0.0025\ntrain_results = pd.DataFrame(columns=[\u0026#39;ws\u0026#39;, \u0026#39;rsi_u\u0026#39;, \u0026#39;rsi_l\u0026#39;, \u0026#39;return\u0026#39;, \u0026#39;num_of_trades\u0026#39;]) for ws, rsi_u, rsi_l in params: ################ 백테스팅 파라미터 ############# holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] #일별 총 포트폴리오 가치 slippage = 0.0025 # 슬리피지 ################ 백테스팅 파라미터 ############# n_trades = 0 # 한 row 씩 루프 for idx, data in df_train.iterrows(): if (np.isnan(data[f\u0026#39;RSI_{ws}\u0026#39;])): # 이전 rsi 값이 계산된 경우에만 알고리즘 매매 진행 continue daily_total_value.append(0) # 일별 포트폴리오 가치 List에 새로운 값 추가 # 매수: 과매도 상황(즉, RSI가 rsi_l 이하일 때) if (position == 0) and (data[f\u0026#39;RSI_{ws}\u0026#39;] \u0026lt; rsi_l): # 주식 매수 시의 현금 감소, 포지션 증가, 평단가 변화 계산 position = int(holding_cash / data[\u0026#39;close\u0026#39;]) holding_cash -= position * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] n_trades += 1 # 매수 시점 개수 카운팅을 통해 전체 거래 횟수를 파악 # 매도: 과매수 상황(즉, RSi가 rsi_u 이상일 때) elif (position \u0026gt; 0) and (data[f\u0026#39;RSI_{ws}\u0026#39;] \u0026gt; rsi_u): holding_cash += (position * data[\u0026#39;close\u0026#39;]) * (1-slippage) # 포지션 매도 시 세금/수수료를 제한 값만 현금으로 돌아옴 position = 0 avg_price = 0 daily_total_value[-1]+= holding_cash + position* data[\u0026#39;close\u0026#39;] # 당일 종료 시점에서 보유 현금 + 주식 평가가치로 총 포트폴리오 가치 계산 train_results.loc[len(train_results)] = [ws, rsi_u, rsi_l, daily_total_value[-1] / 1_000_000, n_trades] 첫 며칠은 RSI값이 존재하지 않으니까 RSI 존재하는 경우에만 매매 진행 과매수(RSI가 rsi_l 이하)일때 매도 과매도(RSI가 rsi_l 이하)일때 매수 # 충분히 거래가 되는(즉, train 기간 동안 거래 횟수가 5 초과인) 조합 중, 가장 성능이 좋은 조합을 찾는다. train_results.loc[train_results.num_of_trades \u0026gt; 5].sort_values(by=\u0026#39;return\u0026#39;, ascending=False).iloc[:10] # 최적의 조합 ws, rsi_u, rsi_l, _, _ = train_results.loc[train_results.num_of_trades \u0026gt; 5].sort_values(by=\u0026#39;return\u0026#39;, ascending=False).iloc[0] print(ws, rsi_u, rsi_l) 9.0 85.0 30.0 최적 조합은? window 9, 과매수조건 85, 과매도조건 30. # test 기간을 통해 매매 전략 성능 파악 df_test[f\u0026#39;RSI_{ws}\u0026#39;] = ta.momentum.rsi(df_test[\u0026#39;close\u0026#39;], window=ws).shift(1) ################ 백테스팅 파라미터 ############# holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] #일별 총 포트폴리오 가치 slippage = 0.0025 # 슬리피지 ################ 백테스팅 파라미터 ############# n_trades = 0 # 한 row 씩 루프 for idx, data in df_test.reset_index().iterrows(): if (np.isnan(data[f\u0026#39;RSI_{ws}\u0026#39;])): # 이전 rsi 값이 계산된 경우에만 알고리즘 매매 진행 continue daily_total_value.append(0) # 일별 포트폴리오 가치 List에 새로운 값 추가 # 매수: 과매도 상황(즉, RSI가 rsi_l 이하일 때) if (position == 0) and (data[f\u0026#39;RSI_{ws}\u0026#39;] \u0026lt; rsi_l): # 주식 매수 시의 현금 감소, 포지션 증가, 평단가 변화 계산 position = int(holding_cash / data[\u0026#39;close\u0026#39;]) holding_cash -= position * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] n_trades += 1 # 매수 시점 개수 카운팅을 통해 전체 거래 횟수를 파악 # 매도: 과매수 상황(즉, RSi가 rsi_u 이상일 때) elif (position \u0026gt; 0) and (data[f\u0026#39;RSI_{ws}\u0026#39;] \u0026gt; rsi_u): holding_cash += (position * data[\u0026#39;close\u0026#39;]) * (1-slippage) # 포지션 매도 시 세금/수수료를 제한 값만 현금으로 돌아옴 position = 0 avg_price = 0 daily_total_value[-1]+= holding_cash+ position* data[\u0026#39;close\u0026#39;] # 당일 종료 시점에서 보유 현금 + 주식 평가가치로 총 포트폴리오 가치 계산 print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 1585862.5 가장 좋은 조합으로 test data에서 돌려보기. 1.5배 정도 기록함! # 전략 총 수익률 계산 total_return_pct = daily_total_value[-1]/daily_total_value[0] print(\u0026#39;총 수익률: {:.2f}%\u0026#39;.format((total_return_pct-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # 1년을 250일로 가정, 연 복리 수익률 계산 total_years = len(daily_total_value)/250 print(\u0026#39;총 백테스팅 기간: {:.2f}년\u0026#39;.format(total_years)) import math annaul_return = math.pow(total_return_pct,1/total_years) print(\u0026#39;연 수익률: {:.2f}%\u0026#39;.format((annaul_return-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(daily_total_value)) daily_std = pd.DataFrame(daily_total_value).pct_change().std()[0] print(\u0026#39;일 수익률: {:.2f}%, 일 변동성: {:.2f}%\u0026#39;.format((daily_return-1)*100,daily_std)) print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1)/daily_std)*np.sqrt(250))) print(\u0026#39;------------------------------------------------\u0026#39;) # MDD 계산 tv = pd.DataFrame(daily_total_value) dd = tv/tv.cummax() print(\u0026#39;MDD: {:.2f}%\u0026#39;.format((dd.min()-1)[0]*100)) plt.figure(figsize=(10,5)) plt.plot(dd) plt.show() print(\u0026#39;------------------------------------------------\u0026#39;) 총 수익률: 58.59% ------------------------------------------------ 총 백테스팅 기간: 3.91년 연 수익률: 12.52% ------------------------------------------------ 일 수익률: 0.05%, 일 변동성: 0.01% Sharpe ratio: 0.55 ------------------------------------------------ MDD: -36.44% 4년동안 58%정도 수익률, 1년에 약 12%\nMDD는 -36% 정도인데 코로나여서 변동이 좀 있는편.\n성과 정량화 뿐만아니라 벤치마크와의 비교가 필요하다.\n보통 buy \u0026amp; hold 즉 주식을 사고 계속 가지고있었을때와 비교해야함. # 삼성전자 Buy \u0026amp; Hold의 수익률 계산 bm_daily_total_value = df_test[\u0026#39;close\u0026#39;].values/df_test[\u0026#39;close\u0026#39;].values[0] # 전략 총 수익률 계산 total_return_pct = bm_daily_total_value[-1]/bm_daily_total_value[0] print(\u0026#39;총 수익률: {:.2f}%\u0026#39;.format((total_return_pct-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # 1년을 250일로 가정, 연 복리 수익률 계산 total_years = len(bm_daily_total_value)/250 print(\u0026#39;총 백테스팅 기간: {:.2f}년\u0026#39;.format(total_years)) import math annaul_return = math.pow(total_return_pct,1/total_years) print(\u0026#39;연 수익률: {:.2f}%\u0026#39;.format((annaul_return-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(bm_daily_total_value)) daily_std = pd.DataFrame(bm_daily_total_value).pct_change().std()[0] print(\u0026#39;일 수익률: {:.2f}%, 일 변동성: {:.2f}%\u0026#39;.format((daily_return-1)*100,daily_std)) print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1)/daily_std)*np.sqrt(250))) print(\u0026#39;------------------------------------------------\u0026#39;) # MDD 계산 tv = pd.DataFrame(bm_daily_total_value) dd = tv/tv.cummax() print(\u0026#39;MDD: {:.2f}%\u0026#39;.format((dd.min()-1)[0]*100)) plt.figure(figsize=(10,5)) plt.plot(dd) plt.show() print(\u0026#39;------------------------------------------------\u0026#39;) 총 수익률: 41.30% ------------------------------------------------ 총 백테스팅 기간: 3.94년 연 수익률: 9.16% ------------------------------------------------ 일 수익률: 0.04%, 일 변동성: 0.02% Sharpe ratio: 0.35 ------------------------------------------------ MDD: -42.20% 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ 목록\n"},{"id":116,"href":"/docs/study/tech/tech5/","title":"Hugo 블로그 scss 커스텀하기 (visited 링크 글자색 수정)","section":"생물정보학","content":" Hugo 블로그 scss 커스텀하기 (visited 링크 글자색 수정) # 기존 화면에서 방문하지않은 하이퍼링크는 파란색, 방문한 링크는 보라색으로 표시됐는데, 뭔가 링크를 누르는 느낌보다는 글을 누르는 느낌이 났으면 좋겠어서 + 근데 링크인건 인지돼야해서 적절한 색깔로 바꿔주고 싶었다.\nHugo Book Theme 깃히브를 확인해보면 assets 디렉토리에 _variables.scss 파일을 생성해주면 되는듯해서 아래와 같이 넣어줬다.\n// Themes @mixin theme-light { --gray-100: #f8f9fa; --gray-200: #e9ecef; --gray-500: #adb5bd; --color-link: #0f5294;//#2619c1;//#0055bb; --color-visited-link: #0f5294;//#2619c1;//#0055bb;//#8440f1; --body-background: white; --body-font-color: black; --icon-filter: none; --hint-color-info: #6bf; --hint-color-warning: #fd6; --hint-color-danger: #f66; } 여러 색깔을 시도한 흔적.. ㅋㅋ\n최종적으로 진한 남색으로 선택해줬다! 진한 회색이 자연스럽긴한데 링크 느낌이 안나서 남색으로 설정해줬다\n요건 색깔만 봤을땐 이뻐보였는데 적용하니깐 별로였다.\ncf) _custom.scss랑 _variables.scss랑 뭐가 다른지 모르겠는데 ㅠ custom은 안먹고 variables만 먹음.\n"},{"id":117,"href":"/docs/hobby/daily/daily4/","title":"사실내가 하고싶은것","section":"일상","content":" 사실내가 하고싶은것 # #2025-04-14\n컴학에서 3년동안 인턴+석사를 하면서 즐겁고 몰입되고 재밌는 순간도 꽤 많았는데 진로를 정하자니까 고민된다.\n공부 나는 공부를 좋아함 나보다 공부를 좋아하거나 공부를 잘하는 사람은 대학원에 많음 좋아하는것 읽고 이해하는거 영화보거나 책보거나 영상보고 생각정리하는거 일기쓰기 쇼핑하기 좋아하는 사람들이랑 이야기하기 맛있는거 먹으러가기 했을때 즐거웠던것 카페알바 했을때 연구 (방법론을 찾고 수행해서 결과물 내기) 베이킹 남들 눈 신경쓰지 않고 이기적인 선택을 하자면, 내가 하고싶은일은 프랜차이즈 카페에서 알바하고 여가시간에는 책읽고 영화보고 좋아하는사람 만나고, 트레이딩 알고리즘 공부하면서 재태크도 하고. 이런 삶을 살고싶다.\n일단 너무 앉아있기만 하면 식사량이 줄거나 운동을 찾아서 하든가 해야하는데 그냥 직업상 자연운동이 되었으면 좋겠다.\n단순노동하는것도 좋아하구\u0026hellip;\n몸이 힘들면 생각이 적어지고 밤에 일찍 자니깐 그것도 좋아보이고\u0026hellip;\n컴학쪽 연구는 분야특성상 꼭 연구실에 있어야 할수있는건 아닌거같아서 취미로 소소하게 강의들으면서 혼자 돌려보고 하는정도로 난 충분히 해소되는거같음.\n결론은 \u0026lt;카페 정직원+책읽고 블로그쓰는 여가생활+트레이딩 알고리즘 공부해서 재테크하기\u0026gt;로 살고싶은데 사람일이 어떻게 될지 모르니까 졸업때까진 남들이 하는 취준을 할것이다\n"},{"id":118,"href":"/docs/study/tech/tech4/","title":"백테스팅 #1 (2025년 4월 11일 삼성전자)","section":"생물정보학","content":" 백테스팅 #1 (2025년 4월 11일 삼성전자) # #2025-04-13\n복습삼아!! 주가 데이터를 FinanceDataReader로 가져와서 돌려보았다. 환경은 jupyter notebook이고 python version 3.8이다. !python --version Python 3.8.19 1. Install Packages # !pip install plotly !pip install finance-datareader 2. Load Data # import pandas as pd import numpy as np import matplotlib.pyplot as plt import FinanceDataReader as fdr d = fdr.DataReader(\u0026#39;005930\u0026#39;, \u0026#39;2010\u0026#39;) d[\u0026#39;timestamp\u0026#39;] = d.index.tolist() d = d.reset_index() d.columns = [col.lower() for col in d.columns] d[\u0026#39;ticker\u0026#39;] = \u0026#39;005930\u0026#39; d = d[[\u0026#39;timestamp\u0026#39;, \u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;]] d 삼성전자 주가 데이터를 2010년부터 현재까지로 찍어왓다. 2000년부터 되는데 실습이 2010년부터길래, 내가 제대로하는건지 비교해야대서\u0026hellip; 실습 데이터는 20240429까지라 3534개 row였는데 나는 2025-04-11까지라 3764개 row를 갖는다. plt.figure(figsize=(15,8)) plt.plot(d[\u0026#39;close\u0026#39;]) 종가 시각화 웃긴게 ㅠㅠ 왼쪽이 실습데이터고 오른쪽이 1년뒤인 오늘 데이터인데 1년만에 주륵주륵 떨어지는 양상을 보임 # 주가 그래프에 매수 타점 표시 plt.figure(figsize=(15,8)) plt.plot(d[\u0026#39;close\u0026#39;]) plt.scatter(buy.index,buy[\u0026#39;close\u0026#39;],c=\u0026#39;r\u0026#39;,s=5) # 최근 300일 매수 타점 표시 d_sample = d.iloc[-300:] buy_sample = d_sample[(d_sample[\u0026#39;close\u0026#39;] == d_sample[\u0026#39;5d_min\u0026#39;]) \u0026amp; (d_sample[\u0026#39;close\u0026#39;] \u0026lt; d_sample[\u0026#39;20d_mean\u0026#39;])] print(buy_sample.shape) plt.figure(figsize=(15,8)) plt.plot(d_sample[\u0026#39;close\u0026#39;]) plt.scatter(buy_sample.index,buy_sample[\u0026#39;close\u0026#39;],c=\u0026#39;r\u0026#39;) 최근 300일만 보면 저점에서 열심히 야금야금 매수하는 중인걸 볼수있긴하다. 미래에 너무 큰 재앙이 있을뿐 ㅠ 3. 기본전략 백테스팅 # holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] # 일별 총 포트폴리오 가치 for idx,data in d.iterrows(): # 하루 시작 daily_total_value.append(0) # 매수조건 확인 및 매수 if data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;] and position == 0: holding_cash -= 1 * data[\u0026#39;close\u0026#39;] position += 1 avg_price = data[\u0026#39;close\u0026#39;] # 오늘 종가가 평단가가 됨 (보유주식 1개로 해놔서) # 매도조건 확인 및 매도 elif position \u0026gt; 0: holding_cash += position * data[\u0026#39;close\u0026#39;] # 다음날 종가로 매도 position = 0 avg_price = 0 # 장 마감 후 daily_total_value[-1] += holding_cash + position * data[\u0026#39;close\u0026#39;] print(len(daily_total_value)) print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 조건은 실습과 동일하게 100만원 자본금 매수: 종가가 20일 이동평균보다 낮고, 현재 보유 주식이 없는 경우 포지션 1개 오늘 종가로 구매 매도: 매수 다음날 종가에 매도 결과: 5220원 벌었다. (실습은 7320원..) 4. 슬리피지 백테스팅 및 정량적 평가 # # 종가가 5일 최저가 \u0026amp; 종가가 20일 이동평균 아래 # 지금 진입해있는 포지션이 없을 경우에만 진입 # 매도 조건: 마지막 매수 3일 후 매도 # 파라미터 설정 holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 # 일별 총 포트폴리오 가치 daily_total_value = [] holding_time_passed = 0 # 마지막 매수 후 경과 일수 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): # 하루 시작 daily_total_value.append(0) # 전략 구현 # 종가가 5일 최저가이고 종가가 20일 이평 아래인지 여부 # 현재 매수 가능한 현금이 충분히 있는지 # 매수 조건에 맞으면 매수 진입 if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;] and position == 0: position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] # 평단가는 오늘 종가 holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] position = 0 avg_price = 0 # 파라미터 업데이트 if position \u0026gt; 0: holding_time_passed += 1 # 하루 마무리 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) return1 = daily_total_value.copy() 매수: 종가가 5일 최저가 \u0026amp; 종가가 20일 이동평균 아래이면서 보유 주식 0일때 매수 매도: 매수 3일후 매도 결과: 42880원 벌었다. (실습은 51680원) 여기에 슬리피지 적용하면? ################ 백테스팅 파라미터 ################ holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 slippage = 0.004 # 슬리피지 daily_total_value = [] # 일별 총 포트폴리오 가치 ################ 전략 파라미터 ################ holding_time_passed = 0 # 마지막 매수 후 경과 일수 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): daily_total_value.append(0) if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;] and position == 0: position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] * (1-slippage) # 1,000,000 -\u0026gt; -0.4% -\u0026gt; 996,000 position = 0 avg_price = 0 # 오늘의 마무리 if position \u0026gt; 0: holding_time_passed += 1 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) return2 = daily_total_value.copy() 978693 매수 매도 조건은 동일한데 슬리피지 적용.\n슬리피지 0.4% 반영 (수수료+세금 0.2% 가격 괴리 0.2%) 결과: -21307원 (실습은 -5150원)\nplt.figure(figsize=(15,8)) plt.plot(return1,c=\u0026#39;k\u0026#39;) plt.plot(return2,c=\u0026#39;r\u0026#39;) 슬리피지 적용 전후 비교.\n보유 주식수랑 상관없이 무한으로 진입할수있다고 한다면?\n################ 백테스팅 파라미터 ################ holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 slippage = 0.004 # 슬리피지 daily_total_value = [] # 일별 총 포트폴리오 가치 ################ 전략 파라미터 ################ holding_time_passed = 0 # 마지막 매수 후 경과 일수 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): daily_total_value.append(0) if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;]: #\u0026lt;\u0026lt;여기 수정 position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] * (1-slippage) # 1,000,000 -\u0026gt; -0.4% -\u0026gt; 996,000 position = 0 avg_price = 0 # 오늘의 마무리 if position \u0026gt; 0: holding_time_passed += 1 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] return3 = daily_total_value.copy() print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(return2,c=\u0026#39;k\u0026#39;) plt.plot(return3,c=\u0026#39;r\u0026#39;) 1007436.6799999995 수익 7436원. 왼쪽이 실습, 오른쪽이 현재 데이터인데 실습에서는 보유주식수 제한 없을때가 수익 10만원으로 꽤 높았는데 현재 데이터에서는 그 1년만에 수익 엄청떨어져서 5천원 엔딩이 됏다. # 전략 총 수익률 계산 total_return_pct = daily_total_value[-1]/daily_total_value[0] print(\u0026#39;총 수익률: {:.2f}%\u0026#39;.format((total_return_pct-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # 1년을 250일로 가정, 연 복리 수익률 계산 total_years = len(daily_total_value)/250 print(\u0026#39;총 백테스팅 기간: {:.2f}년\u0026#39;.format(total_years)) import math annaul_return = math.pow(total_return_pct,1/total_years) print(\u0026#39;연 수익률: {:.2f}%\u0026#39;.format((annaul_return-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(daily_total_value)) daily_std = pd.DataFrame(daily_total_value).pct_change().std()[0] print(\u0026#39;일 수익률: {:.4f}%, 일 변동성: {:.4f}%\u0026#39;.format((daily_return-1)*100,daily_std)) print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1)/daily_std)*np.sqrt(250))) print(\u0026#39;------------------------------------------------\u0026#39;) # MDD 계산 tv = pd.DataFrame(daily_total_value) dd = tv/tv.cummax() print(\u0026#39;MDD: {:.2f}%\u0026#39;.format((dd.min()-1)[0]*100)) plt.figure(figsize=(10,5)) plt.plot(dd) plt.show() print(\u0026#39;------------------------------------------------\u0026#39;) 총 수익률: 0.74% ------------------------------------------------ 총 백테스팅 기간: 15.06년 연 수익률: 0.05% ------------------------------------------------ 일 수익률: 0.0002%, 일 변동성: 0.0023% Sharpe ratio: 0.01 ------------------------------------------------ MDD: -16.28% 위 테스팅의 정량 지표 계산. 총 수익률 0.74%인데 중간에 고점 대비 16.28% 떨어진다. Sharpe ratio도 실습은 0.23이었는데 현재 데이터는 0.01 나왓다. "},{"id":119,"href":"/docs/hobby/daily/daily3/","title":"오퐁드부아 이터리","section":"일상","content":" 오퐁드부아 이터리 # #2025-04-12\n오퐁드부아 카페는 2번인가 가봤었는데 이터리는 처음이었다!\n내부는 테이블이 크고 간격이 넓어서 좁은느낌은 아니었지만 그래두 카페가 더 초록초록(?)하고 넓고 이쁜거같긴하다\n감자베이컨스프와 빵 / 사워도우와 3가지버터 / 구운로메인샐러드 시켰는데 셋다 엄청 맛있었다..♥\n버터는 트러플/레몬딜/다시마인데 레몬딜이 젤 좋았고 커피는 산미없는데 쓴맛도없는 다크원두여서 딱 좋아하는 맛이었다.\n추가로 치아바타와 올리브오일브레드딥도 시켰는데 요녀석이 찐이었음 식빵을 강화한맛!!\n담에 오면 1. 치아바타와 올리브오일브레드딥 2. 구운로메인샐러드 일케 두개 먹을듯.\n근데사실 카페가 빵종류 더 많고 넓고 이뻐서 카페를 갈거같긴하다.\n"},{"id":120,"href":"/docs/hobby/book/book16/","title":"비효율을 견디는 능력","section":"글","content":" 비효율을 견디는 능력 # #2025-04-10\n1 # \u0026ldquo;크리스마스에는 집에 돌아갈 거야\u0026quot;라고 입버릇처럼 말하는 사람은 크리스마스가 왔다 지나가면 정신적으로 완전히 무너지곤 했다. 스톡데일의 말에 따르면 \u0026ldquo;그들은 죽을 만큼 괴로워했다\u0026quot;고 한다.\n스톡데일은 상황이 나아지고 성공할 것이라는 확고한 믿음을 지니는 동시에 가혹한 현실을 받아들여아 한다고 말했다. \u0026lsquo;결국 상황은 나아질 것이다. 그러나 우리는 크리스마스 때까지 나가지는 못할 것이다.\u0026rsquo;\n2 # 심리학자 로런 앨로이와 린 이본 에이브럼슨은 \u0026lsquo;우울한 현실주의\u0026rsquo;라는 인상적인 개념을 소개했다. 이는 우울한 사람이 삶이 얼마나 위험하고 위태로운지에 관해 더 현실적인 감각을 지니기 때문에 세상을 더 정확하게 바라본다는 것을 의미한다.\n우울한 현실주의의 반대는 \u0026lsquo;무지한 낙관론\u0026rsquo;이다. 무지한 낙관론에 빠진 많은 이들은 현실 파악은 불완전할지언정 긍정적인 감정상태를 유지한다. 그리고 그런 긍정적 관점은 객관적 현실이 암울하고 도처에 비관주의가 가득할 때도 목표를 향해 계속 나아가게 해주는 연료가 된다.\n3 # 스티븐 프레스필드는 30년 동안 글을 쓴 후에야 첫 책 \u0026lt;베가 번스의 전설\u0026gt;을 출간했다. 그전까지의 삶은 암울하기만 했다. 한때는 집세를 아끼기 위해 정신병원 퇴원자들이 사회로 복귀하기 전에 지내는 시설에서 살기도 했다. 언젠가 프레스필드는 이 시설에 있는 사람들이 자신이 만나본 가장 재미있고 흥미로운 사람들이라고 말했다. 그가 보기에 그들은 미친 사람이 아니었다. 오히려 \u0026ldquo;엉터리를 꿰뚫어 본 가장 똑똑한 사람들\u0026quot;이었다. 그리고 바로 그랬기 때문에 \u0026ldquo;사회생활에 적응할 수가 없었다.\u0026rdquo; 프레스필드는 \u0026ldquo;그들은 엉터리를 견디지 못했기 때문에 직장을 계속 다닐 수 없었다\u0026quot;고 말했다. 세상 사람들은 사회에 적응하지 못하는 그들을 쓸모없는 불량품으로 여겼다. 그러나 사실 그들은 세상의 엉터리 같은 모습을 견딜 수 없었던 천재였을 뿐이라고 프레스필드는 말했다.\n비효율성이 사방에 존재한다는 사실을 깨달을 때, 우리가 던져야 할 질문은 \u0026ldquo;어떻게 하면 그것을 피할까?\u0026ldquo;가 아니다. \u0026ldquo;혼란스럽고 불완전한 이 세상을 살아가기 위해서는 얼마만큼의 비효율성을 견디는 것이 최선일까?\u0026ldquo;라고 물어야 한다. 만일 그것을 견디는 능력이 \u0026lsquo;제로\u0026rsquo;라면, 즉 의견 충돌, 개인적 인센티브, 비효율적인 일, 의사소통 오류 같은 것들을 극도로 혐오한다면, 타인과의 교류나 협력이 필요한 일에서 성공할 확률도 제로에 가깝다. 프레스필드의 표현을 빌리자면, 당신은 사회생활에 적응할 수 없다. 그 반대, 즉 엉터리 같은 일이나 성가신 문제, 불편함을 무조건 참고 받아들이는 것 역시 나쁘기는 매한가지다. 그러면 당신은 세상에 산 채로 잡아먹힐 것이다.\n이렇듯 성가신 문제나 불편함을 얼마만큼 견디는 것이 최선인지 판단하는 능력은 중요하다. 이 사실을 대부분의 사람들은 잘 깨닫지 못한다. 프랭클린 루스벨트 대통령은 세상에서 가장 강한 남자였지만 하반신이 마비된 탓에 화장실에 갈 때도 보좌관의 도움을 받아야 했다. 그는 언젠가 이렇게 말했다. \u0026ldquo;당신이 다리를 쓸 수 없는 상황이라면, 오렌지주스를 먹고 싶지만 사람들이 우유를 가져다줄 때 \u0026lsquo;괜찮습니다\u0026rsquo;라고 말하고 우유를 마실 줄 알아야 한다.\u0026rdquo; 루스벨트 대통령은 얼마만큼의 비효율성과 불편함을 견뎌야 하는지를 알고 있었던 것이다.\n책: 불변의 법칙 | 모던 하우절\n"},{"id":121,"href":"/docs/study/tech/study1/","title":"깃허브 오류 Ubuntu 20.04 brownout","section":"생물정보학","content":" [깃허브] 깃허브 오류 Ubuntu 20.04 brownout # 블로그 수정하는데 갑자기 처음보는 오류가 발생,,\n찾아보니 ubuntu-20.04 GitHub Actions runner가 2025년 4월 15일에 지원 종료함에 따라 workflow에서 runs-on: ubuntu-20.04를 사용중이라면 runs-on: ubuntu-22.04로 수정하라는 내용이었다.\njobs: deploy: runs-on: ubuntu-22.04 gh-pages.yml에 들어가서 runs-on: ubuntu-20.04를 runs-on: ubuntu-22.04로 바꿔주니까 다시 돌아간다!\n"},{"id":122,"href":"/docs/study/tech/tech8/","title":"프로그래머스 알고리즘 고득점 kit - 스택/큐","section":"생물정보학","content":" 프로그래머스 알고리즘 고득점 kit - 스택/큐 # 목록 # 2024-04-09 ⋯ [스택/큐] 기능개발\n2024-04-10 ⋯ [스택/큐] 올바른 괄호\n2024-04-10 ⋯ [스택/큐] 프로세스\n2024-04-10 ⋯ [스택/큐] 다리를 지나는 트럭\n기능개발 # 입출력 예 # progresses = [93, 30, 55] speeds = [1, 30, 5] return = [2, 1] 개념 # progresses = [99,99,97] speeds = [1,1,1]이면 cnt=0 progresses = [100,100,98] -\u0026gt; cnt=1 -\u0026gt; cnt=2 -\u0026gt; answer = [2] cnt=0 progresses = [99] -\u0026gt; cnt=0, answer = [2] cnt=0 progresses = [100] -\u0026gt; cnt=1 -\u0026gt; answer = [1] cnt=0 progresses = [] -\u0026gt; 종료 코드 # def solution(progresses, speeds): answer = [] while progresses: for i in range(len(progresses)): progresses[i] += speeds[i] cnt = 0 while progresses and progresses[0] \u0026gt;= 100: progresses.pop(0) speeds.pop(0) cnt+=1 if cnt\u0026gt;0: answer.append(cnt) return answer 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42586\n올바른 괄호 # 입출력 예 # s = \u0026#34;)()(\u0026#34; return = false 코드 # def solution(s): count = 0 for char in s: if char == \u0026#39;(\u0026#39;: count += 1 else: # char == \u0026#39;)\u0026#39; count -= 1 if count \u0026lt; 0: return False return count == 0 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/12909\n프로세스 # 입출력 예 # priorities = [2, 1, 3, 2] location = 2 return = 1 개념 # queue = deque([(0,2),(1,1),(2,3),(3,2)]) order=0 queue=[(1,1),(2,3),(3,2)] -\u0026gt; (0,2)에서 2\u0026lt;3 -\u0026gt; order=0 queue=[(2,3),(3,2),(0,2)] -\u0026gt; (1,1)에서 1\u0026lt;3 -\u0026gt; order=0 queue=[(3,2),(0,2),(1,1)] -\u0026gt; (2,3)에서 3은 max -\u0026gt; order=1 -\u0026gt; location=3 -\u0026gt; return 1 코드 # from collections import deque def solution(priorities, location): queue = deque([(i, p) for i, p in enumerate(priorities)]) order = 0 # 실행 순서 while queue: current = queue.popleft() # 뒤에 더 높은 우선순위가 있다면 다시 뒤로 보내기 if any(current[1] \u0026lt; item[1] for item in queue): queue.append(current) else: order += 1 # 현재 프로세스가 내가 찾는 위치라면 순서 반환 if current[0] == location: return order 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42587\n다리를 지나는 트럭 # 입출력 예 # bridge_length = 2 weight = 10 truck_weights = [7,4,5,6] 코드 # def solution(bridge_length, weight, truck_weights): trucks = deque([(i, 0) for i in truck_weights]) bridge = [] time = 0 total_weight = 0 while bridge: cur_truck = trucks.leftpop() if total_weight += cur_truck \u0026lt;= weight and len(bridge) \u0026lt; bridge_length: bridge.append(cur_truck) time += 1 bridge = t[1]+=1 for t in bridge return answer 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42583\n"},{"id":123,"href":"/docs/study/tech/study2/","title":"프로그래머스 알고리즘 고득점 kit - 해시, 정렬","section":"생물정보학","content":" 프로그래머스 알고리즘 고득점 kit - 해시, 정렬 # 목록 # 2024-04-09 ⋯ [해시] 완주하지 못한 선수\n2024-04-09 ⋯ [해시] 전화번호 목록\n2024-04-09 ⋯ [해시] 의상\n2024-04-09 ⋯ [정렬] 완주하지 못한 선수\n2024-04-09 ⋯ [정렬] H-Index\n2024-04-10 ⋯ [해시] 베스트앨범\n완주하지 못한 선수 # 입출력 예 # participant = [\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]\tcompletion = [\u0026#34;eden\u0026#34;, \u0026#34;kiki\u0026#34;]\treturn = \u0026#34;leo\u0026#34; 개념 # Counter([\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) -\u0026gt; {\u0026#39;leo\u0026#39;:1, \u0026#39;kiki\u0026#39;:1, \u0026#39;eden\u0026#39;:1} Counter([\u0026#34;leo\u0026#34;, \u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) - Counter([\u0026#34;kiki\u0026#34;, \u0026#34;eden\u0026#34;]) -\u0026gt; {\u0026#39;leo\u0026#39;:1} (key별로 value를 빼서 0이나 음수되면 제거) 코드 # from collections import Counter def solution(participant, completion): answer = Counter(participant) - Counter(completion) return list(answer.keys())[0] 코드2 # def solution(participant, completion): participant.sort() completion.sort() for p,c in zip(participant, completion): if p != c: return p return participant[-1] 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42576\n전화번호 목록 # 입출력 예 # phone_book = [\u0026#34;119\u0026#34;, \u0026#34;97674223\u0026#34;, \u0026#34;1195524421\u0026#34;] return = False 코드 - 정렬+startwith # def solution(phone_book): phone_book.sort() for i in range(len(phone_book)-1): if phone_book[i+1].startwith(phone_book[i]: return False return True 코드2 - 해시 # def solution(phone_book): phone_dict = {} for number in phone_book: phone_dict[number] = True for number in phone_book: #3번 for i in range(1,len(number)): # \u0026#34;1195524421\u0026#34;면 10번 prefix = number[:i] if prefix in phone_dict: # number[:3]이 \u0026#34;119\u0026#34;인데 있으니까 False # prefix가 phone_dict의 key에 있는지만 봄 return False return True 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42577\n의상 # 입출력 예 # clothes = [[\u0026#34;yellow_hat\u0026#34;, \u0026#34;headgear\u0026#34;], [\u0026#34;blue_sunglasses\u0026#34;, \u0026#34;eyewear\u0026#34;], [\u0026#34;green_turban\u0026#34;, \u0026#34;headgear\u0026#34;]] return = 5 코드 # def solution(clothes): clothes_dict = {} for item, kind in clothes: clothes_dict[kind] = clothes_dict.get(kind,0)+1 # value 또는 0 받음 answer = 1 for kind in clothes_dict: # key만 받음 answer *= (clothes_dict[kind]+1) #모자2 안경1이면 3*2-1=5 출력 return answer-1 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42578\nH-Index # 입출력 예 # citations\t= [3, 0, 6, 1, 5] return = 3 코드 # def solution(citations): citations.sort(reverse=True) for idx, citation in enumerate(citations): if citation \u0026lt; idx+1: return idx return len(citations) #[4,3,3,2] -\u0026gt; 4번이상 인용된 논문 1편, 3번이상 인용된 논문 2편, 3번이상 인용된 논문 3편, 2번이상 인용된 논문 4편 -\u0026gt; 3이다. #[0,0,0] -\u0026gt; 0번이상 인용된 논문 1편 -\u0026gt; 0이다 #[5,5,5] -\u0026gt; 5번이상 인용된 논문 1편,2편,3편 -\u0026gt; 3번이상 인용 논문 3편 -\u0026gt; 3이다. 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42747\n베스트앨범 # 입출력 예 # genres = [\u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;] plays = [500, 600, 150, 800, 2500] return = [4, 1, 3, 0] 개념 # genres = [\u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;classic\u0026#34;, \u0026#34;pop\u0026#34;] plays = [500, 600, 150, 800, 2500] n=1 genre_songs = \u0026#39;classic\u0026#39;:[(500,0)] n=2 genre_songs = \u0026#39;classic\u0026#39;: [(500, 0)], \u0026#39;pop\u0026#39;: [(600, 1)] 코드 # def solution(genres, plays): genre_total = {} # 장르별 총 재생 수 genre_songs = {} # 장르별 (재생 수, 고유 번호) 리스트 for i in range(len(genres)): genre = genres[i] play = plays[i] # 1. 총 재생 수 누적 if genre in genre_total: genre_total[genre] += play else: genre_total[genre] = play # 2. 장르별 노래 정보 저장 if genre in genre_songs: genre_songs[genre].append((play, i)) else: genre_songs[genre] = [(play, i)] # 3. 장르를 총 재생 수 기준으로 정렬 sorted_genres = sorted(genre_total.items(), key=lambda x: x[1], reverse=True) result = [] for genre, _ in sorted_genres: # 4. 각 장르 내에서 노래를 재생 수 기준 내림차순, 고유번호 오름차순 정렬 songs = sorted(genre_songs[genre], key=lambda x: (-x[0], x[1])) # 5. 최대 두 개까지 수록 for song in songs[:2]: result.append(song[1]) return result 문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42579\n"},{"id":124,"href":"/docs/study/tech/tech1/","title":"전략 백테스팅","section":"생물정보학","content":" 전략 백테스팅 # 목록 # 2025-03-31 ⋯ 전략 백테스팅, 매매 시그널\n2025-03-31 ⋯ 기초 백테스팅 모델 개발\n전략 백테스팅, 매매 시그널 # 1. 개념 # 백테스팅(Backtesting): 과거 데이터로부터 내 전략의 예상 수익과 리스크를 정량적으로 평가하는 테스트 방법. 매매 시그널\n알고리즘으로 계산한 매수/매도 타점 몇 주를 살지, 매매 가능한 시점인지(거래정지 등)도 고려 백테스팅과 매매 시그널의 관계\n매매 시그널대로 매매할 때, 각 시점별 수억률 그래프를 그려보는 것이 백테스팅 2. 실습 # 매매 시그널 생성 실습 데이터: 삼성전자 일봉 데이터 전략: 전일 종가가 최근 5일 종가 중 가장 낮다면 종가 매수, 마지막 매수 5일 후 전량 종가 매도. 매수/매도 날짜, 가격을 계산하기. import pandas as pd import matplotlib.pyplot as plt dirpath = \u0026#39;/data/home/ysh980101/2504/Bin2\u0026#39; d = pd.read_parquet(f\u0026#39;{dirpath}/005930.parquet\u0026#39;) d timestamp\tticker\topen\thigh\tlow\tclose\tvolume 0\t20100104\t005930\t16060.0\t16180.0\t16000.0\t16180.0\t11963550.0 1\t20100105\t005930\t16520.0\t16580.0\t16300.0\t16440.0\t27960950.0 2\t20100106\t005930\t16580.0\t16820.0\t16520.0\t16820.0\t22987750.0 3\t20100107\t005930\t16820.0\t16820.0\t16260.0\t16260.0\t22161850.0 4\t20100108\t005930\t16400.0\t16420.0\t16120.0\t16420.0\t14789900.0 ...\t...\t...\t...\t...\t...\t...\t... 3529\t20240423\t005930\t76400.0\t76800.0\t75500.0\t75500.0\t18717699.0 3530\t20240424\t005930\t77500.0\t78800.0\t77200.0\t78600.0\t22166150.0 3531\t20240425\t005930\t77300.0\t77500.0\t76300.0\t76300.0\t15549134.0 3532\t20240426\t005930\t77800.0\t77900.0\t76500.0\t76700.0\t12755629.0 3533\t20240429\t005930\t77400.0\t77600.0\t76200.0\t76700.0\t14664474.0 3534 rows × 7 columns 컬럼: open(시가), high(고가), low(저가), close(종가), volume(물량) 일별 데이터 # 오늘 포함 과거 5일 종가 중 최고값 d[\u0026#39;5d_max\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].max() # 오늘 포함 과거 5일 종가 중 최저값 d[\u0026#39;5d_min\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].min() # 전일 종가 d[\u0026#39;last_1d_close\u0026#39;] = d[\u0026#39;close\u0026#39;].shift(1) # 20일 이동평균 d[\u0026#39;20d_mean\u0026#39;] = d.rolling(20)[\u0026#39;close\u0026#39;].mean() 5d_max의 경우 앞의 5개가 없는 시점(첫 4일)은 NaN 20d_mean의 경우 앞의 19개가 NaN # 종가가 5일 최저가이고, 20일 이동평균보다 낮은 시점만 뽑기 buy = d[(d[\u0026#39;close\u0026#39;] == d[\u0026#39;5d_min\u0026#39;]) \u0026amp; (d[\u0026#39;close\u0026#39;] \u0026lt; d[\u0026#39;20d_mean\u0026#39;])] buy timestamp\tticker\topen\thigh\tlow\tclose\tvolume\t5d_max\t5d_min\tlast_1d_close\t20d_mean 19\t20100129\t005930\t16000.0\t16020.0\t15600.0\t15680.0\t22864250.0\t16840.0\t15680.0\t16160.0\t16402.0 20\t20100201\t005930\t15680.0\t15700.0\t15300.0\t15540.0\t25052100.0\t16300.0\t15540.0\t15680.0\t16370.0 21\t20100202\t005930\t15800.0\t15800.0\t15400.0\t15440.0\t19690150.0\t16160.0\t15440.0\t15540.0\t16320.0 24\t20100205\t005930\t15160.0\t15220.0\t14940.0\t15000.0\t25751700.0\t15540.0\t15000.0\t15520.0\t16148.0 25\t20100208\t005930\t14940.0\t15080.0\t14820.0\t14960.0\t21980400.0\t15540.0\t14960.0\t15000.0\t16099.0 ...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t... 3524\t20240416\t005930\t81200.0\t81300.0\t79400.0\t80000.0\t31949845.0\t84100.0\t80000.0\t82200.0\t81400.0 3525\t20240417\t005930\t80700.0\t80800.0\t78900.0\t78900.0\t22611631.0\t84100.0\t78900.0\t80000.0\t81705.0 3527\t20240419\t005930\t78300.0\t78700.0\t76300.0\t77600.0\t31317563.0\t82200.0\t77600.0\t79600.0\t81755.0 3528\t20240422\t005930\t77400.0\t77500.0\t75100.0\t76100.0\t30469477.0\t80000.0\t76100.0\t77600.0\t81615.0 3529\t20240423\t005930\t76400.0\t76800.0\t75500.0\t75500.0\t18717699.0\t79600.0\t75500.0\t76100.0\t81480.0 723 rows × 11 columns 매매 시그널 뽑기: 오늘 종가가 최근 5일 종가의 최솟값보다 작으면서, 20일의 일평균보다 낮은 경우만 뽑기 총 723개 시점 # plt.plot을 활용해 주가 그래프 출력 plt.figure(figsize=(15,8)) plt.plot(d[\u0026#39;close\u0026#39;]) 삼성전자 일별 종가 시각화 # 주가 그래프에 매수 타점 표시 plt.figure(figsize=(15,8)) plt.plot(d[\u0026#39;close\u0026#39;]) plt.scatter(buy.index,buy[\u0026#39;close\u0026#39;],c=\u0026#39;r\u0026#39;) buy(매수 시그널)에서 종가 타점 표시 시각화를 보면 내려가는부분만 사는 느낌.. 3500일 대신 최근 300일만 시각화하면? # 최근 300일만 뽑아 매수 타점 표시 d_sample = d.iloc[-300:] buy_sample = d_sample[(d_sample[\u0026#39;close\u0026#39;] == d_sample[\u0026#39;5d_min\u0026#39;]) \u0026amp; (d_sample[\u0026#39;close\u0026#39;] \u0026lt; d_sample[\u0026#39;20d_mean\u0026#39;])] # 매수 시그널 print(buy_sample.shape) plt.figure(figsize=(15,8)) plt.plot(d_sample[\u0026#39;close\u0026#39;]) plt.scatter(buy_sample.index,buy_sample[\u0026#39;close\u0026#39;],c=\u0026#39;r\u0026#39;) (63, 11) 총 63개의 매수 타점 떨어질때 계속 매수하다가 올라갈때는 안사고있음. 빨간색에서 매수하고 5일 후에 팔기. 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ top\n기초 백테스팅 모델 개발 # 1. 개념 # 상용 백테스팅 툴: Tradingview, Zipline, Backtesting.py 등\n백테스팅 모듈 기초 변수\n단일 종목 백테스팅 모듈 1일씩 이동하며 각 시점의 보유현금/보유종목/수익률 등을 기록하는 구조 4개 변수 사용 변수 정보\nholding cash: 보유 현금 position: 현재 보유 주식 수 Avg_price: 평단가 slippage: 슬리피지(세금+수수료+백테스팅과 실거래간 체결가격 차이) 2. 실습 # import pandas as pd import matplotlib.pyplot as plt d = pd.read_parquet(\u0026#39;005930.parquet\u0026#39;) d[\u0026#39;5d_max\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].max() # 오늘 포함 과거 5일 종가 중 최고값 d[\u0026#39;5d_min\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].min() # 오늘 포함 과거 5일 종가 중 최저값 d[\u0026#39;last_1d_close\u0026#39;] = d[\u0026#39;close\u0026#39;].shift(1) # 전일 종가 d[\u0026#39;20d_mean\u0026#39;] = d.rolling(20)[\u0026#39;close\u0026#39;].mean() # 20일 이동평균 buy = d[(d[\u0026#39;close\u0026#39;] == d[\u0026#39;5d_min\u0026#39;]) \u0026amp; (d[\u0026#39;close\u0026#39;] \u0026lt; d[\u0026#39;20d_mean\u0026#39;])] 가장 옛날 시점이 첫 줄에 가있는지 확인 holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] # 일별 총 포트폴리오 가치 for data in d.iterrows(): print(data) break (0, timestamp 20100104 ticker 005930 open 16060.0 high 16180.0 low 16000.0 close 16180.0 volume 11963550.0 5d_max NaN 5d_min NaN last_1d_close NaN 20d_mean NaN Name: 0, dtype: object) 파라미터 설정\n보유 현금: 100만원 보유중인 삼성전자 주식 수: 0 평단가: 0 (보유중인 주식 없음) 일별 수익률 저장할 리스트: daily_total_value d.iterrows()하면 (idx, row) 튜플이 나온다.\nfor idx,data in d.iterrows(): # 하루 시작 daily_total_value.append(0) # 매수조건 확인 및 매수 if data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;] and position == 0: holding_cash -= 1 * data[\u0026#39;close\u0026#39;] position += 1 avg_price = data[\u0026#39;close\u0026#39;] # 오늘 종가가 평단가가 됨 (보유주식 1개로 해놔서) # 매도조건 확인 및 매도 elif position \u0026gt; 0: holding_cash += position * data[\u0026#39;close\u0026#39;] # 다음날 종가로 매도 position = 0 avg_price = 0 # 장 마감 후 daily_total_value[-1] += holding_cash + position * data[\u0026#39;close\u0026#39;] 로직 설정\n종가가 20일 이동평균보다 낮고, 현재 보유 주식이 없는 경우에만 포지션 1개 오늘 종가로 구매 무조건 다음날 종가에 청산 오늘 시점의 포트폴리오 총평가가치 입력: 보유현금 + 가진 포지션 + 당일 종가\nprint(len(daily_total_value)) print(daily_total_value[-1]) 3534 1007320.0 7320원 번거\u0026hellip;맞나 ㅋㅋ plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 일별 수익률 시각화 결과 수익률이 좋지는 않다. cf) 40일 평균으로 해보기\n# 40일 이동평균 d[\u0026#39;40d_mean\u0026#39;] = d.rolling(40)[\u0026#39;close\u0026#39;].mean() holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] # 일별 총 포트폴리오 가치 for idx,data in d.iterrows(): daily_total_value.append(0) if data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;40d_mean\u0026#39;] and position == 0: holding_cash -= 1 * data[\u0026#39;close\u0026#39;] position += 1 avg_price = data[\u0026#39;close\u0026#39;] elif position \u0026gt; 0: holding_cash += position * data[\u0026#39;close\u0026#39;] position = 0 avg_price = 0 daily_total_value[-1] += holding_cash + position * data[\u0026#39;close\u0026#39;] print(len(daily_total_value)) print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 3534 1016070.0 16070원으로 오름..^^ cf2) if문 오류 나면?\nholding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 daily_total_value = [] # 일별 총 포트폴리오 가치 for idx,data in d.iterrows(): daily_total_value.append(0) if data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;] and position == 0: holding_cash -= 1 * data[\u0026#39;close\u0026#39;] position += 1 avg_price = data[\u0026#39;close\u0026#39;] if position \u0026gt; 0: #\u0026lt;\u0026lt;여기 오류라면?? holding_cash += position * data[\u0026#39;close\u0026#39;] position = 0 avg_price = 0 daily_total_value[-1] += holding_cash + position * data[\u0026#39;close\u0026#39;] print(len(daily_total_value)) print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 3534 1000000.0 매수하자마자 매도하는게 된다. cf3) 매수 다음날 대신 매수 5일차 종가에 매도\nholding_cash = 1_000_000 position = 0 avg_price = 0 buy_day = None # 매수한 날짜의 인덱스를 저장 daily_total_value = [] for idx, data in d.iterrows(): # 하루 시작 daily_total_value.append(0) # 매수조건 확인 및 매수 (포지션 없는 경우) if position == 0 and data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]: holding_cash -= data[\u0026#39;close\u0026#39;] position = 1 avg_price = data[\u0026#39;close\u0026#39;] buy_day = idx # 매수일 저장 # 보유 중이고, 매수 후 5일이 지난 경우 매도 elif position \u0026gt; 0 and (idx - buy_day) == 4: holding_cash += position * data[\u0026#39;close\u0026#39;] position = 0 avg_price = 0 buy_day = None # 장 마감 후 평가가치 기록 daily_total_value[-1] += holding_cash + position * data[\u0026#39;close\u0026#39;] print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) 1070830.0 7마넌으로 올랐다! 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ top\n"},{"id":125,"href":"/docs/study/tech/tech2/","title":"정량적 백테스팅 성과 분석","section":"생물정보학","content":" 정량적 백테스팅 성과 분석 # 목록 # 2025-03-31 ⋯ 전략 백테스팅과 수익률 그래프 그리기\n2025-03-31 ⋯ 정량적 백테스팅 성과 분석\n2025-03-31 ⋯ 전략과 벤치마크 성과지표 비교\n전략 백테스팅과 수익률 그래프 그리기 # 1. 개념 # 로직 설명\n삼성전자 일봉 사용 최근 5일 종가 중 당일 종가 가격이 가장 낮고, 20일 이동평균보다 종가가 더 낮은 경우 매수 (여기까지만 있다면? 주가가 무한히 떨어지면 무한 매수하게되므로 실거래시에는 사용하기어려움. 그래서 아래 2개 추가) 단, 현재 보유 종목이 있다면 추가매수 없음 매수 3일차 종가에 매도 슬리피지 적용\n수수료+세금을 슬리피지에 함께 포함 실제 전략 운용시, 내 물량으로 인해 호가가 바뀌거나 주문속도의 차이로 인해 기대 체결가격과 실제 체결가격간 괴리가 발생하는 등 백테스팅에서 고려하기 어려운 괴리가 존재. 위 케이스들을 고려해 백테스팅 툴을 만들수도 있지만, 고난이도의 작업이기때문에 슬리피지를 넉넉히 적용하는 것으로 어느정도 문제 해결 슬리피지 = 수수료+세금+매매가격차이 2. 실습 # import pandas as pd import matplotlib.pyplot as plt d = pd.read_parquet(\u0026#39;005930.parquet\u0026#39;) d[\u0026#39;5d_max\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].max() d[\u0026#39;5d_min\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].min() d[\u0026#39;last_1d_close\u0026#39;] = d[\u0026#39;close\u0026#39;].shift(1) d[\u0026#39;20d_mean\u0026#39;] = d.rolling(20)[\u0026#39;close\u0026#39;].mean() # 종가가 5일 최저가 \u0026amp; 종가가 20일 이동평균 아래 # 지금 진입해있는 포지션이 없을 경우에만 진입 # 매도 조건: 마지막 매수 3일 후 매도 # 파라미터 설정 holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 # 일별 총 포트폴리오 가치 daily_total_value = [] holding_time_passed = 0 # 마지막 매수 후 경과 일수 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): # 하루 시작 daily_total_value.append(0) # 전략 구현 # 종가가 5일 최저가이고 종가가 20일 이평 아래인지 여부 # 현재 매수 가능한 현금이 충분히 있는지 # 매수 조건에 맞으면 매수 진입 if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;] and position == 0: position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] # 평단가는 오늘 종가 holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] position = 0 avg_price = 0 # 파라미터 업데이트 if position \u0026gt; 0: holding_time_passed += 1 # 하루 마무리 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) return1 = daily_total_value.copy() 1051680.0 51680원 벌었다. ################ 백테스팅 파라미터 ################ holding_cash = 1_000_000 # 보유 현금 position = 0 # 현재 보유 포지션 avg_price = 0 # 평단가 slippage = 0.004 # 슬리피지 daily_total_value = [] # 일별 총 포트폴리오 가치 ################ 전략 파라미터 ################ holding_time_passed = 0 # 마지막 매수 후 경과 일수 슬리피지 0.4% 반영 (수수료+세금 0.2% 가격 괴리 0.2%) 매수 시점에 슬리피지 반영하는게 조금더 보수적임 매수 매도 따로 슬리피지 반영하는게 조금더 디테일함 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): daily_total_value.append(0) if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;] and position == 0: position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] * (1-slippage) # 1,000,000 -\u0026gt; -0.4% -\u0026gt; 996,000 position = 0 avg_price = 0 # 오늘의 마무리 if position \u0026gt; 0: holding_time_passed += 1 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(daily_total_value) return2 = daily_total_value.copy() 994850.1199999993 삼성전자의 가격이 튈때를 반영했을때 이런 그래프가 나온다. 즉 수익률이 좋지 않다. plt.figure(figsize=(15,8)) plt.plot(return1,c=\u0026#39;k\u0026#39;) plt.plot(return2,c=\u0026#39;r\u0026#39;) 슬리피지 유무에 따른 수익률 차이. 슬리피지가 없을때는 올라가는 느낌이었는데 수수료, 세금 떼고나니까 빠지는중. 현실적인 요인을 반영하면 좋아보였던 전략도 좋지 않을 수 있다. # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): daily_total_value.append(0) if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;]: #\u0026lt;\u0026lt;여기 수정 position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] * (1-slippage) # 1,000,000 -\u0026gt; -0.4% -\u0026gt; 996,000 position = 0 avg_price = 0 # 오늘의 마무리 if position \u0026gt; 0: holding_time_passed += 1 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] return2 = daily_total_value.copy() print(daily_total_value[-1]) plt.figure(figsize=(15,8)) plt.plot(return1,c=\u0026#39;k\u0026#39;) plt.plot(return2,c=\u0026#39;r\u0026#39;) 보유 주식수랑 상관없이 무한으로 진입할수있다고 한다면? 1099929.4799999993 수익률이 크게 바뀐다. 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ 목록\n정량적 백테스팅 성과 분석 # 1. 개념 # 전략의 성과를 정확하게 분석하기 위해서는 직관적 분석이 아닌 정량적 분석이 필요하다.\n연 수익률, 변동성, 최대 손실, 변동 대비 수익률 등의 지표를 활용한다. 퀀트 트레이딩 회사에선 정량적 분석으로 성과 평가하고 비중 조절하는 것이 당연시 됨. 주요 정량평가 지표\n연 수익률: 내 전략의 1년 수익률\n변동성: 일별 수익률의 표준편차. 수익이 얼마나 일정하게 나는지, 포트폴리오 평가가치가 얼마나 빠르게 움직일 수 있는지.\nSharpe 지수\n수익률/변동성 한 단위만큼의 변동성을 가질 대 얼마나 수익이 잘 나는지 분석. Sharpe 지수가 높을수록 좋음. 가장 많이 보는 지표로, 그만큼 변동을 줄이고 수익을 높이는것이 중요하단것을 의미. \u0026lsquo;수익률\u0026rsquo; 대신 \u0026lsquo;수익률 - Risk free ratio\u0026quot;를 적용할 수 있음 (Risk free ratio는 보통 채권 수익률) 이 지표를 변형한 Sortino Ratio가 있음. MDD (Maximum DrawDown)\n고점 대비 최대 하락폭 전략에서 발생할 수 있었던 최대 손실폭을 확인해, 전략의 최대 리스크 파악 Sharpe 지수와 함께 가장 중요하게 보는 지표 중 하나 2. 실습 # import pandas as pd import matplotlib.pyplot as plt import numpy as np d = pd.read_parquet(\u0026#39;005930.parquet\u0026#39;) d[\u0026#39;5d_max\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].max() d[\u0026#39;5d_min\u0026#39;] = d.rolling(5)[\u0026#39;close\u0026#39;].min() d[\u0026#39;last_1d_close\u0026#39;] = d[\u0026#39;close\u0026#39;].shift(1) d[\u0026#39;20d_mean\u0026#39;] = d.rolling(20)[\u0026#39;close\u0026#39;].mean() ################ 백테스팅 파라미터 ################ holding_cash = 1_000_000 position = 0 avg_price = 0 slippage = 0.004 daily_total_value = [] ################ 전략 파라미터 ################ holding_time_passed = 0 # for 문으로 하루씩 백테스팅 진행 for idx,data in d.iterrows(): daily_total_value.append(0) if (data[\u0026#39;close\u0026#39;] \u0026lt; data[\u0026#39;20d_mean\u0026#39;]) and (data[\u0026#39;close\u0026#39;] == data[\u0026#39;5d_min\u0026#39;]): if holding_cash \u0026gt; 1*data[\u0026#39;close\u0026#39;]: #and position == 0: position += 1 holding_cash -= 1 * data[\u0026#39;close\u0026#39;] avg_price = data[\u0026#39;close\u0026#39;] holding_time_passed = 0 # 마지막 매수 3일 후 매도 if position \u0026gt; 0 and holding_time_passed == 3: holding_cash += position * data[\u0026#39;close\u0026#39;] * (1-slippage) position = 0 avg_price = 0 # 오늘의 마무리 if position \u0026gt; 0: holding_time_passed += 1 daily_total_value[-1] = holding_cash + position * data[\u0026#39;close\u0026#39;] 위 백테스팅 다시 수행. 근데 보유주식수 제한 없는 버전으로 (10만원 벌었던 버전) # 전략 총 수익률 계산 total_return_pct = daily_total_value[-1]/daily_total_value[0] print(\u0026#39;총 수익률: {:.2f}%\u0026#39;.format((total_return_pct-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # 1년을 250일로 가정, 연 복리 수익률 계산 total_years = len(daily_total_value)/250 print(\u0026#39;총 백테스팅 기간: {:.2f}년\u0026#39;.format(total_years)) import math annaul_return = math.pow(total_return_pct,1/total_years) print(\u0026#39;연 수익률: {:.2f}%\u0026#39;.format((annaul_return-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(daily_total_value)) daily_std = pd.DataFrame(daily_total_value).pct_change().std()[0] print(\u0026#39;일 수익률: {:.4f}%, 일 변동성: {:.4f}%\u0026#39;.format((daily_return-1)*100,daily_std)) print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1)/daily_std)*np.sqrt(250))) print(\u0026#39;------------------------------------------------\u0026#39;) # MDD 계산 tv = pd.DataFrame(daily_total_value) dd = tv/tv.cummax() print(\u0026#39;MDD: {:.2f}%\u0026#39;.format((dd.min()-1)[0]*100)) plt.figure(figsize=(10,5)) plt.plot(dd) plt.show() print(\u0026#39;------------------------------------------------\u0026#39;) 총 수익률: 9.99% ------------------------------------------------ 총 백테스팅 기간: 14.14년 연 수익률: 0.68% ------------------------------------------------ 일 수익률: 0.0027%, 일 변동성: 0.0019% Sharpe ratio: 0.23 ------------------------------------------------ MDD: -6.38% 전략평가지표 측정\n총수익률: 일별 총 평가가치 / 초기자금(100만원) 연 수익률 총 수익률은 10%인데 1년에는 몇%인가? 1년은 250일로 가정한다. 영업일 기준 그냥 총 수익률 9.99/14.14 하면 되는거 아닌가? 하면 아님. 복리 수익률 계산해야함. x에 14.14승을 해야 9.99%가 나오는거니까 역으로 계산해준다 MDD -6.38%: 최대 고전 대비 6.38%가 빠질 수 있다.\n연 수익률이 0.68%인데 MDD가 6.38%이면\u0026hellip; 10년 번게 한번에 빠질수도있다는 것임. 내 수익률 대비 dd가 얼마나 큰지도 계산해야된다. # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(daily_total_value)) daily_std = pd.DataFrame(daily_total_value).pct_change().std()[0] Rf = 0.05/250 #\u0026lt;\u0026lt;연 5% print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1 - Rf)/daily_std)*np.sqrt(250))) Sharpe ratio: -1.48 Sharpe Ratio 계산할때 risk free ratio를 빼줄수도 있다. 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ 목록\n전략과 벤치마크 성과지표 비교 # # 삼성전자 Buy \u0026amp; Hold의 수익률 계산 bm_daily_total_value = d[\u0026#39;close\u0026#39;].values/d[\u0026#39;close\u0026#39;].values[0] 삼성전자 Buy \u0026amp; Hold를 벤치마크로 삼고 성과지표 비교. plt.plot(d[\u0026#39;close\u0026#39;]) 삼성전자 기본 수익률 그래프를 보면 주가이기때문에 시작값이 10000인데 1로 환산한 값을 사용한다. # 전략 총 수익률 계산 total_return_pct = bm_daily_total_value[-1]/bm_daily_total_value[0] print(\u0026#39;총 수익률: {:.2f}%\u0026#39;.format((total_return_pct-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # 1년을 250일로 가정, 연 복리 수익률 계산 total_years = len(bm_daily_total_value)/250 print(\u0026#39;총 백테스팅 기간: {:.2f}년\u0026#39;.format(total_years)) import math annaul_return = math.pow(total_return_pct,1/total_years) print(\u0026#39;연 수익률: {:.2f}%\u0026#39;.format((annaul_return-1)*100)) print(\u0026#39;------------------------------------------------\u0026#39;) # Sharpe Ratio daily_return = math.pow(total_return_pct,1/len(bm_daily_total_value)) daily_std = pd.DataFrame(bm_daily_total_value).pct_change().std()[0] print(\u0026#39;일 수익률: {:.2f}%, 일 변동성: {:.2f}%\u0026#39;.format((daily_return-1)*100,daily_std)) print(\u0026#39;Sharpe ratio: {:.2f}\u0026#39;.format(((daily_return-1)/daily_std)*np.sqrt(250))) print(\u0026#39;------------------------------------------------\u0026#39;) # MDD 계산 tv = pd.DataFrame(bm_daily_total_value) dd = tv/tv.cummax() # 각 시점별 누적 최댓값으로 나눠준다. print(\u0026#39;MDD: {:.2f}%\u0026#39;.format((dd.min()-1)[0]*100)) plt.figure(figsize=(10,5)) plt.plot(dd) plt.show() print(\u0026#39;------------------------------------------------\u0026#39;) 총 수익률: 374.04% ------------------------------------------------ 총 백테스팅 기간: 14.14년 연 수익률: 11.64% ------------------------------------------------ 일 수익률: 0.04%, 일 변동성: 0.02% Sharpe ratio: 0.42 ------------------------------------------------ MDD: -42.20% 첫해에 삼성전자를 샀으면 매년 복리로 수익률이 11.64% 나왔을것이다. 14.14년동안 총 수익이 374% 나왔다. dd가 굉장히 큰데 30% 이상 떨어지고 60%까지 떨어질수도 있다. mdd는 정확히는 42.2%였다. 총수익은 374% 나오지만 중간에 42%의 최대 손실을 감내할수있어야함.. plt.figure(figsize=(15,8)) plt.plot(daily_total_value,c=\u0026#39;k\u0026#39;) plt.twinx().plot(bm_daily_total_value*daily_total_value[0],c=\u0026#39;r\u0026#39;) 전략 수익률과 벤치마크 수익률 비교 (twinx 줘서 y축 분리) plt.figure(figsize=(15,8)) plt.plot(daily_total_value,c=\u0026#39;k\u0026#39;) plt.plot(bm_daily_total_value*daily_total_value[0],c=\u0026#39;r\u0026#39;) 요 방법으로 plotting 하면 상대적으로 수익률이 안좋은 우리 전략이 아래에 깔리는것을 볼수있다. 강의 링크 https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A3%BC%EC%8B%9D%EB%A7%A4%EB%A7%A4%EB%B4%87-%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5\n⏶ 목록\n"},{"id":126,"href":"/docs/study/tech/tech3/","title":"Kaggle 타이타닉 EDA","section":"생물정보학","content":" Kaggle 타이타닉 EDA # 목록 # 2025-03-30 ⋯ Kaggle API 사용법\nKaggle API 사용법 # 1. 사전 설정 # !pip install kaggle Collecting kaggle Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB) Requirement already satisfied: bleach in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (6.1.0) Requirement already satisfied: certifi\u0026gt;=14.05.14 in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (2024.7.4) Requirement already satisfied: charset-normalizer in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (3.3.2) Requirement already satisfied: idna in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (3.7) Collecting protobuf (from kaggle) Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes) Requirement already satisfied: python-dateutil\u0026gt;=2.5.3 in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (2.9.0) Collecting python-slugify (from kaggle) Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB) Requirement already satisfied: requests in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (2.32.3) Requirement already satisfied: setuptools\u0026gt;=21.0.0 in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (75.1.0) Requirement already satisfied: six\u0026gt;=1.10 in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (1.16.0) Collecting text-unidecode (from kaggle) Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB) Collecting tqdm (from kaggle) Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB) Requirement already satisfied: urllib3\u0026gt;=1.15.1 in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (2.2.2) Requirement already satisfied: webencodings in /opt/anaconda3/envs/workspace/lib/python3.8/site-packages (from kaggle) (0.5.1) Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB) Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB) Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB) Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB) Downloading tqdm-4.67.1-py3-none-any.whl (78 kB) Installing collected packages: text-unidecode, tqdm, python-slugify, protobuf, kaggle Successfully installed kaggle-1.7.4.2 protobuf-5.29.4 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.67.1 캐글 설치 cd ~ mkdir .kaggle cd .kaggle mv /Users/yshmbid/Desktop/kaggle.json . chmod 600 kaggle.json # 사용자 권한을 사용자 ID만 읽고 쓸수있게 하겠다는 의미 Kaggle API 토큰 생성 kaggle.json 파일을 위 경로에 넣기 !kaggle competitions download -c titanic titanic 데이터 다운로드. bash에서 확인해보면? $ pwd /Users/yshmbid/project/bin $ ls 1-kaggle.ipynb\ttitanic.zip titanic.zip이 생겼다! 강의 링크 https://www.inflearn.com/course/%EC%B2%98%EC%9D%8C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8/dashboard\n⏶ 목록\n"},{"id":127,"href":"/docs/hobby/daily/daily12/","title":"카페 오딘","section":"일상","content":" 카페 오딘 # #2025-03-29\n바다뷰가 넘 예뻤던 카페\n저 크림들어간 크로와상이 완전 짱맛..! 샐러드두 맛있었당\n바깥뷰도 이뻐서 나가서 걸어주기에좋다\n이날 기분이별로였어서 아쉽다 담에가면 더마싯게먹고 잘구경하고다닐텐데!!!! 아쉬움이남아서 다시가고싶다 ㅎ\n"},{"id":128,"href":"/docs/study/bioinformatics/bi18/","title":"국민건강보험공단 2025년도 제1차 전문인력 채용","section":"생물정보학","content":" 국민건강보험공단 2025년도 제1차 전문인력 채용 # 전산 \u0026gt; AI\n연구 \u0026gt; 보건·의료통계연구\n전산 \u0026gt; 빅데이터\n공고 전문\n공고 바로가기\n"},{"id":129,"href":"/docs/study/bioinformatics/bi19/","title":"한국의약품안전관리원 2025년 1차 직원 채용 ","section":"생물정보학","content":" 한국의약품안전관리원 2025년 1차 직원 채용 # 특수 \u0026gt; 통계\n공고 바로가기\n"},{"id":130,"href":"/docs/study/bioinformatics/bi12/","title":"대학원생 면접대비캠프","section":"생물정보학","content":" 대학원생 면접대비캠프 # #2025-01-01\n대학원생 대상으로 면접대비 강의가 있길래 신청해봤다!\n화수목은 5시부터 9시이구 금요일은 1시부터 6시반이라서 금요일은 일찍 퇴근할수있으면 퇴근하고 듣는게 좋을듯. 토요일은 10시부터 오프라인으로 한다.\n이번주 랩미팅이 목요일 2시에 있고 논문 미팅은 금요일 아침 9시라서 크게 겹치지는 않아 매우 다행이다!!\ncf)\n원래 이런문자 다 무시하는데 ㅋㅋ 갑자기 눈에 들어와서 신청함..\n1일차 - 면접 기초 # 1.9 오후 7시에 질문 받음. 1.10은 논문 기반 세미나 pt 면접 시뮬레이션. 목요일에 신청받는다. 토요일 모의면접 일정(오늘 9시에 신청) -\u0026gt; 경북대학교 복지관 4층. https://www.onoffmix.com/event/315732\n채용트렌드 분석과 면접 준비전략\n직무관련 경험을 제일 중요하게 본다. 석사는 학위시절의 경험과 연구주제를 가지고 판단함.\n인턴이 가져야할 역량을 단어적 표현으로 정해놓고 이 표현이 많으면 높은점수 줬다. 걸러진 인원을 사람이 최종평가함. 글자수 채우는것도 중요하다. 챗지피티 자소서 vs 합격자 자소서 구별하기 어렵다. AI면접은 일관성만 있으면 붙는다.\n실제면접은 ai면접 기반으로 예상질문이 면접관한테 이런식으로 주어짐. 인재상. 공동작업에서 나랑 다른견해를 가진 사람과 공동의 성과를 얻으려면 커뮤니케이션이 중요함. 그리고 책임감 키워드로 설명하면 좋음. 문제 정답은? 4번이 맞지않나? -\u0026gt; 4번이었음 ㅎ\n이미지메이킹으로 만들어지는 인성면접\n면접은 역량면접, 인성면접. 면접관으로 누가 참여하는지가 다르다. -\u0026gt; 조직에 융화되는 책임감이 제일 중요하다.\n내가 이 연구에서 얼마나 열심히 수행했는지 말고 왜 이연구를 했는지, 우리 회사의 업무를 보는데 어떻게 활용되는지를 어필해야함.\n인성면접에서 중요한것.\n첫인상 좋게주는게 인성면접에서는 매우중요. 첫번째 질문은 1분자기소개 or 지원동기인데 첫번째 질문으로 첫인상이 결정된다. 완벽준비를 해가야한다. 어필포인트가 정리돼있어야한다. 어떤방식으로 문제해결하는 사람인지? 책임감을 바탕으로/소통과 협업으로 등등. 연계되는 경험이 붙어야 한다. 2가지 모습을 정하고 들어가야한다. 책임감있는사람 성실한사람 등등. 책임감을 추천함. 자소서에서 질문포인트 예측하고 정리해야함. 기업에 대한 이해 필수. 기업의 최근 이슈 사항. 이 연구가 인상적이고 내가 어떤역할을 할수있고 등등.-\u0026gt; 연구실 컨텍 메일 썰 풀기. 답변길이는 30초가 적당. 말이 길어지면 앞뒤논리가 안맞을수있고 한문장으로 답변하면 싫어함 점수화 질문이 있고 떨어뜨릴사람 찾는 질문이 있다. -\u0026gt; 얘는 점수화 질문.\n1-\u0026gt; 키워드를 던져줄수있어야한다. 살아오면서 가장 힘들었던 경험이 뭐냐고 하면 거기도 키워드를 던져야함. 석사학위 논문을 낼때 힘들었는데 분석력 책임감 성실함의 중요성을 알수잇었다. 답변은 두괄식으로 해야함. 4-5문장으로 답변. -\u0026gt; 얘는 합불질문이므로 뻔한답변을 하고 넘어가야함.\n1-\u0026gt; 법적 윤리적 지시가 아닌경우 상사의 지시를 따른다. 신입이므로 노하우를 잘 모를수있기때문에 적극적으로 따르고 우려하던 부당한 부분이 추후에 나타난다면 그때 물어본다. 이전에 그런일이 있었는데 어쩌고.\n2-\u0026gt; 상사의 행동을 공유한다. 내규나 메뉴얼을 찾아서 어떻게 대응하면되는지 찾고 찾지못하면 높은 직급의 상사에게 물어본다. 직접 판단하지 않고 조직에서 해당 상사에 대한 판단을 맡기겠다.\n3-\u0026gt; 야근은 필연적으로 발생할수밖에 없으므로 야근을 해서 적극적으로 참여하고 상사과 야근식사를 하고 적극적으로 친해지겠다.\n1분자기소개 만들기 초안 keyword와 직무가 들어가는 글. 청자지향 커뮤니케이션: 2개 키워드/두괄식 중요한거 강약조절로 말의리듬을 살려야한다 \u0026lsquo;솔\u0026rsquo;톤으로 \u0026lsquo;웃으면서\u0026rsquo; 말해야한다. 말안할때는 안웃어도됨.. 예상질문. -\u0026gt; 이 질문의 정답은 \u0026lsquo;나\u0026rsquo;가 아닌 조직과 팀의 관점에서 답변. 다른 연구원들이 춝근하는 시간을 보고 선배들과 소통하기 좋은 시간을 선택하겠다. -\u0026gt; 회사는 조직이라서 약속을 지키는게 중요해서 기한 지키기가 당연히 중요함. -\u0026gt; 업무지시의 순응도를 봄. -\u0026gt; 정당한 사유 없이 업무지시 거부하면 해고사유. -\u0026gt; 협업작업이 중요하다. FAQ 면접관이 평가할때 가장 중요하게 생각하는것은? 시간관리 질문이든 뭐든 혼자하는거 좋아한다고 하지말기. 2일차 - 면접유형별 대응 전략 # AI면접, 비대면면접\n비대면 면접 주의점 학점이 안좋다는 질문이 들어오면 일단 공감하고 하지만 다른부문으로 고치려고해봤다(자기견해)고 어필하기. 처음부터 어필하려하면안됨. 반박하는게 제일안좋다.\nPT면접\nPT면접 종류 pt면접, ap면접, 세미나 pt면접이 있다.\n전공pt면접준비할때. 이런식으로 챗지피티에 쳐서 대상 기업에서 다루는 메인 공정 얻고\n이런식으로 전공 특이적으로 답변 얻고\n공정이슈 상위 3개 얻고\n제시된 이슈상황을 바탕으로 면접질문화한다.\n3일차 - 면접답변 노하우 # 이력서 자소서 2부씩 출력해오기.\n세미나pt면접\n10페이지. 석사는 내가 한 과제들이 회사의 어떤 연구과제와 연계가 된다.라는거는 보여주면됨. 관련성도 중요하지만 해당 연구가 어떤 목적성으로 진행됐는지 어떤 기술/학문적 요소를 사용해서 성장이 있었는지. 한페이지에서 얘기하고자하는게 정해져있어야함. 그리고 확장성도 얘기해야한다. 향후에 어떤 연구를 진행하겠다. 갑자기 크림빵 먹고싶당..\n프로세스. 학문/분야적으로 어떤 목적성이 있었는지 제시돼야한다. 두세가지면 123을 두세번 반복하고 45는한번. 자기소개pt\n이 기업이 어떤사업을 하고 어떤상품서비스를 만드는지 이 직무는 어떤역할을 하는지 명확한 이해에서 시작해야함. 특정 모습이 있어야된다. 직업적 핵심포인트. 그걸 함양하기위한 경험적 요소 나열. 챗지피티로 사업현황, 사업구조 분석. 공정로직까지 분석해서 알려준다.\n예시 예시2 세미나 면접이나 자기소개pt나 중요한것은 한페이지마다 전달하고싶은 메시지가 명확해야함.\n면접질문\n질문의도는 해당 직무로 뽑아도 오겠느냐는 의도. 다른회사의 같은직무도 지원했냐는 꼬리질문이 있을수있음. 학부시절부터 이 직무에 관심이 있었기 때문에 지원하게되었다. 경쟁기업의 동일한 직무에도 지원했다. 이 직무로 경력개발 하고싶다. 석사는 도움이 되기위해 했다. 다른 직무로 가지않을것이다. / 석사과정동안 여러 연구를 하면서 여러 소재 개발에 흥미를 느꼈으나 회사에서는 개발한 소재가 합리적인 공정을 거쳐 최적의 생산이 이루어져야하는게 중요하다는걸 알게되엇고 현장에서 역할을 하고자하기위해 지원했다. 제품의 퀄리티가 개발의 성과를 보여주는것이라고 생각해서 품질 축면에서 컨트롤하는 역할을 하고싶어서 왔다. 방어적인 논리가 필요하다.\n내가 수행한 연구과제가 이런 연관성이 있기 때문에 지원했다. 직무적으로 어떤역할을 하는지 이해하지못한거같을때 이런 질문이 나온다. 혁신 도전 변화 이런단어 들어가면 안좋다.\n자기회사의 브랜드가치에 대해 어떻게 생각하는지. lg가 이렇게 물어봄. 내가 연구한 분야가 이 회사랑 더관련성이 높다고 생각한다. 아예모른다 생각도안해봣다 이런말은 안됨. 이 회사의 브랜드/연구과제적인 가치가 더 높다고 생각한다. 입사의지를 강력하게 표명하기.\n살면서 힘들었던 경험으로부터 가치관이 형성되니까 납득할수있는 힘듦을 말해야한다. 석사는 다힘들다. 힘들었떤 경험이 지금 나의 어떤 모습을 형성하는데 도움됐는지 말하기. 성장과정을 물어보는거랑 동일하다.\n남들보다 로지스틱 회귀모형잘한다 등. 의사소통 스킬 경청 이런거 말하면 안된다. 구체적인 지식이 필요함.\n내 이름으로 출판된 연구가 몇개 이상 되게 하고싶다. 연구자로서의 성장계획. 직책을 말하라는게 아니고 어느정도의 연구를 수행하고싶은지. 박사하고싶다. 이런말하면 큰일남.\n단점이 없을수없으므로 양면성을 갖는 대답해야함. 순진하게 답변했다가 떨어질수있음. 일처리가 늦는게 단점이다 이런말하면안됨. 빨리 이 질문을 탈피하는게 중요함. 말이 많고 수다가 많다는게 단점 대화에 빠져든다 이런식의 답변은 안됨. 완벽을 추구하다보니 개인적으로 스트레스를 많이 받는다. 개선하기위해 나만의 스트레스 해소법을 가지려고 노력하고있고 어떻게보면 업무적 성과와 신뢰를 만들수있는 요소라고 생각한다.\n스트레스 안푸는사람들이 자거나 넷플릭스보거나 주말에 집에있는다. 취미를 물어보는거고 평소에 시간관리를 어떻게하고 적극성이 어케되는지를 물어보는것임. 여가시간에 뭐하냐는 질문임.\n경쟁력. 기술적인 요소를 물어보는게 아니라 어떤 성향의 사람인지 물어보는것. 답변의 방향성은 협업에 적합한 사람들을 선호하므로 책임감 뛰어나다 성실하다 사교성이 뛰어나다 등등. 직무에 따라 좀 다르다.\n계획한대로 잘 안됐던 사례. 설득을 어떻게 했다. 이런식. 일 학습 병행을 잘할수있을줄알고 알바랑 학업 같이했는데 생각보다 어렵고 시간관리 잘하는줄 알았는데 어려웠고 학점이 떨어져서 손해봣다. 시간관리가 중요함을 알았고 그뒤로는 할수있는일인지 아닌지 판단하고 결정했다.\n갈등을 설득으로 해결하는 방법. 갈등은 생각/성향의 차이에서 발생하고 상대방에 대한 수용성, 이해를 바탕으로 적절한 지점을 잡아가는 해결을 제시. 상대는 A안을 제안하고 나는 B안일때 합리적인 안을 제시하는게 이루어졌고 자료기반으로 의사결정했다. 성향의 경우 나는 적극적인데 상대는 신중한 성향일경우 상대방에 대한 이해, 나와 상대방이 어떻게 생각했는지에 대한 적정 포인트를 잡아서 공감했다. 이런 답변 하기.\n세대간 소통스킬이 있는지. 예뻐해주셧다 이렇게말하면 안되고 세대간 소통스킬을 물어보는걸 알고 어떻게 지내는지 대답하기.\n면접관이 충분히 그렇게 바라볼수있을것같고 학점관리 반성중이다. 낮은학점 보완하기위해 어떤 노력이 있었다.\n뭐 잘하는지 말해봐라.\n관련분야 책 말고 하나정도 준비해가기. 북튜버보고 정해가기.\n내 연구과제가 직무랑 잘 맞아서 준비했다. 붙으면 어디갈거냐 물어보면 회사의 로열티를 강조하면서 마무리하면댄다.\n갈등이 발생하지 않는 경우는 공산당이다. 생산성이 있으려면 갈등은 반드시 존재한다. 저사람을 최대한 맞춰주고 피햇다 이런 대답은 안된다. 생산성을 위해, 일이 진행되게하기위해 어떤 일을 했는지.\n없다고하면 안되고 갈등이 있을때도 어떤 성과를 낼것인가. 나는 다 피하는 사람이다 이런건 안좋다.\n조직이라는것에 대해 생각. 조직이기 때문에 납득이 안되는 일이 있어도 틀렷더라도 우선은 따르겠다고 말하기. 신입으로서 해당 분야에 대한 노하우가 부족하다고 생각하기 때문. 잘못된 방향인것 같으면 그때 얘기한다. 조직생활에 있어서 맞지않는 부분이라고 생각한다.\n기존과 다른 방식, 발전적인 변화 등을 말하면됨.\n보고 후 조치를 취한다.\n뻔한 답변 하고 넘어가기. 부모님 여자친구 선물사주겟다. 여행금지. 자기계발 투자하겠다 금지.\n복기하겠다고 답변하면 안됨. 이회사 또지원하겠다!도 안됨. 이 직무에서 사회생활을 하고싶은 진로를 결정했기 때문에 왜 떨어졌는지 생각해보면서 동일한 직무 다른 회사로 지원해서 이 직무에서 경험을 시작하기 위해 노력하는 계기로 삼겠다.\n이 회사가 잘될거기때문에 이회사 주식을 사겠다.\n엄마 아빠 친척 지도교수 안됨. 역사속인물 안됨. 업계 사람으로. 누구나 알수있는사람.\n모르면 인테리어 관점으로 접근하고 논리력이 좋으면 물리력 등을 사용하면됨.\n구체화. 회사와 관련없는 뜬구름잡는 이야기 안하기. 사회적으로 선한 영향력 이런거 말고. 직급에 따라 그 연차에서 달성할수있는 업무적 성과 말하기(챗지피티 사용)\n친화력 스킬. 경청, 배려 이딴거 얘기하면 안좋아한다. 경험 베이스로 본인만의 스킬. 공감 기반 리액션이 중요한것같다. 상황에 맞는 리액션을 잘해서 좋은 결과 낸적있다.\n적응력 질문. 어차피 독립해야하므로 어디든 상관없을것같다.\n둘다중요하다고 생각합니다? 단체에 처음 소속햇을때는 팔로우형이엇을것이고 후에 리더형 포지션이 되면 맡은바를 다하겠다. 환경, 시기에 따라 필요한 역할을 잘 수행하겠다.\n후자를 선택하면 안댐. 많은 사람과 협업해야하는데 혼자하는 자기계발보다는 사회적 관계를 통해 성장하는것이 품질관리 직무에 중요하다고 생각한다.\n단기적으로는 개인차원에서 회사에 적응, 일하는 방법 배우고 전문성 높이기. 장기적으로는 협업하고 조직 역량을 키워주기.\n기회있다고하면 좋아할까?를 생각해야함. 현실적으로 생각했을때 이지역에서 끝까지 성장하고싶다. 지역의 스페셜리스트가 되고싶다. 혹시라도 가야하는 상황이라면 받아들이겟지만 선택해서 가지는 않겟다.\n업무에 지장이 없는 범위 내에서 잘 응하겠다.\n당연히 출근하는거.\n프로젝트가 뭔지 알아야 답변가능..\n해당 직무를 찾아보다보니 기술경쟁력으로 승부하는 회사가 여기라고 생각했다. 이런 제품, 서비스에서 함께 r\u0026amp;d 하면서 함께 성장하고싶어서 지원했다.\n직무에 따라 다른 답변.\n사소한 리스크가 안전문제로 사망사고가 나면 큰 위험이 나타나는거기 떼문에 철저하게 법을 지키고 안전한 환경을 구성해서 작업하겠다. 넘기겠다고 하면 안되고 규정에 맞게 하겠다고 해야함.\n4일차 - 모의면접(PT, 세미나) # 모의면접은 신청자에 한해서 진행했는데, 뭣도 모르고 신청했다가 세미나 pt 준비 해야한대서 당일날 매우 low quality인 피피티 만들었다🥲\n피드백 해주실것도 없을거같은 퀄이었는데 그래두 목소리가 너무 작고 말이 빠르다/아이스브레이킹이 너무 없다 등등의 피드백은 해주셨음\n피피티도 어떤형식으로갈지 모르겠는데 감각도 없어서 ㅠㅠㅠ 그냥 5300원주고 포맷 샀다.ㅋㅋ\nhttps://kmong.com/gig/568376 여기걸로 삼. 근데 나쁘지 않았던거같다.\n5일차 - 모의면접(직무, 인성) # 이날 모의면접두 신청자에 한해서 진행했는데 진짜 취준하고있는사람만 대상으로 했던거같음 ㅠㅠ 나는 작성해둔 이력서나 자소서도 없어서 전날 저녁에 삼양 bioinformatics 공고 보고 거기에 맞춰서 급하게 써갔다\n퍼컬-\u0026gt;직무-\u0026gt;인성 순으로 봤는데 기억남는건\n퍼컬-나는 쿨톤이었다 직무-어설프게 아는거 적어넣으면 면접때 무조건 발각될것같으니 애매한건 무조건 공부해가자. 직무-목소리 크게하고 눈쳐다보면서 얘기하자. 직무-어필포인트를 확실하게 잡아서 그것만 심도있게 얘기하자. 그래야 전문적으로 보이는듯. 인성-예상질문 한 50개 만들어서 답다만들어놓자. 이정도\u0026hellip; 사실 준비가 너무 안된채로가서 면접봐주신분들한테 죄송할 지경이었지만 그래두 신청하길 잘한거같다. 내가얼마나 부족한지 잘알수있었다🥲🥲\n"},{"id":131,"href":"/docs/study/bioinformatics/bi1/","title":"DESeq2 워크플로우","section":"생물정보학","content":" DESeq2 워크플로우 # #2024-12-31\nLoad package # suppressMessages({ library(\u0026#34;DESeq2\u0026#34;) library(pheatmap) library(withr) #library(tidyverse) library(RColorBrewer) library(gplots) library(dplyr) }) Set path # setwd(\u0026#34;/data-blog/bi1\u0026#34;) getwd() \u0026#39;/data-blog/bi1\u0026#39; Run DESeq2 # S1 \u0026lt;- \u0026#39;33\u0026#39; S2 \u0026lt;- \u0026#39;150\u0026#39; countdata \u0026lt;- read.csv(\u0026#34;results.csv\u0026#34;, header=TRUE, sep=\u0026#39;,\u0026#39;) colnames(countdata) \u0026lt;- c(\u0026#39;GeneID\u0026#39;,\u0026#39;150-1\u0026#39;,\u0026#39;150-2\u0026#39;,\u0026#39;150-3\u0026#39;,\u0026#39;33-1\u0026#39;,\u0026#39;33-2\u0026#39;,\u0026#39;33-3\u0026#39;,\u0026#39;con-1\u0026#39;,\u0026#39;con-2\u0026#39;,\u0026#39;con-3\u0026#39;) countdata \u0026lt;- countdata[, c(\u0026#39;GeneID\u0026#39;,\u0026#39;150-1\u0026#39;,\u0026#39;150-2\u0026#39;,\u0026#39;150-3\u0026#39;,\u0026#39;33-1\u0026#39;,\u0026#39;33-2\u0026#39;,\u0026#39;33-3\u0026#39;,\u0026#39;con-1\u0026#39;,\u0026#39;con-2\u0026#39;,\u0026#39;con-3\u0026#39;)] selected_columns \u0026lt;- paste(c(\u0026#39;GeneID\u0026#39;,paste0(S2,\u0026#34;-1\u0026#34;), paste0(S2,\u0026#34;-2\u0026#34;), paste0(S2,\u0026#34;-3\u0026#34;),paste0(S1,\u0026#34;-1\u0026#34;), paste0(S1,\u0026#34;-2\u0026#34;), paste0(S1,\u0026#34;-3\u0026#34;)), sep=\u0026#34;\u0026#34;) countdata \u0026lt;- countdata[, selected_columns] countdata \u0026lt;- countdata[rowSums(countdata[, -1]) != 0, ] sample.names \u0026lt;- paste(c(paste0(S2,\u0026#34;-1\u0026#34;), paste0(S2,\u0026#34;-2\u0026#34;), paste0(S2,\u0026#34;-3\u0026#34;),paste0(S1,\u0026#34;-1\u0026#34;), paste0(S1,\u0026#34;-2\u0026#34;), paste0(S1,\u0026#34;-3\u0026#34;)), sep=\u0026#34;\u0026#34;) conditions \u0026lt;- factor(c(S2,S2,S2,S1,S1,S1)) metadata \u0026lt;- data.frame(Sample = sample.names, group = conditions) metadata N \u0026lt;- dim(countdata)[[2]] cData = countdata[,2:N] GeneID = countdata[,1] rownames(cData) = GeneID dds \u0026lt;- DESeqDataSetFromMatrix(countData = cData, colData = metadata, design = ~group) dds$group \u0026lt;- relevel(dds$group, ref = S1) colData(dds) dds \u0026lt;- DESeq(dds) vsd \u0026lt;- vst(dds, blind=FALSE) rld \u0026lt;- rlogTransformation(dds, blind=FALSE) res \u0026lt;- results(dds, contrast = c(\u0026#34;group\u0026#34;, S2, S1)) res_tbl \u0026lt;- as.data.frame(res) res_tbl$GeneID \u0026lt;- rownames(res_tbl) res_tbl \u0026lt;- res_tbl[order(res_tbl$padj), ] NM_no_NA \u0026lt;- na.omit(res) res_cut \u0026lt;- NM_no_NA[NM_no_NA$padj\u0026lt;0.05,] res_cut # padj val_str \u0026lt;- \u0026#39;padj\u0026#39; cutoff \u0026lt;- 0.05 cutoff_str \u0026lt;- as.character(cutoff) sig_res \u0026lt;- dplyr::filter(res_tbl, padj \u0026lt; cutoff) sig_res \u0026lt;- dplyr::arrange(sig_res, padj) sig_res_file \u0026lt;- paste0(\u0026#39;res_\u0026#39;, S2, \u0026#39;_\u0026#39;, S1, \u0026#39;_\u0026#39;, val_str, cutoff_str, \u0026#39;.csv\u0026#39;) #write.csv(sig_res, file = sig_res_file) print(paste0(S2, \u0026#39; vs \u0026#39;, S1, \u0026#39; | \u0026#39;, val_str, \u0026#39;\u0026lt;\u0026#39;, cutoff_str)) sig_idx \u0026lt;- res$padj \u0026lt; cutoff \u0026amp; !is.na(res$padj) sig_dat \u0026lt;- assay(rld)[sig_idx, ] annC \u0026lt;- data.frame(condition = conditions) rownames(annC) \u0026lt;- colnames(sig_dat) heat_colors \u0026lt;- brewer.pal(6, \u0026#34;RdYlGn\u0026#34;) heat_colors_reversed \u0026lt;- rev(heat_colors) ann_colors \u0026lt;- list(condition = setNames(c(\u0026#34;#F7819F\u0026#34;, \u0026#34;#58D3F7\u0026#34;), c(S2, S1))) A data.frame: 6 x 2 Sample\tgroup \u0026lt;chr\u0026gt;\t\u0026lt;fct\u0026gt; 150-1\t150 150-2\t150 150-3\t150 33-1\t33 33-2\t33 33-3\t33 DataFrame with 6 rows and 2 columns Sample group \u0026lt;character\u0026gt; \u0026lt;factor\u0026gt; 150-1 150-1 150 150-2 150-2 150 150-3 150-3 150 33-1 33-1 33 33-2 33-2 33 33-3 33-3 33 estimating size factors estimating dispersions gene-wise dispersion estimates mean-dispersion relationship final dispersion estimates fitting model and testing log2 fold change (MLE): group 150 vs 33 Wald test p-value: group 150 vs 33 DataFrame with 205 rows and 6 columns baseMean log2FoldChange lfcSE stat pvalue padj \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; ABHD2 50.721 1.352060 0.431587 3.13276 1.73168e-03 4.00143e-02 ADAM12 706.120 -0.571960 0.168494 -3.39454 6.87431e-04 2.03489e-02 ADD2 1819.643 0.868228 0.148791 5.83521 5.37230e-09 9.73246e-07 AIF1L 144.513 1.168923 0.283764 4.11935 3.79938e-05 2.07318e-03 AKAP5 1042.005 -0.637445 0.202189 -3.15271 1.61761e-03 3.81572e-02 ... ... ... ... ... ... ... ZNF655 774.2459 -0.910286 0.198632 -4.58277 4.58855e-06 3.52229e-04 ZNF682 59.7573 -1.382049 0.438336 -3.15295 1.61632e-03 3.81572e-02 ZNF77 76.0271 -1.231188 0.388382 -3.17004 1.52417e-03 3.71126e-02 ZRANB3 536.2301 -0.878732 0.179932 -4.88367 1.04128e-06 9.22422e-05 ZSCAN25 1257.3596 -0.460023 0.149161 -3.08408 2.04184e-03 4.57797e-02 [1] \u0026#34;150 vs 33 | padj\u0026lt;0.05\u0026#34; 출처 # Bioconductor - DESeq2 https://bioconductor.org/packages/release/bioc/html/DESeq2.html\nadditional data # 코드에 사용된 데이터 정보는 github에서 확인 가능하다.\n"},{"id":132,"href":"/docs/study/bioinformatics/bi10/","title":"DESeq2 워크플로우","section":"생물정보학","content":" [코드] DESeq2 워크플로우 # Load package # # Input: genome_positions = list of genomic loci with H-scores # H_scores = dict {position: H_score} # Parameters: # MinPts = 5 # eps_scale = 10 # diminish_factor = 3 initialize hotspots = [] # STEP 1. Search Candidate Core Mutation (CCM) for position in genome_positions: H = H_scores[position] Deps = eps_scale * H neighborhood = get_neighbors_within_deps(position, Deps) avg_H = mean([H_scores[n] for n in neighborhood]) sum_H = sum([H_scores[n] for n in neighborhood]) num_mutants = len([n for n in neighborhood if H_scores[n] \u0026gt; 0]) if H \u0026gt; 0.03 and avg_H \u0026gt; 0.01 and sum_H \u0026gt; 0.05 and num_mutants \u0026gt;= MinPts: mark position as CCM # STEP 2. Cluster Expansion for ccm in CCM_list: cluster = [ccm] current_Deps = eps_scale * H_scores[ccm] for direction in [-1, 1]: step = 1 while True: next_pos = ccm + direction * step if next_pos not in genome_positions: break dist_from_ccm = abs(next_pos - ccm) diminishing_Deps = max(1, int(current_Deps - (dist_from_ccm / diminish_factor))) if dist_from_ccm \u0026gt; diminishing_Deps: break if H_scores[next_pos] \u0026gt; 0: cluster.append(next_pos) step += 1 if len(cluster) \u0026gt;= MinPts: hotspots.append(cluster) # Output: hotspots = list of identified mutation hotspot clusters "},{"id":133,"href":"/docs/study/bioinformatics/bi11/","title":"EBV 유전체 RNA-seq 전처리","section":"생물정보학","content":" EBV 유전체 RNA-seq 전처리 # #2024-12-31\n분석 목적\n제공받은 fastq를 human genome에 매핑해서 전처리, 분석 후 DE 결과 보냄 DE 분석시에 EBV 유전자도 포함해달라는 요청 해야하는것\nfastq를 EBV genome에 매핑해서 전처리, EBV count 생성 human count에 EBV count를 붙이기 통합 count로 DE 분석 재수행 1. Alignment # Load package, Set Path # library(edgeR) library(Rsubread) library(org.Hs.eg.db) setwd(\u0026#34;/data/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#34;) getwd() \u0026#39;/data1/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#39; Build Index # # build index ref \u0026lt;- \u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/NC_007605.1.fa\u0026#34; output_basename \u0026lt;- \u0026#34;NC_007605.1_idx\u0026#34; buildindex(basename = output_basename, reference = ref) Feature Count # # feature.count targets \u0026lt;- read.delim(\u0026#34;targets.txt\u0026#34;, header=TRUE) output.files \u0026lt;- c(targets$FileName) fc \u0026lt;- featureCounts(output.files, annot.inbuilt=FALSE, annot.ext = \u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/NC_007605.1.gtf\u0026#34;, isGTFAnnotationFile = TRUE, GTF.featureType = \u0026#34;exon\u0026#34;, GTF.attrType = \u0026#34;transcript_id\u0026#34;, GTF.attrType.extra = NULL, isPairedEnd=TRUE, countReadPairs=TRUE)# nthreads=30) Save # colnames(fc$counts) \u0026lt;- c(\u0026#39;33-1\u0026#39;,\u0026#39;33-2\u0026#39;,\u0026#39;33-3\u0026#39;,\u0026#39;150-1\u0026#39;,\u0026#39;150-2\u0026#39;,\u0026#39;150-3\u0026#39;,\u0026#39;con-1\u0026#39;,\u0026#39;con-2\u0026#39;,\u0026#39;con-3\u0026#39;) head(fc$counts) group \u0026lt;- factor(targets$Status) y \u0026lt;- DGEList(fc$counts, group=group) fc$sort.counts \u0026lt;- fc$counts[order(rownames(fc$counts)), ] sort.counts.df \u0026lt;- as.data.frame(fc$sort.counts) sort.counts.df$trans_id \u0026lt;- rownames(fc$sort.counts) sort.counts.df \u0026lt;- sort.counts.df[, c(\u0026#34;trans_id\u0026#34;, \u0026#34;33-1\u0026#34;, \u0026#34;33-2\u0026#34;, \u0026#34;33-3\u0026#34;, \u0026#34;150-1\u0026#34;, \u0026#34;150-2\u0026#34;, \u0026#34;150-3\u0026#34;, \u0026#34;con-1\u0026#34;, \u0026#34;con-2\u0026#34;, \u0026#34;con-3\u0026#34;)] write.table(sort.counts.df, file = \u0026#34;count.tsv\u0026#34;, sep = \u0026#34;\\t\u0026#34;, quote = FALSE, row.names = FALSE) 2. Annotation # Load Package # import os import pandas as pd Set Path # os.chdir(\u0026#34;/data/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#34;) os.getcwd() \u0026#39;/data1/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#39; Load GTF file # annotation 파일이 없어서 gtf reference 파일을 사용해서 직접 만들어줫다. gtf_df = pd.read_csv(\u0026#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/NC_007605.1.gtf\u0026#34;, sep=\u0026#39;\\t\u0026#39;, header=None, comment=\u0026#34;#\u0026#34;, names=[\u0026#34;seqname\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;feature\u0026#34;, \u0026#34;start\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;score\u0026#34;, \u0026#34;strand\u0026#34;, \u0026#34;frame\u0026#34;, \u0026#34;attribute\u0026#34;]) gtf_df Make annotation file # split_attributes = gtf_df[\u0026#39;attribute\u0026#39;].str.split(\u0026#39;;\u0026#39;) attribute_1 = [] attribute_2 = [] attribute_3 = [] for attributes in split_attributes: attr_1 = \u0026#39;\u0026#39; attr_2 = \u0026#39;\u0026#39; attr_3 = \u0026#39;\u0026#39; for attribute in attributes: attribute = attribute.strip() if \u0026#39;transcript_id\u0026#39; in attribute: attr_1 = attribute elif \u0026#39;gene_id\u0026#39; in attribute: attr_2 = attribute elif \u0026#39;gene_name\u0026#39; in attribute: attr_3 = attribute attribute_1.append(attr_1) attribute_2.append(attr_2) attribute_3.append(attr_3) gtf_df[\u0026#39;attribute.1\u0026#39;] = attribute_1 gtf_df[\u0026#39;attribute.2\u0026#39;] = attribute_2 gtf_df[\u0026#39;attribute.3\u0026#39;] = attribute_3 annot_df = gtf_df.iloc[:, -4:].copy() annot_df[\u0026#39;transcript_id\u0026#39;] = annot_df[\u0026#39;attribute.1\u0026#39;].str.split(\u0026#39;\u0026#34;\u0026#39;).str[1] annot_df[\u0026#39;gene_id\u0026#39;] = annot_df[\u0026#39;attribute.2\u0026#39;].str.split(\u0026#39;\u0026#34;\u0026#39;).str[1] annot_df[\u0026#39;gene_name\u0026#39;] = annot_df[\u0026#39;attribute.3\u0026#39;].str.split(\u0026#39;\u0026#34;\u0026#39;).str[1] annot_df annot_table = annot_df.iloc[:, -3:].copy() annot_table_dedup = annot_table[~annot_table[\u0026#39;transcript_id\u0026#39;].duplicated(keep=\u0026#39;first\u0026#39;)] annot = annot_table_dedup.iloc[:, -3:].copy() annot = annot.sort_values(by=\u0026#39;transcript_id\u0026#39;) annot.head(50) Save # #annot.to_csv(\u0026#34;annot.tsv\u0026#34;,sep=\u0026#39;\\t\u0026#39;,index=False) 나중에 또 써야대니깐 저장. Load Count # count = pd.read_csv(\u0026#34;count.tsv\u0026#34;,sep=\u0026#39;\\t\u0026#39;,encoding=\u0026#39;cp949\u0026#39;) count 위에서 Rsubread로 만든 EBV count 가져옴 Annotation # merged = count.merge(annot, left_on=\u0026#39;trans_id\u0026#39;, right_on=\u0026#39;transcript_id\u0026#39;, how=\u0026#39;left\u0026#39;) res_df = merged.drop(columns=[\u0026#39;trans_id\u0026#39;]) res_df = res_df[[\u0026#39;transcript_id\u0026#39;, \u0026#39;gene_id\u0026#39;, \u0026#39;gene_name\u0026#39;, \u0026#39;33-1\u0026#39;, \u0026#39;33-2\u0026#39;, \u0026#39;33-3\u0026#39;, \u0026#39;150-1\u0026#39;, \u0026#39;150-2\u0026#39;, \u0026#39;150-3\u0026#39;, \u0026#39;con-1\u0026#39;, \u0026#39;con-2\u0026#39;, \u0026#39;con-3\u0026#39;]] res_df res_df[\u0026#39;gene_name\u0026#39;].value_counts() 중복인 transcript가 엄청많음 res_df[\u0026#39;sum\u0026#39;] = res_df.iloc[:, 3:12].sum(axis=1) res_df = res_df.sort_values(by=\u0026#39;sum\u0026#39;, ascending=False) res_df = res_df.drop_duplicates(subset=\u0026#39;gene_name\u0026#39;, keep=\u0026#39;first\u0026#39;) res_df = res_df.iloc[:, :-1] res_df.reset_index(drop=True, inplace=True) res_df = res_df.dropna(subset=[\u0026#39;gene_name\u0026#39;]) print(res_df.shape) res_df (63, 12)짜리에서 deduplication 결과 (15, 12)가 됏다. res_df.to_csv(\u0026#34;count.annot.tsv\u0026#34;, sep=\u0026#39;\\t\u0026#39;,index=False) Merge # count_hs = pd.read_csv(\u0026#34;/data/home/ysh980101/2311/RNA-seq/count.csv\u0026#34;, encoding=\u0026#39;cp949\u0026#39;) count_hs.columns = [\u0026#34;transcript_id\u0026#34;, \u0026#34;gene_id\u0026#34;, \u0026#34;gene_name\u0026#34;, \u0026#34;33-1\u0026#34;, \u0026#34;33-2\u0026#34;, \u0026#34;33-3\u0026#34;, \u0026#34;150-1\u0026#34;, \u0026#34;150-2\u0026#34;, \u0026#34;150-3\u0026#34;, \u0026#34;con-1\u0026#34;, \u0026#34;con-2\u0026#34;, \u0026#34;con-3\u0026#34;] combined_df = pd.concat([res_df, count_hs], axis=0, ignore_index=True) combined_df combined_drop_df = combined_df[~(combined_df.iloc[:, 4:13] == 0).all(axis=1)] #remove 0 in all samples combined_drop_df combined_drop_df.to_csv(\u0026#34;count.annot.combined.drop.tsv\u0026#34;,sep=\u0026#39;\\t\u0026#39;,index=False) 3. DEG Analysis # Load Packages # suppressMessages({ library(\u0026#34;DESeq2\u0026#34;) library(pheatmap) library(withr) library(RColorBrewer) library(gplots) library(ggplot2) library(dplyr) }) Set Path # setwd(\u0026#34;/data/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#34;) getwd() \u0026#39;/data1/home/ysh980101/2311/RNA-seq_ebv/Rsubread\u0026#39; Run DEG # S1 \u0026lt;- \u0026#39;33\u0026#39; S2 \u0026lt;- \u0026#39;150\u0026#39; countdata \u0026lt;- read.csv(\u0026#34;count.annot.combined.drop.tsv\u0026#34;, header=TRUE, sep=\u0026#39;\\t\u0026#39;) colnames(countdata) \u0026lt;- c(\u0026#39;transcript_id\u0026#39;,\u0026#39;gene_id\u0026#39;,\u0026#39;gene_name\u0026#39;,\u0026#39;33-1\u0026#39;,\u0026#39;33-2\u0026#39;,\u0026#39;33-3\u0026#39;,\u0026#39;150-1\u0026#39;,\u0026#39;150-2\u0026#39;,\u0026#39;150-3\u0026#39;,\u0026#39;con-1\u0026#39;,\u0026#39;con-2\u0026#39;,\u0026#39;con-3\u0026#39;) countdata \u0026lt;- countdata[, c(\u0026#39;transcript_id\u0026#39;,\u0026#39;gene_id\u0026#39;,\u0026#39;gene_name\u0026#39;,\u0026#39;150-1\u0026#39;,\u0026#39;150-2\u0026#39;,\u0026#39;150-3\u0026#39;,\u0026#39;33-1\u0026#39;,\u0026#39;33-2\u0026#39;,\u0026#39;33-3\u0026#39;,\u0026#39;con-1\u0026#39;,\u0026#39;con-2\u0026#39;,\u0026#39;con-3\u0026#39;)] countdata \u0026lt;- countdata[, paste(c(\u0026#39;gene_name\u0026#39;,paste0(S2,\u0026#34;-1\u0026#34;), paste0(S2,\u0026#34;-2\u0026#34;), paste0(S2,\u0026#34;-3\u0026#34;),paste0(S1,\u0026#34;-1\u0026#34;), paste0(S1,\u0026#34;-2\u0026#34;), paste0(S1,\u0026#34;-3\u0026#34;)), sep=\u0026#34;\u0026#34;)] dim(countdata) 16409 * 7 sample.names \u0026lt;- paste(c(paste0(S2,\u0026#34;-1\u0026#34;), paste0(S2,\u0026#34;-2\u0026#34;), paste0(S2,\u0026#34;-3\u0026#34;),paste0(S1,\u0026#34;-1\u0026#34;), paste0(S1,\u0026#34;-2\u0026#34;), paste0(S1,\u0026#34;-3\u0026#34;)), sep=\u0026#34;\u0026#34;) conditions \u0026lt;- factor(c(S2,S2,S2,S1,S1,S1)) metadata \u0026lt;- data.frame(Sample = sample.names, group = conditions) N \u0026lt;- dim(countdata)[[2]] cData = countdata[,2:N] GeneID = countdata[,1] rownames(cData) = GeneID dds \u0026lt;- DESeqDataSetFromMatrix(countData = cData, colData = metadata, design = ~group) dds$group \u0026lt;- relevel(dds$group, ref = S1) dds \u0026lt;- DESeq(dds) vsd \u0026lt;- vst(dds, blind = FALSE) rld \u0026lt;- rlogTransformation(dds, blind = FALSE) res \u0026lt;- results(dds, contrast = c(\u0026#34;group\u0026#34;, S2, S1)) res_tbl \u0026lt;- as.data.frame(res) res_tbl$GeneID \u0026lt;- rownames(res_tbl) res_tbl \u0026lt;- res_tbl[order(res_tbl$padj), ] NM_no_NA \u0026lt;- na.omit(res) res_cut \u0026lt;- NM_no_NA[NM_no_NA$padj \u0026lt; 0.05, ] lfc \u0026lt;- 1.5 res_cut_2 \u0026lt;- res_cut[abs(res_cut$log2FoldChange) \u0026gt; lfc, ] res_cut_2_pos \u0026lt;- res_cut_2[res_cut_2$log2FoldChange \u0026gt; 0, ] res_cut_2_neg \u0026lt;- res_cut_2[res_cut_2$log2FoldChange \u0026lt; 0, ] # Volcano Plot par(mfrow = c(1, 1)) with(NM_no_NA, plot(log2FoldChange, -log10(pvalue), pch = 20, col = \u0026#39;gray\u0026#39;, main = paste0(\u0026#39;Volcano plot_between\\n\u0026#39;, S2, \u0026#39; vs \u0026#39;, S1), xlim = c(-3, 3))) abline(h = -log10(0.05), v = c(-lfc, lfc), col = \u0026#34;gray\u0026#34;, lty = 2) with(res_cut_2_neg, points(log2FoldChange, -log10(pvalue), pch = 20, col = \u0026#39;blue\u0026#39;)) with(res_cut_2_pos, points(log2FoldChange, -log10(pvalue), pch = 20, col = \u0026#39;gold\u0026#39;)) legend(\u0026#34;topright\u0026#34;, legend = c(paste0(\u0026#34;FC ≥ \u0026#34;, lfc, \u0026#34; \u0026amp; p \u0026lt; 0.05\u0026#34;), paste0(\u0026#34;FC ≤ -\u0026#34;, lfc, \u0026#34; \u0026amp; p \u0026lt; 0.05\u0026#34;)), col = c(\u0026#34;gold\u0026#34;, \u0026#34;blue\u0026#34;), pch = 20) # Bar Plot lengths \u0026lt;- c(nrow(res_cut_2_neg), nrow(res_cut_2_pos)) bp \u0026lt;- barplot(lengths, space = 0, col = c(\u0026#34;blue\u0026#34;, \u0026#34;gold\u0026#34;), horiz = TRUE, xlab = \u0026#34;Count of genes\u0026#34;, ylab = paste0(S2, \u0026#34;/\u0026#34;, S1), xlim = c(0, sum(lengths)), width = 0.02, las = 1) title(paste0(\u0026#34;UP, DOWN regulated count (|FC| ≥ \u0026#34;, lfc, \u0026#34;)\u0026#34;)) legend(\u0026#34;topright\u0026#34;, legend = c(\u0026#34;UP\u0026#34;, \u0026#34;DOWN\u0026#34;), fill = c(\u0026#34;gold\u0026#34;, \u0026#34;blue\u0026#34;)) text(lengths, bp, labels = lengths, pos = 4, cex = 0.9) # Save cutoff \u0026lt;- 0.05 sig_res \u0026lt;- dplyr::filter(res_tbl, padj \u0026lt; cutoff) %\u0026gt;% dplyr::arrange(padj) %\u0026gt;% dplyr::select(GeneID, baseMean, log2FoldChange, lfcSE, stat, pvalue, padj) write.csv(sig_res, file = paste0(\u0026#39;../DESeq2/res.\u0026#39;, S2, \u0026#39;vs\u0026#39;, S1, \u0026#39;.csv\u0026#39;), row.names = FALSE) cf) Heatmap 시각화\n# Heatmap sig_idx \u0026lt;- res$padj \u0026lt; cutoff \u0026amp; !is.na(res$padj) sig_dat \u0026lt;- assay(rld)[sig_idx, ] annC \u0026lt;- data.frame(condition = conditions) rownames(annC) \u0026lt;- colnames(sig_dat) ann_colors \u0026lt;- list(condition = setNames(c(\u0026#34;#F7819F\u0026#34;, \u0026#34;#58D3F7\u0026#34;), c(S2, S1))) pheatmap(sig_dat, scale = \u0026#34;row\u0026#34;, fontsize_row = 9, annotation_col = annC, color = rev(brewer.pal(6, \u0026#34;RdYlGn\u0026#34;)), annotation_colors = ann_colors, show_rownames = FALSE, show_colnames = TRUE) "},{"id":134,"href":"/docs/study/bioinformatics/bi16/","title":"EndNote 사용법","section":"생물정보학","content":" EndNote 사용법 # #2024-12-31\n1. EndNote 설치 및 계정 설정 # 계정 설정: 공식 웹사이트에서 End note 계정을 생성한다.\n설치: 나의 경우 여기에서 다운로드해줬다.\n2. 레퍼런스 추가 방법 # Google Scholar에 논문 제목을 검색해서 인용\u0026gt;EndNote를 클릭하면 .enw 파일이 다운로드된다. 3. 레퍼런스 관리 # Endnote에 접속한다. Collect\u0026gt;Import References로 들어간다 파일 선택\u0026gt;아까 저장한 .enw 파일을 선택해준다 Import Option\u0026gt;EndNote Import를 선택해준다 To\u0026gt;New Group을 하면 논문 주제별로 그룹을 생성하여 정리 가능. 생성한 그룹이 이미 있으면 원하는 그룹 선택해준다. Import 해준다 2025EMM_Mutclust 그룹에 45개 레퍼런스를 넣었고 이렇게 뜬다!\n4. Word에서 레퍼런스 인용 # 위에서 EndNote를 설치해줬다면 Word의 상단 탭에 EndNote가 뜬다. 레퍼런스를 넣고싶은 자리에 커서를 두고 EndNote\u0026gt;Insert Citation을 선택해준다. 넣고싶은 논문의 제목 일부나 저자 이름을 넣고 검색\u0026gt;Insert 해준다. 47번째 줄에 성공적으로 레퍼런스가 달렸다! 본문 맨아래를 확인하면 citation도 자동으로 달려있다.\ncf) 만약 citation 형식을 바꾸고 싶으면 Select Other Style로 들어가서 형식을 바꿔주면 된다.\n나는 Last name(full name) \u0026gt; First name(약어) \u0026gt; 제목 \u0026gt; 저널(기울임체) \u0026gt; 버전(bold) \u0026gt; 페이지, 발행년도 순으로 나오고 / 6명 이상인 경우 주저자 1명만 + et al.로 표기되는 형식을 써줘야했고 Nature immunology 포맷을 사용해줬다.\n참고 자료 # EndNote 활용 가이드 https://library.korea.ac.kr/wp-content/uploads/2020/03/EndNote_X9_manualKorean.pdf\n"},{"id":135,"href":"/docs/study/tech/cs2/","title":"Favicon 변경, Giscus 댓글창 추가","section":"생물정보학","content":" Favicon 변경, Giscus 댓글창 추가 # #2024-12-31\nFavicon 변경 # Hugo-book 테마의 github에서 README 파일을 읽어보면, logo와 favicon 이미지의 경로 정보를 찾을 수 있다.\n(logo 정보) (favicon 정보) 확인 결과 static 디렉토리에 각각 logo.png, favicon.png로 저장해두면 반영되는것 같다.\n참고로 Hugo-book 테마의 오리지널 웹사이트는 아래와 같이 디자인되어있고\n최상단 탭에 들어가는 이미지가 logo.png, 블로그 이름 옆에 들어가는 이미지가 favicon.png이다.\n먼저 static 디렉토리에 넣고 싶은 로고와 파비콘을 logo.png, favicon.png 로 저장해준다.\n다음으로, hugo.toml 파일을 열어 아래 내용을 추가해준다.\n# (Optional, default none) Set the path to a logo for the book. If the logo is # /static/logo.png then the path would be \u0026#39;logo.png\u0026#39; BookLogo = \u0026#39;logo.png\u0026#39; 블로그를 들어가보면 설정한 로고와 파비콘이 잘 들어간것을 확인할 수 있다.\nGiscus 댓글창 추가 # Giscus 댓글 시스템을 Hugo 기반 블로그에 연동하기 위해서는 Giscus에 블로그 리포지토리를 연결한 후, js script를 작성하여 블로그 리포지토리의 layouts 디렉토리에 저장하면 된다고 한다.\n이때 연결할 리포지토리는 다음 3가지 조건을 만족해야 한다.\nPublic이어야 함. giscus 앱이 설치되어 있어야 함. Discussions 기능이 해당 저장소에서 활성화되어 있어야 함. 공개 저장소 확인\n블로그 리포지토리의 Settings \u0026gt; General의 맨 하단을 보면 Danger Zone에서 public인지 private인지 확인이 가능하다.\npublic이므로 다음으로 넘어간다.\nGiscus 앱 설치\nhttps://github.com/apps/giscus 에 접속하여 install, configure를 진행하면 쉽게 설치된다.\nRepository access는 All repositories 로 설정했다.\nDiscussion 기능 활성화\n블로그 리포지토리의 Settings \u0026gt; General을 스크롤해보면 Discussions 체크박스가 생긴 것을 확인할 수 있다. 이를 체크해준다.\n위로 스크롤해보면 상단에 Discussions 탭이 생겼다.\n이제 블로그 리포지토리가 Giscus에 연결할 3가지 조건을 만족하였고 블로그를 Giscus로 연결해주면 된다. 연결해주려면 아래 형식의 js 스크립트를 작성하여 layouts/partials/comments.html에 추가해주면 된다.\njs 스크립트는 https://giscus.app/ko에서 파라미터를 선택하면 적절하게 생성해준다!\n\u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;yshghid/yshghid.github.io\u0026#34; data-repo-id=\u0026#34;R_kgDONkMkNg\u0026#34; data-category-id=\u0026#34;DIC_kwDONkMkNs4CloJh\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 해당 내용을 복사해서 블로그 리포지토리의 layouts/partials/docs/comments.html로 생성해주었다.\n성공적으로 댓글창이 추가되었다!!\n참고한 블로그 # https://parker1609.github.io/post/creating-my-blog-with-hugo/\n"},{"id":136,"href":"/docs/hobby/book/book37/","title":"summer","section":"글","content":" summer # #2024-12-31\n#1\n그는 뭘 하면서 지내느냐고 물었다. 테니스. 수영. 밤에 시내로 놀러 나가기. 조깅. 편곡. 독서.\n#2\n모든 것이 올리버가 우리 집에 온 그 여름에 시작되었다. 그것들은 그해 여름에 유행한 곡과 그가 머무는 동안 그리고 떠난 후에 읽은 책들, 뜨거운 날의 로즈메리 냄새부터 오후의 요란한 매미 소리까지 모든 것에 새겨졌다. 여름마다 접해서 익숙해진 냄새와 소리들이 갑자기 나에게 달려들었고, 그 여름의 사건들로 영원히 다른 색조를 띠게 되었다.\n#3\n그는 유대인이라는 사실에 만족했다. 자기 자신에게도 만족했다. 자신의 몸, 얼굴, 특이한 테니스 백핸드, 책과 음악, 영화, 친구를 고르는 취향에도 만족했다. 아끼는 몽블랑 만년필을 잃어버렸을 때도 대수롭지 않게 여겼다. \u0026ldquo;다시 사면 돼.\u0026rdquo; 비판을 받아도 아무렇지 않았다. 언젠가 그 스스로 자랑스럽게 생각하는 원고 몇 장을 우리 아버지에게 보여준 적이 있었다. 아버지는 헤라클레이토스에 대한 통찰은 훌륭하지만 강화할 필요가 있다고 지적했다. 헤라클레이토스라는 철학자의 사상을 그냥 설명하려 하지 말고 모순된 특징이 있음을 받아들여야 한다고. 그는 강화할 필요가 있다는 것도, 모순도 아무렇지 않아 했다. 방향을 처음부터 다시 잡아야 한다는 사실도 괜찮았다.\n그가 우리 집에 온 지 며칠 되지 않았을 때 딱 한번, 의지가 강하지만 호의적이고 느긋하고 흔들림 없으며 당황할 줄 모르는 데다 삶의 많은 것을 대수롭지 않게 여기는 이 스물 넷의 남자가 사실은 사람과 상황을 판단할 때는 철저하게 경계하고 냉정하며 현명하다고 느낀 일이 있었다. 그의 말과 행동은 미리 계획되지 않은 게 하나도 없었다. 그는 모두를 꿰뚫어 보았다. 그가 사람을 정확하게 꿰뚫어 볼 수 있는 것은 타인을 바라볼 때 자신의 내면에서 남들이 보지 말았으면 하는 부분을 가장 먼저 보기 때문이었다.\n#4\n올리버는 어울릴 사람을 원했다. 처음에는 내 테이블을 함께 쓰더니 나중에는 잔디밭에 커다란 담요를 깔고 누워 있는 걸 좋아했다. 옆에는 아무렇게나 펼쳐진 원고와 그가 \u0026lsquo;잡동사니\u0026rsquo;라고 부르는 것들이 놓여 있었다. 레모네이드, 선크림, 책, 에스파듀, 선글라스, 색연필. 그리고 헤드폰을 쓰고 음악을 들었기에 그가 먼저 말을 걸지 않는 이상 말을 걸 수가 없었다. 아침에 작곡 노트나 책을 들고 아래층으로 내려가면 빨간색이나 노란색 수영복 차리므이 그가 벌써 태양 아래 땀을 흘리며 누워 있기도 했다. 함께 조깅이나 수영을 하고 돌아오면 아침 식사가 기다렸다.\n그는 \u0026lsquo;집동사니\u0026rsquo;를 잔디밭에 그대로 둔 채 수영장 바로 옆에 누워 있는 버릇이 생겼다. 그가 \u0026lsquo;여긴 천국이야\u0026rsquo;를 줄여서 \u0026lsquo;천국\u0026rsquo;이라고 부르는 자리였다. 점심을 먹고 나서 라틴어 학자들만의 농담으로 \u0026ldquo;전 이제 천국에 갑니다. 일광욕하러.\u0026ldquo;라고 덧붙이곤 했다. 우리는 그가 수영장가 똑같은 자리에서 선탠오일을 듬뿍 바른 채 몇 시간이고 누워 있다며 놀렸다. 어미니는 \u0026ldquo;오늘 아침에는 천국에 얼마나 있었어요?\u0026ldquo;라고 물었다. \u0026ldquo;두 시간 연속이요. 오늘은 일찍 돌아와서 더 오랫동안 일광욕을 하려고요.\u0026rdquo; 그에게 천국의 가장자리에 간다는 것은 수영장가에 누워서 한쪽 다리는 물에 담그고 헤드폰을 쓰고 얼굴은 밀짚모자로 가리고 있겠다는 뜻이기도 했다.\n무엇 하나 부족할 게 없는 사람이었다. 나는 그 느낌을 이해할 수가 없었다. 그가 부러웠다.\n#5\n\u0026ldquo;마르지아랑 거의 할 뻔했어요.\u0026rdquo; 다음 날 아침을 먹으면서 아버지와 올리버에게 말했다. 아무튼 나는 과시하는 중이었다.\n\u0026ldquo;나중에 다시 해 봐.\u0026rdquo; 올리버가 말했다. 무심한 사람들이 하는 말이었다. 하지만 그가 속마음이 따로 있으며 드러내지 않을 거라는 느낌도 들었다. \u0026lsquo;나중에 다시 해 봐라는 바보 같지만 좋은 의도로 한 말 이면에서 약간의 동요가 느껴졌기 때문이다. 그는 나를 비난하고 있었다. 또는 놀리거나. 아니면 꿰뚫어 보거나.\n그가 마침내 속마음을 드러내자 나는 감정이 상해 버렸다. 나를 완전히 간파한 사람만 할 수 있는 말이었다. \u0026ldquo;나중이 아니면 언제?\u0026rdquo; 올리버는 말이 심했다고 생각했는지 곧바로 덧붙였다. \u0026ldquo;나라면 당연히 다시 해 볼거야. 그리고 또다시 해 볼 거고.\u0026rdquo; 좀 누그러진 표현이기는 했다. 하지만 그는 *\u0026lsquo;나중에 다시 해 봐\u0026rsquo;*라는 베일로 *\u0026lsquo;나중이 아니면 언제?\u0026rsquo;*를 가린 것 뿐이었다.\n#6\n나중에 다시 시도한다는 것은 당장은 용기가 없다는 뜻이었다. 아직 준비되지 않은 것 뿐이었다. 다시 시도해 볼 용기와 의지는 어디세서 찾아야 하는지 알 수 없었다. 하지만 가만히 앉아 있지 말고 뭔가 해야겠다고 결심하면 벌써부터 뭔가 하는 듯한 기분이었다. 있지도 않은 돈으로, 투자하지도 않은 돈으로 수익을 거두는 것처럼.\n하지만 *\u0026lsquo;나중에 다시 해 봐야지\u0026rsquo;*하는 방어적인 태도로 일관하며 살아왔다는 사실도 잘 알고 있었다. 매일 *\u0026lsquo;나중에 다시 해 봐야지\u0026rsquo;*하면서 한 달, 한 계절, 한 해 또는 평생을 보낼 수도 있었다. 나중에 다시 해 보는 것은 올리버 같은 사람에게나 맞았다. 나 같은 사람한테는 *\u0026lsquo;나중이 아니면 언제?\u0026rsquo;*가 더 어울렸다.\n나중이 아니면 언제? 그가 이 말로 나를 간파했고 내 비밀을 하나씩 벗겼다면? 그에게 전혀 관심이 없다는 사실을 알려 줄 필요가 있었다.\n책 콜 미 바이 유어 네임\n"},{"id":137,"href":"/docs/hobby/book/book23/","title":"결핍과 그에 대한 애도의 기간(라디오스타 김영철)","section":"글","content":" 결핍과 그에 대한 애도의 기간(라디오스타 김영철) # #2024-12-31\n1 # https://youtu.be/Qa8zJkZlDF0 은 참 생각지도 못한 순간에, 생각지 못한 계기로 끝나네\n2 # 겨울 아침의 어스름 속에, 아빠 옆에 있는 빛바랜 파란색 소파에 놓인 무언가가 보인다. 엄마가 출근하기 전에 꺼내놓고 간 아빠 웃옷과 바지다. 아빠는 이상한 고집이 있다. 아무리 몸이 아파도 의사를 만나려면 꼭 옷을 제대로 챙겨 입고 가야 한다. 엄마가 출근하러 나갈 때까지만 해도 아빠는 침대에 누워 있었다. 지난주 내내 거의 누워 있었다. 어깨 뒤쪽에 통증이 워낙 심해 대로 숨 쉬기가 어려웠다. 하지만 의사에게는 그런 말을 하지 않았다. 의사는 여기저기 눌러보더니 심장병 이력이 있는 걸 알면서도 디스크일 거라고 했다. 의사가 쉬라고 해서 아빠는 일주일간 쉬었다. 그러다가 오늘은 겨우 일어나 병원에 진단서를 떼러 갈 참이다. 옷을 다 입고 나니, 마지막 차 한 잔을 탈 주전자 물이 끓고 있다.\n벽난로 앞 바닥에 쓰러져 있는 아빠를 사람들이 발견했을 때, 주전자 물은 이미 다 졸아 없이진 지 오래였다.\n아버지가 돌아가신 후 나는 처음엔 끔찍한 죄책감에 휩싸였다. 아버지에게 가봤어야 했다. 그랬다면 아버지를 살릴 수 있었다. 아버지는 옷을 입고 병원에 갈 준비를 하다가 심장마비로 쓰러졌다. 나는 의사가 되려고 5년을 공부했지만 결국 내 아버지도 살리지 못했다. 지금까지 공부한 게 다 무슨 소용인가? 나라는 인간은 무슨 쓸모가 있나? 그러나 끔찍한 죄책감도 가슴 찢어지는 애통함도, 그리 오래가지 않았다. 내가 떨쳐버렸다. 가슴 깊숙이 어딘가서 꾹꾹 눌러 담고 묻어버렸다. 나는 제대로 애통해하지 못했다. 그것도 아주 오랫동안.\n나는 아버지가 돌아가신 후 오랫동안 아버지를 잃었다는 사실을 받아들이지 못했다. 그리고 언제까지나 아버지를 그리워할 것이다. 애통해한다는 것은, 놓아주고 앞으로 나아가는 것이다. 애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다. 이상화된 성자도, 분노와 실망을 쏟아부을 표적도 아닌, 복잡하고 현실적이면서 매우 인간적인 존재로.\n내가 가진 아빠 사진은 한 장뿐이다. 내가 집을 떠나 대학에 가기 얼마 전에 찍은 사진이다. 아빠는 구겨진 셔츠 차림으로 서서 한 팔을 엄마 어깨에 두르고 있고, 엄마는 아빠 손을 꼭 잡아 허리에 붙인 모습니다. 나는 아빠 왼쪽으로 살짝 뒤에 서서 해를 쏘아보고 있고, 동생 이언은 우리 앞에 서 있다. 앨런은 아마 카메라를 들고 있었을 것이다. 아빠는 마치 우리가 모르고 있는 비밀을 알고 있기라도 한 듯 묘한 미소를 엷게 짓고 있다. 엄마는 방금 전까지 다들 싸우기라도 한 듯 억지스러운 미소를 활짝 짓고 있다. 세월이 흐르면서 사진도 점점 빛이 바래 흑백에 가까워져가고, 내 애통한 마음도 흐릿해져간다. 지금은 알 수 있다. 나라는 사람은 결국 아빠가 아니면 아무것도 아니었다는 것을. 아빠는 말로 표현하지 않았지만 행동으로 내게 변치 않는 사랑의 힘을 가르쳐주었고, 내가 지금 모습이 될 수 있게 도와주었다.\n중요한 건 애통한 마음의 변화라고 생각한다. 예컨대 상실의 기억을 떠올릴 때 15년 전이나 지금이나 똑같이 괴롭고 아픔이 생생하다면 진전이 없는 것이다. 감정이 잦아들지 않고 점점 커진다면 그 역시 심각한 신호다. 애도가 제대로 이루어지지 못하면 우울증이 된다. 애통한 마음의 크기를 1에서 10까지의 숫자로 생각해볼 때 그날그날 아주 미미하게라도 줄어들고 있다면, 앞으로 나아가고 있다는 신호다. 조금씩 다시 일상을 마주하고 앞날을 바라보고 있는 것이다. 지나간 일을 조금씩 손에서 놓아가는 것이다.\n3 # 내가 가장 좋아하는 장이 15장 \u0026lt;애도\u0026gt; 였는데 그래서인지 김영철 토크 영상을 보고 눈물이 많이 났다\n책: 당신의 특별한 우울\n"},{"id":138,"href":"/docs/hobby/book/book3/","title":"결혼과 행복","section":"글","content":" 결혼과 행복 # #2024-12-31\n1 # 결혼은 안해도 되는데 한 사람들이 더 행복함.\n사랑이 있든 없든 간에 정신적, 육체적으로 한 명의 남편 혹은 부인에게 초점을 맞추고 가족, 친구, 이웃, 나아가 잠깐 만나는 캐주얼한 섹스 파트너와 전남편 혹은 전 부인까지 양파 껍질처럼 차곡차곡 쌓인 울타리를 만듦으로써 우리 삶은 안정되고 행복해질 수 있다.\n2 # https://youtu.be/vFN_DoqWAL4 우리는 종종 너무나 단순한 걸 놓치고 허우적대곤 하지만요.\n행복한 인생을 살고 싶다면, 성공한 인생을 살고 싶다면, 공략법은 정말로 \u0026lsquo;유일\u0026rsquo;합니다.\n정말 열심히 운동하고 정말 열심히 일하고 정말 열심히 배워서..\n연인과, 친구와, 가족과, 동료를 정말 열심히..\n사랑하는 거죠.\n3 # 내적인 건강만이 열풍처럼 불어 닥친 신경쇠약의 유행으로부터 나를 지켜줄 것입니다. 이 질환은 처음 시작되었던 때와 마찬가지로 사라질 때도 순식간일 것입니다.\n먼저 세 가지 육체적 수단은 충분한 수면과 신선한 공기, 적절한 운동, 그리고 알콜과 육식의 섭취를 줄이라는 것입니다. 두 가지 정신적 수단은 보다 지적인 향상을 바라는 나에 대한 믿음과 지성에 대한 사랑입니다. 이 다섯 가지 외엔 신경쇠약증을 치료할 만한 다른 수단은 없습니다.\n책: 지적 생활의 즐거움\n"},{"id":139,"href":"/docs/hobby/book/book24/","title":"공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다.","section":"글","content":" 공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다. # #2024-12-31\n1 # 카드 다섯 장을 쥐고 하는 포커판에서 나올 수 있는 카드패에는 2,598,960개 종류가 있다고 한다. 즉, 최고의 카드패를 쥘 사람은 약 260만명 중의 한 명이다. 하지만 포커에서 그런 카드패를 갖고 있지 않아도 당신은 이길 수 있다. 그저 포커 게임에 참석한 사람들보다 조금 더 좋은 패를 갖고 있으면 된다. 그러므로 최고의 카드를 받은 잘난 사람들은 무시해라. 그들의 포커판에는 비슷한 사람들이 몰려 있다.\n현재의 위치에서 미래를 미리 계산하여 보고 미리 포기하는 사람들이 당신 주변 사람들이며 그들은 그저 일확천금을 꿈꾸면서 연예인이나 정치인, 스포츠 선수들, 컴퓨터 게임, 채팅, 명품 브랜드, 경마 등에 무지 관심이 많다. 당신이 하는 게임은 바로 그런 사람들과 하는 것이다. 기억하라. 이것 역시 당신에게는 춤을 추고 싶을 정도로 너무나도 기쁘고 다행한 사실이라는 것을.\n\u0026lt;미래의 결단\u0026gt;, \u0026lt;자본주의 이후의 사회\u0026gt; 등으로 우리에게 잘 알려진 미국 미래학의 거두 피터 드러커 역시 높은 성과를 올리는 생산적인 사람, 끊임없이 혁신을 꾀하면서 계속 발전하는 사람, 다른 사람에게 영향을 미치는 비중 있는 사람, 그런 사람이 되는 길은 오직 지속적인 관리와 노력밖에 없다고 말한다. 나도 그의 말에 동의한다.\n부자가 되는 데 있어서 경쟁자는 결국 천재가 아니라 자기 자신이다. 이 지극히 간단한 사실이 독자들 마음 속에 각인되기를 바란다.\n2 # \u0026ldquo;실패를 심각하게 생각하지 말라. 주말에는 교외로 나가 신선한 자연을 벗하라. 일에 쫓기지 말라. 오늘 못한다고 내일 세상이 무너지는 일이란 없다. 긴장을 풀고 살아라. 경쟁심을 버려라. 그들은 그들이고 당신은 당신이다. 실력과 능력이 다가 아니다. 인생은 결과가 아니라 과정이 중요하다. 건강을 생각하며 운동을 하라. 운동은 당신이 생각하는 그 어떤 일보다 중요한 것이다. 자주 친구들과 만나 웃고 떠들며 놀아라. 그것이 정신 건강에 좋다. 느긋하게 천천히 살아라. 그것이 스트레스를 피하는 길이다.\u0026rdquo; 이런 조언에 충실히 따르며 살아간다면 장담하건데 몇년 후에 건강한 신체를 갖게 될는지는 모르겠지만 아마도 하고 있는 일은 망한지 오래이거나, 아니면 직장에서 이미 해고되어 구직 이력서를 서너 통 언제나 준비하여 갖고 다니는 몸 튼튼한 실업자가 되어 있을 것이다. 그래도 건강이 최고라고? 건강을 잃으면 모든 것을 다 잃는다고? 맞는 말이기는 하지만 그렇다고 해서 건강을 지키면 모든 것을 다 갖게 된다는 말은 아니지 않는가.\n왜 스트레스가 생기는가? 어떤 문제가 발생하기 때문이다. 그 문제는 어디서 발생하는 것인가? 일이나 인간관계에서 발생한다. 스트레스는 일이나 인간관계에서 발생한 문제가 풀리지 않아서 생기는 것이다. 왜 문제가 안 풀리는 것일까? 푸는 방법을 모르기 때문이다. 왜 모르는가? 책도 안 읽고 공부도 안 하기 때문이다.\n왜 공부를 스스로 안 하는가? 게으르기 때문이며 스스로의 판단과 생각을 우물 안 개구리처럼 최고로 여기기 때문이다. 한 달에 책 한 권도 안 보고 공부는 학원이나 학교에 가야만 하는 걸로 믿는다. 그러면서도 놀 것은 다 찾아다니며 논다. 그런 주제에 자기는 성실하게 열심히 살아가는데 주변 상황 때문에 스트레스를 받는다고 생각하며 그러면서도 수입이 적다고 투덜투덜댄다.\n문제가 있으면 문제를 해결하려고 덤벼드는 것이 올바른 태도이다. 문제는 그대로 남겨둔 채 그 문제로 인하여 생긴 스트레스만을 풀어 버리려고 한다면 원인은 여전히 남아있는 셈 아닌가. 휴식을 충분히 갖고 쉬라고? 웃으로고? 한 달을 바닷가 해변에서 뒹굴어 보아라. 백날을 하하 호호 웃어 보아라. 문제가 해결되는가? 웃기는 소리들 그만해라.\n기억하라. 제초제를 뿌리는 이유는 뿌리를 죽이기 위함이다. 뿌리를 살려 두는 한 잡초는 다시 살아난다. 스트레스를 없애는 가장 정확한 방법 역시 스트레스를 주는 문제의 원인을 파악하고 그 원인을 뿌리채 뽑아 버리는 것이다. 장담하건대 그 모든 원인은 일이나 인간관계에서 발생한 문제를 어떻게 해결하여야 하는지 모르는 당신의 무지 그 자체이다. 즉, 외부적 상황 때문에 스트레스가 생기는 것이 아니라 그 외부 상황을 어떻게 해야 헤쳐나가는지를 모르고 있는 당신의 두뇌 속 무지 대문에 생긴다는 말이다.\n그리고 그 무지함의 뿌리는 바로 게으름이다. 스트레스를 해소한답시고 빈 맥주병을 쌓아가지 말고 문제를 정면으로 돌파하라. 절대 회피하지 말라. 책을 읽고 방법론을 찾아내라. 그게 바로 스트레스를 없애는 제초제이다.\n3 # 결국 스트레스는 문제를 해결하면 없어지는데 해결책을 찾는 법은?\n아인슈타인은 \u0026ldquo;많은 문제가 무의식중에 해결된다\u0026quot;고 하고, \u0026ldquo;말이 아닌 이미지로 대부분 문제를 해결해 냈다\u0026rdquo;, \u0026ldquo;쓰거나 말하는 단어나 언어는 내 생각의 메커니즘에서 아무 역할도 하지 않는 것 같다. 생각의 요소를 받쳐 주는 듯 보이는 어떤 영적 존재들은 어떤 신호이거나 정도의 차이가 있기는 하나 분명한 이미지들인데 그것들은 스스로 반복되어 나타나기도 하고 결합되어 나타날 수도 있다\u0026rdquo;\n내가 문제 해결을 위해 꽤 오랫동안 사용하여 온 것은 인식 상태에서 미인식 영역을 건드리는 방식이다. 첫째, 샤워장 앞에서 옷을 벗을 때부터 두 눈을 감고 움직이며 샤워를 마칠 때까지 계속 눈을 감고 진행한다. 그렇게 함으로써 평상시에 사용되지 않았던 신경과 감각이 일어나 마인드브레인의 전선들이 재배치되도록 한다. 둘째, 인식 상태에서 들어 본 적 없는 음악 소리를 듣는 것이다. 비록 파리넬리의 노래나 파가니니의 연주를 들으면서 의식을 잃고 졸도한 사람들이 있었다고는 하지만 클래식으로는 안 된다. 최초로 시도했던 것은 아이언 버터플라이Iron Butterfly의 In-A-Gadda-Da-Vida(라이브가 아닌 1968년 스튜디오 녹음)였고 핑크 플로이드Pink Floyd의 Echoes(1971년)가 그 뒤를 이었다가 탠저린 드림Tangerine Dream의 Phaedra(1974년), Rubycon과 Ricochet(1975년), Stratosfear(1976년), Force Majeure(1979년), Tangram(1980년), Logos(1982년) 등을 들었는데 각각 그 음반들이 발표되고 나서 몇 년 후에야 비로소 입수할 수 있었다. 유행가도 아니고 상당히 긴 그런 음악 소리(들어 보면 내가 왜 음악이라고 하지 않고 소리라고 하는지 알게 될 것이고 In-A-Gadda-Da-Vida는 중간 부분만 그렇다)를 듣다가 번쩍 힌트가 스쳐 가는 경험을 나는 아주 많이 했었기에, 적어도 나에게는 그 음악 소리들이 앞에서 설명한 만트라가 되어 전선 재배치를 도와주었다고 믿는다. 시도하여 보아라. 눈을 감고 편안한 자세로 크게 들어야 하며 운전 중에는 절대 듣지 말아라(예전에 지인이 운전 중에 듣다가 사고를 낼 뻔했다고 들었다. 탠저린 드림의 80년대 초반 이후 음반들은 대체로 별로였다). 아, 물론 나에게는 이 방법이 효과가 있었지만 당신에게는 아무런 효과가 없을 가능성도 높다.\n요약 # 게으름 피우지 말고 스트레스를 제거해라. 천재는 쳐다보지 마라.\n책: 세이노의 가르침\n"},{"id":140,"href":"/docs/study/bioinformatics/cs16/","title":"구글 BERT의 정석 | BERT 입문","section":"생물정보학","content":" [딥러닝] 구글 BERT의 정석 | BERT 입문 # 목록 # 2024-12-31 ⋯ 2.3 BERT의 구조\n2024-12-31 ⋯ 2.4 BERT 사전 학습\n2.3 BERT의 구조 # BERT의 전체 구조 # 트랜스포머의 인코더(Encoder) 블록을 여러 개 쌓은 형태. 입력: 문장 (토큰화된 형태) 내부 구조: N개의 Transformer Encoder Blocks (기본 모델은 12개, 큰 모델은 24개) 출력: 각 토큰의 벡터 표현 (Contextual Embedding) cf) BERT의 대표적인 모델 크기\n모델 # 인코더 층 숨겨진 차원 (dmodel) 어텐션 헤드 수 파라미터 수 BERT-Base 12 768 12 110M BERT-Large 24 1024 16 340M BERT의 입력 처리 # 입력 토큰 (Token Embedding) WordPiece Tokenization을 사용하며, 단어를 서브워드(subword) 단위로 분할하고 각 토큰은 고유한 임베딩 벡터로 변환된다. ex) \u0026ldquo;playing\u0026rdquo; -\u0026gt; [\u0026ldquo;play\u0026rdquo;, \u0026ldquo;##ing\u0026rdquo;] 문장 구분 정보 (Segment Embedding) BERT는 두 개의 문장을 함께 입력할 수 있으며, 이때 각 문장이 어디에 속하는지를 구분하기 위해 Segment Embedding을 추가한다. ex) 문장 A: 0 (Segment A) / 문장 B: 1 (Segment B) 위치 정보 (Position Embedding) 트랜스포머는 순서를 고려하지 않는 구조이므로, 단어 순서를 반영하기 위해 위치 임베딩을 추가한다. BERT는 고정된 학습 가능한 위치 임베딩을 사용하며, 트랜스포머에서 사용되는 사인(sine) 및 코사인(cosine) 위치 임베딩을 사용하지 않음. 최종 입력 형식\n[CLS] 문장1 단어1 단어2 \u0026hellip; [SEP] 문장2 단어1 단어2 \u0026hellip; [SEP]\n[CLS]: 문장 전체를 대표하는 분류(Classification) 토큰 (첫 번째 위치) [SEP]: 문장 구분(Sentence Separation) 역할 BERT의 내부 구조 (Transformer Encoder Block) # 트랜스포머 인코더 블록을 여러 개 쌓은 구조.\nMulti-Head Self-Attention BERT는 문장의 양방향 문맥을 학습하기 위해 Multi-Head Self-Attention을 사용한다. 각 단어(토큰)는 문장의 다른 모든 단어와 어텐션을 수행하며, 관계를 학습한다. 즉 장의 다른 모든 단어와 어텐션 스코어를 계산하는데, 스코어가 크면 토큰 간 관계가 강한 것으로 간주된다. BERT는 12~16개의 어텐션 헤드를 사용한다. Feed Forward Network (FFN) 각 어텐션 층을 통과한 결과는 두 개의 완전 연결층(Fully Connected Layers) 을 통과하여 변환된다. 첫 번째 레이어: 선형 변환 + 활성화 함수 (ReLU 또는 GELU) 두 번째 레이어: 최종 출력 변환 FFN은 각 토큰에 대해 독립적으로 작동하며, 모델의 표현력을 증가시키는 역할을 한다. Layer Normalization \u0026amp; Residual Connection Residual Connection: 입력과 출력을 더해줌 (Gradient Flow 안정화) Layer Normalization: 네트워크 안정성 유지, 학습 속도 향상 이 과정을 총 N번 반복하여 최종적으로 컨텍스트 정보를 포함한 벡터가 생성된다.\nBERT의 출력 # BERT의 출력은 크게 두 가지 형태로 활용됨. 문장 수준 출력 ([CLS] 토큰) [CLS] 토큰의 벡터를 활용하여 문장 분류(Classification) 및 회귀(Task-Specific Head) 를 수행. ex) 감성 분석(Sentiment Analysis), 자연어 추론(NLI) 단어 수준 출력 (Token-Level Embeddings) 각 토큰의 벡터를 활용하여 개체명 인식(Named Entity Recognition, NER), 문장 생성 등의 태스크 수행. 2.4 BERT 사전 학습 # 사전 학습 단계에서는 BERT가 대량의 텍스트 데이터를 학습하면서 일반적인 언어 패턴과 문맥(Contextual Representation)을 이해한다.\nMasked Language Model (MLM, 마스킹된 언어 모델) # MLM 기본 개념 입력 문장에서 랜덤하게 15%의 단어를 [MASK]로 바꾼 후, 이를 예측하는 방식. BERT는 문장의 양방향(Bidirectional) 컨텍스트를 활용하여 [MASK]된 단어를 예측한다. 일반적인 언어 모델(예: GPT)은 이전 단어들만 참고하는 단방향 방식이지만, BERT의 MLM은 좌우 문맥을 모두 활용할 수 있다. MLM의 토큰 마스킹 마스킹된 15%의 단어는 다음과 같은 비율로 변환된다.\n80% → [MASK] 토큰으로 변경 10% → 랜덤한 다른 단어로 변경 10% → 원래 단어를 유지 ex) \u0026ldquo;I love deep learning because it is powerful.\u0026ldquo;은 BERT의 입력으로 변환하면 \u0026ldquo;I love [MASK] learning because it is powerful.\u0026ldquo;이고 모델의 목표는 \u0026ldquo;[MASK]\u0026rdquo; → \u0026ldquo;deep\u0026quot;이다.\n일반적인 자동 회귀(autoregressive) 모델은 단방향(Left-to-Right 또는 Right-to-Left)으로 단어를 예측함. 하지만, BERT는 양방향(Bidirectional) 문맥을 고려해야 하므로, 단어 일부를 가려놓고 전체 문맥을 기반으로 예측하는 방식이 적합하다.\nNext Sentence Prediction (NSP, 문장 관계 예측) # NSP 기본 개념 두 개의 문장을 입력으로 받아서, 두 번째 문장이 첫 번째 문장의 다음 문장인지 아닌지를 예측하는 방식. 이는 문장 간 관계를 학습하는 데 유용하며, 질의응답(QA) 및 자연어 추론(NLI) 태스크에 도움됨. NSP의 데이터 구성 학습할 때 두 개의 문장을 선택하여 다음과 같이 구성한다. 50%의 경우 → 실제 연속된 문장 (Positive Example) 50%의 경우 → 무작위로 선택된 문장 (Negative Example) BERT는 [CLS] 토큰을 활용하여 두 문장이 이어지는지 여부를 판단하는 분류 태스크를 수행한다. 이를 통해 질의응답(QA) 및 문장 간 논리적 연결성을 고려하는 태스크에서 강한 성능을 발휘할 수 있다.\nBERT의 사전 학습 과정 (Pre-training Process) # 데이터 준비 BERT는 대량의 비지도 학습 데이터를 사용하여 사전 학습된다. 각 문장을 WordPiece Tokenizer를 이용해 서브워드(subword) 단위로 변환한다. 토큰 임베딩 생성 입력 문장은 다음과 같은 3가지 임베딩을 결합하여 벡터로 변환된다. Token Embedding: 각 단어에 해당하는 임베딩 벡터 Segment Embedding: 문장 A/B를 구분하는 임베딩 Position Embedding: 문장 내 단어의 위치 정보를 나타내는 임베딩 Transformer 인코더 통과 BERT의 본체인 Transformer Encoder (12~24개 블록) 를 통해 입력을 변환한다. MLM 태스크를 위해 일부 토큰이 [MASK] 처리된 상태에서 어텐션(Self-Attention)이 수행됨. NSP 태스크를 위해 [CLS] 토큰의 출력이 사용됨. 두 가지 출력 MLM 출력: [MASK] 위치에 올바른 단어를 예측 NSP 출력: [CLS] 토큰을 사용하여 두 문장이 연속된 문장인지 예측 BERT의 최종 손실(Loss Function) 계산. BERT의 사전 학습 이후 (Fine-tuning) # BERT는 사전 학습을 마친 후, 특정 태스크에 맞춰 미세 조정(Fine-tuning)한다.\n사전 학습된 BERT 모델을 기반으로 특정 태스크 수행 텍스트 분류 (Sentiment Analysis) 질의응답 (SQuAD, Question Answering) 개체명 인식 (NER, Named Entity Recognition) 자연어 추론 (NLI, Natural Language Inference) 미세 조정 방식 사전 학습된 가중치를 초기화한 후, 해당 태스크에 맞게 라벨이 있는 데이터로 추가 학습을 진행한다. [CLS] 토큰을 활용한 분류 태스크 [MASK] 토큰을 활용한 MLM 기반 태스크 "},{"id":141,"href":"/docs/study/bioinformatics/cs17/","title":"구글 BERT의 정석 | BERT의 파생 모델","section":"생물정보학","content":" [딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 2024-12-31 ⋯ 4.1 ALBERT\n2024-12-31 ⋯ 4.3 RoBERTa\n2024-12-31 ⋯ 4.4 ELECTRA\n4.1 ALBERT # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.\n크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다. 펙토라이즈 임베딩 변수화 # BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다. ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션). 행렬1: vocab_size x embedding_size 행렬2: embedding_size x hidden_size 문장 순서 예측 # BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용.\nNSP은 두 문장이 연속적으로 존재하는지를 예측하는 태스크. 문장 간 관계를 학습하는 데 사용됨. ALBERT는 문장 순서 예측 (SOP, Sentence Order Prediction) 라는 새로운 학습 태스크를 도입.\n두 문장이 주어졌을 때, 두 문장의 순서가 올바른지를 예측함. SOP는 문장 간의 순서 관계를 이해하는 데 NSP보다 적합하다.\nALBERT와 BERT 비교 # 크로스 레이어 변수 공유: ALBERT는 여러 레이어에서 파라미터를 공유하여 파라미터 수를 크게 줄임. BERT는 각 레이어마다 독립적인 파라미터 집합을 사용. 펙토라이즈 임베딩: ALBERT는 임베딩 행렬을 분해하여 파라미터 수를 줄임. BERT는 한 번에 큰 임베딩 행렬을 사용. 문장 순서 예측 (SOP): ALBERT는 NSP 대신 SOP를 사용하여 문장 순서를 더 잘 예측할 수 있게 하여, 문장 간 관계 학습을 개선. ALBERT에서 임베딩 추출 # 단어 임베딩\n입력 텍스트의 각 단어를 vocab_size x embedding_size 크기의 행렬을 사용하여 고차원 벡터로 변환함. 이 벡터는 각 단어의 의미를 반영하는 고차원적인 특징을 갖고있다. 레벨 별 임베딩 추출 (Layer-wise Embedding Extraction)\n입력 텍스트가 Transformer 모델을 통과하면서 각 레이어에서 벡터 표현이 점진적으로 변환된다. ALBERT에서는 주로 첫 번째 레이어 또는 최종 레이어에서 추출된 임베딩을 사용할 수 있음. 첫 번째 레이어에는 주로 단어의 기본적인 의미와 구조적 특징 정보. 최종 레이어에는 문장 전체의 복합적인 의미와 문맥이 결합되어, 더 구체화되고 세부적인 정보. 중간 레이어에서의 임베딩 추출\nALBERT는 다중 레이어 구조를 갖기 때문에, 중간 레이어의 출력도 사용할 수 있음. 문장 내 특정 단어의 문맥을 더 잘 반영하는 중간 레이어의 임베딩을 추출할 수 있다. 사용자가 수행하는 작업에 따라, 특정 작업에 적합한 레이어의 출력을 선택. 예를 들어, 문장 분류 작업에서는 모델의 최종 레이어에서 추출된 임베딩이 더 중요할 수 있으며, 개체명 인식(NER) 작업에서는 중간 레이어에서 나온 임베딩이 더 유용할 수 있다. 4.3 RoBERTa # 정적 마스크 대신 동적 마스크 사용 # BERT는 정적 마스크 (Static Masking) 방식을 사용하여 훈련함. 훈련 데이터에서 마스킹할 단어를 고르고, 그 마스크를 모든 훈련 단계에서 동일하게 유지한다. 즉 같은 단어가 훈련 내내 계속 마스크된다. RoBERTa는 동적 마스크 (Dynamic Masking) 방식을 사용. 즉, 각 훈련 배치마다 문장에서 마스크되는 단어가 랜덤하게 변경된다. 정적 마스크에서는 동일한 문맥을 반복해서 학습하므로, 모델이 특정 단어의 패턴을 암기할 수 있는데, 동적 마스크에서는 훈련마다 마스크가 달라져 모델이 더 다양한 방식으로 문맥을 학습할 수 있도록 돕고, 일관된 마스크 패턴에 의한 편향을 줄여 모델이 더 일반화된 특징을 학습할 수 있게 해준다. NSP 테스크 제거 # BERT 모델은 훈련 과정에서 MLM, NSP 테스크를 사용한다. RoBERTa는 NSP 대신 MLM만을 사용하여 훈련을 진행. NSP 제거의 이유는 문장 간의 관계 학습에 NSP가 크게 기여하지 않으며 제거 시 훈련이 더 간단해지고, 모델이 더욱 집중해서 문맥을 학습할 수 있음. 더 많은 데이터로 학습 # RoBERTa는 BERT보다 훨씬 더 많은 데이터로 훈련. BERT는 16GB 크기의 BooksCorpus와 English Wikipedia로 훈련되었지만, RoBERTa는 여기에 추가로 Common Crawl 데이터, CC-News, OpenWebText, Stories 등의 더 많은 데이터를 포함하여 훈련됨. 큰 배치 크기로 학습 # RoBERTa는 훈련에 더 큰 배치 크기를 사용합니다. BERT는 일반적으로 배치 크기를 32 또는 64로 설정하여 훈련하지만, RoBERTa는 배치 크기 8,000까지 사용하여 훈련했습니다. 큰 배치 크기?\n배치가 크면 모델이 더 많은 데이터를 한 번에 처리할 수 있게 해주고, 훈련 속도를 높이는 데 기여함. 학습 안정성을 높여, 학습 과정에서 발생할 수 있는 불안정한 그래디언트 문제를 완화하는 데 도움을 줌. BBPE 토크나이저 사용 # BERT는 WordPiece 토크나이저를 사용하여 텍스트를 서브워드 단위로 분할. RoBERTa는 BBPE (Byte Pair Encoding) 토크나이저를 사용. 단어를 자주 발생하는 문자쌍으로 분할하여 서브워드 토큰을 만든다. 이는 드문 단어나 외래어가 포함된 텍스트에서 더욱 효과적임. BBPE는 단어를 더 작은 조각으로 나누고, 이를 더 자주 사용되는 문자쌍으로 합치는 방식으로 작동함. 효과: 어휘 집합 크기를 줄이면서도 다양한 단어를 처리할 수 있게 해주며, 모델의 효율성을 높이고, 모든 언어에서 유연한 처리가 가능. 4.4 ELECTRA # 교체한 토큰 판별 테스크 # BERT와 같은 기존 모델들은 일부 단어를 마스킹하고 예측하는 방식(Masked Language Modeling, MLM)을 사용해서 모델을 학습. 이 방식은 마스크된 단어의 예측이 실제 문맥을 잘 반영하지 않게 될 수 있다는 단점이 있다. ELECTRA는 교체한 토큰 판별 테스크 (Replaced Token Detection) 를 사용. 이 방식은 문장을 구성하는 각 토큰이 원래의 문장에서 그대로 있었는지 아니면 다른 토큰으로 교체되었는지를 구분하는 문제이며 이렇게 하면 모델은 교체된 단어를 구별하는 법을 배운다. ELECTRA의 생성자와 판별자 # ELECTRA는 두 가지 모델로 구성된다. 생성자 (Generator)\n기존 BERT와 같은 Masked Language Model (MLM) 구조. 입력 문장에서 일부 단어를 [MASK]로 변환한 후, 이를 생성자의 예측 값으로 대체함. 판별자 (Discriminator)\n문장 내 각 토큰이 진짜인지(fake) 가짜인지(real)를 분류하는 이진 분류(Binary Classification) 문제를 해결. ELECTRA 모델 학습 # 생성자 학습\n문장에서 일부 단어를 마스킹한 후, 생성자가 그 단어를 예측. 예측된 단어는 원래 단어 대신 교체된 단어(replaced token)로 사용됨. 판별자 학습\n생성자가 만든 교체된 단어를 포함한 문장을 입력받음. 판별자는 문장 내 각 단어가 원래 단어인지, 교체된 단어인지 판별하는 작업을 수행. 판별자가 더 정확한 예측을 할수록 모델의 언어 이해 능력이 향상됨. 손실 함수 계산\n생성자는 Cross-Entropy Loss (MLM 방식) 판별자는 Binary Classification Loss (Replaced Token Detection 방식) 반복 학습\n생성자의 성능이 향상될수록 판별자의 분류 작업이 더 어려워짐. 결국 판별자가 더 정교한 문맥 이해 능력을 갖도록 최적화됨. 효율적인 학습 방법 탐색 # ELECTRA 모델을 효율적으로 학습시키기 위해서 생성자와 판별자의 가중치를 공유한다.\n기존 BERT는 마스킹된 토큰만 학습에 사용하지만, ELECTRA는 모든 토큰을 판별 작업에 사용하여 훨씬 더 높은 학습 데이터 활용률을 가짐.\n기존의 MLM 방식보다 80% 적은 연산량으로 동일한 성능을 유지, 동일한 연산량을 사용했을 때 BERT보다 2~4배 더 빠르게 학습 가능.\n생성자는 BERT와 같은 크기를 사용할 필요가 없어서, 생성자를 작은 크기의 모델로 설정하여 연산량을 절감.\nELECTRA-Small (14M parameters) → BERT-Small보다 86% 더 높은 성능 / ELECTRA-Large는 BERT-Large보다 적은 연산량으로 더 높은 성능을 보임.\n"},{"id":142,"href":"/docs/study/bioinformatics/cs15/","title":"구글 BERT의 정석 | 트랜스포머 입문","section":"생물정보학","content":" [딥러닝] 구글 BERT의 정석 | 트랜스포머 입문 # 목록 # 2024-12-31 ⋯ 1.2 트랜스포머의 인코더 이해하기\n2024-12-31 ⋯ 1.3 트랜스포머의 디코더 이해하기\n1.2 트랜스포머의 인코더 이해하기 # 셀프 어텐션 # 셀프 어텐션은 문장 내 단어들이 서로 얼마나 중요한지를 계산하는 과정. 트랜스포머는 이를 위해 입력 단어를 쿼리(Query), 키(Key), 밸류(Value) 세 가지 벡터로 변환하여 연관성을 구한다. 어텐션 점수 계산 예제 # \u0026ldquo;The cat sat on the mat.\u0026rdquo;\n각 단어 벡터(예: 512차원)를 가중치 행렬과 곱하여 쿼리(Q), 키(K), 밸류(V)벡터를 생성한다. 어떤 단어가 다른 단어와 얼마나 연관되는지를 측정하기 위해, Q와 K벡터 간의 내적(dot product)을 계산한다. 단어 The cat sat on the mat Query: \u0026ldquo;cat\u0026rdquo; 0.2 1.0 0.8 0.1 0.3 0.5 \u0026ldquo;cat\u0026quot;의 쿼리 벡터와 모든 단어의 키 벡터를 곱해서 점수를 계산하는 경우. 여기서 \u0026ldquo;cat\u0026quot;은 \u0026ldquo;sat\u0026quot;과 가장 연관이 높고(0.8), \u0026ldquo;on\u0026quot;과는 거의 연관이 없다(0.1). 소프트맥스 적용 단어 The cat sat on the mat Softmax 값 0.05 0.4 0.35 0.02 0.08 0.1 위에서 구한 점수에 대해 소프트맥스를 적용하여 확률로 변환 이제 \u0026ldquo;cat\u0026quot;은 \u0026ldquo;sat\u0026rdquo;(0.35)과 \u0026ldquo;cat\u0026rdquo; 자체(0.4)에 높은 가중치를 부여함. 각 단어의 밸류(V) 벡터를 위의 확률로 가중합하여 최종 어텐션 출력을 얻는다. 멀티 헤드 어텐션 # 단어 간의 관계를 한 가지 방식으로만 학습하면, 문맥을 완전히 반영하지 못할 수 있음. 예를 들어, 단어 \u0026ldquo;cat\u0026quot;은 문장에서 다음과 같은 다양한 방식으로 다른 단어와 관계를 맺을 수 있다.\n문법적 관계(Head 1): \u0026ldquo;cat\u0026rdquo; → \u0026ldquo;sat\u0026rdquo; (주어와 동사의 관계) 의미적 관계(Head 2): \u0026ldquo;cat\u0026rdquo; → \u0026ldquo;mat\u0026rdquo; (동물과 사물이 놓여 있는 관계) 위치적 관계(Head 3): \u0026ldquo;on\u0026rdquo; → \u0026ldquo;mat\u0026rdquo; (\u0026ldquo;on\u0026quot;이 \u0026ldquo;mat\u0026quot;과 어떤 방식으로 연결되는지) 만약 하나의 어텐션만 사용한다면, 위 관계 중 하나만 학습할 수 있다. 멀티 헤드 어텐션은 여러 개의 독립적인 어텐션 연산을 수행하여, 이러한 다양한 패턴을 동시에 학습하는 역할을 함.\n멀티헤드 어텐션 수행 과정 # 문장을 입력하면, 각 단어는 일정한 차원의 벡터(예: 512차원)로 변환된다. 각 단어의 벡터를 이용하여 쿼리(Q), 키(K), 밸류(V)를 생성한다.\n여러 개의 어텐션 헤드 생성\n멀티 헤드 어텐션에서는 각 단어 벡터를 여러 개의 서로 다른 가중치 행렬을 사용하여 여러 개의 쿼리(Q), 키(K), 밸류(V)로 변환한다.\n각 헤드는 서로 다른 관계를 학습할 수 있도록 다른 가중치를 가진다.\n각 헤드는 독립적으로 셀프 어텐션(Self-Attention)을 수행한다. 소프트맥스를 적용하여 확률값으로 변환한 후, 밸류(V)에 가중합하여 최종 출력을 생성한다. 어텐션 점수 계산 각 헤드에서 나온 결과를 병합(Concatenation)한 후, 최종적으로 선형 변환을 적용한다. 즉, 여러 개의 어텐션을 병렬로 수행하고, 최종적으로 선형 변환을 적용하여 하나의 벡터로 변환하는 것. 선형 변환 위치 인코딩 # 트랜스포머는 문장을 한 번에 입력받아 병렬로 처리하는 구조이다. 이러한 구조는 속도 면에서 유리하지만, 단어들의 순서(sequence)를 직접적으로 학습할 수 없다. 따라서 위치 정보를 인코딩하여 단어의 순서를 반영하는 기법이 필요함. 즉, 특정 단어의 위치 pos와 벡터의 차원 위치 i에 따라 사인과 코사인 값을 계산하여 각 차원별 위치 인코딩 값을 생성함으로써 위치 정보를 반영한다. 위치 인코딩 예제 # \u0026ldquo;The cat sat on the mat.\u0026rdquo;\n단어 벡터를 4차원으로 설정한다고 가정하고 각 단어에 대해 위치 인코딩 값 계산하기. 첫 번째 차원 (i=0) 계산 (짝수이므로 sin 사용)\n두 번째 차원 (i=1) 계산 (홀수이므로 cos 사용)\n생성된 각 단어의 위치 인코딩 벡터\n단어 위치 인코딩 벡터 (4차원) The [0.000, 1.000, 0.841, 0.540] cat [0.841, 0.540, 0.909, -0.416] sat [0.909, -0.416, 0.141, -0.990] on [0.141, -0.990, -0.757, -0.654] the [-0.757, -0.654, -0.958, 0.283] mat [-0.958, 0.283, -0.279, 0.750] 단어의 임베딩 벡터와 더하면, 위치 정보가 반영된 최종 벡터가 생성된다.\n피드포워드 네트워크(Feedforward Network, FFN) # FFN은 트랜스포머의 각 단어 벡터에 대해 독립적으로 적용되는 두 개의 선형 변환(fully connected layer)과 활성화 함수(ReLU)로 구성된 신경망. 과정 첫 번째 선형 변환 (Fully Connected Layer 1): 입력 벡터를 확장된 차원(2048)의 벡터로 변환한다. ReLU 활성화 함수 적용: 비선형성을 추가하여 복잡한 관계를 학습 두 번째 선형 변환 (Fully Connected Layer 2): 다시 원래 차원(512)으로 축소하여 출력 트랜스포머 인코더 블록에서 FFN의 위치 # 피드포워드 네트워크는 어텐션 이후에 적용됨. 멀티 헤드 어텐션(Self-Attention) 수행\n각 단어가 다른 단어들과의 관계를 학습 어텐션 가중치를 통해 정보를 집계 Add \u0026amp; Norm (Residual Connection + Layer Normalization) 적용 피드포워드 네트워크(FFN) 적용\n개별 단어의 의미 표현을 강화 (독립적인 변환 수행) ReLU를 활용하여 비선형성을 추가 Add \u0026amp; Norm (Residual Connection + Layer Normalization) 적용 Add \u0026amp; Norm # 트랜스포머는 매우 깊은 신경망이다. 깊은 신경망을 학습할 때 흔히 발생하는 문제가 기울기 소실(Vanishing Gradient)과 기울기 폭발(Exploding Gradient). 또한, 모델이 과도하게 변화하면 학습이 불안정해진다. Residual Connection을 사용하면 원래 정보를 유지하면서 학습할 수 있다. Layer Normalization을 사용하면 값의 스케일을 맞추어 학습을 안정화할 수 있다. 트랜스포머 인코더 전체 과정 # 입력 벡터(임베딩 + 위치 인코딩) 생성 멀티 헤드 어텐션 수행하여 단어 간 관계를 학습 Residual Connection 적용 (입력 + 어텐션 출력 더하기) Layer Normalization 적용하여 학습 안정화 피드포워드 네트워크(FFN) 적용하여 단어별 정보를 강화 Residual Connection 적용 (입력 + FFN 출력 더하기) Layer Normalization 적용 다음 인코더 블록으로 전달하여 반복 수행 1.3 트랜스포머의 디코더 이해하기 # 디코더의 구조 # 트랜스포머 디코더는 인코더와 함께 동작할 수도 있고(Google의 원래 Transformer 모델, BART), 독립적으로 동작할 수도 있다(GPT 시리즈). N개의 디코더 블록(stack)이 쌓여 있는 형태로 구성. 디코더 핵심 연산 # Masked Multi-Head Self-Attention 입력 시퀀스 내에서 이전 단어까지만 참고하여 다음 단어를 예측해야 하므로, 일반적인 Multi-Head Self-Attention과 다르게 미래 정보를 차단(masking) 한다. 이를 위해 Casual Masking(Look-Ahead Masking)을 사용하여, 현재 위치 t에서 t+1, t+2, \u0026hellip; 등 미래의 단어들을 보지 못하도록 만든다. 계산 과정 Q, K, V를 입력에서 생성 어텐션 스코어 계산 마스킹 적용: 미래 단어의 스코어를 −∞로 설정하여 Softmax에서 0이 되도록 만듦. Softmax \u0026amp; 가중합하여 최종 출력을 생성. Cross-Attention 인코더에서 생성된 컨텍스트 정보를 활용하는 모듈. 인코더의 출력을 Key \u0026amp; Value로 사용하고, 디코더의 출력을 Query로 사용해서 Attention을 수행. 작동 방식 디코더에서 나온 Query(Q)와 인코더에서 생성된 Key(K) 및 Value(V)를 활용하여 Multi-Head Attention 수행. 이를 통해 코더가 인코더의 정보를 반영하여 다음 토큰을 예측하는 데 도움을 준다. Feed Forward Network (FFN) 각 디코더 블록에는 FFN이 포함되어 있으며, 두 개의 완전 연결층(fully connected layers)으로 구성된다. 구조 입력 차원 dmodel 중간 차원 dff(보통 4dmodel) 활성화 함수 ReLU 또는 GELU 출력 차원 dmodel Residual Connection \u0026amp; Layer Normalization 잔차 연결(Residual Connection): 각 서브 레이어의 입력을 더해줌. Layer Normalization: 학습 안정성을 높이고, 학습 속도를 향상. 디코더의 출력 (Output Processing) # 마지막 디코더 블록에서 나온 결과는 완전 연결층(Dense layer)를 거쳐 차원을 조정한다. 소프트맥스(Softmax) 를 적용하여 단어 확률 분포를 계산한다. 가장 확률이 높은 단어를 선택하여 출력한다. "},{"id":143,"href":"/docs/hobby/book/book28/","title":"그릿을 획득하기 vs 진실로의 창을 열어놓기.","section":"글","content":" 그릿을 획득하기 vs 진실로의 창을 열어놓기. # #2024-12-31\n1 # 나는 전문가들은 이 문제에 관해 뭐라고 이야기하는지 알아보기로 했다. 자기기만이 데이비드와 내 아버지가 경고한 것만큼 그렇게 위험한 것인가 하는 문제 말이다.\n20세기에는 의학 전문가들이 일치된 의견을 내놓았다. 지그문트 프로이트, 에이브러햄 매슬로, 에릭 에릭슨 같은 영향력 있는 심리학자들은 자기기만을 정신적 결함이자 시각에 생긴 문제여서 치료로 교정해야 한다고 보았다. 반면 정확한 시각은 \u0026ldquo;정신의 건강을 보여주는 표지\u0026quot;라고 여겼다.\n그러나 20세기가 기운차게 달려가는 동안, 임상심리학자들은 이상한 일들을 목격하기 시작했다. 그들이 볼 때 더 건강한 환자들, 인생을 더 쉽게 살아가는 사람들, 좌절을 겪은 뒤에도 재빨리 회복하는 사람들, 직업과 친구, 연인을 얻고 인생이라는 회전목마에서 황금기를 가지고 있는 사람들은 장밋빛 자기기만이라는 특징을 지니고 있는 것처럼 보였다.\n반면 그토록 칭송받던 정확한 인식이라는 미덕을 가진 사람들은 어떨까? 짐작했겠지만 그들은 병적인 수준의 우울증에 걸렸다. 살아가는 일을 힘들어했고, 좌절을 겪은 뒤에는 회복이 더 어려웠으며, 일과 사람들의 관계에서도 종종 더 많은 문제를 일으켰다.*\n*내가 느꼈던 바랑 어느정도 일치하는 듯하다. 갖고 싶어 노력했던 것들은 얻지 못하고, 우연찮게 얻게 된 것들은 후에 없어서는 안될 중요한 것들이 되었다. 이렇게나 내 의지와 무관하게 흘러가는게 내 삶이 맞나?라는 생각이 들었다.\n혼자서는 결론을 내리기가 어려운 일들이 많았다. 대다수는 내 존망과 직결되는 문제라서 좀 중요했다. 나는 답을 찾기 전까지는 아무것도 시작할 수 없다고 생각했다.\n많은 감정과 시간을 쏟았지만 해답은 엉뚱한 곳에서 찾았다. 사람은 다른 사람에 섞여 살아야 한다. 사람은 사람이랑 같이 살아야 한다. 혼자서는 답을 찾기 어려운 일들은 다른 머리로 생각했을 땐 의외로 쉬운 질문일 수 있다. 답은 더 엉뚱한 곳에서 나오기도 한다. 영원히 풀리지 않을 문제 같던 일을 어느 새 잊고 사는 것이다. 문제는 문제를 삼아서 문제인지도 모른다. 사실 그 질문은 답이 없었는지도 모른다.\n2 # “자신에게 거짓말을 하는 것이 괜찮을까요?” 내가 윌슨에게 물었다.\n“해로울 게 뭔가요? 두려움을 잠재워주고, 미래에 적응을 방해하는 행동으로 이어지지 않는다면 나는 아무 문제 될 게 없다고 봐요.”\n3 # 더크워스는 왜 어떤 학생은 다른 학생들보다 공부를 더 힘들어하는지 그 이유가 궁금했다.12 성취도가 높은 학생들에게는 무슨 비밀이 있는지 알아내고 싶었다. 몇 년 뒤 더크워스는 그 비밀의 요소라 여겨지는 한 가지 특징을 발견하고 그 특징에 ‘그릿Grit’(끈질긴 투지)이라는 이름을 붙였다. 그릿. 끈질김을 뜻하지만 그보다 귀에 착 붙는 단어, 그릿. “긍정적 피드백”이 없는데도 “매우 장기적인 목표”에 로봇처럼 뛰어들게 해주는 것,13 그릿. 머리로 벽을 반복적으로 들이받을 수 있는 능력. 더크워스는 웨스트포인트(미 육군사관학교) 사관생, 최고경영자, 뮤지션, 운동선수, 셰프 등 거의 모든 직업에서 정상에 선 사람들에게서 그릿을 발견했다.14 재능, 창의력, 친절함, IQ는 다 잊어라. 순수한 그릿이야말로 앞으로 나아가게 해주는 바로 그것인 것 같았다.\n그렇다면 어떤 인지적 결함이 그릿을 획득하는 데 도움이 될까? 바로 긍정적 착각이다.15 다른 연구들도 마찬가지로 긍정적 착각을 갖고 있는 사람이 좌절을 겪은 뒤에 낙담할 가능성이 적다는 것을 보여주었다.16 그릿이란 여러 특성들이 섞인 칵테일 같은 것이지만, 그중 가장 중요한 특징이 바로 이것이다. 좌절을 겪은 뒤에도 계속 나아갈 수 있는 능력, 자신이 추구하는 것이 이루어지리라는 증거가 전혀 없는데도 계속 해나갈 수 있는 능력, 또는 더크워스의 표현을 빌리면 “실패와 역경, 정체에도 불구하고 수년간 노력과 흥미를 유지하는 것”17 말이다.\n그릿의 가장 좋은 부분이자 가장 희망적인 속성이며, 아메리칸드림과도 가장 잘 들어맞는 지점은 이것이 생물학적 기반에서 나오지는 않았을 것이라는 생각이다. 꿈을 현실로 만들어주는 그릿이라는 이 마술적인 특성은 가르쳐서 기를 수 있다는 것이다.\n데이비드는 더크워스가 내린 그릿의 정의를 거의 그대로 복창하듯 자신을 이렇게 묘사했다. “나는 바라는 목표를 향해 끈질기게 일하고 그런 다음 결과를 차분히 받아들이는 데 익숙해졌다. 나아가 나는 일단 일어난 불운에 대해서는 절대 마음 졸이지 않았다.”18\n4 # 그런데 장밋빛 렌즈를 끼고 살아가는 일이 불리하게 작용하기도 할까?\n로빈스와 비어는 스스로 실망을 자초하는 것이라고, 즉 “단기적으로 혜택을 얻는 대신 장기적으로 비용을 치르는” 것이라고 설명했다.29 다시 말해서 기만은 나중에라도 대가를 치르게 된다는 것이다. 장밋빛 렌즈의 힘에는 한계가 수반된다. 그리고 그 힘이 떨어지면 자신이 무력하다는 사실을 정말로 따끔하게 받아들여야 한다.\n5 # 바우마이스터와 부시먼은 높은 자존감이 모두 나쁜 건 아니라는 점도 재빨리 덧붙였다. 그들은 높은 자존감도 아주 좋은 것일 수 있다며, 활짝 편 손바닥을 높이 들어 보이면서 해명해야 하는 상황을 자주 겪었다. 자존감이 높은 사람은 자기 자신을 아주 편안하게 받아들이며, 비판을 받아도 자기 가치가 위협받는다고 느끼지 않으므로 높은 자존감은 당사자를 기이할 정도로 평화롭게(그들의 표현으로는 “이례적으로 비공격적으로”) 만들 수도 있다고 했다. 그들은 자존감이 높기는 하지만 자존감에 대한 위협을 쉽게 느끼는 극히 소수의 사람만이 위험한 이들이라고 생각했다.\n바우마이스터와 부시먼은 이렇게 썼다. “쉽게 말해서 가장 위험한 사람은 자신을 우월한 존재라고 보는 사람들이라기보다 자신을 우월한 존재로 보고 싶다는 욕망이 강한 사람들이다. (…) 거창한 자기상을 확인받는 일에 집착하는 사람들은 비판당하는 것을 몹시 괴로워하며 자기를 비판한 사람을 사납게 공격하는 것으로 보인다.”38\n나는 스탠퍼드에서 보았던 그 오싹한 물고기, 데이비드 스타 조던이 직접 자신의 이름을 붙인 유일한 바닷물고기를 다시 떠올렸다. 서로 반대쪽에 위치한 두 면이 돌돌 말리듯 어디서 만나는지도 모르게 하나로 합쳐지는 뫼비우스 띠 모양의 그 가시 박힌 용 말이다. “모서리가 없는 조던.” 그가 선택한 이 물고기에 어떤 메시지가 숨어 있는 걸까? 그의 매력 아래 도사린 어두운 면에 대한 인정일까?\n루서 스피어는 이렇게 썼다. “조던의 재능 중 특히 양날을 지닌 재능은 자기가 옳은 일을 하고 있다고 자신을 설득하고, 그런 다음 무한해 보이는 에너지로 목표를 추구하는 능력이다. (…) 그는 자신의 관용과 관대함을 자랑스러워했다. (…) 하지만 조던은 파리 한 마리를 잡는 데 대포알을 쓰는 것도 마다하지 않았다.”39\n6 # 다윈은 《종의 기원》의 거의 모든 장에서 “변이”48의 힘을 칭송한다. 동질성은 사형선고와 같다. 한 종에서 돌연변이와 특이한 존재들을 모두 제거하는 것은 그 종이 자연의 힘에 취약하게 노출되도록 만들어 위험을 초래한다.\n이를 달리 표현하자면 “당신의 유전자 포트폴리오를 다양화하라”가 될 것이다.52 상황이 바뀌면 그 상황에 어떤 특징이 더 유용하게 적용될지는 아무도 모르는 법이다. 다윈은 간섭하지 말라고 특별히 강력하게 경고한다.53 그가 보기에 위험한 것은 인간의 눈에서 비롯된 오류 가능성, 복잡성을 이해하지 못하는 우리의 무능력이다. “적합성에 대한 우리의 관점에서는 불쾌하게”54 보일 수 있는 특징들이 사실 종 전체나 생태계에는 이로울 수도 있고, 혹은 시간이 지나고 상황이 바뀌면 이로운 것이 될 수도 있다는 것이다.\n인간의 지력으로 도저히 다 이해할 수 없는 생태의 복잡성에 대한 이러한 조심스러움과 겸손함, 공경하는 마음은 사실 대단히 오래된 것이다. 이는 때로 “민들레 원칙”58이라고도 불리는 철학적 개념이다. 우생학자들은 이런 단순한 상대성의 원칙을 고려하지 못한 것이다. 유전자 풀에서 “필수 불가결한”59 다양성을 제거하려고 노력함으로써 그들은 사실상 지배자 인종을 구축할 최선의 기회를 망쳐버리고 있었던 셈이다.\n7 # 데이비드 스타 조던은 죽는 날까지 열광적인 우생학자로 남았다. 데이비드의 정서적 해부도를 쫙 펼쳐놓고 볼 때 가장 눈에 띄는 원흉은 그 스스로 상당히 자랑스러워했던 두툼한 “낙천성의 방패”가 아닌가 싶다. 특히 시련의 시기에는 더욱더 자기기만에 의존했던 듯하다. 운명의 형태를 만드는 것은 사람의 의지다. “긍정적 착각은 견제하지 않고 내버려둘 경우 그 착각을 방해하는 것은 무엇이든 공격할 수 있는 사악한 힘으로 변질될 수 있다”고 경고한 그 심리학자들의 말이 옳았던 것 같다.\n나는 거꾸로 거슬러 올라가면서 그가 경로를 이탈한 지점을, 그의 방향타를 슬쩍 밀어 그가 그토록 파멸적으로 경로를 벗어나게 만든 사건 혹은 개념을 찾기 시작했다. 그러다 마침내 나는 제비들이 원을 그리며 날아다니는 페니키스 섬의 헛간에서 루이 아가시가 젊은 데이비드의 정신에 관념의 씨앗 하나를 심어놓는 순간에 다다랐다. 그것은 자연 속에 사다리가 내재해 있다는 믿음이었다. 자연의 사다리. 박테리아에서 시작해 인간에까지 이르는, 객관적으로 더 나은 방향으로 향하는 신성한 계층구조. 이 관념이 데이비드의 세계를 다시 건축했다. 그것은 꽃을 수집하던 그의 부끄러운 습관을 “가장 높은 수준의 선교 활동”으로 바꿔놓았다.\n그는 지느러미나 두개골의 형태 속에 도덕적 안내도가 담겨 있다는 믿음을 품고서, 나침반처럼 자연을 읽으며 앞으로 나아갔다. 그는 충분히 꼼꼼하게 살펴보면 누구를 모방해야 할지, 누구를 비난해야 할지 알아낼 수 있을 거라 확신했다. 한마디로 깨달음으로, 평화로, 그 무엇이든 사다리의 꼭대기에 놓여 있을 열매를 향해 나아가는 진실한 경로를 알게 될 거라고. 그리고 인류가 쇠퇴해가는 모습을 목격했다고 생각했을 때, 필요하다면 어떤 수단을 동원해서라도 인류를 구출해야 한다는 소명을 느꼈다. 그는 자연의 질서에 관한 믿음을 칼날처럼 휘두르며, 인류를 구원할 가장 건전한, 아니 유일한 방법은 불임화라고 사람들을 설득했다.\n**이 부분은 의사 결정에서 \u0026lsquo;상자\u0026rsquo;를 선택하는 대신에 \u0026lsquo;나무\u0026rsquo;처럼 생각해야 한다는 카밀라 팡의 의견과 일맥상통한다. \u0026lsquo;상자 속에서 \b생각하는 방식은 대개 감정의 조합이나 배짱으로 의사를 결정한다. 감정이나 배짱은 둘다 신뢰할 수 없다.\u0026rsquo;\n8 # 동물은 인간이 스스로 우월하다고 가정하는 거의 모든 기준에서 인간보다 더 우수할 수 있다. 까마귀는 우리보다 기억력이 좋고,6 침팬지는 우리보다 패턴 인식 능력이 뛰어나며,7 개미는 부상당한 동료를 구출하고,8 주혈흡충은 우리보다 일부일처제 비율이 더 높다.9 지구에 사는 모든 생물을 실제로 검토해볼 때, 인간을 꼭대기에 두는 단 하나의 계층구조를 그려내기 위해서는 상당히 무리해서 곡예를 해야 한다. 우리는 가장 큰 뇌를 갖고 있지도 않고 기억력이 가장 좋은 것도 아니다. 우리는 가장 빠르지도, 가장 힘이 세지도, 번식력이 가장 좋지도 않다. 같은 배우자와 평생을 함께하고, 도구나 언어를 사용하는 것은 인간만이 아니다. 심지어 우리는 지구에 가장 새롭게 나타난 생물도 아니다.\n이것이 바로 다윈이 독자들에게 알려주려고 그토록 노력했던 점이다. 사다리는 없다. 나투라 논 파싯 살툼Natura non facit saltum, “자연은 비약하지 않는다”고10 다윈은 과학자의 입으로 외쳤다. 우리가 보는 사다리의 층들은 우리 상상의 산물이며, 진리보다는 “편리함”을 위한 것이다. 다윈에게 기생충은 혐오스러운 것이 아니라 경이였고,11 비범한 적응성을 보여주는 사례였다. 크건 작건, 깃털이 있건 빛을 발하건, 혹이 있건 미끈하건 세상에 존재하는 생물의 그 어마어마한 범위 자체가 이 세상에서 생존하고 번성하는 데는 무한히 많은 방식이 존재한다는 증거였다.12\n9 # 데이비드는 왜 그걸 보지 못한 걸까? 사다리에 대한 그의 믿음을 반증하는 증거들이 이렇게 산더미처럼 쌓여 있는데. 식물과 동물이 배열되는 방식에 관한 이 자의적인 믿음을 왜 그토록 보호하려 한 걸까? 그 믿음에 도전이 제기되면 왜 더욱 강하게 그 믿음을 고수하고 폭력적인 조치를 합리화하는 데 그 믿음을 사용했을까? 아마도 그 믿음이 그에게 진실보다 더 중요한 무언가를 주었기 때문일 것이다. 그것은 바다와 별들과 현기증 나는 그의 인생을 휘몰아가는, 소용돌이치는 늪을 깔끔하고 빛나는 질서로 바꾸는 방법이었다. 처음 다윈을 읽을 때부터 마지막으로 우생학을 밀어붙일 때까지 어느 시점에서든 그 믿음을 놓아버리는 것은 현기증을 다시 불러일으키는 일이었을 것이다. 그것은 지독히도 방향감각을 앗아가는 일이었을 것이고 혼돈이었을 것이다. 너는 중요하지 않아라는 진실을 흘낏 엿본 바로 그 느낌일 것이다. 그 사다리가 데이비드에게 준 것은 바로 이것이다. 하나의 해독제. 하나의 거점. 중요성이라는 사랑스럽고 따뜻한 느낌. 그런 관점에서 보면 나는 그가 자연의 질서라는 비전을 그토록 단단하게 붙잡고 늘어졌던 이유를 이해할 수 있을 것 같다. 도덕과 이성과 진실에 맞서면서까지 그가 그렇게 맹렬하게 그 비전을 수호한 이유를.\n10 # 나는 살면서 내 인생의 많은 좋은 것들을 망쳐버렸다. 그리고 이제는 더 이상 나 자신을 속이지 않으려 한다. 그 곱슬머리 남자는 결코 돌아오지 않을 것이다. 데이비드 스타 조던은 나를 아름답고 새로운 경험으로 인도해주지 않을 것이다. 혼돈을 이길 방법은 없고, 결국 모든 게 다 괜찮아질 거라고 보장해주는 안내자도, 지름길도, 마법의 주문 따위도 없다. 희망을 놓아버린 다음에는 무슨 일을 해야 하지? 어디로 가야 할까?\n요약 # 자기가 옳은 일을 하고 있다고 자신을 설득하는데 성공하면 무한해 보이는 에너지로 목표를 추구할 수 있다. 하지만 믿음을 반증하는 증거가 나타났을 때도 맹목적으로 그 믿음을 보호하게 될수 있다. 그리고 근거가 *\u0026lsquo;실제로 옳은 일이기 때문\u0026rsquo;*이 아니라 *\u0026lsquo;깔끔한 질서를 잃고 이전의 혼돈으로 되돌아가기 때문\u0026rsquo;*일 수 있다. 결국\n긍정적 착각은 그릿을 획득하는 데 도움이 되지만 \u0026lsquo;궁극적인 진실을 받아들이는 능력\u0026rsquo;을 대가로 치러야 한다.\n책: 물고기는 존재하지 않는다\n"},{"id":144,"href":"/docs/study/tech/cs5/","title":"깃허브 오류 There was an error committing your changes: File could not be edited","section":"생물정보학","content":" [깃허브] 깃허브 오류 There was an error committing your changes: File could not be edited # 갑자기 모든 파일의 수정이 안되고 page deployment도 오류가 났다. 브라우저 캐시 문제인가 해서 방문기록이랑 캐시를 모두 삭제해보았다. 그래도 오류가 났다. 구글링하니까 내 경우랑 맞아떨어지는 한국인 블로그글이 있어서 시키는대로 https://www.githubstatus.com/에 들어가봤다. 블로그 글이랑 같은 창이 떴는데 그냥 기다려야된다길래 그냥 기다림. 2시간 뒤에 들어가니까 이 창으로 바뀌었다. 그리고 된다. 또 블로그 부셔진줄\u0026hellip; 다행이다\u0026hellip;.\n"},{"id":145,"href":"/docs/hobby/book/book21/","title":"깔끔한 상자 모서리는 든든하지만 환상일 뿐이다.","section":"글","content":" 깔끔한 상자 모서리는 든든하지만 환상일 뿐이다. # #2024-12-31\n1 # 더 나은 의사 결정을 하기 위해, 정보에 접근하고 해석하는 방식을 더 체계화할 필요는 없다. 머신러닝이 우리를 그런 방향으로 이끌 것이라고 예상하게 되지만 사실 그 반대다. 알고리즘은 복잡성과 무작위성 속에서 역할을 수행하며, 환경의 변화에 효율적으로 반응하는 능력이 탁월하다. 단순한 패턴을 추구하는 경향은 아이러니하게도 인간의 사고방식에서 나타난다.\n기계는 복잡한 현실을 전체적인 데이터 집합의 또 다른 일부로 여겨 단순하게 접근하는 데 반해, 정작 그로부터 도피하는 것은 우리 인간이다. 단순하거나 직접적이지 않은 대상을 더 복잡한 방식으로 사고하는 통찰력과 자발성이 인간에게 필요한 것이다.\n2 # 비지도 학습 머신러닝 중 클러스터링은 데이터를 A, B, C로 분류하려는 선입견 없이 \u0026ldquo;공통점\u0026quot;을 기준으로 분류한다. 미리 정한 결론에 꿰맞추기보다 데이터 자체가 말해주기를 바랄 때 특히 유용하다.\n3 # 상자는 유용한 증거와 대안을 모아 정돈된 형태로 만든 것이다. 상자 속 사고방식은 깔끔하기 때문에 선택을 분명하게 인지할 수 있다. 이에 반해 나무는 유기적으로 자란다. 나무는 우리를 사방으로 이끌 수 있고, 그중 상당수는 의사 결정의 막다른 길이나 완벽한 미궁으로 밝혀진다. 그러면 어느 쪽이 나을까? 상자, 아니면 나무? 정답은 \u0026lsquo;둘 다 필요하다\u0026rsquo;이다.\n4 # 상자 속에서 생각하는 사람이었던 나는 내 주변 세상과 사람들에 관해 모든 것을 알고 싶었고, 내가 더 많은 데이터를 모을수록 더 나은 결정을 내릴 수 있다고 스스로를 안심시켰다. 하지만 모은 정보를 효과적으로 처리할 방법이 없었기에 쓸모없는 잡동사니로 가득 찬 상자만 점점 늘어났다. 나는 이 과정 때문에 거의 움직일 수 없게 되었고, 때로는 몸을 어느 각도로 유지해야 하는지에 집중하느라 침대에서 벗어날 때조차 고군분투 해야했다.\n물론 분류는 강력한 도구이며 어떤 옷을 입을지, 무슨 영화를 볼지 같은 문제에서 즉각적으로 결정하는 데 유용하다. 그러나 정보를 처리하고 해석하며, 미래를 알기 위해 과거의 증거를 이용해서 까다로운 결정을 내리는 능력을 심각하게 억압한다.\n5 # 우리는 모두 모순과 불가측성, 무작위성을 헤쳐나간다. 이들은 삶을 현실로 만드는 요소다. 우리는 둘 이상의 선택지 중에서 골라야 하며, 고려해야 할 증거들은 파일로 정리되어있지 않다. 깔끔한 상자 모서리는 든든하지만 환상일 뿐이다. 현실의 그 무엇도 그렇게 딱 떨어지지 않기 때문이다. 상자는 고정되어 있고 휘어지지도 않지만, 우리의 삶은 역동적이며 계속 변한다.\n6 # 좋은 의사 결정은 보통 확실성을 가정하는 데서 나오지 않으며 혼돈, 다른 말로는 증거라는 것에서 나온다. 의사 결정을 둘러싼 데이터 집합을 충분히 깊이 탐색하지 않고 다양한 가능성과 결과를 고려하지 않는다면, 그리고 다양한 의사 결정으로 이어지는 나뭇가지가 일제히 닫히거나 열리지 않는다면 사실상 눈가리개를 한 채 선택하는 셈이다. 우리는 미래를 예측할 수 없지만, 데이터 포인트를 충분히 수집하고 가능성이 큰 계획을 구상하면 대부분 상황에서 제대로 된 지도를 손에 쥘 수 있다. 관행이나 미리 정해놓은 결과가 아니라 증거가 의사결정을 이끌 것이고, 다양한 결과와 각 결과가 미치는 영향을 스스로 고려할 수 있을 것이다.\n책: 자신의 존재에 대해 사과하지 말 것\n"},{"id":146,"href":"/docs/hobby/book/book2/","title":"당신의 특별한 우울 | 린다 개스크","section":"글","content":" 당신의 특별한 우울 | 린다 개스크 # #북마크\n애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다.\n불행한 것과 우울한 것.\n#기타\n결핍과 그에 대한 애도의 기간(라디오스타 김영철)\n"},{"id":147,"href":"/docs/study/bioinformatics/cs14/","title":"딥러닝을 이용한 자연어 처리 입문 | BERT","section":"생물정보학","content":" [딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT # 목록 # 2024-12-31 ⋯ 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)\n2024-12-31 ⋯ 17-03 구글 BERT의 마스크드 언어 모델\n2024-12-31 ⋯ 17-04 한국어 BERT의 마스크드 언어 모델\n2024-12-31 ⋯ 17-05 구글 BERT의 다음 문장 예측\n2024-12-31 ⋯ 17-06 한국어 BERT의 다음 문장 예측\n17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) # BERT?\nBERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식. 입/출력 구조 입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환. 출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것. 문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다. 구조와 연산 BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조. 각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다. BERT의 서브워드 토크나이저: WordPiece\n서브워드 토크나이저: 자주 등장하는 단어는 단어 단위로, 드물게 등장하는 단어는 서브워드(subword) 단위로 분리하는 방식의 토크나이저. WordPiece의 작동 원리 훈련 데이터로부터 단어 집합을 생성하는데, 자주 등장하는 단어는 단어 단위로 추가하고 드물게 등장하는 단어는 더 작은 단위(서브워드)로 쪼개어 추가한다. 토큰화: 단어가 단어 집합에 존재하면 그대로 사용하고 단어가 단어 집합에 없으면 서브워드로 분리한다. ex) 단어 \u0026ldquo;embeddings\u0026quot;가 단어 집합에 없으면 서브워드로 분리: em, ##bed, ##ding, #s. ##는 단어의 중간이나 끝에서 온 서브워드라는 표시이다. BERT는 서브워드 단위로 토큰화를 수행한 입력 데이터를 받아 문맥 정보를 반영한 임베딩을 생성한다. 요약\nBERT는 모든 단어가 서로를 참고하도록 트랜스포머 인코더(셀프 어텐션)를 활용해 문맥 정보를 포함한 임베딩을 생성한다. WordPiece 토크나이저는 단어를 자주 등장 여부에 따라 단어 또는 서브워드로 분리하여 토큰화를 수행하는데 서브워드 표기(##)를 통해 단어 복원이 가능하며, 단어 집합의 크기를 줄이면서 표현력을 높인다. transformers 패키지를 사용하여 BERT 토크나이저 사용하기 import pandas as pd from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) # Bert-base의 토크나이저 result = tokenizer.tokenize(\u0026#39;Here is the sentence I want embeddings for.\u0026#39;) print(result) print(tokenizer.vocab[\u0026#39;here\u0026#39;]) #print(tokenizer.vocab[\u0026#39;embeddings\u0026#39;]) [\u0026#39;here\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;want\u0026#39;, \u0026#39;em\u0026#39;, \u0026#39;##bed\u0026#39;, \u0026#39;##ding\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;.\u0026#39;] 2182 \u0026lsquo;Here is the sentence I want embeddings for.\u0026lsquo;라는 문장을 BERT의 토크나이저가 어떻게 토큰화하는지 확인하기. embeddings라는 단어는 단어 집합에 존재하지 않으므로 em, ##bed, ##ding, #s로 분리되었다. BERT의 단어 집합에 \u0026ldquo;here\u0026quot;가 있는지 조회 -\u0026gt; 단어 here이 정수 인코딩을 위해서 단어 집합 내부적으로 2182라는 정수로 맵핑되어져 있다. \u0026ldquo;embeddings\u0026quot;가 있는지 조회 -\u0026gt; KeyError: \u0026rsquo;embeddings\u0026rsquo; 발생. # BERT의 단어 집합을 vocabulary.txt에 저장 with open(\u0026#39;vocabulary.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: for token in tokenizer.vocab.keys(): f.write(token + \u0026#39;\\n\u0026#39;) df = pd.read_fwf(\u0026#39;vocabulary.txt\u0026#39;, header=None) print(\u0026#39;단어 집합의 크기 :\u0026#39;,len(df)) 단어 집합의 크기 : 30522 자료 출처\nhttps://wikidocs.net/115055\n17-03 구글 BERT의 마스크드 언어 모델 # from transformers import TFBertForMaskedLM, AutoTokenizer model = TFBertForMaskedLM.from_pretrained(\u0026#39;bert-large-uncased\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(\u0026#34;bert-large-uncased\u0026#34;) TFBertForMaskedLM: 마스크드 언어 모델(Masked Language Model, MLM)을 위한 BERT 구조 AutoTokenizer: 해당 모델 학습 시 사용된 토크나이저. inputs = tokenizer(\u0026#39;Soccer is a really fun [MASK].\u0026#39;, return_tensors=\u0026#39;tf\u0026#39;) print(inputs[\u0026#39;input_ids\u0026#39;]) tf.Tensor([[ 101 4715 2003 1037 2428 4569 103 1012 102]], shape=(1, 9), dtype=int32) 사전 학습된 BERT로 마스크드 언어 모델 생성함. 예제 문장: \u0026lsquo;Soccer is a really fun [MASK].\u0026lsquo;에 대해 토크나이저로 정수 인코딩을 수헹. [MASK] 토큰 예측하기? from transformers import FillMaskPipeline pip = FillMaskPipeline(model=model, tokenizer=tokenizer) pip(\u0026#39;Soccer is a really fun [MASK].\u0026#39;) [{\u0026#39;score\u0026#39;: 0.7621169686317444, \u0026#39;token\u0026#39;: 4368, \u0026#39;token_str\u0026#39;: \u0026#39;sport\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;soccer is a really fun sport.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.2034207135438919, \u0026#39;token\u0026#39;: 2208, \u0026#39;token_str\u0026#39;: \u0026#39;game\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;soccer is a really fun game.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.01220863126218319, \u0026#39;token\u0026#39;: 2518, \u0026#39;token_str\u0026#39;: \u0026#39;thing\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;soccer is a really fun thing.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.001863038633018732, \u0026#39;token\u0026#39;: 4023, \u0026#39;token_str\u0026#39;: \u0026#39;activity\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;soccer is a really fun activity.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.0013354964321479201, \u0026#39;token\u0026#39;: 2492, \u0026#39;token_str\u0026#39;: \u0026#39;field\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;soccer is a really fun field.\u0026#39;}] 모델과 토크나이저를 파이프라인에 지정. FillMaskPipeline을 사용하여 문장에서 [MASK] 위치에 들어갈 단어를 예측 결과는 [MASK]에 들어갈 가능성이 높은 단어 5개와 각 단어의 관련 정보 예제 결과 sport가 가장 높은 확률 0.7621을 가짐. 문장이 자연스럽고 문맥상 가장 적합하기 때문에 MLM 모델이 이를 첫 번째 후보로 예측했다. game은 두 번째로 높은 확률 0.2034을 가짐. thing, activity, field는 1.2%, 0.19, 0.13% 확률을 가짐. 자료 출처\nhttps://wikidocs.net/153992\n17-04 한국어 BERT의 마스크드 언어 모델 # \u0026lsquo;축구는 정말 재미있는 [MASK]다\u0026rsquo;를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측한다.\nfrom transformers import TFBertForMaskedLM from transformers import AutoTokenizer model = TFBertForMaskedLM.from_pretrained(\u0026#39;klue/bert-base\u0026#39;, from_pt=True) tokenizer = AutoTokenizer.from_pretrained(\u0026#34;klue/bert-base\u0026#34;) inputs = tokenizer(\u0026#39;축구는 정말 재미있는 [MASK]다.\u0026#39;, return_tensors=\u0026#39;tf\u0026#39;) print(inputs[\u0026#39;input_ids\u0026#39;]) print(inputs[\u0026#39;token_type_ids\u0026#39;]) print(inputs[\u0026#39;attention_mask\u0026#39;]) tf.Tensor([[ 2 4713 2259 3944 6001 2259 4 809 18 3]], shape=(1, 10), dtype=int32) tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32) tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32) klue/bert-base의 토크나이저를 사용해서 \u0026lsquo;축구는 정말 재미있는 [MASK]다\u0026rsquo;를 변환. 토크나이저로 변환된 결과: inputs input_ids: 정수로 변환된 토큰 시퀀스. token_type_ids: 문장 구분 (한 개 문장이므로 모두 0). attention_mask: 패딩 토큰 구분 (패딩 없음 → 모두 1). from transformers import FillMaskPipeline pip = FillMaskPipeline(model=model, tokenizer=tokenizer) pip(\u0026#39;축구는 정말 재미있는 [MASK]다.\u0026#39;) [{\u0026#39;score\u0026#39;: 0.8963565230369568, \u0026#39;token\u0026#39;: 4559, \u0026#39;token_str\u0026#39;: \u0026#39;스포츠\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;축구는 정말 재미있는 스포츠 다.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.025957893580198288, \u0026#39;token\u0026#39;: 568, \u0026#39;token_str\u0026#39;: \u0026#39;거\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;축구는 정말 재미있는 거 다.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.010034064762294292, \u0026#39;token\u0026#39;: 3682, \u0026#39;token_str\u0026#39;: \u0026#39;경기\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;축구는 정말 재미있는 경기 다.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.007924459874629974, \u0026#39;token\u0026#39;: 4713, \u0026#39;token_str\u0026#39;: \u0026#39;축구\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;축구는 정말 재미있는 축구 다.\u0026#39;}, {\u0026#39;score\u0026#39;: 0.007844261825084686, \u0026#39;token\u0026#39;: 5845, \u0026#39;token_str\u0026#39;: \u0026#39;놀이\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;축구는 정말 재미있는 놀이 다.\u0026#39;}] FillMaskPipeline으로 [MASK] 위치에 들어갈 수 있는 상위 5개 후보 단어 예측. \u0026ldquo;스포츠\u0026quot;가 문맥상 가장 적합한 단어로 높은 점수를 받았다. 자료 출처\nhttps://wikidocs.net/152922\n17-05 구글 BERT의 다음 문장 예측 # import tensorflow as tf from transformers import TFBertForNextSentencePrediction, AutoTokenizer model = TFBertForNextSentencePrediction.from_pretrained(\u0026#39;bert-base-uncased\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(\u0026#39;bert-base-uncased\u0026#39;) prompt = \u0026#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\u0026#34; next_sentence = \u0026#34;pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\u0026#34; encoding = tokenizer(prompt, next_sentence, return_tensors=\u0026#39;tf\u0026#39;) print(encoding[\u0026#39;input_ids\u0026#39;]) print(tokenizer.cls_token, \u0026#39;:\u0026#39;, tokenizer.cls_token_id) print(tokenizer.sep_token, \u0026#39;:\u0026#39; , tokenizer.sep_token_id) print(tokenizer.decode(encoding[\u0026#39;input_ids\u0026#39;][0])) print(encoding[\u0026#39;token_type_ids\u0026#39;]) tf.Tensor( [[ 101 1999 3304 1010 10733 2366 1999 5337 10906 1010 2107 2004 2012 1037 4825 1010 2003 3591 4895 14540 6610 2094 1012 102 10733 2003 8828 2007 1996 2224 1997 1037 5442 1998 9292 1012 1999 10017 10906 1010 2174 1010 2009 2003 3013 2046 17632 2015 2000 2022 8828 2096 2218 1999 1996 2192 1012 102]], shape=(1, 58), dtype=int32) [CLS] : 101 [SEP] : 102 [CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP] tf.Tensor( [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32) 모델과 토크나이저를 로드하고, 토크나이저로 두 문장을 정수 인코딩했다. input_ids는 정수로 변환된 토큰 시퀀스이다. 여기서 101과 102는 특별 토큰인 [CLS] 토큰과 [SEP] 토큰이다. 정수 인코딩 결과를 다시 디코딩해서 현재 입력의 구성을 확인해보면 BERT에서 두 개의 문장이 입력으로 들어갈 경우에 맨 앞에는 [CLS] 토큰, 문장이 끝나면 [SEP] 토큰, 두번째 문장이 종료되었을 때 다시 [SEP] 토큰이 추가된다 token_type_ids는 두 문장을 구분하기 위한 세그먼트 인코딩이다. 첫 번째 문장은 0, 두 번째 문장은 1. logits = model(encoding[\u0026#39;input_ids\u0026#39;], token_type_ids=encoding[\u0026#39;token_type_ids\u0026#39;])[0] softmax = tf.keras.layers.Softmax() probs = softmax(logits) print(probs) print(\u0026#39;최종 예측 레이블 :\u0026#39;, tf.math.argmax(probs, axis=-1).numpy()) tf.Tensor([[9.9999714e-01 2.8381860e-06]], shape=(1, 2), dtype=float32) 최종 예측 레이블 : [0] 다음 문장 예측하기 BERT 모델에 입력 데이터를 넣어 logits(예측 점수)를 반환 소프트맥스를 적용해 각 레이블(0 또는 1)에 대한 확률 계산. 예측 결과 이어지는 문장일 확률(레이블 0): 99.9997% 이어지지 않는 문장일 확률(레이블 1) 0.00028% 최종 예측은 레이블 0으로써 두 문장이 이어진다고 판단함. # 상관없는 두 개의 문장 prompt = \u0026#34;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\u0026#34; next_sentence = \u0026#34;The sky is blue due to the shorter wavelength of blue light.\u0026#34; encoding = tokenizer(prompt, next_sentence, return_tensors=\u0026#39;tf\u0026#39;) logits = model(encoding[\u0026#39;input_ids\u0026#39;], token_type_ids=encoding[\u0026#39;token_type_ids\u0026#39;])[0] softmax = tf.keras.layers.Softmax() probs = softmax(logits) print(\u0026#39;최종 예측 레이블 :\u0026#39;, tf.math.argmax(probs, axis=-1).numpy()) 최종 예측 레이블 : [1] 이어지지 않는 두 개의 문장으로 테스트 예측 결과: 이어지지 않는다고 판단. 자료 출처\nhttps://wikidocs.net/156767\n17-06 한국어 BERT의 다음 문장 예측 # import tensorflow as tf from transformers import TFBertForNextSentencePrediction from transformers import AutoTokenizer model = TFBertForNextSentencePrediction.from_pretrained(\u0026#39;klue/bert-base\u0026#39;, from_pt=True) tokenizer = AutoTokenizer.from_pretrained(\u0026#34;klue/bert-base\u0026#34;) # 이어지는 두 개의 문장 prompt = \u0026#34;2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\u0026#34; next_sentence = \u0026#34;여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\u0026#34; encoding = tokenizer(prompt, next_sentence, return_tensors=\u0026#39;tf\u0026#39;) logits = model(encoding[\u0026#39;input_ids\u0026#39;], token_type_ids=encoding[\u0026#39;token_type_ids\u0026#39;])[0] softmax = tf.keras.layers.Softmax() probs = softmax(logits) print(\u0026#39;최종 예측 레이블 :\u0026#39;, tf.math.argmax(probs, axis=-1).numpy()) 최종 예측 레이블 : [0] 모델과 토크나이저 로드\nTFBertForNextSentencePrediction.from_pretrained(\u0026lsquo;BERT 모델 이름\u0026rsquo;)을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드. AutoTokenizer.from_pretrained(\u0026lsquo;모델 이름\u0026rsquo;)을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드. 예측 결과: 두 문장이 이어진다고 판단.\n# 상관없는 두 개의 문장 prompt = \u0026#34;2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\u0026#34; next_sentence = \u0026#34;극장가서 로맨스 영화를 보고싶어요\u0026#34; encoding = tokenizer(prompt, next_sentence, return_tensors=\u0026#39;tf\u0026#39;) logits = model(encoding[\u0026#39;input_ids\u0026#39;], token_type_ids=encoding[\u0026#39;token_type_ids\u0026#39;])[0] softmax = tf.keras.layers.Softmax() probs = softmax(logits) print(\u0026#39;최종 예측 레이블 :\u0026#39;, tf.math.argmax(probs, axis=-1).numpy()) 최종 예측 레이블 : [1] 이어지지 않는 두 개의 문장으로 테스트 예측 결과: 이어지지 않는다고 판단. 자료 출처\nhttps://wikidocs.net/156774\n"},{"id":148,"href":"/docs/hobby/book/book13/","title":"루틴의 힘 | 댄 애리얼리, 그레첸 루빈, 세스 고딘 외","section":"글","content":" 루틴의 힘 | 댄 애리얼리, 그레첸 루빈, 세스 고딘 외 # 흔들리지 않고 끝까지 계속하게 만드는 루틴의 힘\n#북마크\n전문가의 세상으로 나가는것에 대한 두려움\n진전의 가시화\n"},{"id":149,"href":"/docs/hobby/book/book22/","title":"모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까?","section":"글","content":" 모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까? # #2024-12-31\n1 # 시간과 공간은 고정된 것도 아니고, 무한한 것도 아니며, 서로 독립적인 것도 아니다. 우주를 이해하려면 이들을 합쳐서 4차원, 즉 공간을 나타내는 세 축과 시간을 나타내는 한 축으로 시각화해야 한다.\n호킹 박사는 \u0026lsquo;시공(spacetime)\u0026rsquo; 이라는 개념을 시각화할 때 광원뿔(light cone) 이미지를 활용해 과거와 미래의 사건이 어떻게 연결되는지 보여주었다. 빛은 발산될 때 연못의 물결처럼 퍼져나가면서 원뿔 형태를 형성한다. 빛의 속도보다 빠른 것은 없으므로 (과거에) 기여하거나 (미래에서) 시작된 현재 순간의 모든 사건은 이 원뿔 안에서 빛의 속도나 그보다 느린 속도로 일어나야만 한다.\n호킹은 원뿔 밖에서 일어나는 사건은 다른 곳에 있다고 말한다. 따라서 그 사건들은 현재를 바꿀 수 없고 현재에 의해 바뀔 수도 없다. 이를 설명하기 위해 호킹은 어느 날 갑자기 태양이 죽는다는 시나리오를 얘기했다. 이 사건은 과거의 광원뿔에서 일어나지 않았고, 태양에서 지구까지 빛이 도착하려면 8분이 걸리기 때문에 현재에 영향을 미치지 않는다. 오직 이 지점에서만, 미래의 광원뿔까지의 어느 정도 거리에서만 이 사건이 우리의 현실과 교차하고 현실을 변화시킨다. 우리는 사건이 실제로 일어났을 때가 아니라 우리으 ㅣ의식을 가로지르기 시작한 순간에 그 사실을 인정한다.\n우리는 모두 우리에게 일어난 일을 통해 배우고 다음에 일어날 일을 바꿀 방법을 찾는다. 우리는 확실성을 원하지만 기회도 원한다. 미래가 안전하다고 느끼기를 바라지만 동시에 가능성에 고무되기를 바란다. 우리가 영향력을 행사하지 못하는 것이 있음을 인정하면서, 그럼에도 우리가 바꿀 수 있는 것이 무엇인지 알고 싶어 한다. 우리는 목표를 설정하고, 판단에 따른 결정을 내리고, 우선순위를 미세하게 조정하는 더 나은 방법을 바란다. 미래를 효율적으로 계획할 도구뿐만 아니라 현재를 살아가는 방법도 필요하다.\n다행히 이런 질문은 잠들지 못해 깨어있는 밤이나, 올해 목표와 다짐을 적는 새해 아침에만 고민하는 질문이 아니다. 이론물리학은 우리를 위해 어려운 부분을 상당히 많이 해결했다. 이론물리학은 삶의 사건을 시각화해서 앞으로 나아갈 길을 계획하고 원하는 결과를 얻는 가능성을 극대화하는 방법을 알려준다. 심지어 더 좋은 점은 내가 여덟 살의 나를 안심시켰듯이, 이론물리학이 알려주는 방법은 이진법 모델과 냉혹한 광원뿔의 경계선에 의존하지 않는다는 것이다. 이 장에서 소개할 개념인 네트워크이론, 토폴로지, 경사하강법을 활용하면 인간만큼이나 유연하고 변하기 쉬운 삶을 계획할 수 있다. 그리고 그에 따라 목표를 설정할 수 있다.\n아마도 삶의 계획과 목표를 세울 때 마주하는 가장 중요한 질문은 \u0026lsquo;무엇에 집중할까?\u0026lsquo;일 것이다. 현재와 미래, 어느 쪽에 집중해야 할까? 지금 느낄 만족감인가, 아니면 뒤로 미룰 기쁨인가? 끊임없이 장기 계획을 세우느라 현재의 삶을 즐기지 못하는가? 아니면 현재에 너무 집중한 나머지 다가올 미래를 제대로 준비하지 못하는가?\n모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까?\n이 딜레마를 두고 너무 고심하느라 걱정한 적이 있다면, 양자역학이 당신을 안심시켜줄 것이다. 양자역학은 우리가 아는 한 가장 작은 입자인 아원자입자(원자보다 더 작은 입자)를 연구하는 이론물리학의 한 분야다. 하이젠베르크의 불확정성 원리는 아원자입자의 위치를 더 정확하게 측정할수록 입자의 운동량을 측정하기는 더 어려워진다고 우리에게 말한다. 역의 명제도 똑같이 적용된다. 다시 말하면 물리학은 우리에게 위치와 운동 속도를 동시에 정확하게 측정할 수 없다고 말해준다. 한쪽에 집중할수록 다른 쪽의 측정은 부정확해진다.\n어디서 들어본 것 같은가? 하이젠베르크는 양자입자에 관해 썼겠지만, 같은 원리가 거시 세계인 우리의 일상에도 적용되는 듯하다. 정밀 측정 장비에도 한계가 있듯이, 집중하고 우선순위를 매기는 우리의 능력도 마찬가지다. 훌륭한 파티를 주최하는 동시에 파티를 즐길 수는 없다. 파티에 대해 고민하든지 파티를 즐기든지, 재미있는 시간을 보내든지 다른 사람은 어떤지 걱정하든지 둘 중 하나다. 하나를 하면 다른 하나를 하는 능력이 억제된다. 특히 나처럼 \u0026lsquo;재미있게 노는 법\u0026rsquo;을 준비하려고 구글에 검색해야 한다몀ㄴ 말이다.\n이는 성인의 딜레마로, 우리는 끊임없이 모순되는 두 개의 욕구를 인식한다. 현재를 즐기거나, 미래를 계획하거나. 동시에 두 가지 모두 챙기려는 욕망은 둘 중 하나를 적절하게 성취할 능력을 조금씩 갉아먹는다. 우리는 앞으로 무엇이 다가올지 걱정하느라 현재를 즐기지 못하거나, 너무나 즐겁게 지내느라 미래를 대비할 여유를 갖지 못한다. 정보 중심의 연구에 기반을 둔 삶을 즐기는 나조차, 그저 배움을 멈추고 세계에 무지한 채 행복에 젖어 진실로 순간을 살아가는 아이로 되돌아가고 싶을 때가 있다.\n아빠와 부엌에서 생선을 요리하거나 정원에서 놀고, 마음 가는 대로 수많은 모래성을 만들고, 멋지고 다채로운 색상의 수영복을 입은 채 루 해변의 \u0026lsquo;밀리의 바위\u0026rsquo;에 앉아있기도 했다. 일곱 살에는 체크무늬를 좋아했고, 엄마의 푸른색 덴마크산 그릇으로 영화의 한 장면을 재현하거나 나를 두근거리게 하는 남성과의 미래를 상상하는 것도 좋아했다. 물론 그 남성은 스티븐 호킹이었다. 모든 기억의 색, 맛, 냄새가 20년이 지난 지금도 생생하게 내 마음에 남아있다. 타인이 어떻게 생각하든 개의치 않고 무엇이든 내가 원하는 것을 했던 시절, 즐거운 삶이었다.\n온갖 취미가 뒤섞인 이 주머니는 무작위였을 수도 있고 일정한 형태가 없어 보이기도 하지만, 모두 과거의 광원뿔을 형성하는 일부로서 지금 여기까지 나를 이끌어왔다. 내 흥미와 독자성, 개성을 강화하는 경험의 축적이다. 이 기억들은 대세에서 나만 소외되리라는 두려움이나 다음에 무슨 일이 일어날지에 대한 걱정이 없었던 때를 상기시킨다.\n균형을 잡으려 노력하던 나는 시간과 공간을 이동하는 파동을 연구하는 또 다른 양자역학 분야에서 영감을 받았다. 이는 전통적인 하이젠베르크 문제, 즉 특정 순간에는 파동의 운동량이나 파동의 위치 둘 중 하나만 정확하게 기술할 수 있다는 문제를 가리킨다. 양손 손가락을 동시에 마주 대려 해보라. 자꾸 어긋나서 쉽지 않을 것이다. 이 문제를 해결하기 위해 우리는 확률파동(wave packet)이라는 것을 만들었다. 확률파동은 수많은 다양한 파동을 합성해서 시각화한 것으로, 과학자들은 파동들이 나타내는 총체적 행동을 연구한다. 하나의 파동은 분명하게 정의하기 힘들지만, 여러 파동 \u0026lsquo;뭉치(packet)\u0026lsquo;은 더 효율적으로 연구할 수 있다. 목표를 설정하고 삶의 계획을 세우는 일도 크게 다르지 않다. 따로 떼어놓고 보면 하나하나의 결정이나 목표가 올바른지 알기 힘들다. 이럴 때는 큰 그림과 맥락, 즉 전체 \u0026lsquo;뭉치\u0026rsquo;를 살펴야 지금 이 순간뿐만 아니라 미래 전체의 최상의 결과와 비교해서 우리가 가능한 최고의 선택을 하는지 알 수 있다.\n가상의 확률파동을 만들면서 나는 삶을 숙고하는 두 가지 사고방식이 이루는 또 다른 균형에 부딪혀야 했다. 모멘텀 사고(momentom thinking)는 시간에 따라 살면서 한 시간에서 다른 시간으로 옮겨 가도록 하며, 이 사고에 따르면 행복은 우리가 성취하고 계획한 것으로 정의된다(즉, 책임이라는 어른의 세계다). 반면, 포지션 사고(position thinking)는 현재를 살면서 현재 순간과 현재가 주는 느낌에 사로잡혀 다른 모든 것을 차단하고 그저 존재하게 하는데, 여기에는 죄책감까지 따른다. 포지션 사고를 받아들이기는 매우 힘든데, 그것이 \u0026lsquo;제대로 된 어른\u0026rsquo;이 되려면 해야 한다고 들어왔던 것과 완전히 어긋나기 때문이다. 그러나 이 역시도 꼭 필요하다. 가만히 서 있다고 해서 멈춰 있다는 뜻은 아니다. 오히려 더 창의적이고, 현재 거치는 과정을 재평가하며, 감각의 힘을 통해 살아가고, 미래를 위해 더 많은 가능성을 탐색한다.\n다음에 무엇을 할지 집착하고 거의 모든 삶의 순간에 끼어들며 현재 이 순간의 즐거움을 부정하는 모멘텀 사고의 연결 고리를 끊을 방법이 필요했다. 나는 명확한 미래에 대하 ㄴ끊임없는 욕구를 희생하지 않으면서도 순간을 살아가는 능력을 회복하고 싶었다. 그래서 2013년, 변화가 사회적으로 수용되는 시기인 사순절(부활절 전 40일 동안의 기간. 단식을 하기도 한다) 직전에 특별한 팬케이크 한 접시를 먹으며 실험을 개시했다. 완벽하고 엄격하게 해야 할 일을 확인하고 모든 우선 사항을 처리했다. 나머지 절반은 포지션 사고를 하며 살았다. 모든 순간을 즐기고 미래에 관한 생각은 전혀 하지 않았다.\n이제 당신은 아마 이 계획이 잘되지 않았으리라고 짐작할 만큼은 나를 잘 알 것이다*. 지금의 나를 만든, 지극히 중요하지만 실패한 또 하나의 실험이었다. 실험하면서도 현재의 즐거움이든 미래의 명확성이든, 실험을 침식하는 무엇인가를 놓치고 있다는 생각을 멈출 수 없었다. 파티를 열고도 파티가 끝난 후 해야 할 설거지 생각을 멈출 수 없었다. 나는 과정을 관찰하는 것만으로도 관찰자가 근본적으로 결과에 영향을 미치고 결과를 바꿀 수 있다는 또 다른 양자역학 교리, 즉 관찰자 효과의 희생자가 되었던 것이다. 이를 설명할 때 가장 많이 드는 예시로는 현미경으로 전자를 관찰하는 사례가 있다. 관찰자가 광자를 투사하는 데 의존하면 이 행위가 광자의 운동 방향을 바꿀 것이다. 이처럼 내가 내 실험을 관찰하는 행위는 당연히 결과를 왜곡했다. 나는 무엇을 빠뜨렸는지 생각하느라 너무 바빠서 그 순간의 나를 즐길 수 없었다.\n실패한 실험 덕분에 나는 포지션 사고와 모멘텀 사고, 현재와 미래 사이의 어디쯤에서 타협할 수 있었다. 평범한 날의 각기 다른 순간에, 나는 바로 그 특정 순간에 내게 가장 필요한 사고로 전환되기를 바라면서 두 사고 사이를 반복해서 왔다 갔다 할 것이다. 나는 지금 당장 모든 것을 원하며 시간이라는 개념 자체가 없는 ADHD와 싸우면서, 현재를 사는 것과 미래를 계획하는 것 사이에서 적당히 춤출 것이다. 불확정성의 원리를 알기만 해도 올바른 균형을 이루는 데 도움이 된다. 내가 발견했듯이 이 둘을 완벽하게 구분하기란 불가능하지만, 그저 이 둘이 양립할 수 없다는 사실을 수용하는 것만으로도 자유로워질 수 있다. 지금 하지 않는 일을 할 시간이 나중에 있을 것이며, 오후에 햇볕을 쬐면서, 혹은 모두가 밖에서 즐기는 동안 안에서 계획을 세우면서 죄책감을 느끼지 않아야 한다는 점을 깨달으면, 우리가 하지 않는 일에 대한 걱정을 덜 수 있다.\n그러나 현재를 사는 것과 미래를 계획하는 것이 다르다는 사실을 인식하고 두 사고방식이 정확히 맞물리게 노력하는 것만으로는 충분하지 않다. 현재와 미래가 어떻게 연결되는지 시각화할 방법도 필요하다. 그러면 목표를 설정하는 방법을 명확하게 선택하고 우리의 여행 속도에 안심할 수 있을 것이다. 바로 여기서, 내 삶에서 가장 신뢰하는 동맹인 네트워크이론이 진가를 발휘한다.\n*사실 저자를 이해한 것이 아님. 내게도 계획은 항상 어그러지는 쪽이었다. a와 b중 a 로 방향을 틀자마자 세상은 b 방향으로 휘어진다. 왜일까? a를 선택하자마자 갑자기 세상에서 a에 대한 반례가 속출하고 다시 양 갈래 길로 돌아오게 된다. 관찰자 효과였을수도 있겠다는 생각이 드네..\n2 # \u0026lt;짧고 쉽게 쓴 \u0026lsquo;시간의 역사\u0026rsquo;\u0026gt;를 읽은 후, 나는 광원뿔의 고정된 경계선보다 내 요구를 더 잘 충족해줄 예측 모델을 찾아 헤맸다. 나는 전통적인 인간의 모순, 즉 확실성에 대한 욕구와 정해진 한계에 대한 좌절감의 모순에 사로잡혔다. 다음에 무슨 일이 일어날지 모르는 것을 제외하면 내게 주어진 계획의 한계만큼 나를 놀라게 하는 것은 없다. 이런 두꺼운 직선을 필요에 따라 구부리고 주변을 탐색할 구불구불한 선으로 바꾸려면 유연성이 필요하다.\n나는 집을 나서는 데만 다섯 시간이 걸리는 끝없는 준비의 필요성과, 오랜 시간 신중하게 생각해 온 것을 극심한 조바심이 폭발하는 순간 모두 파기해버리는 성향, 두 가지 픅면 모두를 고려한 계획법이 필요했다. 이런 성향은 일종의 심리적인 뇌 정지 상태로, 오늘 하루가 레몬 셔벗과 비슷할 것으로 생각하는 순간, 바닐라 아이스크림과 더 비슷해지는 것과 같다. 현재와 미래를 조화시키려는 나의 하이젠베르크식 전투는, ADHD의 시간 왜곡과 나를 계속 바닥으로 짓누르는 정신 가속기 덕분에 더 치열해진다.\n이 모든 것을 처리하는 데 네트워크이론이 나의 구원자가 되었다. 이 이론은 상당히 단순한 개념이다. 연결된 대상을 그래프로 나타내고, 총체적으로 형성되는 네트워크를 시각화하며, 이런 연결성이 우리에게 알려주는 것이 무엇인지 연구한다. 네트워크이론과 그래프 이로닝라는 연관된 기술을 이용해서, 우리는 복잡하고 밀접하며 동적인 계를 분석할 수 있다.\n네트워크는 대상이나 사람들이 연결된 연속체다. 당신과 친구, 이웃은 여러 사회적 네트워크로 연결되어 있다. 런던 지하철은 서로 다른 노선으로 연결된 정거장 네트워크다. 토스터 플러그 속에 든 전기회로도 네트워크다. 와이파이와 무선 근거리통신망 일부에 연결된 채 여러분 옆에 놓여 있을 스마트폰은 아마 현재 네트워크의 일부일 것이다. 인터넷은 그 자체가 물리적으로나 무선으로 연결된 컴퓨터들의 메가 네트워크로, 그를 통해 방대한 양의 자료가 움직인다.\n물질세계에서 디지털 세계까지, 사회에서 과학까지, 네트워크는 어디에나 있다. 네트워크는 무형이지만 분명히 실재하는 구조이며, 우리가 수십 년에 걸쳐 경력을 쌓는 과정부터 지금 우리가 인터넷에 연결되는 방법까지, 모든 것에 영향을 미친다.\n네트워크는 장기간 및 단기간의 삶을 계획하고 시각화하는 이상적인 방법을 제공하기도 한다. 우리는 너무나 많은 것에 영향받으며 사방으로 밀고 밀리므로, 미리 계획을 세우는 투두 리스트보다 더 복잡하고 반복적이며 적용하기 쉬운 모델이 필요하다. 네트워크이론이 바로 이것을 제공하며, 특히 토폴로지는 네트워크 구성 요소인 노드(node, 컴퓨터과학의 기초 단위. 보통 네트워크에 연결된 하나의 기기를 뜻한다)가 연결되는 방식과 형성되는 구조를 알려준다. 토폴로지(네트워크의 요소들을 물리적으로 연결하는 방식)는 경직된 직선을 유동성 있는 가능성의 네트워크로 바꿔준다. 어둠 속에 감춰진 것을 밝은 곳으로 끌어내고, 정점에 이른 내 불안을 느슨하게 풀어준다. 한때는 유용했던 원리가 더는 쓸모없을 때나, 싹트는 생각이 이제 번성할 준비가 되었을 때를 알아차리게 돕기도 한다.\n토폴로지의 본질은 매우 중요하다. 여섯 개의 단추로 패턴을 만들 때, 당신은 선이나 원, V자를 만들 수 있다. 토폴로지는 네트워크의 기능, 즉 역량과 한계를 결정한다. 우리가 살면서 의사 결정을 하고 우선순위를 설정할 때도 똑같은 일을 한다. 즉, 단기간 및 장기간의 결과를 결정할 유용한 증거와 선택을 패턴으로 배열한다.\n미래의 삶을 하나의 거대한 네트워크로 생각해보면 이 네트워크의 노드는 사람부터 희망, 두려움, 목표까지, 무엇이든 될 수 있다. 이것은 내가 발견한 계획법 중에서 너무 단순하지도, 불편할 정도로 제한적이지도 않은 최고의 방법이다. 역동적으로 당신의 환경이 그렇듯 적응력이 있어서 유용하다. 게다가 무엇이 정말 중요하고 중요하지 않은지 알 수 있도록 도와주므로 명확하다. 또 연결성에 초점을 맞추므로, 연결된 노드를 확인하여 어떤 노드가 영향을 주고받는지 살피며 특정 경로가 어디로 이어질지 알려준다.\n네트워크는 호킹이 알려준 대로 시간과 공간의 맥락에서 광원뿔의 궤도에 한정되지 않고 생각하게 해준다. 또 우리가 시간과 공간이라는 이중 캔버스에서 사람, 특정 목표, 삶의 단계 사이의 근접성과 거리를 탐색하게 돕는다. 어떤 사건이 일어나야 하는지, 그 사건이 일어나게 하려면 언제 어디에 있어야 하는지 알려준다. 시간이 지나면서 나는 호킹의 다이어그램에 선이 존재하는 이유를 깨달았다. 소음에서 신호를 찾아내고, 길이나 자기 삶을 잃을 것 같은 불안을 극복하려면 우리에게 방향성이 필요하기 때문이다*. 그러나 네트워크는 이런 직선을 구불구불한 선으로 부드럽게 바꾸며, 시간이 흐르면서 고정된 광원뿔을 다른 면이 빛에 노출되도록 스스로 접히고 돌돌 말리는 잎사귀 모양으로 바꾼다. 우리에게 구조를, 따라갈 길을, 유연성 있는 움직임을 준다**.\n시간과 공간에 걸쳐서 네트워크를 만들 때 필요한 능력, 즉 다음에 무슨 일이 일어나야 하는지 명확하게 인식하는 능력이 있어야만 현재에 대한 과도한 불안을 피하고 미래에 대한 두려움을 없앨 수 있다. 목표 목록 자체는 도움이 되지 않는다. 목록에는 맥락이 없고, 서로 연결되어 있다는 감각도 없으며, 선호도를 설정할 방법도 없기 때문이다. 그것은 삶의 선형성에는 적절할 수 있지만 의사 결정에는 목표와 함께 사람과 장소를 계획하는 네트워크가 필요하며, 이 네트워크는 특정 형태를 고수할 필요 없이 오직 당신의 의도에만 맞으면 된다. 그러나 이 중 어느 것도 우리가 자신의 토폴로지를 친구나 동료의 것과 비교하면서 불안해하거나 부러워하지 않을 거라고, 갖고 싶은 것과 가질 수 있는 모든 것을 궁금해하지 않을 거라고, 대열의 끝으로 밀려날 것을 걱정하지 않을 거라고 보장해주지는 않는다. 네트워크이론은 당신을 자신만 뒤쳐지거나 소외될까봐 두려워하는 마음에서 구원할 수는 없지만, 최소한 당신이 유연하게 형태를 만들어나가면 시간이 흐르면서 진화할 방향과 목적은 알려줄 수 있다.\n일단 네트워크를 만들었다면 탐색을 시작해서, 대량의 정보와 구성 요소 중에서 어떤 것이 성공을 향해 나아가는 길을 보여주는지 알아내야 한다. 어떻게 해야 최적의 경로를 발견하고 발전시켜서, 상황이 바뀔 때마다 움직일 수 있는 부분을 계속 뒤섞을 수 있을까?\n​*삶에서 중요한 것들은 기준이 필요하다. 중요하고 필요한 요소들은 선으로 정해두기. 중요하고 소중한 것은 규정하지 않고 존재하는 그대로 건드리지 말고 두기. 그 상태 그대로도 선명하게 드러나게 하려면 중요하고 필요한 요소들이 선명하게 배경을 형성해줘야한다.\n**열심히 생각해서 가장 적절한 해를 내놓는 식을 통해 결과를 내야 하는 일이 있고, \u0026ldquo;결정\u0026rdquo; 방법을 \u0026ldquo;식\u0026rdquo; 같은게 아니라 그 사안만의 결정하는 방법대로 두고 어느순간 결정할만큼 선명해졌을때, 그 시점이 정답이라고 믿고 그 시점에서의 위치를 결과로 내야하는일도 있고. 그런것같네\n3 # 경사하강법은 머신러닝에서 가장 기본적인 기술의 하나이며, 삶의 네트워크를 탐색하는 우리 모두에게 여러 가지 교훈을 주는 개념이다. 첫 번째 교훈은 우리는 경로 전체를 미리 볼 수 없으며, 심지어 대부분을 볼 수도 없다는 것이다. 노드를 연결하고 군집을 확인할 수는 있지만 결국 길을 따라 아래로, 즉 미래로 갈수록 우리의 시야는 흐릿해진다. 하지만 그래도 괜찮다. 경사하강법의 두 번째 교훈은 현재의 전후 사정이 당신이 지금 당장 알아야 할 모든 것을 말해준다는 것이기 때문이다. 알고리즘이 결정 과정에서 경사도를 시험하듯이, 우리도 우리만의 기준에 따라 특정 경로의 가치를 판단해야 한다. 이 길이 우리를 더 행복하게 하는가, 성취감이 더 큰가, 더 의미 있는가? 우리는 미래에 어떤 일이 어떻게 진행될지 예측할 수 없지만, 여행의 방향을 시험해보고 삶의 비용함수를 최소화하는 방향으로 나아갈 수는 있다. 여기서 가치와 목적에 대한 감각을 개발하고, 매슬로의 욕구 단계의 상층을 충족하는 일이 가능해진다. 매슬로의 욕구 단계는 일단 음식과 쉼터처럼 가장 기본적인 인간 욕구를 충족하면 우리의 관심은 더 덧없는 문제, 즉 성취감을 느끼고 존경받고 문제를 해결하고 창의적으로 생각하는 능력 같은 것으로 이동한다고 말한다.\n만약 그 방향에 대한 선호도가 떨어지기 시작하면, 즉 경사도가 차츰 감소하면 당신의 모멘텀도 줄어들면서 침체되거나 멍해지거나 그저 뭔가 잘못된 것 같은 기분이 들면서 변화하게 된다.* 경사하강법 알고리즘은 선택에 한해서는 감상적이지 않다. 만약 가장 가파른 하강 경로로 되돌아갈 수 있다면 기꺼이 두 단계 뒤로 물러난다. 우리도 그렇게 해야 한다. 우리도 경로를 선택하고 적응하는 과정을 반복해야 하며, 언제든 목표와 행복에 가까워지는 것이 아니라 멀어지고 있다고 느끼면 경로를 바꿔야 한다.** 또한 곧고 완벽하고 유일한 길은 없으며, 다만 발견해서 따라가기까지 기꺼움, 흥미, 인내심을 가져야 하는 길만 있다는 걸 받아들여야 한다. 당신이 선택하는 최고의 경로는 항상 객관적인 안정성 보다는 여러 요인에 좌우될 것이다. 이는 선택 사항을 탐색할 시간이 얼마나 있는가, 그리고 당신이 어느 정도의 완벽주의자인가에 달렸다.***\n목표를 설정하고 추구하는 일은 두려울 수 있지만, 내가 사랑하는 스포츠인 암벽 등반처럼 이것도 그저 적절한 장비와 개인의 노력 문제다. 하이젠베르크는 우리에게 빌레이(암벽 등반에서 등반자의 추락을 방지하기 위한 로프 조작 기술)를, 네트워크이론은 밧줄을, 경사하강법은 경로를 제공한다.\n*학습에서 멈춰야할 지점은 어디일까? 학습의 목표가 되는 적용 분야에의 적합성보다는 학습 자체가 목적이 되어버리는 어느 지점에서 멈춰야 할 것이다. 그 말은? 너무 정확해지면. 또는 너무 어디서 본것같아지면. 그럼 학습을 멈추고 남은 역량은 어디에 써야하는가? 목표에 안전하게 도착하는데 써야한다. 역량은 조금 남아야 제대로 분배한거다.\n**그리디 알고리즘의 반대 발명하면 좋을듯. 대인배 알고리즘 aka 상남자 알고리즘. 너무 멀어질때만 수정하면됨. (ㅋㅋ)\n***\u0026ldquo;동전 던지기 결과\u0026quot;를 예측하기 위한 학습을 수행한다고 하자. 1/2 라는 답보다 아주 적절한 양의 적은 오차를 넣는게 정답에 더 가까울 때도 있을 것이다. 아닐 때도 있ㅇ르 것이다. 고장난 시계가 하루에 두번 맞듯이. 그렇다고 해서 \u0026ldquo;동전\u0026quot;이 앞면과 뒷면 말고 다른 면을 갖고있는 것은 아니다. 동전은 정확히 앞면과 뒷면만 존재하며 1을 둘에 할당하면 (미묘한 무게차이 같은걸 신경쓰지 않으면) 1/2 를 할당하는 것 말고 다른 방법은 (상식적으로) 억지이다. 하지만 10번 던졌을때 정확히 5번이 나오는가 하면, 10번 던지기를 10번 해보면 5번이 나오는 게 더 적을 것이다. 목적을 확실히 해야한다. \u0026ldquo;동전\u0026quot;의 특성을 알아내는 것인가? 아니면 \u0026ldquo;동전 던지기 결과\u0026quot;를 정확히 예측하는 것인가? \u0026ldquo;동전\u0026quot;의 특성을 알아내는게 목적이라면 10번을 10번 한 뒤 smoothing을 해서 1/2 로 결정하면 된다. \u0026ldquo;동전 던지기 결과\u0026quot;를 정확히 예측하는 것이라면, 그리고 더 정확히는 \u0026ldquo;동전 던지기 결과를 누구보다 가장 정확히 맞히는것\u0026rdquo; 이 목적이며 다른 결과는 무의미하다면, 1/2에 난수를 더하는게 목적에는 더 부합할지 모른다. 예측이란 그런것이다\u0026hellip;\n책: 자신의 존재에 대해 사과하지 말 것\n"},{"id":150,"href":"/docs/hobby/book/book12/","title":"불변의 법칙 | 모던 하우절","section":"글","content":" 불변의 법칙 | 모던 하우절 # #2024-12-31\n세상 모든 일은 예측 불가능한 방식으로 서로 영향을 주고받고, 혼합되고, 그 결과가 증폭된다. 세상은 운과 우연에 이토록 취약하다.\n1. 북마크 # ﹂사건의 복리효과\n﹂비효율을 견디는 능력\n2. 플레이리스트 # https://youtu.be/XstIT_dY6eE?si=fbXi6ohmBhTIVztz 들으면서 타이핑하니까 딱이다!!\n3. [일상] 김지민 생일 Birthday - 쿠쿠크루(Cuckoo Crew) # https://youtu.be/5f_Lx-RbrJc?si=imPXt3rVSR3GOq_R\nQ) 뜨겁잖아요! 대체 어떻게 한 거죠?\n??) 뜨거워도 개의치 않는 거지.\nㅋㅋㅋ\n4. 미얀마 강진 생존자 인터뷰 # https://www.youtube.com/watch?v=uO43DJm2hFk 오늘 멋진사람이 나오는 유튜브를 봤는데 \u0026lsquo;10 마법이 일어나는 순간\u0026rsquo;에서 설명하는 장면같아서 생각나서 써봄. 사람은 소중한걸 지킬수있는힘을 가져야된다..\n"},{"id":151,"href":"/docs/hobby/book/book20/","title":"불행한 것과 우울한 것.","section":"글","content":" 불행한 것과 우울한 것. # #2024-12-31\n1 # \u0026ldquo;기분이 최고로 좋았을 때를 10이라고 하면, 지금 기분은 1에서 10 중 몇 정도인가요?\u0026rdquo; 그녀는 조용히 내 답을 기다렸다.\n\u0026ldquo;6에서 7 정도요.\u0026rdquo;\n정말 답하기 어려운 질문이다. 나는 환자들에게 생각하지 말고 직감적으로 답하라고 요구한다. 하지만 \u0026lsquo;7\u0026rsquo;이란 건 내 솔직한 느낌이었을까, 아니면 일반 환자 대신 상담 시간을 차지한 내 행동을 합리화하려는 의도였을까?\n난 내 우울증의 원인을 오랫동안 탐구했다. 어떤 힘든 일이 닥치면 며칠도 안 되어 극심한 절망에 빠지는 이유가 뭘까. 정신역동치료는 과거의 인간관계가 현재에 미치는 영향을 통찰해보려는 쪽이다. 반면 인지행동치료는 현실을 자신에게 해로운 관점으로 보기 때문에 \u0026lsquo;지금 이곳에서\u0026rsquo; 우울해진다고 보고 그런 관점을 개선하려는 쪽이다.\n모든 역사가 그렇듯, 개인의 역사도 불변의 존재가 아니다. 남에게 이야기하고 반복해 서술하는 과정에서 유기체처럼 변한다. 어느 시점에서건, 내가 \u0026lsquo;진짜\u0026rsquo; 아는 건 그때그때 느끼는 감정뿐이다. 1년 전 느꼈던 감정, 품었던 고민이 아무리 해도 기억나지 않을 때가 있다. 어쩌면 일부러 잊는 건지도 모르겠지만, 지금부터 할 이야기는 지금의 나에 대해 내가 아는 이야기다. 그리고 나와 비슷한 문제를 겪는 사람들에게 도움이 되리라고 생각되는 이야기다. 이 일을 하면서 배웠지만, 의사는 환자가 안고 있는 문제의 \u0026lsquo;이력을 알아내는\u0026rsquo; 데 그치지 말고 환자의 이야기에 귀를 기울여야 한다.\n그는 아버지의 폭력에 몸만 다친 게 아니었다. 10대 시절 우울증을 앓았던 것도, 20대 중반인 지금 기분이 심하게 침체되어 있는 것도 이해가 된다. 그는 성장기의 어려움을 극복하고 좋은 회사에 취직했지만, 어릴 때부터 앓았던 당뇨병이 합병증을 일으키면서 그간 노력해 얻은 것들을 다 잃게 되었다고 느꼈다. 어머니도 당뇨병이 있었다. 리처드는 최근 시력이 나빠졌고, 어린 시절 경험 때문에 우울증에도 대단히 취약해진 상태였다. 물론 충분히 이해는 된다. 그럴 수 있다. 당뇨병처럼 큰 병을 앓는 게 얼마나 힘들지도 짐작이 된다. 하지만 그런 상황이라고 해서 모든 사람들이 다 심각할 정도로 기분이 침체되지는 않는다. 알아서 살 길을 찾아나간다. 리처드는 그러지 못하고 있다.\n의사들이 가끔 하는 실수는, 환자가 현재 처한 상황에 비추어볼 때 기분이 가라앉는 것을 \u0026lsquo;이해할 만하다\u0026rsquo;고 넘겨짚어 버리는 것이다. \u0026ldquo;그런 일이 있으면 누구든 기분이 처지는 게 당연하죠. 저라도 그러겠어요!\u0026rdquo; 이런 식으로 말이다.\n하지만 문제는 그리 간단치 않다. 환자는 우울한 것일 수도 있다. 우울은 불행한 감정과는 다르다. 우울은 불행보다 훨씬 더 깊고 큰 절망감으로, 세상을 보는 눈에 색을 덧입히고 일상생활을 해나가기 어렵게 만든다.\n2 # 출발점으로 돌아왔을 때 나는 엉엉 울고 있었다. 아빠는 먼저 훌쩍 내렸다. 나와 일행이 아닌 척 하는 것 같았다. \u0026lsquo;이 울보 여자애 내 딸 아니야\u0026rsquo; 하고 말하는 듯했다. 어린 나이에도 나는 아빠의 기질을 파악했다. 우리 둘은 여러모로 많이 닮았으면서도 또 달랐다. 나는 쉽게 불안해하고 겁이 많았지만, 아빠는 위험한 상황에서도 힘이 세고 용감했다.\n\u0026ldquo;뭐가 문제야?\u0026rdquo; 아침마다 학교 가기 전에 티셔츠를 몇 번이나 입었다 벗었다 하는 앨런에게 나는 묻곤 했다. 엄마 아빠 둘 다 7시 30분까지 출근해야 해서, 내가 두 남동생을 아침마다 준비시켜야 했다. 나보다 열한 살 어린 막내 이언은 골치를 썩이지 않았다. 알아서 시리얼을 우걱우걱 맛있게 먹었다. 하지만 나의 일곱 살 터울인 앨런은 알 수 없는 무언가로 늘 괴로워했다.\n\u0026ldquo;저리 가! 나 좀 가만 놔둬.\u0026rdquo; 앨런이 소리쳤다.\n\u0026ldquo;왜 그러는 건데.\u0026rdquo; 나는 이유를 말해달라고 구슬렸다.\n\u0026ldquo;주름이 너무 많아.\u0026rdquo; 앨런은 중얼거리거나 울면서 외치곤 했다.\n\u0026ldquo;우리 늦었어.\u0026rdquo;\n\u0026ldquo;상관없어! 나 좀 놔둬.\u0026rdquo;\n동생은 그렇게 옷과 씨름하다가 화를 못 이겨 옷을 갈가리 찢기도 했다. 밤에도 쉽지 않았다. 깜깜한 방에서 침대에 눕지도 않고 몇 시간을 서 있었다. 자기 전에 치러야 하는, 자신도 설명하지 못하는 어떤 복잡한 절차에 문제가 생겼기 때문이었다.\n아빠는 절망감에 빠졌다. \u0026ldquo;앨런, 제발 잠옷 좀 입어, 응?\u0026rdquo;\n\u0026ldquo;싫어.\u0026rdquo;\n\u0026ldquo;여보, 이제 자정이야.\u0026rdquo; 엄마가 문간에 서서 애걸했다.\n\u0026ldquo;그냥 놔둬. 서 있다가 알아서 불 끄고 자라고 해.\u0026rdquo;\n어슴푸레한 어둠 속에서, 동생은 자기 침대 옆에 돌처럼 꼼짝 않고 서 있었다. 그러다 문이 꽝 닫혔고, 방 안에서는 흐느끼는 울음소리만 흘러나왔다. 결국 아빠도 포기하고, 실망과 분노로 피폐해진 채 방에 들어가 잠이 들었다. 앨런은 여러 해가 지나서야 비로소 강박 장애 진단을 받았다.\n그때는 병명을 몰랐지만, 아빠는 사회공포증이 점점 심해졌다. 구체적으로는 공공장소에서 남들과 대화하는 것을 두려워했다. 그래서 엄마는 아빠가 살 만한 옷이나 신발 따위를 집에 가져가서 먼저 좀 입혀보겠다고 가게 주인에게 사정해야 했다. 심지어 아빠는 도서관에 가서 책을 빌려오지도 못할 정도로 불안이 심했다. 술을 마시면 불안이 좀 가라앉긴 했지만 아빠는 술을 잘 마시지 않았다. 대신 담배를 하루에 40개비까지 피웠다.\n부모님은 앨런과 함께 가족 치료를 받으러 다녔다. 아빠는 의사가 자기를 빤히 쳐다보기만 하고 아무 설명도 해주지 않는다며 질색했다. \u0026ldquo;뭘 하겠다는 건지 도무지 알 수가 없어. 죄책감만 잔뜩 주고.\u0026rdquo; 의사가 나도 함께 오라고 했지만 나는 거부했다. 나와는 관계없는 일이라고 생각하려 했다.. 난 학교 공부에 너무 바빴다.\n당시엔 정신질환의 생물학적 근거라는 것이 거의 알려져 있지 않았다. 뇌의 배선 결함이 아닌 양육의 문제로 보는 것이 보통이었다. 지금은 유전과 양육 어느 한 쪽의 문제라기보다 둘이 복잡하게 얽힌 경우가 많다고 알려져 있다. 나는 동생 앨런이 불안 장애 성향을 부모 양쪽에게서 물려받았을 것으로 짐작한다. 동생은 난산 끝에 태어났다. 생사의 갈림길을 걷던 몇 분 동안 심장박동이 잡히지 않았는데, 그때 경미한 저산소성 뇌 손상이 일어났을 가능성도 있다. 크면서는 엄마 아빠를 애먹여 두 사람 사이에 긴장을 조성했고, 그로 인해 자신도 더 불안해졌다. 이는 옷 입기나 취침과 관련된 이상행동과 분노와 반항, 또다시 이상행동의 증가로 이어지는 악순환을 낳았다.\n나는 유전적으로 신경증적 성향을 타고나기도 했지만, 안전하고 정서적으로 안정된 성장 터전을 가족에게서 제공받지 못했다는 사실이 늘 괴로웠다. 아이가 자신 있게 세상에 부딪칠 줄 아는 사람으로 커나가기 어려운 환경이었다. 엄마는 불안이 있음에도 천성적으로 매사에 태도가 당당한 사람이었지만, 나는 아빠의 과묵한 내향성을 더 많이 물려받은 것 같다. 어릴 때 엄마보다 아빠와 훨씬 친하기도 했다. 그러나 그 애착은 10대 시절 점점 불안과 두려움으로 바뀌어갔다. 그러한 변화는 인생의 시련을 버티는 내 능력의 한계를 더욱 낮추는 구실을 했다.\n3 # 나는 모종의 이유로 인해 점점 취약성이 높아졌다. 마음에 안드는 옷을 입고 외출하는 것, 책상에서 볼펜을 떨어뜨려서 허리를 숙여야 하는 것, 침대에 누워서 과자를 먹고 봉지를 휴지통에 버리기 위해 팔을 뻗는 것, 문 밖의 누군가의 발소리를 듣는 것. 생각하기에 따라 큰 불행이 아닐 수도 있는 것들이 나에겐 견디기 힘든 큰 불행처럼 느껴졌다.\n어제는 밤에 불행해서 죽고싶어서 울었다. 그 이유는 엄마 아빠와 평생 함께 있고 싶은데 미래의 어느 날은 죽을 것임을 떠올렸기 때문이다. 한참 울은 뒤에 나는 한 가지 처방을 내리고 마음이 편안해져서 잠이 들었다. 내가 내린 처방은 엄마 아빠가 죽을 때 같이 죽겠다는 계획을 세우는 것이었다. 다음 날 아침. 언제 어린애처럼 울었냐는듯이 나는 짐을 챙겨 할일을 하러 집을 나섰다. 출근 전 카페에 와서 읽고 싶던 책도 읽고 맛있는 커피도 마셨다. 기분이 최고로 좋았을 때를 10이라고 하면, 지금 기분은 1에서 10 중 몇 정도인가요? 같은 질문에 7 같다고 답변했다. 그리고 사실 알고있다. 나는 엄마 아빠가 죽는다고 해서 죽지 못할 것이다. 그리고 어제 죽고싶어 울었던 것은 엄마 아빠가 나보다 일찍 죽기 때문이 아니었다. 물론 큰 불행이지만 그 사건이 갖는 \u0026lsquo;시간\u0026rsquo;이라는 특성 때문에 지금의 나에겐 불행의 본질적 크기에 비해 그만한 영향을 주지 못한다. 사실 어제 불행해서 죽고싶어서 울었던 이유는 미팅에서 부정적인 피드백을 받았기 때문이다. 의아한 점은 그 피드백을 온전히 이해했으며 더 안좋은 결과가 나타날 수도 있던 상황을 성공적으로 회피하기까지 했다는 것이다. 피드백 자체는 아무런 문제가 없었지만, 작은 불행 하나가 내 안의 기폭제를 밀었고 벼랑을 구르며 점점 커졌으며 우울한 감정이 발생했다. 만약 덧입혀질 불행이 없다면 그대로 축적되어 다음 발생할 우울을 조금 당길 예정이었으나, 어제는 먼 미래의 불행이 떠오름에 따라 감정이 덧입혀져 발현될 수 있었으며 그렇기에 나는 죽을 듯이 울었던 것이다.\n책: 당신의 특별한 우울\n"},{"id":152,"href":"/docs/study/tech/cs1/","title":"사이트 생성, 깃허브 배포","section":"생물정보학","content":" 사이트 생성, 깃허브 배포 # #2024-12-31\nHugo 설치 # $ brew install hugo $ hugo version hugo v0.131.0+extended darwin/arm64 BuildDate=2024-08-02T09:03:48Z VendorInfo=brew Hugo v0.112.0 이상인지 확인하면 된다.\nHugo 사이트 생성 # 작업하고 싶은 위치에 Hugo 디렉토리를 만들어준다.\n$ mkdir Hugo $ cd Hugo Hugo로 들어가서 hugo 사이트 틀을 생성해준다. 나는 blog라는 이름으로 생성하였다.\n$ pwd /Users/yshmbid/Hugo $ hugo new site blog blog 디렉토리에 빈 Git 저장소를 초기화한다.\n$ cd blog $ pwd /Users/yshmbid/Hugo/blog $ git init hint: Using \u0026#39;master\u0026#39; as the name for the initial branch. This default branch name hint: is subject to change. To configure the initial branch name to use in all hint: of your new repositories, which will suppress this warning, call: hint: hint: git config --global init.defaultBranch \u0026lt;name\u0026gt; hint: hint: Names commonly chosen instead of \u0026#39;master\u0026#39; are \u0026#39;main\u0026#39;, \u0026#39;trunk\u0026#39; and hint: \u0026#39;development\u0026#39;. The just-created branch can be renamed via this command: hint: hint: git branch -m \u0026lt;name\u0026gt; /Users/yshmbid/Hugo/blog/.git/ 안의 빈 깃 저장소를 다시 초기화했습니다 위에서 Using \u0026lsquo;master\u0026rsquo; as the name for the initial branch. 언급이 나온다. 여기서 확인해줘야 할 부분이 있다.\n레포지토리 생성 페이지에서 Add a README file.을 체크하면 This will set main as the default branch.라는 안내가 뜬다. 이를 통해 default가 main임을 확인할수있다.\n따라서 위의 경우에는 master가 아닌 main으로 바꿔줘야 한다.\n$ pwd /Users/yshmbid/Hugo/blog $ git config --global init.defaultBranch main $ git branch -m main 다음으로 선택한 테마를 Git 서브모듈로 프로젝트에 추가한다. 나는 hugo-book이라는 테마를 사용했다.\n$ pwd /Users/yshmbid/Hugo/blog $ git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book 다음으로, 블로그의 기본 설정들을 세팅해준다. blog 디렉토리 내 파일들은 대략적으로 아래와 같이 구성되어 있다.\n$ ls archetypes\tdata\ti18n\tresources assets\thugo.toml\tlayouts\tstatic content\tpublic\tthemes 이 중에서 content와 hugo.toml만 수정할것이다. content에는 작성한 게시물이 들어가고, hugo.toml에는 기본 세팅을 위한 config 변수들이 들어간다.\nhugo-book 테마의 경우에는 content에 대해 이와 같이 언급하고 있다. 해당 테마는 국가별로 여러 content 디렉토리가 존재해서, 그 중 main이 되는 content.en의 내용만을 시키는대로 복사해준다.\n$ cp -R themes/hugo-book/exampleSite/content.en/* ./content 다음으로 hugo.toml에 선택한 테마를 설정해주고 열어서 확인해본다.\n$ echo \u0026#34;theme = \u0026#39;hugo-book\u0026#39;\u0026#34; \u0026gt;\u0026gt; hugo.toml $ view hugo.toml 1 baseURL = \u0026#39;https://example.org/\u0026#39; 2 languageCode = \u0026#39;en-us\u0026#39; 3 title = \u0026#39;My New Hugo Site\u0026#39; 4 theme = \u0026#39;hugo-book\u0026#39; 여기서 base가 되는 내용만 수정해줬다.\n1 baseURL = \u0026#39;https://yshghid.github.io/\u0026#39; 2 languageCode = \u0026#39;en-us\u0026#39; 3 title = \u0026#39;\bLifelog 2025\u0026#39; 4 theme = \u0026#39;hugo-book\u0026#39; # i를 누르면 편집모드로 전환된다. # 편집이 끝났으면 esc를 누르고 :wq!를 입력하면 완료된다. 기본적인 설정이 끝났으므로 로컬에서 실행시켜보자! http://localhost:1313에 접속하면 local 환경에서 어떻게 실행 중인지 확인할수있다.\n$ hugo server 이쁘게 잘 나온다 ㅎㅎ\n변경 사항을 픽스하려면 hugo를 수행해서 public 디렉토리에 static site 코드를 생성해준다.\n$ pwd /Users/yshmbid/Hugo/blog $ hugo Start building sites … hugo v0.131.0+extended darwin/arm64 BuildDate=2024-08-02T09:03:48Z VendorInfo=brew WARN Expand shortcode is deprecated. Use \u0026#39;details\u0026#39; instead. | EN -------------------+----- Pages | 58 Paginator pages | 0 Non-page files | 0 Static files | 78 Processed images | 0 Aliases | 11 Cleaned | 0 Total in 66 ms Hugo 사이트 배포 # hugo로 만든 static site를 github page를 활용해서 배포할것이다. 이를 위해서 \u0026lt;user-id\u0026gt;.github.io 리포지토리를 생성해준다.\n이때 Add a README file 을 선택할 경우 push 할때 오류가 날 수 있으므로 체크 해제해서 생성해주는게 좋다.\n다음으로, .github/workflows 경로에 gh-pages.yml 파일을 만들어준다. gh-pages.yml은 GitHub Actions 워크플로우를 정의하여 코드가 커밋되거나 푸시될 때 자동으로 Hugo 사이트를 빌드하고 GitHub Pages에 배포할 수 있도록 하는 파일이다.\n$ pwd /Users/yshmbid/Hugo/blog $ mkdir -p .github/workflows $ cd .github/workflows $ touch gh-pages.yml 아래 내용은 HUGO 공식 문서에서 제공한 워크플로우인데, 나의 경우에는 오류가 났다.\n# Sample workflow for building and deploying a Hugo site to GitHub Pages name: Deploy Hugo site to Pages on: # Runs on pushes targeting the default branch push: branches: - main # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued. # However, do NOT cancel in-progress runs as we want to allow these production deployments to complete. concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: false # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.128.0 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v4 with: submodules: recursive fetch-depth: 0 - name: Setup Pages id: pages uses: actions/configure-pages@v5 - name: Install Node.js dependencies run: \u0026#34;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026#34; - name: Build with Hugo env: HUGO_CACHEDIR: ${{ runner.temp }}/hugo_cache HUGO_ENVIRONMENT: production TZ: America/Los_Angeles run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.pages.outputs.base_url }}/\u0026#34; - name: Upload artifact uses: actions/upload-pages-artifact@v3 with: path: ./public # Deployment job deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 위의 워크플로우를 사용한다면 line 8의 main를 확인해주자면 default가 master라면 master로 바꿔줘야 한다.\n나의 경우는 위 워크플로우로는 오류가 났어서 아래의 수정된 내용을 넣어줬다.\nname: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Create .nojekyll run: echo \u0026#39;\u0026#39; \u0026gt; .nojekyll - name: Setup Hugo uses: peaceiris/actions-hugo@v3 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v4 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GH_TOKEN }} publish_dir: ./public 이어서 GH_TOKEN를 정의해줘야 하는데\n리포지토리의 Settings -\u0026gt; Secretes and Variables -\u0026gt; Actions 에서 Repository secretes와 Repository variables를 생성해준다.\nSecret 자리에 토큰을 입력해주면 된다.\n작성이 완료되었다면, 생성한 \u0026lt;user-id\u0026gt;.github.io 리포지토리에 연결한 후 커밋, 푸시해준다.\n$ pwd /Users/yshmbid/Hugo/blog $ git remote add origin https://github.com/yshghid/yshghid.github.io.git $ git add . $ git commit -m \u0026#34;first commit\u0026#34; $ git push origin main 마무리 # 이로써 블로그 생성과 배포는 끝이지만!! 추가로 확인하면 좋은 부분이 있다.\nActions Actions에서 초록색 체크박스가 뜨는지 확인하기. 오류가 난다면 해당 오류의 로그를 읽어보고 그에 맞게 수정해주면 된다.\nSources, Branch 공식 문서에서는 Deploy from a branch에서 Github Actions로 바꿔주라고 나온다. 바꿔도 상관없으나 나는 그냥 뒀다.\n브랜치는 보통은 gh-pages 브랜치가 기본 Github Pages 브랜치로 설정되어 있지만 혹시 안되어 있다면 gh-pages로 바꿔주면 된다.\n구조 /Users/yshmbid/Hugo/blog ├── hugo.toml ├── content/ ├── layouts/ ├── static/ └── .github/ └── workflows/ └── gh-pages.yml blog 디렉토리가 이와 같은 구조를 띤다면 제대로 작성된 것이다.\n참고한 블로그 및 문서 # HUGO 공식 문서 - https://gohugo.io/getting-started/quick-start/ HUGO 공식 문서2 - https://gohugo.io/hosting-and-deployment/hosting-on-github/ hugo-book github - https://github.com/alex-shpak/hugo-book.git https://c11oud.tistory.com/entry/GitHub-깃허브-블로그-만들기1 https://github.com/Integerous/Integerous.github.io https://kzeoh.github.io/posts/make-blog/ "},{"id":153,"href":"/docs/hobby/book/book32/","title":"새롭게얻은 부와 충동","section":"글","content":" 새롭게얻은 부와 충동 # #2024-12-31\n#1\n머스크는 여름이 끝날 무렵 스탠퍼드대학원에 진학하여 재료과학을 공부할 계획을 세웠다. 여전히 커패시터에 매료된 그는 그것으로 전기자동차에 전력을 공급할 수 있는 방법을 연구하고 싶었다. “첨단 칩 제조 장비를 활용하여 자동차의 주행거리를 늘리기에 충분한 에너지 밀도를 가진 고체 소자 울트라 커패시터를 만들어볼 생각이었어요.” 그는 말한다. 하지만 등록기간이 가까워지면서 걱정이 들기 시작했다. “스탠퍼드에서 몇 년을 보내고 박사학위까지 받았는데 그 커패시터가 실현 불가능한 것으로 밝혀지면 어떻게 해야 할 것인가, 하는 걱정이 들었어요. 사실 대부분의 박사학위는 무의미해요. 실제로 그 부류 가운데 세상에 진정한 변화를 가져오는 사람은 거의 없잖아요.” 머스크의 말이다.\n그 무렵 그는 마치 ‘만트라’처럼 되새기고 되새길 인생의 비전을 마음속에 품고 있었다. “인류에게 진정으로 영향을 미칠 수 있는 것이 무엇인지 생각했어요. 그리고 세 가지를 떠올렸지요. 인터넷, 지속 가능한 에너지, 우주여행.” 1995년 여름, 머스크는 그중 첫 번째인 인터넷이 그가 대학원을 마칠 때까지 기다려주지 않을 거라는 사실을 깨달았다. 얼마 전 웹이 상업용으로 개방되었으며, 8월 초에 브라우저 스타트업 넷스케이프Netscape가 IPO를 단행해 하루 만에 시가총액 29억 달러의 기업으로 날아오른 상황이었다.\n머스크는 사실 펜실베이니아대학교 졸업반 시절에 구상한 인터넷 기업에 대한 아이디어를 하나 갖고 있었다. 뉴욕 및 뉴잉글랜드 지역 전신전화 회사인 나이넥스NYNEX의 한 임원이 학교 강연회에 와서 옐로페이지(미국의 업종별 전화번호부-옮긴이)의 온라인 버전 출시 계획에 대해 밝혔을 때 떠올린 아이디어였다. ‘빅옐로Big Yellow’라는 이름의 그 온라인 버전은 인터랙티브 기능을 갖추어 사용자들이 자신의 필요에 따라 정보를 맞춤화할 수 있다는 것이 그 임원의 설명이었다. 하지만 머스크는 나이넥스가 진정한 인터랙티브의 구현 방법을 전혀 모른다고 생각했다(결과적으로 그것은 올바른 판단이었다). 그는 킴벌에게 “우리가 직접 만드는 게 어떨까?”라고 제안했다. 킴벌은 사업체 목록과 지도 데이터를 결합할 수 있는 코드를 작성하기 시작했고, 거기에 ‘버추얼 시티내비게이터Virtual City Navigator’라는 이름을 붙였다.\n스탠퍼드대학원 등록 마감일 직전, 머스크는 노바스코샤 은행의 피터 니콜슨에게 조언을 구하기 위해 토론토로 갔다. 버추얼 시티내비게이터에 대한 아이디어를 계속 추구해야 할까요, 아니면 박사과정을 시작하는 게 나을까요? 스탠퍼드에서 박사학위를 받은 니콜슨은 애매하게 둘러말하지 않았다. “인터넷 혁명 같은 것은 일생에 단 한 번 올까 말까 한 기회라네. 물 들어올 때 노 저으라는 말이 있지 않은가.” 니콜슨은 머스크와 함께 온타리오 호숫가를 따라 걸으며 말했다. “대학원은 나중에라도 뜻만 있으면 얼마든지 갈 수 있지.” 머스크는 팰로앨토로 돌아와 렌에게 결심을 굳혔다고 말했다. “다른 모든 것은 보류하기로 했어. 지금은 인터넷의 물결에 올라탈 때야.”\n하지만 그는 사실 자신의 베팅에 보험을 들었다. 스탠퍼드에 정식 등록하고 즉시 휴학을 신청한 것이다. “실은 제가 최초로 인터넷 지도와 전화번호부를 갖춘 소프트웨어를 개발했습니다.” 머스크는 재료과학과 담당교수인 빌 닉스에게 이렇게 말했다. “아마 실패할 겁니다. 실패하는 경우 다시 돌아오고 싶습니다.” 닉스는 머스크가 학업을 연기하는 것은 문제 될 게 없다고 말했다. 그러면서 그는, 그렇지만 머스크가 다시 돌아오지 않을 것이라고 예측했다.\n#2\n머스크 형제는 수익금 가운데서 아버지에게 30만 달러를, 어머니에게 100만 달러를 드렸다. 일론은 50평짜리 콘도를 구입하고 당시 가장 빠른 양산차인 맥라렌 F1 스포츠카를 100만 달러에 구입하는 등 나름대로 궁극의 사치를 부렸다. 그는 그의 집에서 차가 배달되는 모습을 촬영하게 해달라는 CNN의 요청을 받아들였다. “불과 3년 전만 해도 YMCA에서 샤워를 하고 사무실 바닥에서 잠을 자던 제가 이제 100만 달러짜리 차를 갖게 되었습니다.” 머스크는 트럭에서 차가 내려지는 동안 이렇게 말한 후 거리를 이리저리 껑충껑충 뛰어다녔다.\n충동적으로 자신의 욕구를 분출한 이후, 그는 새롭게 발견한 자신의 부에 대한 취향을 경솔하게 과시하는 것이 꼴사나운 짓임을 깨달았다. “어떤 사람들은 이 차를 구입한 것을 보고 건방진 제국주의자의 전형적인 행동방식으로 해석할 수도 있습니다. 제 가치관이 변했을지 모르지만, 저는 제 가치관이 변했다는 것을 의식적으로 자각하지 못하고 있습니다.”\n과연 그가 변한 걸까? 새롭게 얻은 부로 그는 자신의 욕망과 충동에 거의 제약을 받지 않게 되었지만, 그런 상황은 항상 보기 좋은 것은 아니었다. 하지만 그의 진지하고 사명감 넘치는 강렬함은 조금도 변함이 없었다.\n작가 마이클 그로스는 실리콘벨리에서 티나 브라운의 번지르르한 잡지인 \u0026lt;토크\u0026gt;에 새로 부자가 된 테크노브랏techno-brat, 즉 기술 열풍을 타고 벼락부자가 된 젊은 리더들에 대한 기사를 쓰고 있었다. “날카롭게 비판해도 될 만한 허세 가득 찬 주인공을 찾고 있었습니다.” 그로스는 몇 년 후 이렇게 회상했다. “하지만 2000년에 만난 머스크는 삶의 환희가 넘치는, 너무 호감 가는 인물이라 비판할 수가 없었지요. 그는 지금과 마찬가지로 주변의 기대에 대해 무관심하고 무심했지만, 편하고 개방적이며 매력적이고 재미난 인물이었어요.”\n책: 일론 머스크\n"},{"id":154,"href":"/docs/hobby/book/book35/","title":"생존법","section":"글","content":" 생존법 # #2024-12-31\n#1\n머스크는 러시아인들이 받아내려 했던 터무니없는 가격을 곱씹으면서 제 1원리First Principles(다른 경험적 데이터를 필요로 하지 않는 \u0026lsquo;자명한 진리\u0026rsquo;)에 입각한 사고를 동원해 그 상황에 대한 기본 물리학을 파고들었고 거기서부터 차근차근 쌓아 올려나갔다. 그리고 이를 통해 완제품이 기본 재료비보다 얼마나 더 비싼지 계산하는 \u0026lsquo;바보 지수idiot index\u0026rsquo;를 개발했다. 제품의 \u0026lsquo;바보 지수\u0026rsquo;가 높으면 보다 효율적인 제조기술을 고안하여 비용을 크게 줄일 수 있다는 것을 의미했다.\n로켓은 \u0026lsquo;바보 지수\u0026rsquo;가 극도로 높았다. 머스크는 로켓에 들어가는 탄소섬유와 금속, 연료 및 기타 재료의 원가를 계산하기 시작했다. 기존의 방법을 사용한 완제품의 제작비용은 머스크가 계산한 원가보다 최소 50배 이상 비쌌다.\n인류가 화성에 가려면 로켓 기술이 획기적으로 개선되어야 했다. 중고 로켓, 특히 러시아의 오래된 로켓에 의존해서는 기술을 발전시킬 수 없었다.\n그래서 집으로 돌아오는 비행기에서 그는 노트북을 꺼내 중형 로켓을 만드는 데 들어가는 모든 재료와 비용을 세세히 나열하며 스프레드시트를 만들기 시작했다. 뒷자리에 앉은 캔트렐과 그리핀은 술을 주문하며 웃었다. “우리의 저 천재백치께서는 대체 지금 뭘 하고 있는 걸까요?” 그리핀이 캔트렐에게 물었다. 머스크가 몸을 돌려 “이것 좀 봐요, 여러분”이라고 말하며 자신이 만든 스프레드시트를 보여주었다. “이런 로켓을 우리가 직접 만들 수 있을 것 같아요.” 캔트렐은 숫자를 살피며 혼잣말로 중얼거렸다. “헐, 내 책을 다 빌려가더니만 결국 이러려고 그랬군.” 그러고는 승무원에게 술을 한 잔 더 달라고 했다.\n#2\n킴벌은 일론과 저스틴, 아기와 함께 병원으로 향했다. 네바다는 뇌사 판정을 받은 상태로 3일 동안 생명유지장치를 달고 생을 유지했다. 마침내 호흡기를 끄기로 결정했을 때, 일론은 아기의 마지막 심장 박동을 느꼈고 저스틴은 아기를 품에 안고 죽음의 떨림을 느꼈다. 일론은 주체할 수 없이 흐느꼈다. “마치 늑대처럼 울었어요.” 그의 어머니는 말한다. “늑대처럼….”\n일론이 도저히 집으로 돌아가지 못하겠다고 해서 킴벌은 부부가 베벌리윌셔 호텔에 머물도록 조처했다. 호텔 지배인은 그들에게 프레지덴셜 스위트를 내주었다. 일론은 그에게 호텔로 가져왔던 네바다의 옷과 장난감을 치워달라고 부탁했다. 일론이 가까스로 집에 가서 한때 아들의 방이었던 곳을 보기까지 3주가 걸렸다.\n일론은 슬픔을 조용히 감내했다. 퀸스대학교에서 사귄 친구 나베이드 패룩은 그가 집에 돌아오자마자 로스앤젤레스로 날아와 곁을 지켰다. 패룩은 말한다. “저스틴과 나는 그간의 일에 대한 대화에 일론을 끌어들이려 했지만, 그는 그 일에 대해 이야기하고 싶어 하지 않았지요.” 그래서 그들은 대신 영화를 보고 비디오 게임을 하며 시간을 보냈다. 오랜 침묵의 시간이 흐른 후 패룩이 물었다. “기분은 어때? 잘 견디고 있는 거지?” 하지만 일론은 그런 대화 자체를 완전히 차단했다. “그의 표정을 읽을 수 있을 정도로 오랫동안 그를 알고 지내온 사이였기에 그가 그 일에 대해 이야기하지 않기로 결심했다는 것을 알 수 있었어요.” 패룩의 말이다.\n반대로 저스틴은 자신의 감정에 매우 솔직했다. “남편은 내가 네바다의 죽음에 대한 감정을 표출하는 것을 달가워하지 않았어요.” 그녀는 말한다. “그는 내가 감정을 숨김없이 털어놓으면서 감정적으로 자기를 조종하려 한다고 말하기도 했어요.” 저스틴은 그가 그렇게 감정을 억압하는 것이 어린 시절에 발달된 방어기제 때문이라고 생각한다. “그는 어두운 상황에 처하면 감정을 차단해버려요. 그에게는 그것이 생존을 위한 방법인 것 같아요.”\n#3\n요하네스버그에서 출발한 비행의 첫 번째 구간을 마치고 노스캐롤라이나 주 랠리에 도착했을 때, 에롤은 델타항공 담당자로부터 호출을 받았다. \u0026ldquo;나쁜 소식이 있습니다.\u0026rdquo; 담당자가 말했다. \u0026ldquo;아드님께서 손자 네바다가 사망했다는 소식을 전해달라고 하셨습니다.\u0026rdquo; 일론은 그 내용을 직접 말할 자신이 없었기에 항공사 담당자에게 대신 전해달라고 부탁한 것이다.\n에롤이 전화를 받자 킴벌은 상황을 설명하며 말했다. \u0026ldquo;아버지, 오시면 안돼요.\u0026rdquo; 킴벌은 아버지에게 발길을 돌려 남아공으로 돌아가라고 설득했지만, 에롤은 거부했다. \u0026ldquo;아니다, 이미 미국에 도착했으니 로스앤젤레스에 가봐야 되겠다.\u0026rdquo;\n에롤은 베벌리윌셔 호텔 펜트하우스의 규모를 보고 놀랐던 기억을 떠올렸다. \u0026ldquo;아마도 그때까지 내가 본 호텔 방 중 가장 놀랍지 않았나 싶어요.\u0026rdquo; 일론은 넋이 나간 듯 보였지만, 복잡한 심정으로 애정에 목말라 있기도 했다. 그는 거칠고 거만한 성격의 아버지가 그런 나약한 모습의 자신을 보는 것이 불편했지만, 아버지가 떠나기를 원하지도 않았다. 결국 그는 아버지와 그의 새 가족이 로스앤젤레스에 머물 것을 종용하기에 이르렀다. \u0026ldquo;남아공으로 돌아가지 않으셨으면 좋겠어요.\u0026rdquo; 그가 말했다. \u0026ldquo;제가 여기에 집을 사드릴게요.\u0026rdquo;\n킴벌은 깜짝 놀랐다. \u0026ldquo;아냐, 아냐, 좋은 생각이 아니야.\u0026rdquo; 그가 일론에게 말했다. \u0026ldquo;형은 아버지가 얼마나 음흉한 인간인지 벌써 잊었어? 그러지 마, 형. 이건 자학이나 마찬가지라고.\u0026rdquo; 하지만 동생이 설득하려고 애쓸수록 일론은 더욱 슬퍼졌다. 수년 후, 킴벌은 어떤 갈망이 형에게 그런 동기를 부여했는지 다시 한 번 되짚었다. \u0026ldquo;아들이 죽는 것을 지켜본 일이 아버지가 곁에 있기를 원하도록 이끈 게 분명해요.\u0026rdquo; 그가 내게 말했다.\n#4\n어느 날 에롤이 보트에 올라 있을 때 일론으로부터 메시지 한 통이 날아왔다. “상황이 좋아지기는커녕 엉망이 되고 있으니” 에롤에게 남아공으로 돌아가라는 내용이었다. 에롤은 그렇게 했다. 몇 달 후, 그의 아내와 아이들도 남아공으로 돌아갔다. “아버지를 더 나은 방향으로 바꾸기 위해서 협박도 하고 보상도 하고 논쟁도 벌이고 별의별 시도를 다 했지요.” 일론이 나중에 한 말이다. “그런데 그는…” 머스크는 오랜 시간 말을 잇지 못했다. “말도 안 되게도, 더 나빠졌어요.” 인적 네트워크는 디지털 네트워크보다 복잡하기 마련이다.\n책: 일론 머스크\n"},{"id":155,"href":"/docs/hobby/book/book5/","title":"세이노의 가르침","section":"글","content":" 세이노의 가르침 # 원래 같으면 조금 읽고 덮었을 것 같은데 취준시즌에 읽어서 꽤 많이 읽음. \u0026lsquo;나\u0026rsquo;에게 도움이 되는 책인지는 모르겠는데 \u0026lsquo;취준하는 나\u0026rsquo;에게는 매우 유용한 책이었다! ㅋㅋ\n#북마크\n공부를 안해서 오는 스트레스는 사실 공부를 해야 없어진다.\n잘할 수 있는 일을 찾기 vs 일을 잘하기\n인테그리티\n"},{"id":156,"href":"/docs/hobby/book/book34/","title":"아이러니서클","section":"글","content":" 아이러니서클 # #2024-12-31\n#1\n레브친은 머스크를 어떻게 이해하면 좋을지 고민이 됐다. 그의 팔씨름 제안은 진담이었을까? 바보 같은 유머와 게임 플레이로 간간이 중단되곤 하는 일련의 광적인 격렬함은 계산된 것일까, 아니면 그저 발광일 뿐인가? 레브친은 말한다. “그가 하는 모든 일에는 아이러니가 있어요. 그는 11까지 올라가지만 4 이하로는 내려가지 않는 아이러니 설정 상태에서 움직입니다.” 머스크의 힘 중 하나는 다른 사람들을 자신의 아이러니 서클로 끌어들여 자기들만 아는 농담을 공유할 수 있게 하는 것이다. “그는 자신의 아이러니 화염방사기를 켜고 일론 클럽의 회원이라는 배타적인 의식을 만들어내죠.”\n하지만 레브친에게는 그런 방식이 잘 먹히지 않았다. 그는 진지함이라는 자신의 방패로 머스크의 아이러니 화염방사기를 막아내고 있었다. 그는 머스크의 과장을 탐지하는 데 탁월한 레이더를 보유했다. 합병 과정에서 머스크는 엑스닷컴의 사용자가 2배 가까이 많다고 계속 주장했고, 레브친은 엔지니어들에게 확인하여 실제 사용자 수를 알아내곤 했다. “머스크는 단순히 과장하는 데서 그치는 게 아니라 없는 얘기를 지어내기도 했어요.” 레브친의 말이다. 그의 아버지가 종종 보여주던 행태였다.\n하지만 레브친은 그에 반하는 사례를 접하면서 경탄하기도 했다. 머스크가 박학다식으로 그를 놀라게 했을 때가 대표적인 경우다. 어느 날 레브친과 그의 엔지니어들은 사용 중인 오라클 데이터베이스와 관련한 어려운 문제로 씨름하고 있었다. 다른 일로 그 방에 들어선 머스크는 자신의 전문 분야는 오라클이 아닌 윈도였지만, 대화의 맥락을 즉시 파악하고 정확한 기술적인 답변을 내놓은 후 확인을 기다리지도 않고 방을 나갔다. 레브친과 그의 팀은 오라클 매뉴얼로 돌아가 머스크가 설명한 내용을 찾아보았다. “하나씩 하나씩 들여다보며 우리 모두 ‘젠장, 머스크 말이 맞네’라고 했지요.” 레브친의 회상이다. “머스크는 말도 안 되는 소리를 지껄이기도 하지만, 때로는 다른 사람의 전문 분야에 대해 그보다 훨씬 더 많이 알고 있어 사람들을 놀라게 하곤 하죠. 나는 그가 사람들에게 동기를 부여하는 방법 중 상당 부분이 바로 때때로 드러내는 그런 예리함에 있다고 생각합니다. 그를 헛소리꾼이나 바보로 잘못 알고 있던 사람들이 전혀 기대하지 않고 있다가 그런 면모에 세게 한 방 맞은 기분이 드는 거지요.”\n#2\n이사회에서 투표를 통해 머스크의 해임을 결정했을 때, 머스크는 지금까지 그의 격렬한 투쟁을 지켜본 사람들을 놀라게 할 만큼 차분하고 품위 있게 대응했다. 그는 직원들에게 보낸 이메일에 이렇게 썼다. “엑스닷컴을 다음 단계로 끌어올릴 경험 많은 CEO를 영입할 때가 되었다고 결정했습니다. 그 작업이 완료되면 3~4개월 정도 안식 기간을 갖고 몇 가지 아이디어를 검토해본 다음 새로운 회사를 설립할 계획입니다.”\n머스크는 길거리 싸움꾼이었음에도 의외로 패배에 현실적으로 대처할 수 있는 능력이 있었다. 나중에 옐프Yelp를 창업하는 머스크의 추종자 제러미 스토플먼이 이사회 결정에 대한 항의의 표시로 자신과 다른 몇몇이 사직해야 하는 거 아니냐고 물었을 때, 머스크는 아니라고 답했다. “회사는 나의 아기였고, 솔로몬 이야기에 나오는 어머니처럼 나는 회사가 살아남을 수 있도록 기꺼이 포기할 수 있었어요.” 머스크는 말한다. “나는 틸 및 레브친과의 관계를 회복하기 위해 열심히 노력하기로 결심했어요.”\n#3\n머스크는 3년 만에 두 번째로 회사에서 쫓겨났다. 그는 사람들과 잘 어울리지 못하는 선지자였다. 페이팔의 동료들이 머스크의 가차 없고 거친 스타일에 더하여 놀랐던 것은 리스크를 감수하려는 그의 의지, 심지어 욕망이었다. “기업가는 사실 리스크를 감수하는 사람이 아니지요.” 로로프 보타는 말한다. “기업가는 리스크를 완화하는 사람이에요. 리스크를 감수하면서 번창하려 하지도 않고 리스크를 증폭시키려 하지도 않죠. 대신 통제 가능한 변수를 파악해서 리스크를 최소화하려고 노력하지요.” 하지만 머스크는 그렇지 않았다. “그는 리스크를 증폭시키고 우리가 물러설 수도 없게 배를 불태워버리는 데 몰두했어요.” 보타가 보기에 머스크의 맥라렌 사고는 그런 성향을 상징적으로 보여주는 것이었다. 가속페달을 있는 대로 밟고 얼마나 빨리 달리는지 보려다 난 사고였기 때문이다.\n이것이 항상 리스크를 제한하는 데 집중하던 틸과 머스크가 근본적으로 다른 점이었다. 한번은 틸과 호프먼이 페이팔에서의 경험을 담은 책을 집필할 계획을 세웠다. 그들은 머스크에 관한 장의 제목을 “‘리스크’라는 단어의 의미를 이해하지 못한 남자”로 잡기로 했다. 하지만 그의 리스크 중독은 불가능해 보이는 일을 하도록 사람들을 이끈다는 면에서는 유용할 수도 있었다. 호프먼은 말한다. “머스크는 놀랍도록 성공적으로 사람들이 사막을 가로질러 행진하게 만들곤 하지요. 그는 모든 칩을 테이블 위에 올려놓을 수 있을 정도의 확신을 가지고 움직입니다.”\n이는 단순한 비유가 아니었다. 수년 후 레브친은 한 독신 친구의 아파트에서 머스크 등과 함께 어울렸다. 몇몇 사람들은 판돈을 크게 걸고 텍사스 홀덤이라는 포커 게임을 하고 있었다. 머스크는 카드 플레이어가 아니었음에도 테이블로 다가갔다. “카드를 외우고 확률을 계산하는 데 능한 컴퓨터광들과 타짜 수준의 꾼들이 모여 있었지요.” 레브친의 설명이다. “일론은 모든 판에서 올인을 걸었고, 당연히 졌지요. 그러자 칩을 더 사서 더블 다운을 하고, 계속 그런 식으로 플레이했어요. 그렇게 여러 판에서 돈을 잃은 후에 마침내 올인을 걸고 이겼지요. 그랬더니 ‘좋아, 여기까지’라고 하면서 일어서더군요.” 칩을 테이블에서 거두지 않고 계속 리스크를 감수하는 것, 그것은 그의 인생의 주제가 되었다.\n그리고 그것은 그에게 좋은 전략인 것으로 드러났다. 틸은 말한다. “그가 이어서 설립한 두 회사, 스페이스X와 테슬라를 보세요. 실리콘밸리의 통념에 따르면 이 두 회사는 모두 엄청나게 미친 도박이었지요. 하지만 모두가 불가능하다고 생각하던 두 개의 미친 회사가 성공한다면, 사람들은 무슨 생각이 들까요? ‘일론은 리스크와 관련해 다른 사람들이 알지 못하는 무언가를 이해하고 있는 게 틀림없어.’ 이렇게 생각하지 않을까요?”\n책: 일론 머스크\n"},{"id":157,"href":"/docs/hobby/book/book19/","title":"애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다.","section":"글","content":" 애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다. # #2024-12-31\n1 # 지금까지 의사로 일하면서, 인생 계획을 완벽하게 할 수 있다고 생각하는 사람들을 많이 보았다. 그런 사람은 자녀들 인생까지도 그런 식으로 계획하려고 한다. 그리 생각하는 게 무리가 아닐지도 모른다. 살면서 정말 나쁜 일을 당해본 적이 한 번도 없고 모든 일이 기대한 대로 풀린 사람이라면 그럴 수 있다. 그러다가 상실을 경험하게 되면 그것이 본인의 자아정체감이나 인생의 이정표와 관련이 클수록 받아들이기가 더 힘들어진다. 나는 시험에 떨어지면서 계획이 일시적으로 틀어졌다. 주도면밀하게 그려놓았던 인생 계획이 어그러졌다. 누가 만들어준 계획은 분명히 아니었다. 오로지 내 생각만으로 만든 계획이라고 믿었다. 나도 어쩌면 대니얼처럼, 아버지의 마음에 들려고 애쓰고 있는 건지도 모른다는 사실은 무시했다. 게다가 이미 돌아가시고 세상에 있지도 않은 아버지였으니. 지금 생각해보면 나는 그때, 아버지가 돌아가신 후로 계속 나타나고 있던 균열을 적당히 땜질만 하며 수습하고 있었다. 그때는 길을 잠깐 잃었다가 다시 찾았다고만 생각했고, 다른 생각은 하지 못했다. 하지만 내게 정말 필요했던 약은, 운명이라 생각했던 길에서 완전히 탈선하는 것이었을지도 모른다. 후에 깨달았지만, 삶이라는 열차가 탈선하여 내달리는 그 혼돈의 순간에는 때로 중요한 메시지가 담겨 있다. 앞으로 무엇을 바꾸면서 살아야 할지, 너무 늦기 전에 생각해보라는 메시지다. 그런 의문에 답할 수 있다면, 자신만의 목표를 향해 다시 앞으로 나아갈 수 있다. 자신이 스스로 정한 목표는 이룰 가능성도 더 높은 법이다.\n2 # 생각해보면 그때부터 나는 깨닫기 시작했던 것 같다. 내 모든 결점과 허물까지 있는 그대로 받아들이지 않고서는, 삶을 다시 살아갈 수 없다는 것을. 심리치료사들은 자기애에 대해 이야기한다. 간혹 자기애를 이기심과 같은 것으로 오해하기도 하지만 둘은 다르다. 진부하게 들릴 수도 있겠지만, 진정으로 남을 아껴줄 수 있으려면 자신을 먼저 사랑할 줄 알아야 한다는 말이 백 번 틀리지 않다. 자신만의 장점을 인정하고, 단점을 시인하고, 받아들이며, 그 모든 것을 평온하게 바라볼 줄 알아야 한다. 나는 이미 저지른 실수를 반복하지 않으려면 지금까지 살아오면서 해온 선택들에 책임을 져야 한다는 것을 차츰 깨달았다. 그렇다고 해서 잘못된 선택을 더 이상 하지 않는 건 물론 아니었다. 특히 연애에 성급히 빠져드는 문제는 고쳐지지 않았다. 하지만 조금씩 달라질 수 있을 듯 했다.\n3 # 아버지가 돌아가신 후 내 결혼 생활의 부족한 점을 직시하지 못했던 건 외로움에 대한 두려움 때문이었다는 사실도 차츰 깨달았다. 내 삶도 정서적으로 \u0026lsquo;보류된\u0026rsquo; 상태였던 것이다. 미래가 뒤로 미루어진 상태였다.\n나는 물방앗간 집 옆 바위에 앉아 풍경을 바라보며 제니퍼를 생각했다. 바람에 이는 파도의 물보라, 바다 건너편에 수면과 맞닿아 있는 자줏빛 산들, 넋을 빼앗길 만큼 아름다운 풍경이었다. 나는 외로움의 아픔이 어떤 것인지 안다. 그것은 남은 평생을 혼자 살게 되리라는 두려움이었다. 아침에 옆에서 자는 연인의 따뜻한 체온을 느끼며 눈을 뜰 일이 없게 되리라는 두려움이었다. 이제 저녁 식탁에서 내가 정치인들이 의료제도를 개악하고 있다고 불만을 터뜨릴 때 공감해줄 사람도, 나를 안아주면서 일 이야기는 그만하고 어서 식기 전에 먹으라고 말해주는 사람도 없으리라는 두려움이었다. 고독사가 두려웠다. 혼자 사는 할머니가 집 주방에서 몇 주 만에 발견되었는데 \u0026lsquo;자연사\u0026rsquo;한 것으로 보이지만 배고픈 고양이들이 물어뜯어서 정확한 사망 원인은 알 수 없다는 따위의 이야기가 남 이야기가 아닐 것 같았다.\n내 환자들이 많이 그랬듯, 나도 세상으로부터 고립되고 단절될까봐 두려웠다. 고립, 외로움, 우울은 서로 밀접한 관련이 있다. 사람들과 떨어지면 그로 인해 우울해질 수 있고 회복 또한 더뎌질 수 있다. 문제는 우울해지면 남들과 대화하기도, 함께 있기도 힘들고 남들을 믿지도 못하니 스스로를 적극적으로 고립시키곤 한다는 것이다. 그 결과 고립이 심해지고 그에 따라 기분이 더 가라앉는 악순환이 일어난다. 이럴 때는 단순히 사람을 다시 만나는 것이 꼭 해결책이라고도 볼 수 없다. 천성이 사교적인 사람은 다시 사람을 만나고 싶은 마음이 상대적으로 크지만, 내향적인 사람은 상호작용 과다로 인한 스트레스에서 회복하려면 혼자 있는 시간이 필요할 수도 있다. 내 경우도 물론 후자 쪽이다. 우울한 사람은 세상 속에 나가 남들과 어울린다는 것에 대단히 양면적인 감정을 갖기 쉽다.\n숙소 밖에 앉아 주변 경관을 응시하면서, 혼자라는 두려움과 맞닥뜨릴 방법을 조금씩 알 것 같았다. 그 두려움을 어떻게 끌어안고 견뎌내고, 이해해야 할지 조금씩 깨달았다. 글을 읽거나 쓰거나 창작하는 등의 활동을 하려면 꼭 혼자 시간을 보내야 하는 사람들이 많다. 앤서니 스토는 \u0026lsquo;고독의 위로\u0026rsquo;라는 책에서 창작을 하는 사람이건 아니건 혼자 있는 능력이야말로 그 사람의 성숙도를 보여주는 징표이며, 모든 사람이 인간관계를 훌륭하게 영위해야만 삶에서 행복을 얻을 수 있는 것은 아니라는 이야기를 했다. 불교의 사상과 수행에서 유래한 \u0026lsquo;마음챙김\u0026rsquo;이라는 개념이 있다. 마음을 활짝 열고 우리 내면의 자아를 좀 더 잘 알기 위해, 괴로운 생각을 억누르려 하지 말고 그대로 관찰하면서 현재에 집중하는 것이다. 그 당시 나는 마음챙김에 대해 전혀 알지 못했지만, 이곳에서 생활하면서 그날그날 반복되는 일과에 집중하다 보니 - 내가 먹을 음식을 만들고, 3킬로미터 거리의 가게를 걸어서 다녀오고, 창가 책상에 앉아 독서하고 글 쓰고, 바다 풍경을 스케치하고 하면서 - 나도 모르게 마음챙김 기법을 실천하고 있었다. 그리고 그 과정에서, 혼자라는 게 사실 그렇게 나쁘지 않다는 걸 깨달았다.\n많은 사람이 외로움을 두려워한다. 누구나 정도의 차이는 있을지언정 남들과 어울리면서 감정을 나누고 걱정과 근심을 터놓고자 하는 욕구가 있다. 그러지 못한다면 제니퍼처럼 우울해지고, 또 우울에서 벗어나지 못하게 된다. 하지만 나는 고독이라는 것 역시 끌어안을 수 있고, 심지어 즐길 수도 있다고 생각한다. \u0026lsquo;자기 자신과 함께하는\u0026rsquo; 법을 배운다면 가능하다. 그렇게 함으로써 세상 속에서 내가 어떤 사람인지, 또 내가 남들에게 무엇을 줄 수 있는지 더 잘 알 수 있다. 우리는 친밀과 고독 사이에서 누구나 각자의 이상적인 균형점을 찾아내야 한다.\n4 # 나는 마지못해 그의 말이 맞다는 걸 인정했지만, 그런 공포스러운 감정에 사로잡힐 때 어떻게 벗어나야 할지 도무지 알 수 없었다. 가끔 기분이 가라앉고 몸이 녹초일 때는, 무거운 추가 가슴을 짓눌러 몸을 옴싹달싹할 수 없는 느낌이었다. 그런가 하면 어떤 때는 무엇이든 가능할 것 같은 기분이 들었다. 존의 말이 맞았다. 그럴 때 나는 정말로 통제력을 잃고 현실을 벗어나 버리는 듯 했다. 대개는 잠깐이었지만, 그럴 때면 자살 충동도 다시 느껴졌다. 나는 엘리자베스 워첼이 \u0026lsquo;프로작 네이션\u0026rsquo;이라는 책에서 묘사한, 끝없는 정서적 혼돈 상태가 무엇을 말하는지 너무나 잘 안다. 내가 특히 공감한 부분은, 저자가 원하는 치료사란, 어른답게 행동할 수 있도록 도와줄 사람, 그리고 우울증이 심해 전화 요금도 내지 못하는 이용자의 사정 따위는 전화 회사가 신경쓰지 않는 세상에서, 살아갈 방법을 알려줄 사람이라고 한 대목이었다.\n5 # \u0026ldquo;지금 어머니에 대해서는 어떤 감정이세요?\u0026rdquo; 내가 물었다.\n\u0026ldquo;제 어머니예요. 그러니까 물론 사랑하지요\u0026hellip;\u0026rdquo; 그녀는 그렇게 말하고는 잠시 생각하더니 입을 열었다. \u0026ldquo;하지만 밉기도 해요. 정말, 진짜 미워요.\u0026rdquo; 그리고 나를 보며 얼굴을 살짝 붉혔다. \u0026ldquo;제가 어떻게 그런 나쁜 말을\u0026hellip; 신부님에게 고해성사해야 할 것 같아요.\u0026rdquo; \u0026ldquo;아니요, 전혀 나쁜 말 같지 않은데요. 본인의 감정인 걸요. 이제 그 감정을 안고 살아갈 방법을 찾아봐야죠.\u0026rdquo;\n6 # 세상에 단일한 진실이란 없다. 저마다 몇 개의 안경 너머로 각자의 삶을 바라보는 다양한 관점이 있을 뿐이다. 남들의 기억과 인식과 가치관을 자기 것으로 삼아야 할 이유는 없다. 사람은 자기 필요에 맞는 진실을 만들어간다. 좋건 나쁘건 본인이 생각하는 자신의 모습과 자신의 스토리에 부합되는 방향으로 친구들과 이야기하면서, 일기를 쓰면서, 심리치료를 받으면서 만들어간다. 그러면서 우리는 과거를 조금씩 되돌아볼 수 있고, 과거가 어떻게 지금의 우리를 만들었는지 차츰 이해할 수 있다. 그리고 마침내는, 지금도 우리를 이리저리 휘두르는 과거의 횡포에 맞서 그 힘을 무력화할 수 있다.\n7 # 30년이 넘는 세월이 흐른 후 존과 함께 다시 찾은 그곳은, 회청색 갈매나무와 가시금작화 수풀 사이로 새로 깔린 판잣길이 모래언덕까지 이어져 있었다. 마침내 깨끗한 모래사장에 파도가 부서지는 해변에 이르자, 나는 워시만의 바닷물에 발을 담갔다. 차갑지만 상쾌했다. 어찌 보면 모질고 변덕스러운 바다였지만 나는 이곳에 오면 늘 기분이 새로웠다. 아이 때도 10대 때도 여름날 저녁이면 아빠와 함께 자주 와서, 바다에서 수영하는 아빠를 지켜보았던 바로 그곳이었다. 그때의 장면들이 새록새록 떠올랐다. \u0026ldquo;아빠가 항상 저 모래언덕에 앉았어.\u0026rdquo; 내가 존에게 외쳤다. \u0026ldquo;아빠가 여기를 정말 좋아했어. 수영을 워낙 잘했거든.\u0026rdquo; 힘차게 바다로 헤엄치던 아빠의 검게 탄 어깨가 떠올랐다. 그때는 아빠와 함께 있으면 무척 안전하게 느껴졌다. 아빠가 너무 좋았다. 잠시 아빠의 모습이 보였다. 아빠는 바다 저쪽, 아빠가 좋아했던 그 자리에 앉아 있었다. 힘찬 모습으로 살아서, 검게 탄 긴 팔을 석양에 번들거리며 나를 향해 흔들고 있었다. 그러고는 다시 물에 들어가더니 거센 물살을 헤치며 나를 향해 헤엄쳐왔다.\n나는 아버지가 돌아가신 후 오랫동안 아버지를 잃었다는 사실을 받아들이지 못했다. 그리고 언제까지나 아버지를 그리워할 것이다. 애통해한다는 것은, 놓아주고 앞으로 나아가는 것이다. 애통해할 수 있게 되면 잃어버린 사람을 그 사람 그대로 기억할 수 있게 된다. 이상화된 성자도, 분노와 실망을 쏟아부을 표적도 아닌, 복잡하고 현실적이면서도 매우 인간적인 존재로. 내가 가진 아빠 사진은 한 장 뿐이다. 내가 집을 떠나 대학에 가기 얼마 전에 찍은 사진이다. 아빠는 구겨진 셔츠 차림으로 서서 한 팔을 엄마 어깨에 두르고 있고, 엄마는 아빠 손을 꼭 잡아 허리에 붙인 모습이다. 나는 아빠 왼쪽으로 살짝 뒤에 서서 해를 쏘아보고 있고, 동생 이언은 우리 앞에 서 있다. 앨런은 아마 카메라를 들고 있었을 것이다. 아빠는 마치 우리가 모르고 있는 비밀을 알고 있기라도 한듯 묘한 미소를 엷게 짓고 있다. 엄마는 방금 전까지 다들 싸우기라도 한 듯 억지스러운 미소를 활짝 짓고 있다. 세월이 흐르면서 사진도 점점 빛이 바래 흑백에 가까워져가고, 내 애통한 마음도 흐릿해져간다. 지금은 알 수 있다. 나라는 사람은 결국 아빠가 아니면 아무것도 아니었다는 것을. 아빠는 말로는 표현하지 않았지만 행동으로 내게 변치 않는 사랑의 힘을 가르쳐주었고, 내가 지금 모습이 될 수 있게 도와주었다.\n8 # 중요한 건 애통한 마음의 변화라고 생각한다. 예컨대 상실의 기억을 떠올릴 때 15년 전이나 지금이나 똑같이 괴롭고 아픔이 생생하다면 진전이 없는 것이다. 감정이 잦아들지 않고 점점 커진다면 그 역시 심각한 신호다. 애도가 제대로 이루어지지 못하면 우울증이 된다. 애통한 마음의 크기를 1에서 10까지의 숫자로 생각해볼 때 그날그날 아주 미미하게라도 줄어들고 있따면, 앞으로 나아가고 있다는 신호다. 조금씩 다시 일상을 마주하고 앞날을 바라보고 있는 것이다. 지나간 일을 조금씩 손에서 놓아가는 것이다.\n책: 당신의 특별한 우울\n"},{"id":158,"href":"/docs/hobby/book/book33/","title":"어른들의지휘","section":"글","content":" 어른들의지휘 # #2024-12-31\n#1\n신규 가입 고객의 이름을 모니터링하던 중, 머스크는 이름 하나에 시선이 머물렀다. 바로 피터 틸이었다. 그는 엑스닷컴과 같은 건물에 있다가 지금은 거리 아래쪽으로 사무실을 옮긴 컨피니티Confinity라는 회사의 창업자 중 한 명이었다. 틸과 그의 주요 공동창업자 맥스 레브친은 모두 머스크만큼이나 열정적이었지만, 비교적 절제된 태도를 견지하는 사람들이었다. 엑스닷컴과 마찬가지로 컨피니티도 개인 간 결제 서비스를 제공했는데, 컨피니티의 시스템은 페이팔PayPal이라고 불렸다.\n2000년 초 인터넷 거품이 꺼질 조짐이 보이기 시작하던 무렵, 엑스닷컴과 페이팔은 신규 고객을 유치하기 위해 치열한 경쟁을 벌이고 있었다. “고객이 가입하고 친구를 추천하도록 유도하기 위해 양사 모두 엄청난 보너스를 지급하는 미친 경쟁을 벌이고 있었지요.” 틸의 설명이다. 나중에 머스크는 이렇게 표현했다. “어느 쪽이 먼저 돈이 바닥나는지 끝까지 가보자는 경쟁이었어요.” 머스크는 비디오 게임에 쏟던 열정으로 경쟁에 임했다. 반면에 틸은 냉정하게 계산하고 리스크를 완화하는 편을 좋아했다. 두 사람 모두 네트워크 효과(먼저 규모를 키우는 회사가 더욱 빠르게 성장하는 현상)로 인해 어느 한 회사만 살아남는다는 사실을 곧 깨달았다. 따라서 ‘모탈 컴뱃’ 게임식의 경쟁으로 치닫는 것보다는 합병하는 것이 합리적이라고 생각하게 되었다.\n머스크와 신임 CEO 빌 해리스는 팰로앨토에 있는 그리스 레스토랑 에비아의 별실에서 틸과 레브친을 만났다. 양측은 각자의 고객 보유 현황을 적은 메모를 교환했는데, 머스크는 거기에 평소처럼 나름의 과장을 섞어 넣었다. 틸은 머스크에게 잠재적 합병조건을 어떻게 구상하고 있는지 물었다. 머스크는 “합병된 회사의 90퍼센트는 우리가 소유하고 10퍼센트는 당신들이 소유하는 것”이라고 대답했다. 레브친은 머스크의 말을 어떻게 받아들여야 할지 알 수 없었다. 진담인가? 두 회사의 고객 기반은 거의 비슷했다. 레브친은 말한다. “머스크는 농담하는 게 아니라는 듯 매우 진지한 표정을 짓고 있었지만, 그 이면에 무언가 아이러니한 구석이 있는 것 같았어요.” 머스크는 나중에 레브친의 말을 인정하며 말했다. “사실 우리는 게임을 하고 있었던 거예요.”\n점심을 먹고 나오며 레브친은 틸에게 이렇게 말했다. “이 거래는 절대 성사될 수 없을 것 같네요. 그냥 우리끼리 다음 행보를 밟기로 하죠.” 하지만 틸은 사람을 읽는 데 더 능숙했다. 그래서 레브친에게 말했다. “이제 막 시작했을 뿐이에요. 머스크 같은 친구는 인내심을 갖고 상대해야 해요.”\n밀고당기는 협상 과정은 2000년 1월 내내 계속되었고, 머스크는 저스틴과의 신혼여행을 연기해야 했다. 엑스닷컴의 주요 투자자였던 마이클 모리츠는 샌드힐로드에 있는 자신의 사무실에서 양측이 만나도록 주선했다. 틸은 머스크의 맥라렌을 함께 타고 샌드힐로드로 향했다. “그래서, 이 차의 특별한 장점은 무엇인가요?” 틸이 물었다. “한번 보시죠.” 머스크는 그렇게 답하곤 추월차선으로 들어가 가속페달을 있는 힘껏 밟았다. 갑자기 뒷차축이 부러졌고 차가 빙글빙글 돌다가 갓길 경사면에 부딪힌 후 비행접시처럼 공중을 날았다. 차체 일부가 찢어졌다. 평소 자유주의를 실천하던 틸은 안전벨트를 매고 있지 않았지만, 다친 데 없이 빠져나왔다. 그는 지나가던 차를 얻어 타고 샌드힐로드의 세쿼이아 사무실까지 갈 수 있었다. 머스크도 다치지 않았고, 차를 견인시키기 위해 30분 정도 그 자리에 머물렀다가 세쿼이아로 왔다. 그는 해리스에게 무슨 일이 있었는지 말하지 않고 회의에 참석했다. 나중에 머스크는 웃으며 말했다. “적어도 내가 위험을 두려워하지 않는 사람이라는 것을 틸에게 보여준 거죠.” 틸은 동의한다. “맞아요, 그가 좀 미친 사람이라는 걸 깨달았죠.”\n#2\n머스크는 여전히 합병에 반대했다. 두 회사 모두 이베이의 전자결제를 위해 등록한 약 20만 명의 고객을 보유하고 있었지만, 그는 좀 더 광범위한 은행 서비스를 제공하는 엑스닷컴이 더 가치 있는 회사라고 믿었다. 그래서 그는 해리스와 갈등을 빚었고, 해리스는 만약 머스크가 합병 협상을 무산시키려 들면 사임하겠다고 위협하기에 이르렀다. “해리스가 그만두면 재앙이 닥칠 수 있는 상황이었어요. 인터넷 시장이 위축되고 있던 터라 더 많은 자금을 조달하기 위해 애쓰고 있었거든요.” 머스크의 말이다.\n머스크가 틸과 레브친과 다시 한번 점심식사를 하며 유대감을 형성하는 시간을 가지면서 상황은 달라졌다. 이번에 그들은 팰로앨토에 있는, 하얀 식탁보가 인상적인 이탈리아 레스토랑 일포르나이오에서 만났다. 음식을 기다리는 시간이 길어지자 해리스가 주방으로 뛰어들어가 어떤 요리부터 나올 수 있는지 살폈다. 머스크와 틸, 레브친은 서로를 바라보며 의미심장한 눈빛을 나누었다. 레브친은 말한다. “해리스는 극도로 외향적인 사업개발자 유형이었어요. 마치 가슴에 S자를 새긴 슈퍼맨처럼 행동했지요. 반면에 우리 셋은 뭐랄까, 비사교적인 괴짜들 같았다고나 할까요. 우리는 절대로 해리스처럼 나서서 설치진 않을 사람들이라는 점에서 유대감을 느꼈습니다.”\n양측은 엑스닷컴이 합병회사의 지분 55퍼센트를 갖는 조건에 합의했지만, 머스크가 곧이어 레브친에게 도둑질을 하고 있다고 비난하는 바람에 상황이 크게 꼬여버렸다. 격분한 레브친은 없던 일로 하자고 위협했다. 해리스는 레브친의 집으로 차를 몰고 가 빨래 개는 것을 도와주며 그를 진정시켰다. 계약 조건은 다시 한번 수정되어 기본적으로 50대 50으로 합병하되, 엑스닷컴이 존속법인으로 남는 것으로 합의되었다. 2000년 3월, 거래가 성사되었고 최대 주주였던 머스크가 의장으로 취임했다. 몇 주 후, 그는 레브친과 함께 해리스를 몰아내고(ㅋㅋ) CEO 자리도 되찾았다. 어른들의 지휘는 더 이상 환영받지 못했다.\n책: 일론 머스크\n"},{"id":159,"href":"/docs/hobby/book/book14/","title":"우리가 빛의 속도로 갈 수 없다면","section":"글","content":" 우리가 빛의 속도로 갈 수 없다면 # #2024-12-31\n#순례자들은 왜 돌아오지 않는가\n소피. 마지막으로 한 가지 말할 것이 남았어. 내가 처음으로 마을에 대해 의문을 품게 되었던 계기, 그 오두막 뒤에 있던 귀환자 말야. 정해진 성년식보다 조금 더 빨리 지구에 가기로 결심했을 때 나는 그 남자에게 몰래 찾아가 물었어. 혹시 지구에서 무슨 일이 있었던 거냐고.\n그는 슬픈 진실을 말해주었지. 지구에서 그가 사랑했던 사람과 그의 쓸쓸한 죽음에 관해. 그가 남겼던, 행복해지라는 유언에 관해.\n나는 말했어. 당신의 마지막 연인을 위해 당신이 할 수 있는 일이 있지 않겠냐고. 나는 그에게 지구로 다시 함께 가겠냐고 물었어.\n떠나겠다고 대답할 때 그는 내가 보았던 그의 수많은 불행의 얼굴들 중 가장 나은 미소를 짓고 있었지.\n그때 나는 알았어.\n우리는 그곳에서 괴로울 거야.\n하지만 그보다 많이 행복할 거야.\n소피, 이제 내가 먼저 떠나는 이유를 이해해줄 거라고 믿어.\n그럼 언젠가 지구에서 만나자.\n그날을 고대하며,\n데이지가.\n#스펙트럼\n1\n할머니는 마지막 순간들에 대해 구체적으로 이야기하지 않았다. 그때의 일을 다시 떠올리는 것이 너무나 괴롭기 때문이라고 했다. 하지만 나는 할머니가 그 이상으로 무언가를 숨기고 싶어 한다는 느낌을 지울 수 없었다.\n마지막 이야기에는 거짓이 있다. 할머니는 그 행성에서 구조 신호를 발신한 적이 없다. 할머니의 셔틀이 구조된 장소는 망망대해 같은 우주의 진공 한가운데였다. 할머니는 무리인들의 행성에서 10년을 보냈다고 했지만, 실제로 할머니가 구조된 건 조난 이후 40년 만이었다. 시공간 여행의 시차를 고려하더라도 할머니는 20년 이상을 다시 혼자가 되어 떠돌았다는 이야기가 된다. 그 오랜 시간동안 할머니는 대체 무엇을 한 걸까? 어쩌면 할머니는 어떻게든 행성에서 멀리 떠날 방법을 찾아냈던 것인지도 모른다. 그리고 누구도 그 행성의 위치를 추적할 수 없을 장소에 도달한 다음에야 마침내 구조 신호를 보낸 것인지도.\n어쨌든 모든 것은 추측에 불과하다. 할머니는 단 한 번도 그 시간의 빈틈에 대해서는 이야기해준 적이 없다. “루이는 정말로 죽었을까요?” 그런 질문에도 할머니는 빙긋 미소만 지었을 뿐이다.\n2\n행성의 위치에 대해 어떤 단서조차 내놓지 않겠다는 할머니의 고집은 이해할 수 없을 정도로 완고했다. 정부와 기업, 연구소에서 수도 없이 사람을 보내 할머니를 설득했지만 할머니는 굳게 입을 다물었다. 수십 년의 고독과 외로움에 지쳐 상상 속에서 허구의 세계를 만들어낸 것이라고 사람들이 수군거렸던 것도 그렇게 이상한 일만은 아닌 셈이었다.\n3\n우리가 그들을 다시 만날 때는, 우리는 더는 유약한 이방인이 아닐 것이다. 루이와 할머니의 관계는 재현될 수 없을 것이다. 나는 할머니를 이해할 수 있었다. 마지막 탈출 때 할머니가 협곡에서 가지고 올 수 있었던 것은 오직 한 뭉치의 종이뿐이었다. 할머니의 말대로 종이 위의 색채들은 마치 누군가 수백 종의 물감을 흩뿌려놓은 것처럼 다채로웠다. “이건 루이가 나를 기록하고 관찰한 일기였어. 일종의 연구노트라고 할까. 내가 그들을 관찰하고 탐색한 것처럼 루이에게도 나는 연구대상이었던 셈이지. 어쩌면 그들은 내가 아주 먼 곳에서 온, 도구가 없어 무력한 학자임을 이미 알고 있었는지도 몰라.” 할머니는 나에게 루이가 쓴 기록의 내용을 읽어주셨다. 지구에 돌아온 이후로 할머니는 여생을 색채 언어의 해석에만 몰두했다. 내용의 대부분은 그렇게까지 시간을 들여가며 알아낼 필요가 있었을까 싶을 정도로 정말 평범한 관찰 기록이었다. 그러나 그중 잊히지 않는 한 문장만큼은 지금도 떠오른다. “이렇게 쓰여 있구나.” 할머니는 그 부분을 읽을 때면 늘 미소를 지었다. “그는 놀랍고 아름다운 생물이다.”\n숨을 거두기 전 할머니는 연구노트의 처분을 나에게 맡겼다. 나는 기록의 사본을 남기고, 원본은 할머니와 함께 화장했다. 찬란했던 색채들이 한 줌의 재로 모였다. 나는 할머니의 유해를 우주로 실어 보내 별들에게 돌려주었다.\n#공생 가설\n만약에 뇌 속의 ‘그들’이 인간에게 태생적으로 존재하는 것이 아니라 외부에서 유입되는 것이라면 어떨까? 마치 기생충이나 미생물이 사람에게서 다른 사람으로 전염되듯 말이다. 그들은 공기 중에 분포해 있거나, 바이러스처럼 환경에 널리 퍼져 있을 수도 있다. 하지만 어느 쪽이든 감염을 위한 최초의 접촉이 필요할 것이다. 그렇기에 상자 속의 아이들이 밖으로 나오기 전까지 ‘그들’을 받아들일 기회가 없었던 것이라면? 어쩌면 가장 중요한 특성은 인간 밖에서 오는 것인지도 모른다. 수빈은 그 증거를 확인하려 하고 있었다.\n수빈은 영상에서 소리 데이터를 추출해서 전환기에 넣었다. 그냥 듣기에는 다른 평범한 아기들과 별반 다를 바 없는 울음이었다. 그러나 만약 ‘그들’의 유무가 아기들에게 영향을 미친다면, 여기서는 다른 결과가 나타날 것이다. 그들의 대화가 아닌 아기들의 욕구를 확인하게 될 것이다.\n「배고파」 「졸려」 「무서워」\n수빈은 다음에 일어난 일 역시 알고 있었다. 그 아기들은 사람들이 기대한 대로 성장하지 않았다. 상자 속의 아기들은 이타성을 획득하지 못했다.\n*재밌게 읽어서 하는 말이지만 인간의 \u0026lsquo;이타성\u0026rsquo;이 \u0026lsquo;그들\u0026rsquo; 즉 외부로부터 온다는 가정을 증명하는 위 부분에서 \u0026lt;태어난 아이들을 상자 속에 집어넣는 실험\u0026gt; 설정은 좀 거슬린다. \u0026lt;태어난 아이들을 충분히 빨리 상자 속에 집어넣음 -\u0026gt; 접촉이 일어나지 않음\u0026gt;인건데 거슬리는 부분은 \u0026lsquo;충분히\u0026rsquo;이다. 얼마나 빨리 집어넣었길래 혹은 접촉이 어떻게 일어나길래? 미토콘드리아처럼 공생한다고 했으면 의문이 안들었을것 같음. 빈틈없는 논리를 중요하게 생각하지 않는 편인데 내 눈에 보이는거면 매끄럽지 않은 진행이 맞는 듯하지만. 뭐 중요한가? 사실 이 말도 재밌게 읽었기 때문에 하는 말이다. ㅎ\n#우리가 빛의 속도로 갈 수 없다면\n기술 발전만 보고 달리니까 다른 중요한 가치를 인간이 따라가지 못하는 것에 대한 비판. 예전에 유튜브에서 돌고래와 소통하는 실험을 봤던 게 생각났다.\nhttps://youtu.be/1NfgR7LZ3sI?si=q9eMkyp5v9k_bI03 ![image](https://github.com/user-attachments/assets/f150b7ea-0701-4b02-9abe-9b222cd11389\n#감정의 물성\n나는 보현의 서랍장 위에서 수십 개의 감정의 물성 제품들을 발견했다. 하나같이 전부 ‘우울’이었다. 그 옆에는 병원에서 처방받아 온 항우울제가 있었다. 나는 이제 그녀가 우울에 빠져 죽고 싶은 것인지, 아니면 살아남고 싶은 것인지 도저히 알 수가 없었다.\n“널 이해 못 하겠어.” 보현은 딜레마에 빠져 있었다. 발목이 잡혀 있었다. 한때 사랑했던 사람들이 그녀를 억압하고 있었다. 그렇다고 이런 방식으로 해결하려는 건 더더욱 이해할 수 없었다. ‘우울체’가 그녀의 슬픔을 어떻게 해결해주는가? “물론 모르겠지, 정하야. 너는 이 속에 살아본 적이 없으니까. 하지만 나는 내 우울을 쓰다듬고 손 위에 두기를 원해. 그게 찍어 맛볼 수 있고 단단히 만져지는 것이었으면 좋겠어.” 테이블 위의 휴대폰이 울렸다. 보현은 말을 이어갔다. “어떤 문제들은 피할 수가 없어. 고체보다는 기체에 가깝지. 무정형의 공기 속에서 숨을 들이쉴 때마다 폐가 짓눌려. 나는 감정에 통제받는 존재일까? 아니면 지배하는 존재일까? 나는 허공중에 존재하는 것 같기도 아닌 것 같기도 해. 그래. 네 말대로 이것들은 그냥 플라시보이거나, 집단 환각일 거야. 나도 알아.” 보현은 우울체를 손으로 한 번 쥐었다가 탁자에 놓았다. 우울체는 단단하고 푸르며 묘한 향기가 나는, 부드러운 질감을 가진, 동그랗고 작은 물체였다. “하지만 고통의 입자들은 산산이 흩어져 내 폐 속으로 들어오겠지. 이 환각이 끝나면.” 우울체 하나가 탁자 위를 굴러 바닥으로 툭 떨어졌다. “그게 더 나은 결론일까.”\n나는 시선을 피했고 그 순간 보현이 어떤 표정을 지었는지는 알 수 없었다. 이어지는 진동 소리가 짧은 비명 같았다. 잠시 뒤 그녀가 몸을 돌려 밖으로 나갔다. 문이 달칵 닫혔다. 휴대폰의 진동이 멈췄다. 나는 고개를 들었다. 이제 허공을 가득 채운 침묵이 느껴졌다. 보현을 무슨 말로 위로해야 했을까? 나는 순간 보현을 위로할 수 있는 어떤 언어도 나에게 없다는 사실을 깨달았다. 무언가 중요한 것이 가슴속에서 빠져나가버린 듯 싸늘했고, 나는 그게 생각이나 관념이 아닌 실재하는 감각임을 알았다. 그제야 어설프게 그녀를 이해할 수 있었다. 잠시 머물렀다 사라져버린 향수의 냄새. 무겁게 가라앉는 공기. 문 너머에서 들려오는 흐느끼는 소리. 오래된 벽지의 얼룩. 탁자의 뒤틀린 나뭇결. 현관문의 차가운 질감. 바닥을 구르다 멈춰버린 푸른색의 자갈. 그리고 다시, 정적.\n물성은 어떻게 사람을 사로잡는가. 나는 닫힌 문을 가만히 바라보다 시선을 떨구었다.\n*결말이 이해가 안돼서 여러번 읽었는데 그래도 이해가 안된다. ㅠㅠ\n"},{"id":160,"href":"/docs/hobby/book/book30/","title":"위기모드","section":"글","content":" 위기모드 # #2024-12-31\n#1\n일론 머스크가 물려받은 유산과 혈통은 그의 뇌 배선과 어우러져 때때로 그를 냉담하게도, 충동적이게도 만들었다. 그리고 그것은 또한 리스크에 대한 극도로 높은 수준의 내성으로 이어졌다. 그는 리스크를 냉정하게 계산할 수도 있었고, 열정적으로 수용할 수도 있었다. “일론은 리스크 그 자체를 원합니다.” 페이팔PayPal 초창기에 머스크의 파트너로 일했던 피터 틸은 말한다. “그는 리스크를 즐기는 듯합니다. 때로는 정말 리스크에 중독된 것처럼 보이기도 하고요.”\n머스크는 태풍이 몰려올 때 가장 강력한 생기를 느끼는 그런 사람 중 한 명이다. “나는 폭풍을 위해 태어났어요. 그러니 고요함은 나에게 적합하지 않지요.” 미국의 7대 대통령 앤드류 잭슨이 한 말이다. 일론 머스크도 마찬가지다. 그는 일과 연애 양 측면에서 폭풍과 드라마를 끌어당기는 힘, 때로는 갈망을 발달시켰다(그래서 그가 그렇게 부부 또는 연인관계를 유지하는 데 어려움을 겪은 것이리라). 그는 위기나 데드라인, 할 일의 폭증과 같은 상황에서 번성했다. 복잡하고 난해한 도전에 직면하면, 그로 인한 긴장으로 종종 잠을 이루지 못하거나 심지어 토하기도 했다. 그러나 그런 상황은 그에게 활력도 불어넣었다. “형은 드라마를 끄는 자석과 같아요.” 킴벌이 말한다. “드라마가 그의 강박이자 삶의 주제입니다.”\n#2\n예전에 내가 스티브 잡스에 관해 취재하던 당시, 그의 파트너였던 스티브 워즈니악은 다음과 같은 질문을 제기하는 것이 중요하다고 말했다. “그가 꼭 그렇게 비열하게, 꼭 그렇게 거칠고 잔인하게, 꼭 그렇게 매번 드라마틱하게 굴었어야 했을까?” 인터뷰 말미에 해당 질문과 관련해 본인은 어떻게 다른지를 묻자, 워즈니악은 만약 자신이 애플을 경영했더라면 그보다는 좀 더 온화하게 처신했을 것이라고 답했다. 직원 모두를 가족처럼 대했을 것이고, 즉결로 해고하거나 그러지도 않았을 것이라고 했다. 그런 후 잠시 멈추었다가 이렇게 덧붙였다. “하지만 만약 내가 애플을 경영했더라면, 매킨토시 같은 것은 결코 만들어내지 못했을 겁니다.” 우리는 일론 머스크에 대해서도 유사한 질문을 떠올릴 수 있을 것이다. “만약 그가 괴팍하지 않았다면 과연 우리를 전기차의 미래로, 그리고 화성으로 인도하는 사람이 될 수 있었을까?”\n2022년 초, 스페이스X에서 31차례나 로켓을 성공적으로 발사했고, 테슬라의 자동차가 100만 대 가까이 팔렸으며, 머스크가 지구상에서 가장 부유한 사람으로 등극한 기념비적인 한 해를 보내고 새로운 해를 맞으며 머스크는 극적인 상황을 만들어내는 자신의 충동에 대해 유감스럽다는 듯이 말했다. “아무래도 사고방식을 위기 모드에서 다른 것으로 전환해야 할 필요가 있는 것 같아요.” 그가 나에게 한 말이다. “대략 지난 14년 동안 위기 모드로 살아왔거든요. 아니 거의 평생을 그랬다고 하는 게 맞겠네요.”\n그것은 새해 결심이라기보다는 아쉬움을 담은 말이었다. 그런 맹세를 했음에도 그는 세계 최상의 놀이터라 할 수 있는 트위터의 주식을 비밀리에 사들이고 있었다.\n#3\n머스크는 나중에 자신이 아스퍼거증후군을 앓고 있다고 밝히고 심지어 농담까지 하곤 했다. 아스퍼거증후군은 자폐 스펙트럼 장애의 한 형태에 대한 일반적인 명칭으로, 사회성과 인간관계, 정서적 연결, 자기 조절 능력 등에 영향을 미칠 수 있다. “어렸을 때 실제로 그런 진단을 받은 적은 한 번도 없거든요.” 어머니의 말이다. “하지만 본인이 그렇다고 하니 그 말이 맞겠지요.” 그의 그런 상태는 어린 시절의 트라우마로 악화되었다. 그의 절친한 친구 안토니오 그라시아스에 따르면, 성인이 된 이후에도 그는 괴롭힘을 당하거나 위협을 받는다고 느낄 때면 어린 시절에 얻은 외상후 스트레스장애가 뇌에서 감정을 조절하는 부분인 변연계를 완전히 장악해버렸다.\n그 결과 그는 사회적 신호를 잘 포착하지 못했다. “나는 사람들이 무언가를 말하면 액면 그대로 받아들이곤 했어요.” 그의 말이다. “사람들이 말하는 내용이 항상 진심은 아니라는 것을 오로지 독서를 통해 배웠어요.” 그는 공학, 물리학, 코딩과 같은 보다 정확한 주제를 선호했다.\n모든 심리적 특성이 그렇듯이 머스크의 특성 역시 복합적이고 개별화되어 있었다. 그는 특히 자녀와 관련해서는 매우 따뜻해질 수 있었고, 혼자 있게 되면 불안감을 심하게 느꼈다. 그러나 그에게는 일상적인 친절이나 따뜻함, 사랑받고 싶은 욕구를 만들어내는 감정 수용기가 없었다. 그는 공감 능력을 타고나지 못했다. 덜 전문적인 용어로 표현하자면, 그는 개자식처럼 굴 수도 있었다.\n#4\n하느님에 대한 경외심이 더 돈독했던 아버지는 일론에게 우리의 제한된 감각과 머리로는 알 수 없는 것들이 있다고 설명했다. “조종사 중에는 무신론자가 없는 법이지요.” 그의 말이다. 일론은 나중에 이렇게 덧붙였다. “시험 시간에는 무신론자가 없는 법이지요.” 하지만 일론은 일찍부터 과학이 모든 상황을 설명할 수 있으므로 창조주나 신성을 불러내 삶에 개입시킬 필요가 없다고 믿게 되었다.\n청소년기에 접어든 일론은 무언가 빠졌다는 생각에 시달리기 시작했다. 존재에 대한 종교적 설명과 과학적 설명 모두 ‘우주는 어디에서 왔으며 왜 존재하는가?’와 같은 정말 중요한 질문을 다루지 않았다고 그는 말한다. 물리학은 우주에 대한 모든 것을 가르칠 수 있었지만, 그 존재의 이유는 설명하지 못했다. 그것은 그가 스스로 ‘청소년기의 실존적 위기’라고 부르는 것으로 이어졌다. “나는 삶과 우주의 의미가 무엇인지 알아내려고 노력하기 시작했어요.” 그는 말한다. “그리고 인간의 삶이란 것이 아무런 의미가 없을지도 모른다는 생각에 정말 우울해졌지요.”\n훌륭한 책벌레들이 그러하듯이, 그는 독서를 통해 이런 의문을 해결했다. 처음에 그는 불안한 청소년의 전형적인 실수를 저질렀다. 니체나 하이데거, 쇼펜하우어와 같은 실존주의 철학자들의 책을 읽은 것이다. 이것은 일론의 혼란을 절망으로 바꾸어놓았다. “십대들에게는 니체를 읽으라고 권하면 안 된다고 생각합니다.” 일론은 말한다.\n#5\n머스크의 그런 청소년기에 가장 큰 영향을 미친 공상과학 소설은 더글러스 애덤스의 《은하수를 여행하는 히치하이커를 위한 안내서》였다. 유쾌함과 풍자가 넘치는 이 이야기는 머스크가 나름의 철학을 형성하는 데 도움이 되었고, 그의 진지한 표정에 익살스러운 유머를 더해주었다. “그 책은 내가 실존적 우울증에서 벗어나는 데 실제로 도움이 되었어요. 그 책을 읽는 순간 모든 부분에서 미묘한 방식으로 놀랄 만큼 재미있다고 생각했어요”라고 그는 말한다.\n이 소설에는 초공간 고속도로를 건설하는 외계 문명에 의해 지구가 파괴되기 몇 초 전에 지나가는 우주선에 의해 구조되는 아서 덴트라는 인간이 등장한다. 덴트는 자신을 구해준 외계인과 함께 “불가해성을 예술로 바꾼” 머리 두 개 달린 대통령이 통치하는 은하계의 다양한 구석구석을 탐험한다. 은하계의 주민들은 “생명과 우주, 그리고 모든 것에 대한 궁극적인 의문에 대한 답”을 알아내려고 노력하며 슈퍼컴퓨터를 만들지만, 그 컴퓨터는 700만 년 이상이 지난 후 그 질문에 대해 ‘42’라는 답을 내놓는다. 당황한 외계인들이 어리둥절해하며 법석을 떨자 컴퓨터는 응답한다. “확실히 답이 그렇게 나왔습니다. 솔직히 말해서 문제는 여러분이 질문이 무엇인지 제대로 알지 못한다는 것입니다.” 이 교훈은 머스크에게 그대로 각인되었다. “나는 그 책을 통해 의식의 범위를 확장해야 답을 얻을 수 있는 질문을 더 잘 던질 수 있다는 것을 깨달았어요. 우리 의식의 범위를 우주로 확장해야 하는 거지요.”\n#6\n아버지에 대한 이야기를 나눌 때 일론은 때때로 다소 거칠고 쓴 웃음을 터뜨렸다. 아버지와 비슷한 웃음이었다. 일론이 사용하는 일부 단어와 그가 응시하는 방식, 빛에서 어둠으로 그리고 다시 빛으로 갑작스럽게 변하는 모습은 그의 가족들에게 그의 내부에서 부글부글 끓고 있는 에롤을 떠올리게 한다. “일론이 나에게 들려준 끔찍한 이야기의 그림자가 자신의 행동방식에서 드러나는 것을 보곤 했어요.” 일론의 첫 번째 부인인 저스틴의 말이다. “그것은 우리가 원하든 원치 않든 자신이 성장한 환경의 영향을 받지 않는 것이 얼마나 어려운 일인지를 깨닫게 해주었지요.” 이따금 그녀는 감히 “당신이 아버지로 변하고 있어요”와 같은 말을 입에 올렸다. “사실 그것은 그가 어둠 속으로 들어가고 있음을 경고하는 우리의 암호였어요”라고 그녀는 설명한다.\n그러나 저스틴은 항상 자녀에게 감정적으로 관심을 기울이는 일론이 아버지와는 근본적으로 다르다고 말한다. “에롤을 보면 정말로 주변에서 나쁜 일이 일어날 것 같은 분위기를 느낄 수 있어요. 반면에 좀비가 창궐하는 대재앙이 발생한다면 일론의 팀에 속하고 싶을 거예요. 일론이라면 좀비를 줄 세우는 방법을 알아낼 것이기 때문이죠. 그는 매우 냉혹할 수 있지만, 결국에는 승리할 방법을 찾아낼 것이라는 믿음을 주는 사람이에요.”\n그러기 위해서 그는 앞으로 나아가야 했다. 남아공을 떠날 시간이었다.\n책: 일론 머스크\n"},{"id":161,"href":"/docs/hobby/book/book31/","title":"인간의 사교적인 행동을 배우려는 다른 행성의 관찰자처럼","section":"글","content":" 인간의 사교적인 행동을 배우려는 다른 행성의 관찰자 # #2024-12-31\n#1\n그는 아버지처럼 공학에 끌렸기에 물리학을 전공하기로 결정했다. 그가 느낀 엔지니어의 본질은 어떤 문제든 물리학의 가장 근본적인 원리를 파고들어 해결책을 찾는 것이었다. 그는 또한 공동 학위 과정을 밟아 경영학도 전공하기로 했다. “경영학을 공부하지 않으면 경영학을 공부한 누군가의 밑에서 일하게 될까 봐 걱정이 되었지요.” 그는 말한다. “내 목표는 물리학의 감각으로 제품을 설계 및 제작하는 것, 그리고 경영학을 전공한 보스를 위해 일할 필요가 없게 되는 것이었어요.”\n그는 정치적이지도 사교적이지도 않았지만 학생회 임원 선거에 출마했다. 그의 선거 공약 중 하나는 이력서를 화려하게 채우기 위해 학생회 활동을 하려는 학생들을 조롱하는 내용이었다. 그의 선거 공약 중 마지막 약속은 다음과 같았다. “만약 내가 이력서에 이 경력을 써 넣는다면, 공공장소에서 물구나무를 서서 이 공약서 50부를 씹어 먹겠습니다.”\n다행히도 그는 낙선했고, 덕분에 기질적으로 맞지 않는 학생자치회 유형의 학생들과는 어울릴 필요가 없었다. 대신 그는 과학적 힘과 관련된 영리한 농담을 하고 ‘던전앤드래곤’ 게임 및 비디오 게임에 탐닉하며 컴퓨터 코드 작성을 좋아하는 일단의 컴퓨터광 무리에 편안히 섞여들었다.\n#2\n렌은 머스크가 훗날의 경력 형성과 관계된 세 가지 분야에 집중했다고 회상한다. 중력을 측정하든 중력의 속성을 분석하든 그는 늘 렌과 로켓 제작에 적용되는 물리 법칙에 대해 논의했다. “그는 화성에 갈 수 있는 로켓을 만드는 것에 대해 계속 이야기했습니다.” 렌의 말이다. “물론 나는 그가 환상을 품고 있다고 생각했기에 별로 주의를 기울이지 않았지요.”\n머스크는 전기차에도 집중했다. 그와 렌은 종종 푸드 트럭 중 하나에서 점심을 급히 해결하고 캠퍼스 잔디밭에 앉아 쉬곤 했는데, 그때마다 머스크는 배터리에 관한 학술 논문을 읽곤 했다. 마침 캘리포니아 주에서 2003년까지 차량의 10퍼센트를 전기자동차로 전환할 것을 요구하는 법령이 막 통과된 시점이었다. 머스크는 “내가 그렇게 되도록 만드는 주역이 되고 싶어”라고 말했다.\n머스크는 또한 1994년에 접어들며 급격히 확산되기 시작한 태양광 발전이 지속 가능한 에너지로 나아가는 최선의 길이라고 확신하게 되었다. 그의 졸업논문 제목은 〈태양광의 중요성〉이었다. 기후변화의 위험성뿐만 아니라 화석연료 매장량이 줄어들기 시작할 것이라는 사실도 그에게 동기를 부여했다. 그는 “사회는 곧 재생 가능한 동력원에 집중할 수밖에 없게 될 것이다”라고 썼다. 논문의 마지막 페이지에서는 ‘미래의 발전소’에 대해 설명하고 있는데, 거기에는 태양 전지판에 햇빛을 집중시켜 생성한 전기를 마이크로파 빔을 통해 지구로 다시 보내는, 거울들이 달린 위성이 포함되었다. 교수는 “느닷없이 제시한 마지막 수치만 제외하면 매우 잘 쓴 흥미로운 논문”이라는 평가와 함께 98점을 주었다.\n#3\n레시는 나중에 일론이 약간 무심한 것처럼 보인다는 사실에 놀랐다. “그는 파티에 참석하는 것을 즐겼지만, 완전히 파티에 빠지지는 않았어요. 그가 진정으로 탐닉한 것은 오로지 비디오 게임이었지요.” 레시가 보기에, 일론은 그 많은 파티에 참석하면서도 인간의 사교적인 행동을 배우려는 다른 행성의 관찰자처럼 근본적으로 소외감을 느끼며 물러나 있었다. “일론이 조금 더 행복해지는 방법을 알았으면 좋겠어요”라고 레시는 말했다.\n책: 일론 머스크\n"},{"id":162,"href":"/docs/hobby/book/book26/","title":"인테그리티","section":"글","content":" 인테그리티 # #2024-12-31\n1998년 워런 버핏은 플로리다대학교에서 MBA 학생들에게 사람을 고용할 때 살펴보는 3가지를 언급하였다. 지능이 좋은지(머리가 잘 돌아가는지, 똑똑한지, 어리바리하지는 않은지), 일을 선도적으로 열정을 갖고 이끌어 나갈 수 있는지(시키는 것만 하는지, 해야 할 것들을 알아서 챙기는지), 그리고 integrity가 있는지 살펴봐야 한다. 머리도 좋고 일을 주도적으로 이끌어 나갈 열정도 있으나 integrity가 없는 자는 회사를 망칠 사람이다. integrity가 없는 사람을 고용하면 직원들을 게으름뱅이, 멍청이로 만들려는 것이기 때문이다.\n인테그리티란 자신이 옳다고 믿거나 생각하는 것을 말과 행동을 통해 일관성 있게 실천하는 것이다. 인테그리티를 완벽하게 실천하며 살아가기란 쉽지 않을 수 있다. 하지만 살아가면서 꾸준히 추구해야 할 가치이다.\n책: 세이노의 가르침\n"},{"id":163,"href":"/docs/hobby/book/book4/","title":"자신의 존재에 대해 사과하지 말 것 | 카밀라 팡","section":"글","content":" 자신의 존재에 대해 사과하지 말 것 | 카밀라 팡 # 최정문 북클럽 2023년 07월 도서여서 읽어봤다! 저자가 생물정보학 과학자이다.\n#북마크\n깔끔한 상자 모서리는 든든하지만 환상일 뿐이다.\n모든 것을 가질 수는 없을까? 현재에도 행복하고 미래도 이상적으로 계획할 수 있을까?\n"},{"id":164,"href":"/docs/hobby/book/book27/","title":"자연은 인간의 사정을 봐주지 않는다 vs 운명의 형태를 만드는 것은 사람의 의지다.","section":"글","content":" 자연은 인간의 사정을 봐주지 않는다 vs 운명의 형태를 만드는 것은 사람의 의지다. # #2024-12-31\n1 # “넌 중요하지 않아”라는 말은 아버지의 모든 걸음, 베어 무는 모든 것에 연료를 공급하는 것 같았다. “그러니 너 좋은 대로 살아.” 아버지는 수년 동안 오토바이를 몰고, 엄청난 양의 맥주를 마시고, 물에 들어가는 게 가능할 때마다 큰 배로 풍덩 수면을 치며 물속으로 뛰어들었다. 아버지는 언제나 게걸스러운 자신의 쾌락주의에 한계를 설정하는 자기만의 도덕률을 세우고 또 지키고자 자신에게 단 하나의 거짓말만을 허용했다. 그 도덕률은 “다른 사람들도 중요하지 않기는 매한가지지만, 그들에게는 그들이 중요한 것처럼 행동하며 살아가라”는 것이었다.\n아버지는 반세기 동안 거의 매일 아침 어머니에게 커피를 만들어주었고, 자기 학생들에게 헌신적이었다. 그들은 명절 때 우리 집에 식사하러 오고, 때로는 우리 집에서 살기도 했다. 우리 집 식탁에는 아버지가 떨리는 손으로 새긴 수천 개의 작은 숫자들이 새겨져 있는데, 이는 우리 세 자매에게 수학의 논리를 이해시키려 노력하며 보낸 무수한 밤들의 물리적 기록이다.\n암울한 현실일 수도 있는 것들이 아버지에게는 오히려 인생에 활력을 가득 불어넣고, 아버지가 크고 대범하게 살도록 만들었다. 나는 평생 광대 신발을 신은 허무주의자 같은 아버지의 발자국을 따라 걸으려 노력해왔다. 우리의 무의미함을 직시하고, 그런 무의미함 때문에 오히려 행복을 향해 뒤뚱뒤뚱 나아가려고 말이다.\n2 # 그것이, 바로 그것이 데이비드 스타 조던이 내 주의를 끌었던 이유다. 결코 승리하지 못할 거라는 그 모든 경고에도 불구하고, 그로 하여금 혼돈을 향해 계속 바늘을 찔러 넣도록 한 것이 무엇인지 알고 싶었다. 그가 우연히 어떤 비법을, 무정한 세상에서 희망을 찾을 수 있는 어떤 처방을 발견한 게 아닐까 궁금했다. 게다가 그는 과학자였으므로, 나는 무엇이든 끈질기게 지속하는 일에 대한 그의 정당화가 내 아버지가 심어준 세계관에도 들어맞을 수 있을 거라는 작은 가능성을 꽉 붙잡고 놓지 않았다. 어쩌면 그는 무언가 핵심적인 비결을 찾아냈을지도 몰랐다. 아무 약속도 존재하지 않는 세계에서 희망을 품는 비결, 가장 암울한 날에도 계속 앞으로 나아가는 비결, 신앙 없이도 믿음을 갖는 비결 말이다.\n3 # “낮이나 밤이나 호스로 물을 뿌려. 낮이나 밤이나.”\n해는 뜨고 지고, 뜨고 지고, 데이비드의 동료 두 사람은 고무 덧신을 신고서 물고기들의 살덩이를 향해 호스로 물을 뿌렸다. 이것이야말로 진정한 불굴의 기개가 무엇인지 보여주는 장면이 아닐까? 창밖에는 그들의 선지자가 머리를 거꾸로 처박고 있고, 공기 중에는 먼지가 희부옇게 드리워 있으며, 이 난장판을 어떻게 다시 수습할 수 있을지 알 수 없는 상황에서, 차가운 물과 불확실성을 정면으로 고스란히 받아내며 적어도 당장은 이것들을 마르지 않게 하겠다는 단호한 의지.\n4 # 그래서 그는 자신에게 어떤 말을 속삭였을까? 자기가 평생 해온 작업의 파편들을 쓸어 담을 때, 정체를 밝혀내지 못한 물고기들을 던져버릴 때, 이튿날 밤 작은아들 에릭을 침대에 뉘일 때, (영원히 끝나지 않을, 엄청난 양의) 번개와 세균과 지각변동이 잠복한 채 기다리고 있음을 알면서 이 모든 일을 하고 있을 때, 자신에게 계속 박차를 가하기 위해, 그 모든 일의 허망함에 짓눌려 으스러지지 않기 위해 그는 정확히 어떤 말을 자신에게 들려주었을까?\n5 # 나는 시카고로 옮겨 갔다. 친구 헤더가 몇 주 동안은 자기 집 남는 방에서 지내도 되니 거기서 앞으로 뭘 할지 생각해보라고 했다. 믿을 수 없을 만큼 친절한 제안이었다. 나는 시카고가 좋았다. 시카고의 추위가, 시카고의 익명성이. 나는 누구든 될 수 있었다. 컨버스 스니커즈를 신고, 탄산화 생성물이 약간 포함되어 있는 듯한 까끌까끌한 보도를 따라 걸었다. 나는 폴짝 뛰었다. 내가 되고 싶은 사람이 될 수 있을 것 같은 기분이었다. 바람둥이가 아니라, 우울증 환자가 아니라, 우주적 정의가 실행되는 대상이 아니라, 고향에 행복한 가정이 있는 사람이.\n그러나 헤더가 남자친구와 시내로 외출한 밤, 도시의 자주색 불빛이 창으로 쏟아져 들어올 때면 나는 그 모든 것의 현실성을 무시할 수 없다는 사실을 깨닫곤 했다. 내 인생에 생긴 공백을, 내가 품은 희망의 빛이 나를 더 따뜻이 데워줄수록 점점 더 넓어지고 차가워지기만 하는 그 공백을 말이다.\n그래서였다. 나는 절박했다. 단순하게 말하자. 데이비드 스타 조던의 책에서, 망해버린 사명을 계속 밀고 나아가는 일을 정당화하는 그 정확한 문장을 찾아내는 것이 내게는 절박했다.\n6 # 그는 갈수록 더욱더 내 아버지와 비슷한 소리를 했다. 인간이 살아가는 방법은 매번 숨 쉴 때마다 자신의 무의미성을 받아들이는 것이며, 거기서 자기만의 의미를 만들어내는 것이라고 말이다.\n심지어 절제에 관한 에세이에서도 그것을 찾을 수 있다. 그는 왜 그토록 약에 반대했을까? 그건 약이 사람을 실제보다 더 강력하다고 느끼게 하기 때문이다. 혹은 그의 표현을 빌리자면, 약이 “신경계가 거짓말을 하도록 강요”하기 때문이다.8 예를 들어 알코올은 사람들로 하여금 “실제로는 몸이 차가울 때도 따뜻하게 느끼도록 하고, 아무 근거 없이 기분 좋아지게 하며, 인격 수양의 핵심을 차지하는 제한과 자제에서 해방되었다고 느끼게 한다.” 달리 말하면, 자신에 대한 낙관적인 관점은 자기 발전에 대한 저주라는 것이다. 자신을 정체시키고 자기 발달을 저해하고 도덕적으로 미숙하게 만드는 길이자 멍청이가 되는 지름길이다.\n이런 게 정말 그의 세계관이라면, 그가 그렇게 자기 과신을 경계하는 사람이라면 도대체 어떻게 그런 집요함을 이끌어낼 수 있었을까? 모든 게 사라지고 부서지고 희망이라곤 없는 최악의 날에조차 어떻게 자신을 일으켜 세우고 밖으로 나가게 한 것일까?\n마침내 나는 가장 유의미한 단서가 될 만한 것을 손에 넣었다. 그것은 《절망의 철학》이라는 제목의 작고 검은 책이다.\n7 # 책에서 데이비드는 과학적 세계관이 골치 아픈 점은 삶의 의미를 찾고자 할 때 그 세계관이 보여주는 것은 허망함뿐이라는 사실을 고백한다. “우리가 붙인 불은 숯을 남기고 죽는다. 우리가 지은 성들은 우리 눈앞에서 사라진다. 강은 바닥을 드러내고 사막의 모래만 남긴다. (…) 어느 쪽으로 눈을 돌리든 생명의 과정을 묘사하려면 기운 빠지게 하는 은유를 사용할 수밖에 없다.”9\n그러면 어떻게 해야 한다는 말인가?\n데이비드는 청교도답게 손을 게으름에서 벗어나게 하라고 권한다. “활동적인 야외 생활과 그로 인해 얻게 되는 건강과 함께”10 “영혼의 고통은 사라진다.”11 그는 우리 몸이 일으키는 전기에 구원이 있다고 주장한다. 비슷한 시기에 쓴 한 강의 요강에서 그는 이렇게 말한다. “행복은 행하고, 돕고, 일하고, 사랑하고, 싸우고, 정복하고, 실제로 실행하고, 스스로 활동하는 데서 온다.”12 내 생각에는 너무 많이 생각하지 말라는 것이 그가 말하려는 요점 같다. 여정을 즐기고 작은 것들을 음미하라고 말이다. 복숭아의 “감미로운” 맛,13 열대어의 “호화로운” 색깔,14 “전사가 느끼는 준엄한 기쁨”15을 느끼게 해주는 운동 후 쇄도하는 쾌감 등.\n그러면 나쁜 나날을 보내고 있으면 어떻게 하라는 걸까? 데이비드는 나쁜 하루하루를 보내고 있는 사람에게는 동정심을 거의 느끼지 않는다. 《절망의 철학》의 최종 결론은 절망이 선택이라는 것이기 때문이다. 그는 절망이 청소년기에 자연스럽게 거쳐 가는 단계라고 생각하기는 해도 그런 감정을 떨쳐내지는 못하는 사람들은 경멸한다.\n8 # 나는 익숙한 수치심이 나를 덮치는 것을 느꼈다. 그것은 아버지가 엄청 차가운 호수에 풍덩 뛰어들었다가 개구쟁이 같은 미소를 만면에 띠고 큰 숨을 내쉬며 수면으로 치솟는 모습을 볼 때 느꼈던 바로 그 감정이었다. 나는 왜 아버지처럼 저렇게 살 수 없는 걸까? 내가 잘못하고 있는 게 뭘까? 그 답을 찾으려는 필사적인 마음에 나는 계속 책을 읽으며, 위생과 유머, 외교, 평화주의에 관한 그의 비판문과 시, 강의 노트, 알코올과 립스틱과 전쟁에 관한 논쟁을 뒤졌다. 그리고 마침내, 어느 오후 나는 발견했다. 공포에 대한 해독제, 희망에 대한 처방을 말이다.\n그것은 그가 ‘진화의 철학’이라 이름 붙인 강의 요강의 제일 밑에 묻혀 있었다. 알고 보니 그는 그날 하루의 강의를 내가 풀고자 했던 그 난제, 바로 과학적 세계관을 받아들이는 문제에 바쳤다. “이러한 인생관은 염세주의로 이어지는가?”20 강의가 끝나갈 무렵 그는 학생들에게 일종의 마술 같은 주문을 걸었다. 혼돈이 주는 냉기를 떨쳐버리는 한 가지 방법을 말이다. 특별한 활자체로 된 여덟 개의 단어.\n생명에 대한 이런 시각에는 어떤 장엄함이 깃들어 있다.\n나는 경악했다. 이거였다. 내 아버지가 즐겨 쓰는 바로 그 비법. 오늘날까지도 아버지 책상 위 액자 속에 담겨 있는 바로 그 단어들. 다윈이 외친 투쟁의 권유. 내 아버지와는 다르게—반항적이고, 희망과 신념이 가득한 사람으로—보였던 데이비드지만, 결국 그에게도 내게 알려줄 새로운 건 하나도 없었던 것이다. 내가 늘 들어왔던 말을 또다시 상기시키는 것밖에는.\n장엄함은 존재해. 네가 그걸 보지 못한다면 부끄러운 줄 알아.\n9 # 나는 스탠지에게 데이비드 스타 조던과 그 지진과 바늘에 대한 나의 집착을 이야기했다. “그러니까 말하자면 그건 왜 그러는지에 관한 집착이야”라고 나는 말했다. “한 사람을 계속 나아가도록 몰아대는 건 뭘까?”\n그때 그 친구가 한 말은 “흠”이 다여서 나는 맥이 좀 빠졌지만, 다음 날 오후 이메일을 통해 좀 더 긴 답변을 들을 수 있었다.\n그리고 네가 말한 그 이야기 말이야. 너무나 소중하고, 너무나 정교한 뭔가를 쌓아 올렸다가… 그 모든 게 다 무너지는 걸 목격한 그 사람… 그 사람은 계속 나아갈 의지를 어디서 다시 찾았을까 하는 그 질문. 계속 가고 싶든 그렇지 않든 어쨌든 계속 가게 만드는, 모든 사람의 내면 가장 깊은 곳에 자리한 그것을 카프카는 ‘파괴되지 않는 것’이라고 불렀어. 파괴되지 않는 것은 낙관주의와는 전혀 무관해. 낙관주의에 비하면 훨씬 더 심오하고 자의식은 훨씬 덜하지. 우리는 그 파괴되지 않는 것을 온갖 종류의 다른 상징과 희망과 야심 등으로 가리고 있어. 이런 상징과 희망과 야심은 그 밑에 무엇이 있는지 인정하라고 강요하지 않으니까. 음… 만약 그 모든 잉여를 제거한다면(혹은 제거할 수밖에 없게 된다면), 파괴되지 않는 그것을 찾게 될 거야. 그리고 우리가 일단 그것의 존재를 인정하게 되면(카프카는 여기서 더 깊게 들어가. 그는 우리가 파괴되지 않는 것을 낙관적이거나 긍정적인 것으로 생각하게 해주지 않아), 그것은 실제로 우리를 찢어발기고 파괴할 수도 있어.\n그래도 어쩔 수 없는 거지….\n10 # 나는 파괴되지 않는 것이라는 말이 마음에 들었다. 경이로운 개념이었다. 왜냐하면 그건 내가 비현실적인 목표를 향해 밀고 나아가는 것이 미친 짓인가 아닌가 하는 질문에 답하지 않아도 된다고 허락해주는 개념이기 때문이다. 그 개념은 단지 내가 그것을 거역한다면 나를 부숴버리겠다고만 약속할 뿐이다.\n하지만 나는 그것이 데이비드 스타 조던에게 잘 들어맞는다고는 생각하지 않았다.\n11 # 하지만 나는 확인하고 싶었다. 그래서 다시 그의 회고록으로 돌아갔다. “파괴되지 않는 것”이라는, 아마도 그전까지는 내게 불활성 상태로 있었을 개념에 생기를 불어넣은 이 새로운 단어로 무장한 채, 나는 그 개념이 데이비드가 쓴 글들 속 어딘가에 잠복해 있을지도 모른다고 생각하고 그 증거를 찾아 나섰다.\n그 증거는 긴 발췌문 속에 묻혀 있었다. 지진이 있고 겨우 며칠밖에 지나지 않았을 때, 아직 상처가 아물지 않은 채 샌프란시스코가 입은 피해의 규모를 조사하려 애쓰고 있을 때 데이비드 본인이 쓴 개인적인 에세이○에서 발췌한 글이었다.\n사람이 계획을 세우고 창조하기 시작한 이래, 사람이 노력해서 이룬 결과가 그토록 처참하게 파괴된 일은 한 번도 없었다. 엄청난 규모의 재앙 앞에서 그렇게 푸념하지 않는 인간을 만난 일은 한 번도 없었다. 평범한 한 남자가 자기 자신에게 그토록 희망차고, 그토록 용감하며, 그토록 자신과 자신의 미래를 확신하는 모습을 보여준 일은 그전엔 결코 없었다. 왜냐하면 결국 살아남는 것은 사람이고, 운명의 형태를 만드는 것도 사람의 의지이기 때문이다.\n사람은 결코 흔들리지 않으며 불에 타지 않는다는 것, 그것이 그 지진과 화재가 준 교훈이다. 그가 지은 집은 무너지기 쉬운 카드로 지은 집이지만, 그는 집 밖에 서 있고 다시 집을 지을 수 있다. 위대한 도시를 건설하는 것은 경이로운 일이다. 그보다 더 경이로운 일은 도시가 되는 것이다. 도시란 사람들로 이루어지며, 사람은 영원히 자신이 창조한 것들보다 높이 올라가야 한다. 사람의 내면에 있는 것은 그가 할 수 있는 모든 일보다 더 위대하다.21이 얼마나 경이롭고 분발을 요구하는 투쟁의 권유인가. 이 얼마나 영광스러운 위로이자, 어깨를 움켜쥐는 손길인가. 그런데 작은 문제가 하나 있다. 그가 쓴 단어들을 자세히 들여다보면 당신도 그 문제를 발견할 것이다. 그 진주알을 만든 최초의 작은 모래알 하나가 거짓말이라는 것을.\n운명의 형태를 만드는 것은 사람의 의지다.\n이 말은 그가 자기 자신에게 결코 하지 않겠다고 약속했던 바로 그런 종류의 거짓말이다. 사악함으로 이끌어가는 것이라고 그가 경고했던 그런 종류의 거짓말. 자기 경력을 바쳐 맞서 싸워왔던 그런 종류의 거짓말이자, 그가 죽기를 각오하고 싸울 가치가 있다고 말했던 그런 종류의 거짓말이다. 자연은 인간의 사정을 봐주지 않으니까! 그조차도 절망에 완전히 집어삼켜지지 않으려면 그 거짓말이 진실이기를 믿어야만 했던 것이다.\n책: 물고기는 존재하지 않는다\n"},{"id":165,"href":"/docs/hobby/book/book38/","title":"자전거","section":"글","content":" 자전거 # #2024-12-31\n#1\n\u0026ldquo;당신은 내가 뭘 좋아하는지 몰라요.\u0026rdquo; 나는 냉정하게 쏘아붙였다. \u0026ldquo;전혀요.\u0026rdquo;\n인간의 영혼을 읽는 능력이 조금 덜 예리한 사람이라면 나의 끊임없는 부정에서 키아라를 방어막으로 사용하고 있음을 허둥지둥 시인한다는 끔찍한 신호를 발견했을 것이다.\n하지만 그런 능력이 대단히 날카로운 사람은 내 행동에서 완전히 다른 진실로 이어지는 문을 발견했으리라. 그 문을 열러면 위험을 각오해요. 장담하건대 당신은 진실을 듣고 싶지 않을 거예요. 아직 시간이 있을 때 자리를 피하는 게 좋을 거예요.\n#2\n바다가 내려다보이는 작은 광장에 도착하자 올리버는 담배를 사기 위해 멈추었다. 그는 골루아즈를 피우기 시작한 터였다. 피워 본 적 없는 브랜드라 나도 한대 피워 봐도 되느냐고 물었다. 그가 성냥개비 하나를 꺼내 내 얼굴 가까이에서 양손을 동그랗게 모아 쥐고 담뱃불을 붙여 주었다.\n\u0026ldquo;나쁘지 않지?\u0026rdquo;\n\u0026ldquo;나쁘지 않네요.\u0026rdquo;\n그를, 오늘을 떠올리는 담배가 되리라고 생각했다. 앞으로 한 달도 되지 않아 그는 흔적도 없이 사라질 테니까. 그가 B에서 지낼 날이 얼마나 남았는지 세어 보기로 마음먹은 것은 그때가 처음이었다.\n#3\n\u0026ldquo;이걸 좀 봐.\u0026rdquo; 아래로 완만하게 경사진 언덕이 내려다보이는 광장 끄트머리를 향해 아침 햇살 속에서 자전거를 끌고 천천히 걸으며 그가 말했다.\n저 아래 저 멀리 그림 같은 바다가 펼쳐저 있었다. 거대한 돌고래들이 파도를 부수는 듯 작은 만에 몇 가닥의 하얀 거품이 보였다. 작은 버스 한 대가 언덕길을 오르고 제복 차림의 남자 셋이 자전거를 타고 뒤따라 왔다. 분명히 버스에서 나오는 매연에 대해 불평하고 있으리라.\n“이 근처에서 누가 익사했는지 알겠지?” 그가 물었다. “시인 퍼시 셸리요.” “시신이 발견된 후 그의 아내 메리와 친구들이 어떻게 했는지도 알고?” “Cor cordium, 마음 중의 마음이요.” 부풀어 오른 시신을 해변에서 화장할 때, 불꽃이 시신을 완전히 집어삼키기 전에 친구가 셸리의 심장을 떼어 냈다는 이야기를 떠올리며 대답했다. 시험이라도 치듯 왜 저러는 걸까?\n“넌 모르는 게 없지?” 나는 그를 바라보았다. 지금이 바로 나를 위한 순간이었다. 그 순간을, 나는 잡을 수도 놓칠 수도 있지만 어느 쪽이건 평생 잊지 못할 당혹스러운 순간으로 남을 것이다. 아니면 칭찬에 흡족해할 수도, 나머지 전부를 후회하면서 살 수도 있었다. 내가 할 말을 미리 계획하지 않고 어른에게 말하기는 그때가 처음이었을 것이다. 너무 떨려서 계획할 수도 없었다. “난 아무것도 몰라요, 올리버. 아무것도. 아무것도요.” “넌 여기 그 누구보다 많이 아는데.” 어째서 그는 비극에 가까운 내 말투에 저렇듯 단조롭게 칭찬처럼 답하는 거지? “정말로 중요한 건 잘 모른다는 걸 당신이 몰라서 그래요.” 나는 물을 향해 걸어가고 있었다. 익사하려 하지도, 안전하게 헤엄치려 하지도 않고 그냥 머물기 위해. 진실을 말하거나 암시조차 못 한다고 해도 진실은 항상 우리 주변에 놓여 있을 테니까. 마치 수영하다가 잃어버린 목걸이 이야기를 하듯이. 나는 그것이 어딘가에 있음을 안다. 뻔히 드러나 보이는 사실을 종합해서 무한대보다 큰 숫자를 떠올리도록 내가 기회를 주는 거라는 사실을 그가 알 수만 있다면. 하지만 그가 이해했다면 눈치를 챘다는 뜻이고, 만약 눈치를 챘다면 그동안 결코 만나지 않는 평행선 너머에서 적대적으로 강철처럼 차갑고 무표정하게 다 안다는 듯 허를 찌르는 눈빛으로 나를 쳐다보고 있었으리라.\n그는 뭔가 떠오른 게 틀림없었다. 그게 뭔지는 아무도 알 수 없지만. 어쩌면 그는 놀라지 않은 척하는 건지도 모른다. “중요한 게 뭐지?” 그는 지금 솔직하지 못한 것일까? “뭔지 알잖아요. 지금쯤이면 다른 사람은 몰라도 당신은 알 거예요.” 침묵이 흘렀다. “왜 이런 말을 하는 거지?” “당신이 알아야 한다고 생각했어요.” “내가 알아야 한다고 생각했다…….” 그는 깊은 생각에 잠긴 듯 내 말을 그대로 읊었다. 그 말에 담긴 의미를 파악하고 정리할 시간을 벌려는 듯이. 강철이 뜨겁게 달아오르고 있었다. “당신이 알았으면 해요.” 나도 모르게 튀어나왔다. “당신 말고는 말할 사람이 아무도 없으니까요.” 말해 버렸다. 도대체 말이 되기나 하는 걸까? 나는 바다나 내일 날씨 혹은 아버지가 매년 이맘때면 꼭 약속하는 E 항해가 과연 좋은 생각인지에 대한 이야기로 화제를 돌리려고 했다. 하지만 역시나 그냥 넘길 그가 아니었다. “지금 무슨 말을 하는지 알고 하는 얘기야?” 나는 바다를 쳐다보면서 모호하고 지친 어조로 대답했다. 내 마지막 방향 전환이자 위장막이자 최후의 도피였다. “네. 내가 무슨 말을 하는지 알고 당신도 제대로 받아들이고 있어요. 난 말을 잘 못해요. 다시는 나랑 말하지 않겠다고 해도 괜찮아요.” “잠깐. 내가 생각하는 그런 말이 맞는 거야?” “네에.” 이왕 말을 꺼낸 마당이라 약간 느긋하고 짜증 난 것처럼 굴 수 있었다. 경찰에 항복한 뒤 범행 방법을 몇 번이나 반복해서 진술해야 하는 절도범처럼 말이다. “잠깐 여기서 기다려. 2층에 올라가서 원고를 받아 와야 하니까. 딴 데 가지 마.” 나는 그에게 믿음직한 미소를 보냈다. “내가 아무 데도 안 간다는 걸 당신도 잘 알잖아요.” 이거야말로 내 속마음을 확실하게 인정하는 게 아니고 뭐란 말인가?\n#4\n“말하지 말걸 그랬어요.” 마침내 내가 말문을 열었다. 그렇게 말하는 순간 우리 사이의 미약한 마법의 주문이 깨지리라는 것을 알고 있었다. “못 들은 걸로 할게.” 전혀 예상하지 못한 대답이었다. 뭐든 괜찮다고 말하는 사람이니까. 우리 집에서는 단 한 번도 들어 본 적 없는 말이었다. “그럼 서로 말은 하고 지내는 거예요, 아니면 아닌 거예요?” 그가 생각에 잠겼다. “우린 그런 얘기를 해서는 안 돼. 정말로 안 돼.” 그가 가방을 둘러멨고 우리는 내리막길을 향해 출발했다.\n#5\n“여기는 내 공간이에요. 나만의 공간. 책을 읽으러 와요. 여기서 몇 권이나 읽었는지는 나도 몰라요.” “넌 혼자 있는 게 좋아?” “아뇨. 혼자 있는 걸 좋아하는 사람은 없어요. 난 그걸 견디는 법을 배웠죠.” “넌 항상 그렇게 지혜로우니?” 그도 다른 사람들처럼 밖에 나가서 친구 좀 사귀라고, 사귄 친구들한테 이기적으로 굴지 말라고 거들먹거리는 말투로 설교를 시작하려는 것일까? 아니면 정신과 의사 겸 가족 친구의 역할을 수행하겠다는 신호일까? 아니면 내가 또 그를 완전히 잘못 읽은 걸까? “난 전혀 지혜롭지 않아요. 말했잖아요. 난 아무것도 몰라요. 책은 알죠. 말을 결합할 줄은 알지만 나한테 가장 중요한 얘기를 할 줄 안다는 뜻은 아니에요.” “하지만 지금 그러고 있는데. 어떤 면에서는 말이야.” “그래요. 어떤 면에서는 그렇죠. 난 항상 그런 식으로 말해요.”\n그를 바라보지 않으려고 앞바다로 시선을 향하면서 풀밭에 앉았다. 그가 몇 미터 떨어진 곳에 쭈그려 앉는 모습이 보였다. 언제든 자전거가 있는 곳으로 뛰어가려는 듯 발끝이 들려 있었다.\n그때는 몰랐다. 그를 이곳에 데려온 이유는 단지 그에게 내 작은 세상을 보여 주려는 게 아니라 내 작은 세상이 그를 받아들여 주길 바라서라는 것을. 내가 여름날 오후면 홀로 찾던 장소가 그를 보고 괜찮은 사람인지 판단하여 받아들일 수 있도록. 그래야 훗날 다시 왔을 때도 내가 기존의 세상을 피해 스스로 만든 세상을 찾으러 이곳에 온다는 사실을 기억할 테니 말이다. 그에게 다른 세상으로 출발하는 내 발사대를 소개해 준 셈이었다. 이곳에 서 읽은 책을 나열하면 그는 내가 어디를 여행했는지 알 터였다. “난 네가 말하는 방식이 마음에 드는데, 왜 넌 항상 너를 깎아내리지?” 나는 어깨를 으쓱했다. 지금 나를 비판하는 건가? “모르겠어요. 그러니까 당신도 알 수 없겠죠.” “남들이 어떻게 생각할지 두려워?” 고개를 저었다. 하지만 답을 알지 못했다. 어쩌면 너무 뻔해서 대답할 필요가 없는지도 모른다. 이럴 때면 벌거벗은 것처럼 한없이 연약해지는 기분이었다. 자신을 몰아세우고 초조하게 만들어서 상대방을 몰아세우지 않는 한 다 들켜 버린다. 아뇨. 뭐라고 답할 말이 없었다. 하지만 나는 움직이지도 않았다. 그더러 혼자 집으로 돌아가라 말하고 싶은 충동이 일었다. 나는 점심시간에 맞춰서 가겠다고.\n그는 내 입에서 무슨 말이 나오기를 기다리고 있었다. 나를 빤히 쳐다보았다.\n#6\n“네 기분이 조금이라도 좋아질지 모르겠는데 이젠 나도 참아야 해. 넌 지금쯤이면 참는 법을 배웠겠지만.” “내가 할 수 있는 최선은 신경 쓰지 않는 척하는 거예요.” “그건 이미 서로 알고 있는 거잖아.” 그가 곧바로 쏘아붙였다.\n산산이 부서진 기분이었다. 내가 정원과 발코니, 해변에서 그를 얼마나 쉽게 무시할 수 있는지 보여 줄 때마다 그는 나를 다 꿰뚫어 보았고 짜증이 섞인 지극히 전형적인 수라고 생각한 것이다. 우리 사이의 수로를 다 열어 준 듯한 그의 고백은 오히려 새롭게 솟아나는 내 희망을 삼켜 버렸다. 이제 우리는 어떻게 하지? 여기서 더 할 게 뭐가 있는가? 서로 말하지 않는 척하지만 서로의 사이에 낀 서리가 가짜라는 것을 더 이상 확인할 수 없어지면 어떻게 되는 거지? 우리는 잠시 더 이야기를 나누었지만 대화는 점차 사라졌다. 서로의 마음을 털어놓은 후라 그냥 잡담을 나누는 것처럼 느껴졌다. “그래, 모네가 그림을 그린 곳이라고…….” “집에 가서 보여 줄게요. 여기 풍경을 그린 모네 그림이 실린 책이 있어요.” “그래, 꼭 보여 줘.” 그는 잔소리하기 좋아하는 대역 배우를 연기하고 있었다. 나는 그게 싫었다. 우리는 각자 한 팔로 기대고 누워서 멀리 보이는 경치를 감상했다.\n#7\n우리는 내 언덕에서 출발해 자전거를 타고 달리며 N을 향해 남쪽으로 가는 관광객용 밴 두 대를 보았다. 정오가 가까워진 게 틀림없었다.\n“우리 이제 말하지 말아요.” 내가 영원히 끝나지 않는 내리막길을 달리며 말했다. 그와 나의 머리카락이 바람에 날렸다. “그런 말 하지 마.” “난 알 수 있어요. 그냥 잡담이나 하겠죠. 시시콜콜한 잡담이나. 그게 전부겠죠. 웃긴 건 그래도 난 살 수 있겠죠.” “방금 운율이 딱딱 맞았어.”\n책 콜 미 바이 유어 네임\n"},{"id":166,"href":"/docs/hobby/book/book25/","title":"잘할 수 있는 일을 찾기 vs 일을 잘하기.","section":"글","content":" 잘할 수 있는 일을 찾기 vs 일을 잘하기. # #2024-12-31\n1 # 많은 부자들은 일하는 것이 취미라고 말한다. 재미있게 즐긴다는 뜻이다. 토마스 J. 스탠리는 〈백만장자 마인드〉에서 미국의 백만장자 733명을 표본 조사하여 얻은 자료들을 보여 주는데 미국의 백만장자들 중 86%는 “나의 성공은 내 일과 직업을 사랑한 결과이다”라고 공통적으로 말한다(투자를 잘해야 부자가 된다는 말에 현혹되지 말라! 일이 우선이고 투자는 나중이다, 이 바보들아). 그리고 81%는 “나의 일은 내 능력과 적성을 한껏 발휘할 수 있도록 해 준다”고 말한다.\n하지만 사람들이 자기 능력과 적성에 맞는 일만을 찾아 나서는 것은 내가 볼 때는 정말 어리석은 일이다. 게다가 대다수의 사람들은 ‘자기가 머릿속에서 꿈꾸고 원하여 온 일’을 그 일을 위한 구체적인 준비도 없이 ‘자신이 해야 하는 일’과 동일시하거나 ‘자기가 능력을 갖고 있는 일’, ‘자기 적성에 맞는 일’, ‘자기가 잘할 수 있는 일’로 믿는다. 그러나 능력이니 적성이니 하는 것들은 관련 분야의 지식을 갖춘 뒤 실제로 일을 경험하여 보기 전까지는 뚜렷하게 나타나는 것이 아니다. 적성 검사 결과를 너무 믿지는 말라는 말이다(나는 학교에서 적성 검사를 받을 때마다 뭐 하나 유달리 적성이 뛰어난 것으로 나온 분야가 전혀 없었다).\n정말 그러냐고? 미국 백만장자들의 경우를 좀 더 살펴보자. 그들이 어느 날 아침 갑자기 일어나 자기 능력과 적성에 맞는 일을 하기 시작한 것은 절대 아니다. 그런 일은 천재들에게나 일어난다. 백만장자들이 일을 택하게 된 동기는 그저 우연한 기회(29%), 시행착오(27%), 예전 직업과의 관련성(12%), 이전 고용주가 놓친 기회(7%) 때문이다. 이 수치는 중고등학교 시절부터 공부를 잘해서 의사나 변호사 같은 전문직업인이 되어 부자가 된 사람들도 포함시킨 것이므로 그들을 제외한다면 거의 대다수의 백만장자들은 어떻게 하다 보니까 그렇게 되었다는 말이며, 어쩌다 하게 된 일이 시발점이 되어 돈을 벌었다는 뜻이다.\n진실은 이것이다. 백만장자들은 ‘어떻게 하다 보니까 하게 된 일’에서 기회를 포착하고 그 일을 사랑하고 즐김으로써 ‘능력과 적성을 한껏 발휘할 수 있는 일’로 바꾸어 버렸던 것이다. 내 말을 믿어라. 마크 피셔Mark Fisher와 마크 앨런Marc Allen의 공저 〈백만장자처럼 생각하라〉에서도 ‘성공하는 사람들은 그들의 일을 사랑한다’고 단언한다.\n2 # 명심해라. 내가 믿고 있는 원칙은 단 하나, 모르면 괴롭고 알면 즐겁다는 것이다.\n학창 시절을 돌이켜 생각하여 보아라. 누구나 자기가 잘하는 과목은 공부에 재미를 느끼지만 잘 못하는 과목은 정말 지겨워한다. 무엇인가를 잘하면 재미를 느끼기 마련이고 잘 못하면 재미고 뭐고 없지 않겠는가. 즉, 재미를 느끼느냐는 것과 잘하느냐 못하느냐 하는 데에는 비례 관계가 있는 것이다. 무엇인가를 잘한다는 것은 그것에 대하여 많이 알고 있기에 가능하며, 잘하니까 재미도 생기는 것이다. 학창 시절에 어떤 과목을 지겨워하였는데 그 과목을 가르치는 선생님이 미남 총각이어서(혹은 예쁜 여선생님이어서) 관심을 쏟아 가며 열심히 하게 되었고 하다 보니 많이 알게 되어 잘하게 되고 잘하게 되니 성적도 잘 나오고 칭찬도 받으니 재미도 많이 느끼고… 이런 경험을 가진 사람들이 실제로 주변에 널려 있지 않은가.\n결국 어떤 일에 대한 재미는 그 일에 대하여 얼마나 관심을 쏟고 관련된 지식을 얼마나 많이 갖고서 경험하는가에 따라 좌우되는 문제이다. 부자들은 초기에 무슨 일을 하든 우선은 그 일의 구조 전체를 파악하는 데 필요한 지식을 흡수하고 경험을 하다 보니, 점점 더 많이 알아 가게 되고 더 많이 알기에 재미도 느끼고 돈도 벌게 되니 즐거움도 배가 된다. 하기 싫은 일이란 것이 적어도 부자가 되는 과정에서는 있을 수 없다는 말이다.\n반면에 대개의 사람들은 일을 사랑하지도 않으며 즐기지도 못한다. 그저 목구멍이 포도청이라서 억지로 한다는 생각을 한다.\n3 # 애당초부터 가까이 가서는 안 될 우물도 있다. 하지만 처음부터 가까이 가서는 안 될 우물이 아니라면 어느 우물이건 그 우물 주인처럼 생각하고 행동하라. 즉, 하고 있는 일이 아무리 엿같이 생각되어도 그 구조체와 흐름을 완전히 파악하여야 하며 거기에 필요한 모든 지식을 스펀지처럼 흡수해 나가야 한다.\n부자가 되려면 이 원칙을 평생 잊지 말라. 사람들은 자기가 잘할 수 있는 일이 따로 있을 것이라고 생각하지만 성격상의 문제나 기술적 분야가 아닌 이상 어느 한 분야의 일에서 새는 바가지는 다른 분야의 일터에서도 새기 마련이며, 어느 한 분야에서 귀신이 되는 사람은 다른 일을 해도 중복되는 부분이 반드시 있기 때문에 남들보다 빠른 시간 안에 귀신이 된다.\n이런 말을 들은 적이 있다. \u0026ldquo;세 번은 질리고 다섯 번은 하기 싫고 일곱 번은 짜증이 나는데 아홉 번째는 재가 잡힌다.\u0026rdquo; 재가 잡힌다는 말은 일에 리듬이 생겨 묘미가 생긴다는 말이다. 즉 피곤을 가져오는 \u0026lsquo;노동\u0026rsquo;이 더 이상 아니고 재미를 느끼게 되는 단계인 \u0026lsquo;일\u0026rsquo;이 된다는 말이다.\n4 # 허드렛일을 싫어하는 사람들은 자존심을 내세운다. 내가 이런 일 하려고 취직한 건 아니라고 하면서 말이다.\n정말 자존심이 세다면 낮은 곳으로 내려가라. 성경에도 낮은 곳으로 내려가라는 말이 나온다. 낮은 곳에서 걸레를 누구보다 먼저 잡고 하찮아 보이는 일들을 즐겁고 기쁜 마음으로 하면서 실수 없이 완벽하게 해치울 때 비로소 사람들은 당신을 인정할 것이다.\n당신의 자존심은 그렇게 주변 사람들이 당신을 스스로 낮출 줄 아는 사람으로 인정할 때 저절로 지켜지게 되는 것이다.\n5 # 주 5일제 근무 좋아하지 마라.\n만약에 말이다. 당신은 다른 사람들 역시 이틀이나 되는 주말을 당신처럼 \u0026lsquo;재충전 내지는 삶의 질 향상\u0026rsquo;이라는 명목으로 쉬면서 보낸다고 생각하지만 사실 그들 중 일부는 자기 계발을 위하여 그 주말의 황금시간을 거의 모두 바치면서 일과 관련된 능력과 지식을 \u0026lsquo;독하게\u0026rsquo; 향상시키고 있다면, 그리고 그런 노력이 2년 정도 지속되면 어떻게 되는지 아는가?\n6 # 내가 말하고자 하는 것은 자기계발은 일찍 하면 일찍 할수록 유리하다는 것이다. 결국 부자가 되는 게임은 먼저 실전 지식을 축적한 사람이 이기게 되어 있기 때문이다.\n한줄 요약 # 성공하려면 자기 일을 알고, 재미를 느끼고, 사랑하면 된다. 허드렛일도 해라. 주 5일제 좋아하지 마라.\n책: 세이노의 가르침\n"},{"id":167,"href":"/docs/hobby/book/book17/","title":"전문가의 세상으로 나가는것에 대한 두려움","section":"글","content":" 전문가의 세상으로 나가는것에 대한 두려움 # #2024-12-31\n1 # 자주 하면, 부담이 줄어든다. 일주일 동안의 결과물이 겨우 한 페이지, 블로그 포스팅 한 건, 스케치 하나라면 당연히 \u0026lsquo;특출나게 잘해야 한다\u0026rsquo;는 생각이 들고 작업물의 질에 대해 조바심을 내게 된다. 반면 매일 쓰면 하루치 정도는 그다지 중요하지 않다. 불안감이 사라진 덕분에 결과적으로 일을 더 즐기게 되고, 새로운 실험을 해 보거나 위험을 기꺼이 감수할 수도 있다. 괜찮은 결과물이 나오지 않아도 시간은 충분하니까 다른 방법을 시도하면 되는 것이다. 하지만 하루하루 시간은 지나는데 아무것도 완성되지 않으면 불안과 절망감이 엄습한다. 일을 미룸으로써 발생한 불안감 때문에 도리어 일에 매진하지 못하는 것은 직업 생활의 씁쓸한 아이러니다.\n2 # 사람마다 효율적인 일상의 모습은 각기 다르다. 자기 능력과 성향에 따른 맞춤식이어야 루틴의 효과가 배가된다. 따라서 위에서 소개한 탄탄한 루틴 형성법을 직접 실험해 보고 어떤 조합이 최고의 성과를 내는 데 가장 좋은지 살펴라. 매일의 스케줄이 단조로운 일상이 아닌 창의적인 의식처럼 느껴지기 시작한다면 효과적인 조합이라고 할 수 있을 것이다.\n3 # 단기적인 실천 습관 역량을 가진 사람들이 장기적으로 실천하는 데 어려움을 겪는 이유는, 십중팔구 두려움 때문입니다. 두려움의 저항력은 상당히 은밀하게 작용하죠. 겉으로 봐서는 흔적이 뚜렷하게 남지 않습니다. 그러나 영화계에 파란을 일으킬 단편 영화는 만들 수 있어도 장편 영화 제작에 필요한 자금을 조성하지 못하는 사람, 여기저기서 규모가 작은 프리랜서 일은 하지만 그 일을 제대로 된 직업으로 전환할 줄은 모르는 사람, 이런 사람들은 일종의 자기 파괴 행위를 하는 셈입니다.\n이처럼 이들이 스스로를 망가뜨리는 이유는, 세상에 나온다는 것은 곧 자신이 하는 일을 속속들이 아는 사람들이 있는 곳으로 나서는 것이기 때문이죠. 세상에 나왔을 때 사기꾼으로 비춰질까 봐 두려운 것이죠. 이사회나 회의장에서, 또는 그저 동료 앞에 서서 “저는 이 일에 대해 잘 압니다. 자, 제 작업을 보시죠. 1년 만에 해낸 일입니다. 멋지지 않나요?”라고 말하는 건 그들에게 정말 어려운 일입니다.\n그 이유는 두 가지인데요. 첫째, 비판에 자신을 내놓는 일이기 때문입니다. 둘째, 자신의 일을 속속들이 아는 사람들의 세상으로 나온다는 건, 앞으로 평생 자신의 일에 능통한 전문가가 되겠다고 선언하는 것이나 다름없기 때문입니다.\n4 # 우리의 역량은 한정돼 있다. 하지만 우리가 에너지를 관리하는 방식에는 변화를 줄 수 있다. 능숙하게 에너지를 관리하면 좀 더 지속적으로, 한층 수준 높게, 그것도 좀 더 짧은 시간에 많은 일을 해낼 수 있다.\n그 방법을 두 가지 중요한 과학 연구 결과에서 찾을 수 있다. 첫째는 ‘수면이 음식 섭취보다 중요하다’는 것이다. 우리는 일주일 동안 아무것도 먹지 않아도 버틸 수 있다. 다만 몸무게를 좀 잃게 될 뿐이다. 하지만 단 이틀이라도 잠을 안 자면? 완전히 망가진다. 이런데도 우리는 1시간의 수면을 너무 쉽게 포기한다. 그 1시간만큼 생산성이 더 올라갈 거라는 잘못된 믿음 때문이다. 실상은 수면이 아주 조금만 부족해도, 우리의 인지 능력은 상당한 피해를 입는다. 매우 짧은 수면을 취하고도 제대로 일을 해낼 수 있다는 일부의 얘기는 보통 미신에 불과하다. 인구 40명당 한 사람, 전체 인구의 2.5퍼센트 미만의 사람만 하루 7~8시간의 수면을 취하고도 충분하다고 느낀다.\n두 번째 중요한 연구 결과는 ‘우리 신체는 주기적 리듬을 따른다’는 것이다. 즉 우리 몸은 90분 주기로, 일을 처리할 수 있는 최고 수준의 역량 한계점에 도달한다. 커피나 설탕에 의존하거나 스트레스 호르몬을 자극해서 90분 이상 자신을 밀어붙일 수는 있지만, 그러면 생리적으로 필요한 휴식과 회복의 시간을 무시하는 셈이다. 결국 그렇게 한계점까지 자신을 밀어붙이면 대가를 치러야 한다.\n5 # 요즘 같은 세상에 고독을 찾는 것은 꼭 필요한 일이다. 고독이 주는 교훈을 배울 수 있고, 오롯이 집중하고 창조할 수 있는 공간을 찾을 수 있으며, 고요 속에서 내면의 목소리에 귀 기울일 수 있기 때문이다.\n하루에 20분~1시간만이라도 고독을 위한 시간을 비워 두면 어마어마한 변화가 찾아온다. 이 시간, 고요한 평온 속에서 우리 마음은 나무 위의 원숭이처럼 활기가 넘치게 된다. 마음에 고요가 찾아오면 무엇이 진짜 중요한지 파악할 수 있고, 매일의 업무와 인터넷 생활의 불협화음 속에서 잃어버렸던 자신만의 창조적 목소리에 다시 귀 기울일 수 있다.\n책: 루틴의 힘\n"},{"id":168,"href":"/docs/hobby/book/book29/","title":"좋은 것들이 기다리고 있다는 약속","section":"글","content":" 좋은 것들이 기다리고 있다는 약속 # #2024-12-31\n#1\n나는 그에게 통쾌하게 반박해줄 말이 있었으면 싶었다. 우리는 중요하다고, 우리는 사실 아주 중요하다고 말해줄 방법. 그러나 주먹이 올라가는 게 느껴지자마자 내 뇌가 주먹을 다시 잡아당겼다. 왜냐하면 당연히, 우리는 중요하지 않기 때문이다. 이것이 우주의 냉엄한 진실이다. 정말 이상한 일이지만, 이 진실을 무시하는 것은 정확히 데이비드 스타 조던과 똑같이 행동하는 것이다.\n#2\n천천히 그것이 초점 속으로 들어왔다. 서로서로 가라앉지 않도록 띄워주는 이 사람들의 작은 그물망이, 이 모든 작은 주고받음-다정하게 흔들어주는 손, 연필로 그린 스케치, 나일론 실에 꿴 플라스틱 구슬들-이 밖에서 보는 사람들에게는 그리 대단치 않은 것일지도 모른다. 하지만 그 그물망이 받쳐주는 사람들에게는 어떨까? 그들에게 그것은 모든 것일 수 있고, 그들을 지구라는 이 행성에 단단히 붙잡아두는 힘 자체일 수도 있다.\n바로 이런 점이 내가 우생학자들에 대해 그토록 격노하는 이유다. 그들은 이런 그물망의 가능성을 상상조차 하지 못한다.\n#3\n별이나 무한의 관점, 완벽함에 대한 우생학적 비전의 관점에서는 한 사람의 생명이 중요하지 않아 보일지도 모른다. 그러나 그것은 무한히 많은 관점 중 단 하나의 관점일 뿐이다. 이것이 바로 다윈이 독자들에게 그토록 열심히 인식시키고자 애썼던 관점이다. 자연에서 생물의 지위를 매기는 단 하나의 방법이란 결코 존재하지 않는다는 것. 하나의 계층구조에 매달리는 것은 더 큰 그림을, 자연의, \u0026ldquo;생명의 전체 조직\u0026quot;의 복잡다단한 진실을 놓치는 일이다. 좋은 과학이 할 일은 우리가 자연에 \u0026ldquo;편리하게\u0026rdquo; 그어 놓은 선들 너머를 보려고 노력하는 것, 당신이 응시하는 모든 생물에게는 당신이 결코 이해하지 못할 복잡성이 있다는 사실을 아는 것이다.\n#4\n분기학자들이 등장하던 시기에 \u0026ldquo;수리분류학\u0026quot;이라는 방법이 유행하고 있었다. 이는 컴퓨터가 그 무지막지한 계산 능력으로 진화적 친연성을 판단해줄 거라는 희망에 기초한 방법이다. 종들을 비교할 때 생각해낼 수 있는 특징들(예를 들어 새들을 비교한다면 부리의 유형, 알의 크기, 깃털 색깔, 척추골의 수, 내장의 길이 등)을 그냥 최대한 많이 입력하면, 컴퓨터가 개연성 있는 관계의 패턴을 뽑아내주는 것이다. 이는 두 종 사이에 비슷한 점이 많을수록 둘이 가까운 관계일 거라는 생각에 기초한 방법이다. 그러나 컴퓨터는 전혀 말이 안되는 관계를 제안할 때도 많았다. 인간의 직관을 완전히 제거했더니\u0026hellip; 혼돈만 남은 것이다.\n그러나 분기학자들은 어떤 특징들이 다른 특징보다 더 유용하다는 사실을 깨달았다. 종들이 거쳐 간 시간의 흐름을 가장 신빙성 있게 보여줄 수 있는 것은 그들이 \u0026ldquo;공통의 진화적 참신함\u0026quot;이라고 부른 특징들, 그러니까 새롭게 추가된 특징들이었다. 이를테면 완전히 새로운 더듬이라든가 반짝이는 노란 지느러미 같은 것들 말이다. 모델에 추가된 참신한 업그레이드가 무엇인지 알아낼 수 있다면, 그 새로운 특징을 따라 생물들이 거쳐 간 다양한 버전을 추적할 수 있고, 시간의 화살이 어느 길을 가리키고 있는지 (좀 더 자신 있게) 추측할 수 있고, 더 큰 확신을 갖고 누가 누구를 낳았는지 단언할 수 있다는 것이다.\n그 발견은 단순했고, 미묘했고, 특출났다. 그리고 시간이 지나며 아주 놀라운 관계들을 드러내기 시작했다. 예를 들어 박쥐는 날개가 달린 설치류처럼 보일지 모르지만 사실은 낙타와 훨씬 더 가깝고, 고래는 실제로 유제류(발굽이 있는 동물로, 사슴이 속한 과)라는 사실이 그렇다.\n#5\n\u0026ldquo;어류\u0026quot;라는 범주가 모든 차이를 가리고 있다. 그 범주는 가까운 사촌들을 우리에게서 멀리 떼어놓음으로써 잘못된 거리 감각을 만들어낸다.\n어류는 존재하지 않는다. \u0026ldquo;어류\u0026quot;라는 범주는 존재하지 않는다. 데이비드에게 너무나도 소중했던 그 생물의 범주, 그가 역경의 시간이 닥쳐올 때마다 의지했던 범주, 그가 명료히 보기 위해 평생을 바쳤던 그 범주는 결코, 단 한 번도 존재한 적이 없었다.\n#6\n반세기 동안 분류학자로 일해온 데이브 스미스는 애매하게 얼버무리는 몇 마디를 뱉어내다가 결국 \u0026ldquo;아마 존재하지 않을 겁니다\u0026quot;라고 인정했다. 시간이 지나면서 자기의 일, 생명의 진정한 상호 연관을 밝혀내는 일을 정말로 할 마음이 있다면, 그들이 하는 말을 부인할 수 없다는 것을 깨달았다. \u0026ldquo;어류\u0026quot;라는 것은 그것을 제대로 직시한다면 사실 틀린 범주라는 것을 말이다. 명료하지 않고 날림으로 만든 이 범주-분류학자들의 용어로는 측계통군-에는 그 구성원들의 일부가 빠져 있다. 나중에 나는 미국자연사박물관의 어류분과 수석 큐레이터인 멜라니 스티아스니에게 전화해 긍게서도 어류라는 범주가 사라졌는지 물었다. 멜라니는 \u0026ldquo;어이쿠\u0026rdquo; 하고 운을 떼더니 \u0026ldquo;널리 그렇게 받아들여지죠\u0026quot;라고 말했다. 당신도 상상할 수 있듯이 무덤덤하게.\n\u0026ldquo;맞아요. 직관에 어긋납니다!\u0026rdquo; 자칭 \u0026ldquo;횡설수설하는 분기학자\u0026quot;인 릭 윈터바텀이 내게 한 말이다. 그는 30년 넘게 학생들에게 실제 자연 세계가 우리가 설정한 범주대로 분류되는 것은 아니라는 사실을 확인시키려 노력해왔다. 그리고 그 관념이 학계 밖으로는 도저히 퍼져나가지 않는 것을 보면서 크게 실망했다. 그는 자기가 대적하기에 너무 센 적수를 상대하고 있는 것 같다고 걱정스러워했다. 그 센 적수는 바로 직관이다. 그는 사람들이 결코 편안함을 진실과 맞바꾸지 않을 것이라고 했다.\n#7\n우주가 데이비드 스타 조던에게서 그가 사랑하는 물고기를 빼앗는 모습을 지켜보면서 느낀 약간의 병적인 만족감을 제외하면, 내게 그것이 중요한 일인가? 조금만 넓은 의미에서 보면, 표본들을 유리단지에 정리하는 것이 직업이 아닌 모든 사람에게, 하나의 범주로서 어류가 존재하지 않는다는 사실이 중요한 일일까?\n헤더는 코페르니쿠스를 예로 들었다. 그 시대 사람들이 하늘을 올려다보면서 움직이고 있는 게 별이 아니라는 걸 받아들이기가 얼마나 어려웠을지 이야기했다. 그럼에도 그에 관해 이야기하고, 그에 관해 생각하고, 별들이 매일 밤 그들 머리 위에서 빙빙 돌고 있는 천구의 천정이라는 생각을 사람들이 서서히 놓아버릴 수 있도록 수고스럽게 복잡한 사고를 하는 것은 중요한 일이라고 말이다. \u0026ldquo;왜냐하면 별들을 포기하면 우주를 얻게 되니까\u0026quot;라고 헤더는 말했다.\n물고기를 포기하면 무슨 일이 일어날까? 나는 전혀 알 수 없었다. 하지만 그순간 하나는 알 수 있었다. 물고기의 반대편에 다른 뭔가가 기다리고 있다는 것. 물고기를 놓아주는 일은 그 결과로 또 다른 어떤 실존적 변화를 불러온다는 것. 그리고 그 결과는 사람에 따라 다 다를 거라는 생각이 들었다.\n#8\n나의 아버지는 \u0026ldquo;어류\u0026quot;라는 단어를 포기하고 싶어 하지 않았다. 과학적으로 정확하지 않다는 건 이해하지만 유용한 단어라고 생각했다. 그 단어를 사용함으로써 세계를 경험하는 제한된 방식에 자신을 가두게 되는 것이 걱정되지 않으냐고 내가 묻자, 아버지는 불만스럽게 끙끙거리는 소리를 내더니 이렇게 말했다. \u0026ldquo;아이고, 그게 뭐든, 아직 내가 해방되기에는 너무 늙었어.\u0026rdquo; 큰언니는 물고기를 놓아버리는 데 아무런 문제도 없었다. 언니는 어류라는 범주 전체를 바로 손에서 놓아버렸다. 왜 언니한테는 그게 그렇게 쉬운 거냐고 묻자 이렇게 말했다. \u0026ldquo;왜냐하면 그게 피할 수 없는 사실이니까. 인간은 원래 곧잘 틀리잖아.\u0026rdquo; 언니는 평생 사람들이 자신에 대해 늘 반복적으로 오해해왔다고 말했다. 의사들에게는 오진을 받고, 급우들과 이웃들, 부모, 나에게서는 오해를 받았다고 말이다. \u0026ldquo;성장한다는 건, 자신에 대한 다른 사람들의 말을 더 이상 믿지 않는 법을 배우는 거야.\u0026rdquo;\n정말로 이 물음은 모든 사람마다 다 다르다.\n#9\n나는 시카고를 떠날 때가 되었다는 것을 알았다. 더이상 나의 연옥에 숨어 있을 수만은 없다는 사실을. 나는 내 인생을 계속 살아가야 했고 혼돈 속으로 다시 들어가 무슨 일이 벌어지는지 지켜봐야 했다.\n#10\n\u0026lsquo;나는 이 사람이 없는 인생은 결코 원하지 않아.\u0026rsquo; 이건 내가 그려왔던 인생이 아니었다. 체격이 아주 작고, 나보다 일곱 살이 어리며, 자전거 경주에서 나를 이기고, 툭하면 나를 향해 어이없다는 듯 눈동자를 굴리는 여자를 쫓아다니는 것은. 그러나 이건 내가 원하는 인생이다. 나는 범주를 부수고 나왔다. 자연이 프린트된 커튼 뒤를 들춰보았다. 있는 그대로의 세상을, 무한한 가능성의 장소를 보았다. 모든 범주는 상상의 산물이다. 그건 세상에서 가장 근사한 느낌이었다.\n#11\n마침내 내가 줄곧 찾고 있었던 것을 얻었다. 하나의 주문과 하나의 속임수, 바로 희망에 대한 처방이다. 나는 좋은 것들이 기다리고 있다는 약속을 얻었다. 내가 그 좋은 것들을 누릴 자격이 있어서가 아니다. 내가 얻으려 노력했기 때문이 아니다. 파괴와 상실과 마찬가지로 좋은 것들 역시 혼돈의 일부이기 때문이다. 죽음의 이면인 삶. 부패의 이면인 성장.\n그 좋은 것들, 그 선물들, 내가 눈을 가늘게 뜨고 황량함을 노려보게 해주고, 그것을 더 명료히 보게 해준 요령을 절대 놓치지 않을 가장 좋은 방법은 자신이 보고 있는 것이 무엇인지 전혀 모른다는 사실을, 매 순간, 인정하는 것이다. 산사태처럼 닥쳐오는 혼돈 속에서 모든 대상을 호기심과 의심으로 검토하는 것이다.\n#요약\n우리가 지어낸 질서를 무너뜨리고 그 짜임을 풀어내는 것이 우리가 해야할 일이다.\n라고 하는데, \u0026lsquo;진실이 아닌 모든 것을 믿지 않기\u0026rsquo; 또한 맹목적으로 느껴짐. 유용하다면 취하기 vs 진실이 아닌 모든 것을 믿지 않기. 이 둘 사이를 왔다갔다\u0026hellip; 물고기를 놓아주는 일이 사람에 따라 다 다르듯이 \u0026lsquo;사실\u0026rsquo;의 중요도는 내게 엄연히 다르다. 어떤 사실에 대한 태도를 둘 사이의 어느 지점에 할당할지는 나만의 기준으로 정하면 되는 것이다.\n책: 물고기는 존재하지 않는다\n"},{"id":169,"href":"/docs/hobby/book/book18/","title":"진전의 가시화","section":"글","content":" 진전의 가시화 # #2024-12-31\n1 # Q. 우리가 시간 관리를 좀 더 잘하기 위해서는 무엇에 집중해야 할까요?\nA. 저는 가장 중요한 요소가 ‘진전의 가시화’라고 생각합니다. 대개의 경우 일이 얼마나 진척됐는지 확인하기가 쉽지 않죠. 그런데 이메일 답장 같은 쉬운 일이라면, 1000통의 이메일에 답장한다고 해도 자신이 답장한 이메일을 한눈에 파악할 수 있습니다. 반면 어려운 문제를 처리할 때는 마치 30시간은 헛되이 보냈고 마지막 30분만 유용했던 것처럼 느껴집니다. 왜냐하면 마지막 30분 동안에 아이디어가 떠올랐기 때문이죠.\n일이 진척된다는 감각은 한눈에 파악되지 않습니다. 그러니 제 생각에 관건은, “어떻게 하면 자신이 발전하고 있다는 것을 느낄 수 있는가?”인 겁니다. 일의 진전 여부를 가시화할 수 있다면 다른 많은 것은 작은 장애물에 지나지 않는다고 생각합니다. 쉽게 말해, 펜으로 적으면서 일을 한다면 자신이 처리한 일의 증거물이 남습니다. 자신이 밟아 온 경로를 볼 수 있는 거죠. 이처럼 발전의 기록이 눈에 보이도록 하는 방법들을 생각할 수 있을 겁니다.\n2 # 어떤 일에서 탁월함의 경지에 오르기 위해서는 궁극적으로 관찰과 정련, 적응과 인내가 요구된다. 저명한 소설가인 무라카미 하루키가 자신의 작품을 완성하기 위해 스스로에게 적용하는 자제력 이야기에 귀 기울여 보기 바란다. 저는 소설 쓰기 모드에 돌입했을 때 새벽 4시에 일어나 5-6시간 동안 작업합니다. 오후에는 10킬로미터 달리기나 1500미터 수영을 한 다음(혹은 두 가지를 모두 한 다음), 책을 읽거나 음악을 감상하지요. 밤 9시에는 잠자리에 들고요. 이런 루틴을 변화 없이 매일 지속합니다. 반복 자체가 중요합니다. 반복은 일종의 최면이니까요. 제 자신의 깊은 내면에 접근하기 위해 스스로에게 최면을 거는 겁니다. 하지만 6개월-1년이라는 긴 시간 동안 이런 반복적 생활을 유지하려면 엄청난 정신력과 체력이 요구되지요. 이런 의미에서 장편 소설을 쓴다는 건 생존 훈련과도 같습니다. 예술적 감성만큼 체력이 절실한 일이지요. 창의적인 사람이 되기 위해서는 가장 혼란스러운 환경 속에서도 집중력을 단련하고 창의적 에너지를 모으는 법을 배워야만 한다.\n3 # 자신이 어떤 분야에 관심을 쏟는지가 그 사람의 정체성을 대변한다.에 20분~1시간만이라도 고독을 위한 시간을 비워 두면 어마어마한 변화가 찾아온다. 이 시간, 고요한 평온 속에서 우리 마음은 나무 위의 원숭이처럼 활기가 넘치게 된다. 마음에 고요가 찾아오면 무엇이 진짜 중요한지 파악할 수 있고, 매일의 업무와 인터넷 생활의 불협화음 속에서 잃어버렸던 자신만의 창조적 목소리에 다시 귀 기울일 수 있다.\n책: 루틴의 힘\n"},{"id":170,"href":"/docs/hobby/book/book6/","title":"책 물고기는 존재하지 않는다","section":"글","content":" 책 물고기는 존재하지 않는다 # #2024-12-31\nhttps://blog.naver.com/afx1979/222154049972?trackingCode=blog_bloghome_searchlist\n이 블로그 글에는 이런 말이 나온다.\n미(학)적으로는 우울이나 자살이 아름다워 보일지 몰라도 진선미가 다 우울의 편을 든다고 해도 나는 분노가 더 낫다고 본다. 분노는 삶에 도움이 되고 삶을 더 좋게 변화시키는 원동력이 되기 때문이다.\n미학적으로는 진실만 받아들이는 것이 온전해보인다. 그런데 진실은 조금 밀어놓고 일단 달리기 시작하는 사람도 있다. 하이젠베르크의 불확정성 원리처럼 둘 다를 챙기는 것은 불가능하고 둘 중 하나는 어쩔 수 없이 포기해야 한다.\n1. 북마크 # ﹂자연은 인간의 사정을 봐주지 않는다 vs 운명의 형태를 만드는 것은 사람의 의지다.\n﹂그릿을 획득하기 vs 진실로의 창을 열어놓기.\n﹂좋은 것들이 기다리고 있다는 약속\n2. 플레이리스트 # 읽으면서 듣기에 딱은 아니지만(집중력 흐려짐) 좋았던 부분 타이핑하면서 듣기엔 딱이다. ㅎㅎ\n"},{"id":171,"href":"/docs/hobby/book/book7/","title":"책 일론 머스크","section":"글","content":" 책 일론 머스크 # #2024-12-31\n똑똑하면서 적당히 착한 마음이 있는 사람은 다 좋다.\n머스크는 태풍이 몰려올 때 가장 강력한 생기를 느끼는 그런 사람 중 한 명이다. “나는 폭풍을 위해 태어났어요. 그러니 고요함은 나에게 적합하지 않지요.” 미국의 7대 대통령 앤드류 잭슨이 한 말이다. 일론 머스크도 마찬가지다. 그는 위기나 데드라인, 할 일의 폭증과 같은 상황에서 번성했다. 복잡하고 난해한 도전에 직면하면, 그로 인한 긴장으로 종종 잠을 이루지 못하거나 심지어 토하기도 했다. 그러나 그런 상황은 그에게 활력도 불어넣었다. “형은 드라마를 끄는 자석과 같아요.” 킴벌이 말한다. “드라마가 그의 강박이자 삶의 주제입니다.”\n1. 북마크 # ﹂위기모드\n﹂다른 행성의 관찰자\n﹂새롭게얻은 부와 충동\n﹂어른들의지휘\n﹂아이러니서클\n﹂생존법\n﹂필승법\n2. 화성 북마크 # 알라딘 중고서점 갔다가 화성 북마크 보여서 ㅎㅎ 이책 생각나서 구매함.\n3. [일론 머스크] 제1원리 사고법: 추정이 아닌 근본적인 문제로의 접근 # https://youtu.be/BWxYWnwi08o?si=dC6veL5s0JWogbbz\nI do think there’s a good framework for thinking. It is physics. You know, the sort of first principles reasoning. Generally I think there are.. what I mean by that is boil things down to their fundamental truths and reason up from there, as opposed to reasoning by analogy. Through most of our life, we get through life by reasoning by analogy, which essentially means copying what other people do with slight variations. And you have to do that. Otherwise, mentally you wouldn’t be able to get through the day. But when you want to do something new, you have to apply the physics approach. Physics is really figuring out how to discover new things that are counterintuitive, like quantum mechanics. It’s really counterintuitive. So I think that’s an important thing to do, and then also to really pay attention to negative feedback, and solicit it, particularly from friends. This may sound like simple advice, but hardly anyone does that, and it’s incredibly helpful.\n글쎄요.. 저는 생각을 할 때 써먹기 좋은 어떠한 틀이 있다고 보는데 바로 물리입니다. 일종의 제1원리 사고법 이라고 할까요. 일반적으로 저는.. 그러니까 이게 무슨 말이냐면, 물질의 근본적인 것까지 파고들어 그로부터 다시 생각해 나가는 것인데요, 유추해 나가는 방식과는 반대되는 개념입니다. 우리 대부분은 인생을 살아가면서 유추한 것을 기반으로 살아가죠. 이는 달리 말해 다른 사람들이 하는 것을 약간의 변화만을 주어 따라 한다는 건데요. 평소에는 그렇게 하는게 맞아요. 그렇지 않으면 정신적으로 하루하루를 버텨내기 힘드실테니까요. 그러나 무언가 새로운 걸 하고자 하신다면 물리학적으로 접근하셔야 합니다. 물리학은 직관에서 벗어나 어떻게 하면 새로운 것을 발견할 수 있을지 생각해 나가는 것인데 양자역학을 예로 들 수 있겠네요. 직관에 전혀 의존하지 않습니다. 그래서 저는 이런 사고방식이 중요하다고 생각하고 또한 부정적인 평가에도 귀를 기울일 줄 아셔야 합니다. 특히 친구들로부터 그런 평가를 해달라고 부탁하세요. 이게 정말 평범한 조언같이 들리시겠죠, 거의 대부분은 무시하시니까요. 정말 도움이 되는데도 말입니다.\nI think it’s also important to reason from first principles, rather than by analogy. So the normal way that we conduct our lives is we reason by analogy. It’s… we’re doing this because it’s like something else that was done, or it’s like what other people are doing. Cause it’s kind of mentally easier to reason by analogy, rather than from first principles. But first principle is kind of a physics way of looking at the world. And what that really means is you kind of boil things down to the most fundamental truths, and say, OK, what are we sure is true? Or sure as possible is true? and then reason up from there. That takes a lot more mental energy. Somebody could say, in fact, people do that battery packs are really expensive and that’s just the way the’ll always be because that’s the way they’ve been in the past. I’m like. Well, No. that’s pretty dumb, you know, because if you apply that reasoning to anything new, then you wouldn’t be able to ever get to that new thing. So, you know, it’s.. like, you can’t say. Oh, you know horses.. nobody wants a car because horses are great, and we’re used to them and they can eat grass, there’s lots of grass all over the place and you know, there’s not like, there’s no gasoline that people can buy. So people will never going to get cars. People did say that. And for batteries, they would say, oh, it’s gonna cost.. You know, historically it’s cost 600 dollars per kilowatt hour, and so.. it’s gonna be much better than that in the future. I would say, no, okay, what are the batteries made of? So with the first principles, we say, ok. What are the material constituents of the batteries? What is the stock-market value of the mateiral constituents? So you can say, ok, it’s got cobalt, nickel, aluminum, carbon, and some polymers for separation, and a seal can. So break that down on a material basis and say, if we bought that on the London Metal Exchange, what would each of those things cost? Oh geez, it’s like $80 per kilowatt hour. So clearlly you just need to think of clever ways to take those materials and combine them into the shape of a better call, and you can have batteries that are much, much cheaper than anyone realizes.\n저는 유추를 하는 것보다는 제1원리에서부터 추론을 시작하는 것이 중요하다고 생각하는데요. 우리는 인생의 계획을 세울 때도 보통 유추를 바탕으로 계획을 수립하곤 하는데요. 그렇게 하는 이유는 지금껏 다른 것들도 다른 방식으로 행해져 왔기 때문이겠죠. 아니면 다른 누군가도 그렇게 해왔으니까요. 유추로부터 추론해 나가는 것은 그게 정신적으로 덜 힘들기 때문이겠죠, 제1원리를 따르는 것보다 말이에요. 제1원리는 물리학적 방식으로 세상을 바라보는 것이거든요. 이게 무슨 말이냐면, 가장 근본적인 논거에 이르기까지 어떠한 문제를 압축해 나가는 건데요, 예를 들어, 우리가 정말 참이라고 확신할 수 있는 것에는 무엇이 있을까? 라는 질문으로부터 추론을 시작해 나가는 겁니다. 이렇게 하면 정신적으로는 더 힘이 들겠지만요. 어떤 분들은.. 사실 정말로 이렇게 말씀들을 많이 하시는데 배터리 팩의 가격은 너무 비싸고 앞으로도 계속 비싸겠지. 과거에도 그래왔으니까. 그럼 저는, 아닌데! 그거 참 멍청한 소리 같은데? 무언가 새로운 것을 만드는 데 그런 식의 추론을 적용한다면 절대로 새로운 무언가를 만들어내지 못할 테니까. 그래서.. 뭐 이런 소리를 하면 곤란하겠죠, 말의 경우에는.. 말이 워낙 훌륭해서 차를 원하는 사람은 없을거야. 우리는 말을 타는 게 익숙하다고, 말은 풀도 뜯어 먹고, 여기 온 사방이 풀로 뒤덮여 있잖아. 지금 이곳을 봐, 사람들이 기름을 어디서 사냐고. 그러니 사람들은 절대로 차를 안 살거야. 진짜로 사람들이 이런 말을 했어요. 그래서 배터리 같은 경우에도 사람들이, 비용이 너무 많이 들어..역사적으로 보면 1킬로와트시(kWh) 당 600 달러가 드는데 미래에 이보다 가격이 더 떨어질 것 같지 않아.. 라고 말하면, 저는 이런 질문을 합니다. 그래? 배터리팩은 뭘로 만들어지지? 그러니까 제1원리로 접근하면 이런거죠. 배터리를 구성하는 물질 성분들은 어떤 것들이 있지? 거래소에서 이 물질들의 가치는 어떻게 형성되어 있지? 그러고 나서 이제, 배터리 팩은 코발트, 니켈, 알루미늄, 카본, 가체 분리용 중합체 그리고 밀봉된 캔으로 구성되는구나. 그럼 이러한 성분의 기저로부터 세부적으로 쪼개 들어가, 이 금속들을 런던금속거래소에서 구매한다면, 각각의 금속들은 얼마 정도 할까? 질문하는 겁니다. 그랬더니 뭐야! 1킬로와트시당 80달러 정도밖에 안 드네! 이와 같이 각각의 물질들을 보다 영리하게 접근해서 이를 배터리의 형태로 결합할 수 있는지 생각해 보는 겁니다. 그러면 그 누구도 생각지 못할 만큼 훨씬 저렴한 배터리도 만들 수 있게 되는 거죠.\n"},{"id":172,"href":"/docs/hobby/movie/movie2/","title":"콜 미 바이 유어 네임","section":"영화","content":" 콜 미 바이 유어 네임 # #2024-12-31\n여름 감성 최고봉 영화! 특히 ost가 너무 좋다.\n1. 플레이리스트 # https://www.youtube.com/watch?v=n50Z3HGj4QE\nSufjan Stevens - Mystery of Love https://www.youtube.com/watch?v=XPPp0Gn45_8\n| 𝐩𝐥𝐚𝐲𝐥𝐢𝐬𝐭 | 𝐬𝐨𝐦𝐞𝐰𝐡𝐞𝐫𝐞 𝐢𝐧 𝐧𝐨r𝐭𝐡𝐞𝐫𝐧 𝐢𝐭𝐚𝐥𝐲 🌳🍃 이건 비슷한 감성을 느끼고 싶을때 듣기 좋은 플리.\n⏶ top\n2. 원작 소설 # 콜 미 바이 유어 네임 - 안드레 애치먼 \u0026raquo;\n얼굴 개연성(..)으로만 설명되었던 빠져드는 계기가 천천히 설명되어서 좋았다. 그리고 올리버의 불안정함이 다른 어른들의 시각에 비추어서 어느정도 더 설명되니까 그것도 좋았다.\n감성은 영화랑 비등비등. 영화-\u0026gt;소설 순으로 감상한 건 운이 좋았다!\ncf) 위 소설은 리마스터판이고 이전 버전은 \u0026lt;그해, 여름 손님\u0026gt;이라는 제목으로 출판되었다.\n"},{"id":173,"href":"/docs/hobby/book/book8/","title":"콜 미 바이 유어 네임 | 안드레 애치먼","section":"글","content":" 콜 미 바이 유어 네임 | 안드레 애치먼 # #북마크\nsummer\n자전거\n"},{"id":174,"href":"/docs/hobby/book/book36/","title":"필승법","section":"글","content":" 필승법 # #2024-12-31\n#1\n2002년 1월의 어느 일요일, 창고를 빌려 그 아마추어 엔진의 제작에 열중하던 중 가비가 뮬러에게 일론 머스크라는 인터넷 백만장자가 그를 만나고 싶어 한다고 말했다. 머스크가 저스틴과 함께 도착했을 때, 뮬러는 줄에 매단 80파운드짜리 엔진을 어깨로 떠받친 채 프레임에 고정하기 위해 볼트를 조이고 있었다. 머스크는 다짜고짜 그에게 질문을 퍼붓기 시작했다. \u0026ldquo;그게 추력은 얼마나 되나요?\u0026rdquo; 뮬러는 1만 3,000파운드라고 답했다. \u0026ldquo;더 큰 것도 만들어본 적이 있나요?\u0026rdquo; 뮬러는 얼마 전부터 TRW에서 65만 파운드의 추력을 가진 TR-106의 제작에 참여하고 있다고 설명했다. \u0026ldquo;추진 연료로는 무엇을 쓰나요?\u0026rdquo; 머스크가 또 물었다. 뮬러는 머스크의 속사포 질문에 집중하기 위해 마침내 볼트 결합 작업을 일시 중단했다.\n머스크는 뮬러에게 TRW의 TR-106만큼 큰 엔진을 혼자서 만들 수 있는지 물었다. 뮬러는 자신이 인젝터와 점화기를 직접 설계했고, 펌프 시스템을 잘 알고 있으며, 나머지는 팀과 함께 해결할 수 있다고 답했다. 머스크는 물었다. \u0026ldquo;비용이 얼마나 들까요?\u0026rdquo; 뮬러는 TRW가 1,200만 달러를 들여 그것을 제작하고 있다고 답했다. 머스크는 방금 전에 던진 질문을 재차 반복했다. \u0026ldquo;비용이 얼마나 들까요?\u0026rdquo; \u0026ldquo;오, 이런, 그거 참 답하기 어려운 문제이긴 합니다.\u0026rdquo; 대화가 너무 빨리 구체적인 사안으로 진행되어서 속으로 놀라고 있던 뮬러 역시 그 부분은 재고해볼 필요가 있다고 판단했다.\n그때 긴 가죽 코트를 걸치고 있던 저스틴이 머스크를 쿡 찌르며 이제 갈 시간이 되었다고 말을 건넸다. 머스크는 뮬러에게 다음 일요일에 만날 수 있는지 물었다. 뮬러는 주저했다. \u0026ldquo;마침 슈퍼볼 일요일이었고, 나는 와이드스크린 TV를 막 구입했기에 친구들과 함께 경기를 보고 싶었어요.\u0026rdquo; 하지만 그는 거부해봤자 소용이 없을 것 같은 느낌이 들었고, 그래서 찾아오겠다는 머스크의 제안을 받아들였다.\n\u0026ldquo;우리가 발사체 제작에 대해 얼마나 몰두해서 이야기를 나누었던지 마치 한 편의 연극을 보는 것 같았지요.\u0026rdquo; 뮬러의 기억이다. 그들은 그 자리에서 다른 엔지니어 몇 명과 함께 최초의 스페이스X 로켓에 대한 계획을 계략적으로 세우기까지 했다. 발사체의 1단은 액체산소와 등유를 사용하는 엔진으로 추진하기로 결정했다. \u0026ldquo;제가 그 작업을 쉽게 할 수 있는 방법을 알고 있습니다.\u0026rdquo; 뮬러가 말했다. 머스크는 상단에는 과산화수소를 사용하자고 제안했지만, 뮬러는 그것을 다루기 어려울 것이라고 생각했다. 그래서 사산화질소를 제안했지만, 머스크는 그것이 너무 비싸다고 생각했다. 결국 두 사람은 2단에도 액체산소와 등유를 사용하기로 합의했다. 슈퍼볼은 잊혔다. 로켓이 더 흥미로웠다.\n#2\n뮬러는 스페이스X의 첫 번째 주요 영입자가 되었다. 뮬러가 고집한 한 가지 조건은 머스크가 그의 2년 치 보수를 조건부 날인 증서로 보장해주는 것이었다. 그는 인터넷 백만장자가 아니었기에 벤처가 실패할 경우 보수를 받지 못하게 될 가능성을 감수하고 싶지 않았다. 머스크는 동의했다. 하지만 이 일로 머스크는 뮬러를 스페이스X의 공동창업자가 아닌 직원으로 여기게 되었다. 이것은 머스크가 페이팔 시절에도 중요하게 여겼고, 테슬라를 창업하면서도 마찬가지로 중시할 투자와 관련된 문제였다. 그는 회사에 투자할 의사가 없다면 창업자 자격이 없다고 생각했다. \u0026ldquo;2년치 월급을 조건부 날인 증서로 예치해달라면서 자신을 공동창업자라고 생각해서는 안되는 거지요.\u0026rdquo; 머스크는 말한다. \u0026ldquo;공동창업자가 되려면 영감과 땀, 리스크가 어느 정도 조합이 되어야 하는 겁니다.\u0026rdquo;\n#3\n공장을 설계할 때 머스크는 디자인과 엔지니어링, 제조 팀이 모두 함께 모여 있어야 한다는 자신의 철학을 따랐다. \u0026ldquo;조립라인에 있는 사람들이 즉각적으로 디자이너나 엔지니어를 붙잡아 세우고 \u0026lsquo;대체 왜 이런 식으로 만든 거요?\u0026lsquo;라고 따질 수 있어야 하는 거예요.\u0026rdquo; 머스크가 뮬러에게 설명했다. \u0026ldquo;가스레인지 위에 자기 손을 올려 놓으면 뜨거워지자마자 바로 떼어내지만, 다른 사람의 손이 올라가 있으면 무언가 조치를 하는 데 시간이 더 오래 걸리기 마련이지요.\u0026rdquo;\n팀이 성장함에 따라 머스크는 자신의 리스크에 대한 내성과 의도적인 현실 왜곡 논리를 자신의 팀에도 불어넣었다. \u0026ldquo;부정적으로 생각하거나 무언가를 할 수 없다는 태도를 보이면 다음 회의에 초대받지 못했지요.\u0026rdquo; 뮬러의 회상이다. \u0026ldquo;그는 그저 어떻게든 일을 해낼 사람들을 원했어요.\u0026rdquo; 이는 사람들이 불가능하다고 생각하는 일을 해내도록 유도하는 좋은 방법이었다. 하지만 그것은 나쁜 소식을 전하거나 결정에 의문을 제기하길 두려워하는 사람들에게 둘러싸이기에도 좋은 방법이었다.\n#4 (광적인 긴박감을 유지하라)\n멀린 엔진을 개발할 때, 뮬러는 버전 중 하나를 완성하기 위해 공격적인 일정을 제시했다. 하지만 머스크가 보기엔 충분히 공격적이지 않았다. \u0026ldquo;도대체 왜 이렇게 오래 걸리는 거요? 이건 말도 안 돼. 반으로 줄이세요.\u0026rdquo;\n뮬러는 난색을 표했다. \u0026ldquo;이미 반으로 줄인 일정을 그렇게 다시 반으로 줄일 수는 없습니다.\u0026rdquo; 머스크는 그를 차갑게 쳐다보며 회의가 끝난 뒤에 남으라고 말했다. 둘만 남았을 때 그는 뮬러에게 계쏙 엔진 책임자로 남고 싶은지 물었다. 뮬러가 그렇다고 대답하자 머스크는 \u0026ldquo;그럼 내가 뭔가를 요구하면, 염병할, 그냥 그렇게 해주시오\u0026quot;라고 했다.\n뮬러는 이에 동의하고 임의로 일정을 반으로 줄였다. \u0026ldquo;그리고 어떻게 됐을까요?\u0026rdquo; 뮬러가 물었다. \u0026ldquo;결국 원래 일정에 잡혀 있던 시간을 거의 다 들인 후에야 완성이 되었지요.\u0026rdquo; 머스크의 미친 스케쥴은 때대로 불가능을 가능으로 만들기도 했지만, 매번 그러지는 못했다. 뮬러는 말한다. \u0026ldquo;머스크에게는 절대 안 된다고 말하면 안 된다는 것을 배웠지요. 그냥 해보겠다고 말하고 나중에 잘 안 되면 그 이유를 설명하면 되는 겁니다.\u0026rdquo; (이거 우리 교수님이자나..)\n#5 (실패를 통해 배워라)\n머스크는 설계에 반복적 접근방식을 취했다. 로켓과 엔진의 프로토타입을 빠르게 만들어 테스트하고, 날려버리고, 수정하고, 다시 시도하는 식으로 마침내 제대로 된 게 나올 때까지 반복했다. 빠르게 움직이고, 날려버리고, 반복하라! 뮬러는 말한다. “중요한 것은 문제를 얼마나 잘 피하느냐가 아니거든요. 어떤 문제가 있는지 얼마나 빨리 파악해서 해결하느냐가 진정으로 중요한 겁니다.”\n예를 들면, 새로운 버전의 엔진을 여러 다양한 조건에서 몇 시간 동안 시험 발사해야 하는지에 대한 일련의 국방규격 표준이 있었다. “지루하기 짝이 없는데다가 비용도 많이 드는 접근방식이었지요.” 팀 부자의 설명이다. “일론은 그저 엔진 하나를 만들어서 테스트 스탠드에서 불을 붙여보라고 했어요. 그래서 작동하면 로켓에 장착해 날려보자는 거였지요.” 스페이스X는 민간기업이었고, 머스크는 기꺼이 규칙을 어기는 성향이었기에 그렇게 원하는 대로 리스크를 감수할 수 있었다. 부자와 뮬러는 엔진이 고장 날 때까지 밀어붙여 한계가 어디까지인지 파악하곤 했다. 반복적 설계에 대한 이러한 신념은 곧 스페이스X에 언제든 이용할 수 있는 자유로운 테스트 장소가 필요하다는 것을 의미했다.\n물론 항상 성공하는 것은 아니었다. 머스크는 2003년 말 엔진 연소실 내부의 열 확산 소재에 균열이 발생했을 때도 마찬가지로 색다른 접근방식을 시도했다. “처음에 하나, 이어서 또 하나, 또 하나, 그렇게 우리가 만든 최초의 연소실 세 개에 균열이 생겼어요.” 뮬러의 회상이다. “말 그대로 재앙이었지요.”\n나쁜 소식을 듣자 머스크는 뮬러에게 고칠 방법을 찾으라고 지시했다. “그냥 버릴 수는 없어요.” 뮬러는 “고칠 방법이 없습니다”라고 대답했다.\n머스크를 격분하게 만드는 종류의 발언이었다. 그는 뮬러에게 비행기를 보낼 테니 그 세 개의 연소실을 싣고 로스앤젤레스의 스페이스X 공장으로 날아오라고 지시했다. 그의 아이디어는 에폭시 접착제를 균열에 스며들도록 도포해 문제를 해결하자는 것이었다. 뮬러가 말도 안 되는 미친 아이디어라고 말했고, 둘 사이에는 고성이 오갔다. 그러다 마침내 뮬러가 물러섰다. 그는 팀원들에게 말했다. “그가 결정권자니까.”\n연소실이 공장에 도착했을 때 머스크는 마침 크리스마스 파티에 참석하기로 되어 있던 터라 고급 가죽 부츠를 신고 있었다. 그는 파티에 가지 못했다. 대신 그는 밤새 에폭시 도포 작업을 도왔다. 멋진 부츠가 엉망이 되도록.\n도박은 실패로 돌아갔다. 압력을 가하자마자 에폭시가 떨어져나갔다. 연소실을 다시 설계해야 했고 발사 일정은 4개월 뒤로 미뤄졌다. 하지만 혁신적인 아이디어를 추구하며 기꺼이 공장에서 밤을 새는 머스크를 보면서 엔지니어들은 두려움 없이 색다른 해결책을 시도해볼 수 있다는 생각에 고무되었다.\n그렇게 패턴이 형성되었다. 새로운 아이디어를 시도하고 기꺼이 날려버려라.\n책: 일론 머스크\n"},{"id":175,"href":"/docs/study/bioinformatics/cs12/","title":"혼자 공부하는 딥러닝 | ANN","section":"생물정보학","content":" [딥러닝] 혼자 공부하는 딥러닝 | ANN # 목록 # 2024-12-31 ⋯ 17. 간단한 인공 신경망 모델 만들기\n2024-12-31 ⋯ 18. 인공 신경망에 층을 추가하여 심층 신경망 만들어 보기\n2024-12-31 ⋯ 19. 인경 신경망 모델 훈련의 모범 사례 학습하기\n17. 간단한 인공 신경망 모델 만들기 # 데이터 준비 fashion_mnist 데이터셋에서 학습과 테스트용 이미지 데이터를 가져온다. 학습 데이터는 60,000개의 28x28 픽셀 이미지, 테스트 데이터는 10,000개의 28x28 픽셀 이미지. train_target과 test_target은 각 이미지에 해당하는 레이블(0~9)을 갖고있다.\nfrom tensorflow import keras (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() print(train_input.shape, train_target.shape) print(test_input.shape, test_target.shape) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 29515/29515 [==============================] - 0s 3us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26421880/26421880 [==============================] - 2s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 5148/5148 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4422102/4422102 [==============================] - 0s 0us/step (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) 데이터 시각화 첫 10개의 이미지 샘플을 출력하기.\nimport numpy as np import matplotlib.pyplot as plt fig, axs = plt.subplots(1, 10, figsize=(10,10)) for i in range(10): axs[i].imshow(train_input[i], cmap=\u0026#39;gray_r\u0026#39;) axs[i].axis(\u0026#39;off\u0026#39;) plt.show() print([train_target[i] for i in range(10)]) print(np.unique(train_target, return_counts=True)) [9, 0, 0, 3, 0, 2, 7, 2, 5, 5] (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) np.unique()로 레이블 분포를 확인해보니 각 클래스에 6,000개씩 균일하게 분포해있다.\n로지스틱 회귀 이미지를 0~255의 픽셀 값을 [0, 1] 범위로 정규화한다. 그리고 데이터를 2D 배열로 펼친다. (60000, 28, 28) → (60000, 784). 즉 각 이미지를 784차원 벡터로 변환한다.\nfrom sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_validate train_scaled = train_input / 255.0 train_scaled = train_scaled.reshape(-1, 28*28) print(train_scaled.shape) 로지스틱 회귀모델을 학습한다. 손실함수는 로지스틱 손실함수를 사용한다.\nsc = SGDClassifier(loss=\u0026#39;log\u0026#39;, max_iter=5, random_state=42) scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1) print(np.mean(scores[\u0026#39;test_score\u0026#39;])) (60000, 784) 0.8195666666666668 학습 결과 테스트 세트 정확도는 81.96%이다.\n케라스 신경망 모델 생성 from sklearn.model_selection import train_test_split train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) print(train_scaled.shape, train_target.shape) print(val_scaled.shape, val_target.shape) (38400, 784) (38400,) (9600, 784) (9600,) 학습 데이터를 학습 세트와 검증 세트로 나눴다. 학습 세트는 (38400, 784) 검증 세트는 (9600, 784).\ndense = keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;, input_shape=(784,)) model = keras.Sequential(dense) 2025-01-23 17:30:40.924465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2025-01-23 17:30:40.934329: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Dense Layer는 각 입력 뉴런이 모든 출력 뉴런에 연결되는 신경망의 기본 층이다. Dense(10)으로 10개의 뉴런을 가지는 층을 만들어줬다. 입력 데이터는 784차원 벡터이고, 활성화 함수는 softmax 함수가 사용되었다. keras.Sequential(dense)는 하나의 Dense 층으로 이루어진 간단한 순차 모델을 정의한다.\n다시 말해, Dense Layer는 784차원 입력을 10개 클래스의 출력으로 변환하며, 각 출력은 Softmax를 통해 확률로 계산된다\n모델 컴파일 model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=\u0026#39;accuracy\u0026#39;) print(train_target[:10]) /data1/home/ysh980101/miniconda3/envs/workspace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:165: FutureWarning: The loss \u0026#39;log\u0026#39; was deprecated in v1.1 and will be removed in version 1.3. Use `loss=\u0026#39;log_loss\u0026#39;` which is equivalent. warnings.warn( /data1/home/ysh980101/miniconda3/envs/workspace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:704: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. [9 4 9 0 4 9 3 6 4 7] 모델을 학습하기 전에 손실 함수, optimizer, 평가 지표(metric)을 설정. 모델이 학습 과정에서 어떻게 성능을 평가하고 손실을 줄이고 가중치를 업데이트할지 정의한다.\n손실 함수 (Loss Function)는 모델의 예측값과 정답 사이의 차이를 측정함. sparse_categorical_crossentropy는 레이블이 정수 형태로 제공되는 경우 즉 다중 클래스 분류 문제(Multi-class Classification)에 사용된다. (원 핫 인코딩 아니라)\n모델의 출력값은 softmax 활성화 함수를 통해 각 클래스에 대한 확률 분포를 반환하는데 손실 함수는 정답 클래스와 예측된 확률 분포 간의 교차 엔트로피(Cross Entropy)를 계산한다.\nLoss = $- \\sum_{i=1}^C y_i \\cdot \\log(\\hat{y}_i)$\n$y_i$은정답 레이블의 원-핫 인코딩 값 (sparse일 경우 해당 위치만 1), $\\hat{y}_i$: 모델의 예측 확률값, $C$: 클래스의 총 개수이다. 확률값이 정답 클래스에 가까울수록 손실이 작아진다. 모델의 전체 동작 흐름\n모델은 마지막 Dense 층에서 softmax를 사용해 10개의 클래스 확률을 출력 손실 함수는 정답 레이블(예: 2)과 예측 확률(0.7)의 차이를 교차 엔트로피로 계산. 예측 클래스(가장 높은 확률을 가진 클래스)가 정답 레이블과 일치하면 평가 지표 accuracy 즉 모델이 정확하게 예측한 비율이 높아진다. 손실 값이 최소화되도록 가중치(모델 파라미터)가 옵티마이저에 의해 업데이트된다. 모델 훈련 model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1200/1200 [==============================] - 3s 2ms/step - loss: 0.6326 - accuracy: 0.7853 Epoch 2/5 1200/1200 [==============================] - 3s 3ms/step - loss: 0.4910 - accuracy: 0.8344 Epoch 3/5 1200/1200 [==============================] - 3s 3ms/step - loss: 0.4656 - accuracy: 0.8444 Epoch 4/5 1200/1200 [==============================] - 3s 3ms/step - loss: 0.4512 - accuracy: 0.8499 Epoch 5/5 1200/1200 [==============================] - 3s 3ms/step - loss: 0.4417 - accuracy: 0.8526 \u0026lt;keras.callbacks.History at 0x7fa1e4c12be0\u0026gt; 학습 반복(Epoch)은 5회로 설정되었다. 각 Epoch 결과 손실은 0.6326 → 0.4417로 감소, 정확도(accuracy)는 78.5% → 85.3%로 증가했다. 모델 평가 model.evaluate(val_scaled, val_target) 300/300 [==============================] - 1s 3ms/step - loss: 0.4335 - accuracy: 0.8590 [0.4334854781627655, 0.8589583039283752] 검증 데이터에서 모델 평가 결과 손실은 0.4335, 정확도는 85.9%.\n손실 값(0.4335)은 모델의 예측이 검증 데이터에서 큰 오류를 범하지 않았음을 보여주고 정확도(85.9%)**는 모델이 Fashion MNIST 데이터셋에서 상당히 높은 성능을 보였으며, 의류 이미지를 잘 분류할 수 있음을 나타낸다.\n손실과 정확도는 상관관계가 있지만 동일하지 않음. 손실은 모델의 예측이 얼마나 잘 정답 분포를 따르는지(확률 수준)를 나타내며, 확률이 높은 정답일수록 손실 값이 낮아진다.\n정확도는 모델이 정답을 맞췄는지 여부(0 또는 1)를 측정한다. 손실이 감소해도 정확도는 일정 범위에서 정체될 수 있다. 이는 모델이 정답 분포를 더 잘 학습했지만, 예측 결과가 다른 클래스에 대한 잘못된 선택으로 여전히 분류 문제를 일으킬 수 있기 때문.\n사이킷런-케라스 비교 로지스틱 회귀(SGDClassifier): 정확도 약 81.96%. 단순한 선형 모델. 케라스 신경망 모델: 정확도 약 85.9%. 더 높은 성능을 보였으며, 신경망의 유연성 덕분에 복잡한 데이터를 잘 학습했다. 요약 데이터 준비 → 정규화 → 펼침. 간단한 신경망 모델(1개 층, 10개 뉴런) 설계. 로지스틱 회귀와 비교해 신경망이 더 나은 성능을 보였다. 강의 링크\nhttps://www.youtube.com/watch?v=ZiP9erf5Fo0\u0026list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX\u0026index=17 18. 인공 신경망에 층을 추가하여 심층 신경망 만들기 # 데이터 가져오기 from tensorflow import keras (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() print(train_input.shape, train_target.shape) print(test_input.shape, test_target.shape) from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_validate train_scaled = train_input / 255.0 print(train_scaled.shape) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) (60000, 28, 28) 심층 신경망 dense1 = keras.layers.Dense(100, activation=\u0026#39;sigmoid\u0026#39;, input_shape=(784,)) dense2 = keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) model = keras.Sequential([dense1, dense2]) cf) 층을 추가하는 다른 방법\nmodel = keras.Sequential([ keras.layers.Dense(100, activation=\u0026#39;sigmoid\u0026#39;, input_shape=(784,), name=\u0026#39;hidden\u0026#39;), keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;, name=\u0026#39;output\u0026#39;) ], name = \u0026#39;패션 MNIST 모델\u0026#39;) model = keras.Sequential() model.add(keras.layers.Dense(100, activation=\u0026#39;sigmoid\u0026#39;, input_shape=(784,))) model.add(keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) 렐루 함수와 Flatten 층 model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28,28))) model.add(keras.layers.Dense(100, activation=\u0026#39;relu\u0026#39;)) model.add(keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) model.summary() Model: \u0026#34;sequential_2\u0026#34; Model: \u0026#34;sequential_3\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_5 (Dense) (None, 100) 78500 dense_6 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 옵티마이저 model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=\u0026#39;accuracy\u0026#39;) sgd = keras.optimizers.SGD() model.compile(optimizer=sgd, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=\u0026#39;accuracy\u0026#39;) sgd = keras.optimizers.SGD(learning_rate=0.1) sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True) model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28,28))) model.add(keras.layers.Dense(100, activation=\u0026#39;relu\u0026#39;)) model.add(keras.layers.Dense(100, activation=\u0026#39;softmax\u0026#39;)) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=\u0026#39;accuracy\u0026#39;) model.fit(train_scaled, train_target, epochs=5) model.evaluate(val_scaled, val_target) Epoch 1/5 1500/1500 [==============================] - 59s 39ms/step - loss: 0.8167 - accuracy: 0.7495 Epoch 2/5 1500/1500 [==============================] - 39s 26ms/step - loss: 0.4167 - accuracy: 0.8520 Epoch 3/5 1500/1500 [==============================] - 32s 21ms/step - loss: 0.3710 - accuracy: 0.8665 Epoch 4/5 1500/1500 [==============================] - 33s 22ms/step - loss: 0.3345 - accuracy: 0.8790 Epoch 5/5 1500/1500 [==============================] - 51s 34ms/step - loss: 0.3218 - accuracy: 0.8816 375/375 [==============================] - 4s 11ms/step - loss: 0.3423 - accuracy: 0.8785 [0.34229394793510437, 0.8784999847412109] 강의 링크\nhttps://www.youtube.com/watch?v=JskWW5MlzOg\u0026list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX\u0026index=18 19. 인경 신경망 모델 훈련의 모범 사례 학습하기 # 손실 곡선 model.compile(loss=\u0026#34;sparse_categorical_crossentropy\u0026#34;, metrics=\u0026#34;accuracy\u0026#34;) history = model.fit(train_scaled, train_target, epochs=5, verbose=0) print(history.history.keys()) dict_keys([\u0026#39;loss\u0026#39;,\u0026#39;accuracy\u0026#39;]) plt.plot(history.history[\u0026#39;loss\u0026#39;]) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.ylabel(\u0026#39;epoch\u0026#39;) plt.show() plt.plot(history.history[\u0026#39;accuracy\u0026#39;]) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.ylabel(\u0026#39;accuracy\u0026#39;) plt.show() cf) 더 많은 에포크?\nhistory = model.fit(train_scaled, train_target, epochs=20, verbose=0) plt.plot(history.history[\u0026#39;loss\u0026#39;]) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.show() 검증 손실 history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target)) print(history.history.keys()) plt.plot(history.history[\u0026#39;loss\u0026#39;]) plt.plot(history.history[\u0026#39;val_loss\u0026#39;]) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.legend([\u0026#39;train\u0026#39;,\u0026#39;val\u0026#39;]) plt.show() 드롭아웃 model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28,28))) model.add(keras.layers.Dense(100, activation=\u0026#39;relu\u0026#39;)) model.add(keras.layers.Dropout(0.3)) model.add(keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) model.summary() 모델 저장과 복원 model.save_weights(\u0026#39;model-weights.h5\u0026#39;) model.load_weights(\u0026#39;model-weights.h5\u0026#39;) model.save(\u0026#39;model-whole.h5\u0026#39;) model = keras.models.load_model(\u0026#39;model-whole.h5\u0026#39;) val_labels = np.argmax(model.predict(val_scaled), axis=-1) print(np.mean(val_labels == val_target)) 콜백 checkpoint_cb = keras.callbacks.ModelCheckpoint(\u0026#39;best-model.h5\u0026#39;) model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb]) model = keras.models.load_model(\u0026#39;best-model.h5\u0026#39;) 조기종료 checkpoint_cb = keras.callbacks.ModelCheckpoint(\u0026#39;best-model.h5\u0026#39;) early_stopping_cb = keras.callbecks.EarlyStopping(patience=2, restore_best_weights=True) history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) print(early_stopping_cb.stopped_epoch) plt.plot(history.history[\u0026#39;loss\u0026#39;]) plt.plot(history.history[\u0026#39;val_loss\u0026#39;]) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.legend([\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]) plt.show() 강의 링크\nhttps://www.youtube.com/watch?v=2by0Fz3XC84\u0026list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX\u0026index=19 (여기 코드 왤케 오류 많이나지 ㅠㅠ\u0026hellip;)\n"},{"id":176,"href":"/docs/hobby/daily/daily13/","title":"카페 라파테","section":"일상","content":" 카페 라파테 # #2024-10-12\n\u0026lsquo;범어역 브런치카페\u0026rsquo;의 정석 같은 카페 ㅎㅋ\n빵이 엄청 맛있다!! 이런건 아닌데 정석적인 브런치카페감성을 느끼고 싶을때 가기 좋은곳이다 ㅋㅋ\n브리또 / 베이글 / 에그타르트는 무난했구 소금빵은 요즘 워낙 버터많이넣은 소금빵집이 많아서 풍미가 적은느낌이있지만 갠적으로 다른집들이 너무 많이 넣는다고 생각해서 ㅠ 여기가 오히려 정통같아서 좋았당\n"},{"id":177,"href":"/docs/hobby/baking/baking11/","title":"크림치즈스콘","section":"베이킹","content":" 크림치즈스콘 # #2024-09-07\n식탁일기 크림치즈스콘 레시피가 이뻐보여서 시작한 크림치즈스콘\n비주얼 노릇노릇 넘 이쁘구 맛도 너무맛있다 ㅎㅎ\n첫판에서 반죽이 좀 퍼진거같애서 냉장을 더시켜서 구워줬더니 미친비주얼이.. 너무 맛있게 생겨서 웃김 ㅋㅋㅋ\n같은 판 아님 여기저기 선물한다고 엄청구웠다\n굽기전엔 좀 애매한가? 싶어도\n굽고나면 마싯는 비주얼이 된다.\n선물용으로 엄청 구운 모습\n아빠가 식빵구운거랑 같이 추석선물로 포장ㅎㅎㅋㅋ 며칠동안 집이 빵공장이었다.\n"},{"id":178,"href":"/docs/hobby/baking/baking10/","title":"주말아침의 대파치즈스콘","section":"베이킹","content":" 주말아침의 대파치즈스콘 # #2024-08-24\n여느 주말아침,, 고요비 유튜브 보다가 갑자기 삘받아서 대파치즈스콘 만들었다 ㅋㅋ 레시피는 자도르 콘치즈 스콘 레시피에서 콘 빼고 파 넣었음.\n생각보다 너무너무 맛있게 나와서 행복 ㅎㅎㅎ 특히 아빠가 넘맛있다구 해줬당\n"},{"id":179,"href":"/docs/hobby/baking/baking8/","title":"포카치아","section":"베이킹","content":" 포카치아 # #2024-08-15\n발효빵 중에서도 수공이 꽤많이들어가는편인 포카치아..!! 발효도 16시간정도 엄청 오래 시켜야하구 발효중에도 한번씩 반죽접기 해줘야돼서 해볼까말까 고민했는데, 신경쓸게 많다고 생각하니까 오히려 도전욕구가 자극되었다. ㅎ\n레시피는 자도르 포카치아 레시피에서 변형 없이 그대로 해줬다!\n토마토 정갈하게 썰린게 예뻐서 찍음 ㅎㅎ\n토마토랑 올리브오일 로즈마리로 데코하기. 굽기전인데 벌써 이쁘다\n조금 남아서 시식용도 만듦 ㅋㅋ\n결과물!! 이정도면 성공이라고본다 ㅎㅎㅎ\n단면샷을 안찍어놨는데 구멍이 엄청많진않았지만 포카치아에서 중요한 쫄깃바삭 속성은 충분했구 엄청 맛있게 먹었다 ㅎㅎ\n"},{"id":180,"href":"/docs/hobby/baking/baking9/","title":"황치즈 비스코티","section":"베이킹","content":" 황치즈 비스코티 # #2024-08-15\n올드패션 황치즈 비스코티 레시피 보고 넘 예쁘고 맛있어보여서 만들어봤다 ㅎㅎ\n벽돌아님\u0026hellip; 반죽임\n간단해보였는데 얘도 은근 손이 많이간다. 벽돌상태로 1차 굽기 해준담에 쿠키두께되게 썰어서 펼쳐주고 2차굽기 -\u0026gt; 뒤집에서 3차굽기 해줘야함.\n결과물 ㅎㅎ 생긴건 유튜브보단 투박한데 맛이 진짜 미쳤다. 파마산치즈가루가 신의 한수인듯.\n유명한 베이킹 유튜브들이 많지만 이사람 레시피가 전체적으로 찐인듯거같다. 왜그렇게 느껴지나 생각해봤는데 ㅋㅋ 대부분 유튜브가 본인 기술력으로 쇼부보는데 이사람은 맛있을수밖에 없는 특정 재료를 넣어서 맛을 강화함. 그래서 나같은 초짜가 만들어도 웬만하면 마싯게 출력되는거같다. ㅋㅋ\n"},{"id":181,"href":"/docs/hobby/baking/baking7/","title":"소금빵","section":"베이킹","content":" 소금빵 # #2024-08-11\n식탁일기 소금빵 레시피 따라구운 소금빵!! 근데 버터롤빵때랑 마찬가지로 발효시간이 좀 부족했던거같음.\n나름 소세지 끼운 문어소금빵으로 바리에이션도 줬다. ㅋㅋ\n"},{"id":182,"href":"/docs/hobby/baking/baking6/","title":"버터롤빵","section":"베이킹","content":" 버터롤빵 # #2024-08-08\n구움과자 아니라 발효가 들어가는 빵은 처음 구워봤다!! (그래서 실패할까봐 엄청 조금 굽기..)\n레시피는 식탁일기 버터롤빵 레시피대로 했다.\n반죽성형 해줌\n칼집내서 굽기. 근데 칼집 넘깊게내서 결은 엄청많은데 모양은 좀 깨진거같다 ㅋㅋ\n그리구 무엇보다 촉촉한 느낌보다는 좀 딱딱한느낌이었는데 구운시간의 문제보다는 발효가 부족했던듯. 그래두 맛있게먹었다 ㅎㅎ\n"},{"id":183,"href":"/docs/hobby/baking/baking5/","title":"레몬 쿠키","section":"베이킹","content":" 레몬 쿠키 # #2024-08-07\n색다른 쿠키를 만들고싶어서 레몬 쿠키 도전!\n요 레시피를 따라하긴했는데 내가 쓴 밀가루가 문제인지 정량대로 넣으니까 너무 묽어져서 ㅠㅠ 밀가루 훨씬더넣고 근데 연해져서 레몬제스트 넣고 슈가파우더 넣고\u0026hellip; 점도 산미 단맛 3개만 맞추자 하고 맘대로 커스텀해버려서 재현은 불가능한 쿠키가 됐다.\n노릇노릇 기여운 결과물 ㅎㅎ 레몬쿠키는 요런 클래식한 쿠키커터가 잘 어울리는둣.\n맛도 엄청맛있었다! 근데 다시 만들려면 레시피를 재창조해야대서 아쉽다 ㅠㅠ\n"},{"id":184,"href":"/docs/hobby/baking/baking4/","title":"통밀쿠키 / 빼곰스튜디오 쿠키커터","section":"베이킹","content":" 통밀쿠키 / 빼곰스튜디오 쿠키커터 # #2024-08-04\n빼곰스튜디오랑 치치공작소에서 쿠키커터를 엄청 쇼핑했는데 첫개시하기!!\n레시피는 실패없는 자도르 통밀 쿠키 레시피대로 했다.\n노릇노릇\n굽고나니깐 약간 흐려져서 슬픔 ㅠㅠ\n딸기펜으로 점찍어주면 귀여움 오백배!!\n"},{"id":185,"href":"/docs/hobby/baking/baking3/","title":"휘낭시에","section":"베이킹","content":" 휘낭시에 # #2024-08-04\n조빵이 레시피대로 만든 휘낭시에!!\n첫트라서 웬만하면 그대로 갈려고 했는데 ㅠ 인간적으로 버터랑 설탕이 너무많이들어가서 버터는 정량 / 설탕은 절반 넣었는데 그래도 단것같은 기분 ㅋㅋ ㅠㅠ\n맛은 맛있었지만 휘낭시에의 빠쟉함은 설탕량에서 나오는게 일부 있는거같다. 건강한 맛 바라면 안대는 메뉴니깐 휘낭시에는 그냥 사먹는걸로\u0026hellip;ㅋ\n"},{"id":186,"href":"/docs/hobby/baking/baking2/","title":"무품곰 (무화과 품은 곰) 쿠키","section":"베이킹","content":" 무품곰 (무화과 품은 곰) 쿠키 # #2024-07-30\n보통 아품곰(아몬드 품은 쿠키) 만드는 쿠키틀이지만 아몬드가 없어서 무화과를 넣어보았다. 레전드 귀여움..! ㅋㅋ\n쿠키 레시피는 그냥 자도르 통밀 쿠키 레시피 배합대로 했는데 반죽 문제라기보다는 모양 흐트러질까봐 좀 두껍게 구웠더니 좀 덜 바삭한 쿠키가 되었다.\n그래두 모양이 귀여우니깐 ㅎㅎ 만족\n"},{"id":187,"href":"/docs/hobby/baking/baking1/","title":"홈메이드 그래놀라","section":"베이킹","content":" 홈메이드 그래놀라 # #2024-07-13\n집에 있는 재료 이것저것 넣고 구웠는데 생각보다 너무 맛있었던..!\n재료는 오트밀/호두/아몬드/해바라기씨/크랜베리/꿀 넣었다\n레시피는 자도르 유튜브 봤긴 한데 \u0026lsquo;노릇하게 굽고-\u0026gt;섞어주고-\u0026gt;다시 굽고 반복\u0026rsquo;이라는 개념만 가져가구 나머진 그냥 내 오븐에 맞춰서 했다.\n요건 굽기 전 버전.\n크랜베리 대신 건포도 버전. 근데 크랜베리 넣은게 훨 맛있다.\n"},{"id":188,"href":"/docs/study/bioinformatics/bi20/","title":"한국과학기술원 기간제 근로자(연수연구원) 모집","section":"생물정보학","content":" 한국과학기술원 기간제 근로자(연수연구원) 모집 # 연구 \u0026gt; 연수연구원/IT융합연구소\n연구 \u0026gt; 연수연구원/강화학습(정보전자연구소G)\n연구 \u0026gt; 연수연구원/생명과학(생명과학연구소G)\n연구 \u0026gt; 연수연구원/시스템생물학(정보전자연구소C)\n연구 \u0026gt; 연수연구원/신경과학,인지과학,뇌과학 및뇌공학(생명과학연구소A)\n공고 바로가기\n"},{"id":189,"href":"/docs/hobby/daily/blog39/","title":"오타루☃️","section":"일상","content":" 오타루☃️ # #2025-02-28\n"}]