---
date : 2025-09-10
tags: ['2025-09']
categories: ['AI']
bookHidden: true
title: "ML #1"
---

# ML #1

#2025-09-10

---

### 1

```python
# !pip install numpy
# !pip install pandas
# !pip install seaborn
# !pip install matplotlib
# !pip install -U scikit-learn
# !pip install xgboost
# !pip install lightgbm

import warnings

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    r2_score,
    mean_squared_error,
    root_mean_squared_error,
    mean_absolute_percentage_error,
)

warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", 100)
pd.set_option("float_format", "{:.4f}".format)
sns.set_style("whitegrid")

RANDOM_STATE = 42
```

```python
# 1. Data Definition
_data = load_diabetes()
print(_data.DESCR)
```
```plain text
.. _diabetes_dataset:

Diabetes dataset
----------------

Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline.

**Data Set Characteristics:**

:Number of Instances: 442

:Number of Attributes: First 10 columns are numeric predictive values

:Target: Column 11 is a quantitative measure of disease progression one year after baseline

:Attribute Information:
    - age     age in years
    - sex
    - bmi     body mass index
    - bp      average blood pressure
    - s1      tc, total serum cholesterol
    - s2      ldl, low-density lipoproteins
    - s3      hdl, high-density lipoproteins
    - s4      tch, total cholesterol / HDL
    - s5      ltg, possibly log of serum triglycerides level
    - s6      glu, blood sugar level

Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).

Source URL:
https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html

For more information see:
Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) "Least Angle Regression," Annals of Statistics (with discussion), 407-499.
(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
```
```python
data = _data["data"]
feature_names = _data["feature_names"]

df = pd.DataFrame(data, columns=feature_names)
df["target"] = _data["target"]

df.head()
```

```python
# 2. EDA
# Correlation
plt.figure(figsize=(10, 8))

sns.heatmap(
    data=df.drop("target", axis=1).corr(),
    annot=True,
    cmap="coolwarm",
)

plt.show()
```
```python
# 4. Machine Learning Regression
# Dataset Definition
X = df.drop("target", axis=1)
y = df["target"]

# ë¨¼ì € train+validì™€ testë¡œ ë¶„í•  (80:20)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# train+validë¥¼ ë‹¤ì‹œ trainê³¼ validë¡œ ë¶„í•  (75:25)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE
)

print(f"ë°ì´í„° ë¶„í•  ê²°ê³¼:")
print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Valid: {len(X_valid)} ({len(X_valid)/len(X)*100:.1f}%)")
print(f"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
```
```plain text
ë°ì´í„° ë¶„í•  ê²°ê³¼:
Train: 264 (59.7%)
Valid: 89 (20.1%)
Test: 89 (20.1%)
```
```python
# Model (Vanila)
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(random_state=RANDOM_STATE)
model.fit(X_train, y_train)
model.score(X_train, y_train)
print(model.intercept_, model.coef_)
```
```plain text
1.0
```
```python
# Feature Importance
model.feature_importances_
feature_names
_feature_importances = pd.Series(
    model.feature_importances_,
    index=feature_names,
)

_feature_importances
```
```plain text
array([0.08981708, 0.00592253, 0.43861624, 0.08517564, 0.04463861,
       0.04418316, 0.05650334, 0.05949104, 0.13289137, 0.04276099])
['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']
age   0.0898
sex   0.0059
bmi   0.4386
bp    0.0852
s1    0.0446
s2    0.0442
s3    0.0565
s4    0.0595
s5    0.1329
s6    0.0428
dtype: float64
```
```python
_feature_importances.nlargest().plot(kind="barh")
```
```python
# Prediction
# ê° ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡
y_train_pred = model.predict(X_train)
y_valid_pred = model.predict(X_valid)
y_test_pred = model.predict(X_test)

# ê° ì…‹ì˜ ì„±ëŠ¥ í‰ê°€
def calculate_metrics(y_true, y_pred, set_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = root_mean_squared_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    
    return {
        'Set': set_name,
        'RÂ²': r2,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': mape
    }

# ëª¨ë“  ì…‹ì˜ ì„±ëŠ¥ ê³„ì‚°
train_metrics = calculate_metrics(y_train, y_train_pred, 'Train')
valid_metrics = calculate_metrics(y_valid, y_valid_pred, 'Valid')
test_metrics = calculate_metrics(y_test, y_test_pred, 'Test')

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
results_df = pd.DataFrame([train_metrics, valid_metrics, test_metrics])
results_df = results_df.set_index('Set')

print("=== Train/Valid/Test ì…‹ ì„±ëŠ¥ ë¹„êµ ===")
print(results_df.round(4))
```
```plain text
=== Train/Valid/Test ì…‹ ì„±ëŠ¥ ë¹„êµ ===
           RÂ²       MSE    RMSE   MAPE
Set                                   
Train  1.0000    0.0000  0.0000 0.0000
Valid  0.2095 4330.7978 65.8088 0.3744
Test  -0.2659 6706.9101 81.8957 0.5430
```
```python
# Metrics
# ê³¼ì í•© ë¶„ì„
print(f"\n=== ê³¼ì í•© ë¶„ì„ ===")
train_valid_r2_diff = train_metrics['RÂ²'] - valid_metrics['RÂ²']
valid_test_r2_diff = valid_metrics['RÂ²'] - test_metrics['RÂ²']

print(f"Train-Valid RÂ² ì°¨ì´: {train_valid_r2_diff:.4f}")
print(f"Valid-Test RÂ² ì°¨ì´: {valid_test_r2_diff:.4f}")
```
```plain text

=== ê³¼ì í•© ë¶„ì„ ===
Train-Valid RÂ² ì°¨ì´: 0.7905
Valid-Test RÂ² ì°¨ì´: 0.4754
```

> - ì„ê³„ê°’ 0.1 
>   - ì•„ë˜ ì½”ë“œì—ì„œ 0.1ë¡œ ë¹„êµí•˜ëŠ” ê²ƒì€ ê³ ì • ê·œì¹™ì´ ì•„ë‹ˆë¼ ê²½í—˜ì  ê°€ì´ë“œ ì…ë‹ˆë‹¤ 
>   - ëª¨ë¸/ë°ì´í„°ë³„ ì°¨ì´ê°€ ìˆì–´ ë™ì¼ ê¸°ì¤€ìœ¼ë¡œ ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ 
>   - í•´ë‹¹ íŒŒì¼ì—ì„œëŠ” ìš°ì„ ì ìœ¼ë¡œ ê²½í—˜ì  ê°€ì´ë“œë¥¼ ì ìš©í•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤ 
>     
> - ê°ê´€ì  íŒë‹¨ì„ ìœ„í•´ì„œëŠ” "ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’" í†µí•œ ë¹„êµ í•„ìš” => (ëª¨ë¸ ìµœì í™” ê³¼ì •ì—ì„œ í™•ì¸) 
>   - k-foldì—ì„œ val R^2ì˜ í‘œì¤€í¸ì°¨ Ïƒ_valì„ êµ¬í•´
>   - ê³¼ì í•© : (train_mean - val_mean) > max(0.05, 2*Ïƒ_val)
>   - ì¼ë°˜í™” ë¬¸ì œ : |test - val_mean| > 2*Ïƒ_val

```python
if train_valid_r2_diff > 0.1:
    print("âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.")
else:
    print("âœ… ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ì´ ìœ ì‚¬í•©ë‹ˆë‹¤.")

if valid_test_r2_diff > 0.1:
    print("âš ï¸  ì¼ë°˜í™” ë¬¸ì œ: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.")
else:
    print("âœ… ì¼ë°˜í™” ì„±ëŠ¥ ì–‘í˜¸: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ì´ ìœ ì‚¬í•©ë‹ˆë‹¤.")
```
```plain text
âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.
âš ï¸  ì¼ë°˜í™” ë¬¸ì œ: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.
```
```python
# ëª¨ë¸ ì„¤ëª…ë ¥ í•´ì„
print(f"\n=== ëª¨ë¸ ì„¤ëª…ë ¥ í•´ì„ ===")
print(f"í›ˆë ¨ ë°ì´í„° RÂ²: {train_metrics['RÂ²']:.4f} ({train_metrics['RÂ²']*100:.2f}%)")
print(f"ê²€ì¦ ë°ì´í„° RÂ²: {valid_metrics['RÂ²']:.4f} ({valid_metrics['RÂ²']*100:.2f}%)")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° RÂ²: {test_metrics['RÂ²']:.4f} ({test_metrics['RÂ²']*100:.2f}%)")
print(f"â†’ ìµœì¢… í…ŒìŠ¤íŠ¸ì—ì„œ ëª¨ë¸ì´ {test_metrics['RÂ²']*100:.1f}%ì˜ ë¶„ì‚°ì„ ì„¤ëª…í•©ë‹ˆë‹¤.")
```
```plain text
=== ëª¨ë¸ ì„¤ëª…ë ¥ í•´ì„ ===
í›ˆë ¨ ë°ì´í„° RÂ²: 1.0000 (100.00%)
ê²€ì¦ ë°ì´í„° RÂ²: 0.2095 (20.95%)
í…ŒìŠ¤íŠ¸ ë°ì´í„° RÂ²: -0.2659 (-26.59%)
â†’ ìµœì¢… í…ŒìŠ¤íŠ¸ì—ì„œ ëª¨ë¸ì´ -26.6%ì˜ ë¶„ì‚°ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
```
```python
# ì˜¤ì°¨ ì •í™•ë„ í•´ì„
print(f"\n=== ì˜¤ì°¨ ì •í™•ë„ í•´ì„ ===")
print(f"Train RMSE: {train_metrics['RMSE']:.2f}")
print(f"Valid RMSE: {valid_metrics['RMSE']:.2f}")
print(f"Test RMSE: {test_metrics['RMSE']:.2f}")
print(f"Test MAPE: {test_metrics['MAPE']:.4f} ({test_metrics['MAPE']*100:.2f}%)")
print(f"â†’ ìµœì¢… í…ŒìŠ¤íŠ¸ì—ì„œ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ì— ëŒ€í•´ í‰ê· ì ìœ¼ë¡œ Â±{test_metrics['MAPE']*100:.1f}% ì •ë„ì˜ ì˜¤ì°¨ë¥¼ ë³´ì…ë‹ˆë‹¤.")
```
```plain text
=== ì˜¤ì°¨ ì •í™•ë„ í•´ì„ ===
Train RMSE: 0.00
Valid RMSE: 65.81
Test RMSE: 81.90
Test MAPE: 0.5430 (54.30%)
â†’ ìµœì¢… í…ŒìŠ¤íŠ¸ì—ì„œ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ì— ëŒ€í•´ í‰ê· ì ìœ¼ë¡œ Â±54.3% ì •ë„ì˜ ì˜¤ì°¨ë¥¼ ë³´ì…ë‹ˆë‹¤.
```
```python
# ì‹œê°í™”: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ë¹„êµ
plt.figure(figsize=(15, 5))

# í›ˆë ¨ ë°ì´í„°
plt.subplot(1, 3, 1)
plt.scatter(y_train, y_train_pred, alpha=0.6, color="blue")
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], "r--", lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title(f"Train (RÂ² = {train_metrics['RÂ²']:.4f})")

# ê²€ì¦ ë°ì´í„°
plt.subplot(1, 3, 2)
plt.scatter(y_valid, y_valid_pred, alpha=0.6, color="green")
plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], "r--", lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title(f"Valid (RÂ² = {valid_metrics['RÂ²']:.4f})")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
plt.subplot(1, 3, 3)
plt.scatter(y_test, y_test_pred, alpha=0.6, color="red")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title(f"Test (RÂ² = {test_metrics['RÂ²']:.4f})")

plt.tight_layout()
plt.show()
```
```python
# ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸
metrics_to_plot = ['RÂ²', 'RMSE', 'MAPE']
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, metric in enumerate(metrics_to_plot):
    values = [train_metrics[metric], valid_metrics[metric], test_metrics[metric]]
    sets = ['Train', 'Valid', 'Test']
    
    bars = axes[i].bar(sets, values, color=['blue', 'green', 'red'], alpha=0.7)
    axes[i].set_title(f'{metric}')
    axes[i].set_ylabel(metric)
    
    # ê°’ í‘œì‹œ
    for bar, value in zip(bars, values):
        height = bar.get_height()
        axes[i].text(bar.get_x() + bar.get_width()/2., height,
                    f'{value:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()
```
```python
# actual vs predicted
plt.figure(figsize=(10, 5))

plt.plot(y_test.values, label="Actual", marker="o", color="blue")
plt.plot(y_test_pred, label="Predicted", marker="x", color="red")

plt.title("Actual vs Predicted Values")
plt.ylabel("Disease Progression")
plt.legend()
plt.show()
```

```python
# Tree Structure
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 12))
plot_tree(model, filled=True)
plt.show()
```
```python
plt.figure(figsize=(10, 7))

plot_tree(
    model,
    filled=True,
    max_depth=1,
    feature_names=feature_names,
)

plt.show()
```
```python
# Retraining (feat.Prunning)
models = {
    "DecisionTree": DecisionTreeRegressor(
        max_depth=3,          # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
        min_samples_split=5,  # ë…¸íŠ¸ ë¶„í•  ì‹œ, ìµœì†Œí•œ 5ê°œì˜ ìƒ˜í”Œì´ ìˆì–´ì•¼ í•¨
        min_samples_leaf=5,   # ë¦¬í”„ ë…¸ë“œì—ëŠ” ìµœì†Œí•œ 5ê°œì˜ ìƒ˜í”Œë¦¬ ìˆì–´ì•¼ í•¨
        random_state=RANDOM_STATE,
    )
}
model.fit(X_train, y_train)
model.score(X_train, y_train)
```
```plain text
1.0
```
```python
plt.figure(figsize=(15, 10))

plot_tree(
    model,
    filled=True,
    max_depth=2,  # ì‹œê°í™” ìœ„í•´ ì œí•œ
    feature_names=feature_names,
)

plt.show()
```
```python
# Feature Importance
_feature_importances = pd.Series(
    model.feature_importances_,
    index=feature_names,
)

_feature_importances
```
```plain text
age   0.0898
sex   0.0059
bmi   0.4386
bp    0.0852
s1    0.0446
s2    0.0442
s3    0.0565
s4    0.0595
s5    0.1329
s6    0.0428
dtype: float64
```
```python
_feature_importances.nlargest().plot(kind="barh")
```
```python
# Prediction
# ê° ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡
y_train_pred = model.predict(X_train)
y_valid_pred = model.predict(X_valid)
y_test_pred = model.predict(X_test)

# ëª¨ë“  ì…‹ì˜ ì„±ëŠ¥ ê³„ì‚°
train_metrics = calculate_metrics(y_train, y_train_pred, 'Train')
valid_metrics = calculate_metrics(y_valid, y_valid_pred, 'Valid')
test_metrics = calculate_metrics(y_test, y_test_pred, 'Test')

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
results_df = pd.DataFrame([train_metrics, valid_metrics, test_metrics])
results_df = results_df.set_index('Set')

print("=== Train/Valid/Test ì…‹ ì„±ëŠ¥ ë¹„êµ ===")
print(results_df.round(4))
```
```plain text
=== Train/Valid/Test ì…‹ ì„±ëŠ¥ ë¹„êµ ===
           RÂ²       MSE    RMSE   MAPE
Set                                   
Train  1.0000    0.0000  0.0000 0.0000
Valid  0.2095 4330.7978 65.8088 0.3744
Test  -0.2659 6706.9101 81.8957 0.5430
```
```python
# ê³¼ì í•© ë¶„ì„
print(f"\n=== ê³¼ì í•© ë¶„ì„ ===")
train_valid_r2_diff = train_metrics['RÂ²'] - valid_metrics['RÂ²']
valid_test_r2_diff = valid_metrics['RÂ²'] - test_metrics['RÂ²']

print(f"Train-Valid RÂ² ì°¨ì´: {train_valid_r2_diff:.4f}")
print(f"Valid-Test RÂ² ì°¨ì´: {valid_test_r2_diff:.4f}")

if train_valid_r2_diff > 0.1:
    print("âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.")
else:
    print("âœ… ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ì´ ìœ ì‚¬í•©ë‹ˆë‹¤.")

if valid_test_r2_diff > 0.1:
    print("âš ï¸  ì¼ë°˜í™” ë¬¸ì œ: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.")
else:
    print("âœ… ì¼ë°˜í™” ì„±ëŠ¥ ì–‘í˜¸: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ì´ ìœ ì‚¬í•©ë‹ˆë‹¤.")
```
```plain text
=== ê³¼ì í•© ë¶„ì„ ===
Train-Valid RÂ² ì°¨ì´: 0.7905
Valid-Test RÂ² ì°¨ì´: 0.4754
âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.
âš ï¸  ì¼ë°˜í™” ë¬¸ì œ: ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤.
```

###

### 2. ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ í™•ì¸

> (ëŒ€ìƒ) Decision Tree, randomForest, Lasso, Ridge, XGB, LGBM
> 1. ìœ„ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ê°€ì¥ ìµœì ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ í™•ì¸ 
>     - í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” cross validationì€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ  
> 2. ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ feature importance í™•ì¸ 
> 3. ìµœì ì˜ ëª¨ë¸ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ ì„±ëŠ¥ í™•ì¸ (RMSE)

```python
models = {
    "DecisionTree": DecisionTreeRegressor(
        max_depth=4,              # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ ì œí•œìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
        min_samples_split=10,     # ë…¸ë“œ ë¶„í•  ì‹œ ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”
        min_samples_leaf=5,       # ë¦¬í”„ ë…¸ë“œì—ëŠ” ìµœì†Œ 5ê°œ ìƒ˜í”Œ í•„ìš”
        random_state=RANDOM_STATE,
    ),
    "RandomForest": RandomForestRegressor(
        n_estimators=100,         # 100ê°œì˜ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ë¡œ ì•™ìƒë¸” êµ¬ì„±
        max_depth=8,              # ê°œë³„ íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ 
        min_samples_split=5,      # ë…¸ë“œ ë¶„í•  ì‹œ ìµœì†Œ 5ê°œ ìƒ˜í”Œ í•„ìš”
        min_samples_leaf=2,       # ë¦¬í”„ ë…¸ë“œì—ëŠ” ìµœì†Œ 2ê°œ ìƒ˜í”Œ í•„ìš”
        random_state=RANDOM_STATE,
    ),
    "Lasso": Lasso(
        alpha=0.01,               # ì •ê·œí™” ê°•ë„ (ì‘ì„ìˆ˜ë¡ ëœ ì •ê·œí™”)
        random_state=RANDOM_STATE,
    ),
    "Ridge": Ridge(
        alpha=0.1,                # ì •ê·œí™” ê°•ë„ (ì‘ì„ìˆ˜ë¡ ëœ ì •ê·œí™”)
        random_state=RANDOM_STATE,
    ),
    "XGBRegressor": XGBRegressor(
        n_estimators=100,         # 100ê°œì˜ ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ
        max_depth=6,              # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
        learning_rate=0.1,        # í•™ìŠµë¥  (ì‘ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼)
        subsample=0.8,            # ìƒ˜í”Œë§ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)
        colsample_bytree=0.8,     # íŠ¹ì„± ìƒ˜í”Œë§ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€) 
        random_state=RANDOM_STATE,
    ),
    "LGBMRegressor": LGBMRegressor(
        n_estimators=100,         # 100ê°œì˜ ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ
        max_depth=6,              # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
        learning_rate=0.1,        # í•™ìŠµë¥ 
        subsample=0.8,            # ìƒ˜í”Œë§ ë¹„ìœ¨
        colsample_bytree=0.8,     # íŠ¹ì„± ìƒ˜í”Œë§ ë¹„ìœ¨
        random_state=RANDOM_STATE,
        force_col_wise=True,      # ê²½ê³  ë©”ì‹œì§€ ë°©ì§€
    ),
}
```
```python
# ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ train, valid, test ì…‹ì—ì„œ í‰ê°€
results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    
    y_train_pred = model.predict(X_train)
    y_valid_pred = model.predict(X_valid)
    y_test_pred  = model.predict(X_test)
    
    # ì„±ëŠ¥ ê¸°ë¡
    results[name] = {
        "Train_R2": r2_score(y_train, y_train_pred),
        "Valid_R2": r2_score(y_valid, y_valid_pred),
        "Test_R2":  r2_score(y_test, y_test_pred),
        "Train_RMSE": root_mean_squared_error(y_train, y_train_pred),
        "Valid_RMSE": root_mean_squared_error(y_valid, y_valid_pred),
        "Test_RMSE":  root_mean_squared_error(y_test, y_test_pred),
    }

results_df = pd.DataFrame(results).T
print("=== ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (RÂ² & RMSE) ===")
display(results_df.round(4))
```
```plain text
=== ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (RÂ² & RMSE) ===
	Train_R2	Valid_R2	Test_R2	Train_RMSE	Valid_RMSE	Test_RMSE
DecisionTree	0.6137	0.3196	0.3113	49.0068	61.0539	60.4061
RandomForest	0.8542	0.4615	0.4318	30.1061	54.3170	54.8663
Lasso	0.5185	0.5229	0.4514	54.7135	51.1284	53.9115
Ridge	0.5105	0.5201	0.4558	55.1620	51.2744	53.6941
XGBRegressor	0.9978	0.4334	0.3810	3.7360	55.7150	57.2664
LGBMRegressor	0.8889	0.3987	0.4168	26.2792	57.3986	55.5853
```

```python
# Metrics
# ëª¨ë“  ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
print("\n=== í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ì¤€ ëª¨ë¸ ë¹„êµ (RÂ² & RMSE) ===")
print(results_df[["Test_R2", "Test_RMSE"]].sort_values(by="Test_R2", ascending=False))
```
```plain text
=== í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ì¤€ ëª¨ë¸ ë¹„êµ (RÂ² & RMSE) ===
               Test_R2  Test_RMSE
Ridge           0.4558    53.6941
Lasso           0.4514    53.9115
RandomForest    0.4318    54.8663
LGBMRegressor   0.4168    55.5853
XGBRegressor    0.3810    57.2664
DecisionTree    0.3113    60.4061
```

```python
# Best Model
# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ
best_model_name = results_df["Test_R2"].idxmax()
best_model = models[best_model_name]

print(f"\nâœ… Best Model: {best_model_name}")
print(f"Test RÂ² : {results_df.loc[best_model_name, 'Test_R2']:.4f}")
print(f"Test RMSE : {results_df.loc[best_model_name, 'Test_RMSE']:.4f}")
```
```plain text
âœ… Best Model: Ridge
Test RÂ² : 0.4558
Test RMSE : 53.6941
```


```python
# Feature Importance
# Best Modelì˜ feature importance ë¶„ì„

if hasattr(best_model, "feature_importances_"):  # íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸
    feature_importances = pd.Series(
        best_model.feature_importances_,
        index=feature_names
    ).sort_values(ascending=False)
    
    print("\n=== Feature Importance ===")
    display(feature_importances)
    
    feature_importances.plot(kind="barh", figsize=(8, 6), title=f"{best_model_name} Feature Importance")
    plt.show()

elif hasattr(best_model, "coef_"):  # ì„ í˜• ëª¨ë¸ (Ridge, Lasso)
    feature_importances = pd.Series(
        best_model.coef_,
        index=feature_names
    ).sort_values(key=abs, ascending=False)  # ì ˆëŒ“ê°’ ê¸°ì¤€ ì •ë ¬
    
    print("\n=== Coefficients (ì ˆëŒ“ê°’ ê¸°ì¤€ ì¤‘ìš”ë„) ===")
    display(feature_importances)
    
    feature_importances.plot(kind="barh", figsize=(8, 6), title=f"{best_model_name} Coefficients (Importance)")
    plt.show()
```
```plain text
=== Coefficients (ì ˆëŒ“ê°’ ê¸°ì¤€ ì¤‘ìš”ë„) ===
bmi    486.3793
s5     403.8977
bp     281.9268
sex   -201.4770
s3    -185.6912
s4     161.8174
s2    -125.0469
s6     111.1299
s1    -103.1486
age     39.9838
dtype: float64
```

```python
# Prediction
# Best Modelë¡œ ì˜ˆì¸¡ ìˆ˜í–‰

# ë°ì´í„°ì…‹ë³„ ì˜ˆì¸¡
y_train_pred = best_model.predict(X_train)
y_valid_pred = best_model.predict(X_valid)
y_test_pred  = best_model.predict(X_test)

# ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜ ì¬ì‚¬ìš©
def calculate_metrics(y_true, y_pred, set_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = root_mean_squared_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    
    return {
        "Set": set_name,
        "RÂ²": r2,
        "MSE": mse,
        "RMSE": rmse,
        "MAPE": mape,
    }

# ëª¨ë“  ì…‹ ì„±ëŠ¥ ê³„ì‚°
train_metrics = calculate_metrics(y_train, y_train_pred, "Train")
valid_metrics = calculate_metrics(y_valid, y_valid_pred, "Valid")
test_metrics  = calculate_metrics(y_test, y_test_pred, "Test")

# ê²°ê³¼ DataFrame ì •ë¦¬
prediction_results = pd.DataFrame([train_metrics, valid_metrics, test_metrics]).set_index("Set")

print("\n=== Best Model (Ridge) ì„±ëŠ¥ ë¹„êµ ===")
display(prediction_results.round(4))
```
```plain text
=== Best Model (Ridge) ì„±ëŠ¥ ë¹„êµ ===
RÂ²	MSE	RMSE	MAPE
Set				
Train	0.5105	3042.8483	55.1620	0.4105
Valid	0.5201	2629.0688	51.2744	0.3042
Test	0.4558	2883.0570	53.6941	0.3663
```

```python
# Best Modelë¡œ ì„±ëŠ¥ ê³„ì‚°
print("\n=== ìµœì  ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===")
print(f"Best Model : {best_model_name}")
print(f"Train RÂ² : {train_metrics['RÂ²']:.4f}, RMSE : {train_metrics['RMSE']:.4f}, MAPE : {train_metrics['MAPE']*100:.2f}%")
print(f"Valid RÂ² : {valid_metrics['RÂ²']:.4f}, RMSE : {valid_metrics['RMSE']:.4f}, MAPE : {valid_metrics['MAPE']*100:.2f}%")
print(f"Test  RÂ² : {test_metrics['RÂ²']:.4f}, RMSE : {test_metrics['RMSE']:.4f}, MAPE : {test_metrics['MAPE']*100:.2f}%")

print(f"\nğŸ‘‰ ìµœì¢… Test ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ {best_model_name} ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
```
```plain text
=== ìµœì  ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===
Best Model : Ridge
Train RÂ² : 0.5105, RMSE : 55.1620, MAPE : 41.05%
Valid RÂ² : 0.5201, RMSE : 51.2744, MAPE : 30.42%
Test  RÂ² : 0.4558, RMSE : 53.6941, MAPE : 36.63%

ğŸ‘‰ ìµœì¢… Test ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ Ridge ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.
```


```python
# actual vs predicted visualization
plt.figure(figsize=(15, 5))

# 1. ì‚°ì ë„: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_test_pred, alpha=0.6, color="red", edgecolor="k")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "b--", lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title(f"{best_model_name} - Actual vs Predicted (Test)\nRÂ² = {test_metrics['RÂ²']:.4f}, RMSE = {test_metrics['RMSE']:.2f}")

# 2. ë¼ì¸ ê·¸ë˜í”„: ì‹œê³„ì—´ ìˆœì„œ ê¸°ì¤€ ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ
plt.subplot(1, 2, 2)
plt.plot(y_test.values, label="Actual", marker="o", color="blue", alpha=0.7)
plt.plot(y_test_pred, label="Predicted", marker="x", color="red", alpha=0.7)
plt.title(f"{best_model_name} - Actual vs Predicted (Test)")
plt.ylabel("Disease Progression")
plt.legend()

plt.tight_layout()
plt.show()
```