---
date : 2024-12-31
tags: ['2024-12']
categories: ['deep learning']
bookHidden: true
title: "구글 BERT의 정석 | BERT 입문"
bookComments: true
---

# [딥러닝] 구글 BERT의 정석 | BERT 입문

## 목록

*2024-12-31* ⋯ [2.3 BERT의 구조](https://yshghid.github.io/docs/study/cs/cs16/#23-bert%ec%9d%98-%ea%b5%ac%ec%a1%b0)

*2024-12-31* ⋯ [2.4 BERT 사전 학습](https://yshghid.github.io/docs/study/cs/cs16/#24-bert-%ec%82%ac%ec%a0%84-%ed%95%99%ec%8a%b5)

---

## 2.3 BERT의 구조

### BERT의 전체 구조

- 트랜스포머의 인코더(Encoder) 블록을 여러 개 쌓은 형태.
  - 입력: 문장 (토큰화된 형태)
  - 내부 구조: N개의 Transformer Encoder Blocks (기본 모델은 12개, 큰 모델은 24개)
  - 출력: 각 토큰의 벡터 표현 (Contextual Embedding)

> cf) BERT의 대표적인 모델 크기
> | 모델       | # 인코더 층 | 숨겨진 차원 (dmodel) | 어텐션 헤드 수 | 파라미터 수 |
> |-----------|----------|----------------------|-------------|------------|
> | BERT-Base | 12       | 768                  | 12          | 110M       |
> | BERT-Large | 24       | 1024                 | 16          | 340M       |

### BERT의 입력 처리

1. 입력 토큰 (Token Embedding)
- WordPiece Tokenization을 사용하며, 단어를 서브워드(subword) 단위로 분할하고 각 토큰은 고유한 임베딩 벡터로 변환된다.
  ex) "playing" -> ["play", "##ing"]

2. 문장 구분 정보 (Segment Embedding)
- BERT는 두 개의 문장을 함께 입력할 수 있으며, 이때 각 문장이 어디에 속하는지를 구분하기 위해 Segment Embedding을 추가한다.
  ex) 문장 A: 0 (Segment A) / 문장 B: 1 (Segment B)

3. 위치 정보 (Position Embedding)
- 트랜스포머는 순서를 고려하지 않는 구조이므로, 단어 순서를 반영하기 위해 위치 임베딩을 추가한다.
- BERT는 고정된 학습 가능한 위치 임베딩을 사용하며, 트랜스포머에서 사용되는 사인(sine) 및 코사인(cosine) 위치 임베딩을 사용하지 않음.

**최종 입력 형식**

> [CLS] 문장1 단어1 단어2 ... [SEP] 문장2 단어1 단어2 ... [SEP]

- [CLS]: 문장 전체를 대표하는 분류(Classification) 토큰 (첫 번째 위치)
- [SEP]: 문장 구분(Sentence Separation) 역할
- [PAD]: 패딩 (짧은 문장을 일정 길이로 맞추기 위해 사용)


### BERT의 내부 구조 (Transformer Encoder Block)

> 트랜스포머 인코더 블록을 여러 개 쌓은 구조.

1. Multi-Head Self-Attention
- BERT는 문장의 양방향 문맥을 학습하기 위해 Multi-Head Self-Attention을 사용한다.
- 각 단어(토큰)는 문장의 다른 모든 단어와 어텐션을 수행하며, 관계를 학습한다.
  - 즉 장의 다른 모든 단어와 어텐션 스코어를 계산하는데, 스코어가 크면 토큰 간 관계가 강한 것으로 간주된다.
    ![image](https://github.com/user-attachments/assets/6b72b53a-0916-47c4-bc15-5c4dc38bae61)
  - BERT는 12~16개의 어텐션 헤드를 사용한다.

2. Feed Forward Network (FFN)

- 각 어텐션 층을 통과한 결과는 두 개의 완전 연결층(Fully Connected Layers) 을 통과하여 변환된다.
  - 첫 번째 레이어: 선형 변환 + 활성화 함수 (ReLU 또는 GELU)
  - 두 번째 레이어: 최종 출력 변환
- FFN은 각 토큰에 대해 독립적으로 작동하며, 모델의 표현력을 증가시키는 역할을 한다.

3. Layer Normalization & Residual Connection
- Residual Connection: 입력과 출력을 더해줌 (Gradient Flow 안정화)
- Layer Normalization: 네트워크 안정성 유지, 학습 속도 향상

> 이 과정을 총 N번 반복하여 최종적으로 컨텍스트 정보를 포함한 벡터가 생성된다.


### BERT의 출력

- BERT의 출력은 크게 두 가지 형태로 활용됨.

1. 문장 수준 출력 ([CLS] 토큰)
- [CLS] 토큰의 벡터를 활용하여 문장 분류(Classification) 및 회귀(Task-Specific Head) 를 수행.
- ex) 감성 분석(Sentiment Analysis), 자연어 추론(NLI)

2. 단어 수준 출력 (Token-Level Embeddings)
- 각 토큰의 벡터를 활용하여 개체명 인식(Named Entity Recognition, NER), 문장 생성 등의 태스크 수행.

---

## 2.4 BERT 사전 학습

> 사전 학습 단계에서는 BERT가 대량의 텍스트 데이터를 학습하면서 일반적인 언어 패턴과 문맥(Contextual Representation)을 이해한다.

### Masked Language Model (MLM, 마스킹된 언어 모델)

1. MLM 기본 개념

- 입력 문장에서 랜덤하게 15%의 단어를 [MASK]로 바꾼 후, 이를 예측하는 방식.
- BERT는 문장의 양방향(Bidirectional) 컨텍스트를 활용하여 [MASK]된 단어를 예측한다.
- 일반적인 언어 모델(예: GPT)은 이전 단어들만 참고하는 단방향 방식이지만, BERT의 MLM은 좌우 문맥을 모두 활용할 수 있다.

2. MLM의 토큰 마스킹

- 마스킹된 15%의 단어는 다음과 같은 비율로 변환된다.
  - 80% → [MASK] 토큰으로 변경
  - 10% → 랜덤한 다른 단어로 변경
  - 10% → 원래 단어를 유지
 
  ex) "I love deep learning because it is powerful."은 BERT의 입력으로 변환하면 "I love [MASK] learning because it is powerful."이고 모델의 목표는 "[MASK]" → "deep"이다.

> 일반적인 자동 회귀(autoregressive) 모델은 단방향(Left-to-Right 또는 Right-to-Left)으로 단어를 예측함. 하지만, BERT는 양방향(Bidirectional) 문맥을 고려해야 하므로, 단어 일부를 가려놓고 전체 문맥을 기반으로 예측하는 방식이 적합하다.


### Next Sentence Prediction (NSP, 문장 관계 예측)

1. NSP 기본 개념

- 두 개의 문장을 입력으로 받아서, 두 번째 문장이 첫 번째 문장의 다음 문장인지 아닌지를 예측하는 방식.
- 이는 문장 간 관계를 학습하는 데 유용하며, 질의응답(QA) 및 자연어 추론(NLI) 태스크에 도움됨.

2. NSP의 데이터 구성

- 학습할 때 두 개의 문장을 선택하여 다음과 같이 구성한다.
  - 50%의 경우 → 실제 연속된 문장 (Positive Example)
  - 50%의 경우 → 무작위로 선택된 문장 (Negative Example)


> BERT는 [CLS] 토큰을 활용하여 두 문장이 이어지는지 여부를 판단하는 분류 태스크를 수행한다. 이를 통해 질의응답(QA) 및 문장 간 논리적 연결성을 고려하는 태스크에서 강한 성능을 발휘할 수 있다.

### BERT의 사전 학습 과정 (Pre-training Process)

1. 데이터 준비
- BERT는 대량의 비지도 학습 데이터를 사용하여 사전 학습된다.
- 각 문장을 WordPiece Tokenizer를 이용해 서브워드(subword) 단위로 변환한다.

2. 토큰 임베딩 생성
- 입력 문장은 다음과 같은 3가지 임베딩을 결합하여 벡터로 변환된다.
  - Token Embedding: 각 단어에 해당하는 임베딩 벡터
  - Segment Embedding: 문장 A/B를 구분하는 임베딩
  - Position Embedding: 문장 내 단어의 위치 정보를 나타내는 임베딩

3. Transformer 인코더 통과

- BERT의 본체인 Transformer Encoder (12~24개 블록) 를 통해 입력을 변환한다.
  - MLM 태스크를 위해 일부 토큰이 [MASK] 처리된 상태에서 어텐션(Self-Attention)이 수행됨.
  - NSP 태스크를 위해 [CLS] 토큰의 출력이 사용됨.

4. 두 가지 출력
   - MLM 출력: [MASK] 위치에 올바른 단어를 예측
   - NSP 출력: [CLS] 토큰을 사용하여 두 문장이 연속된 문장인지 예측
   - BERT의 최종 손실(Loss Function) 계산.
     > ![image](https://github.com/user-attachments/assets/69e5a74a-328e-4a80-bb3e-b44ae2f97cf3)


### BERT의 사전 학습 이후 (Fine-tuning)

> BERT는 사전 학습을 마친 후, 특정 태스크에 맞춰 미세 조정(Fine-tuning)한다.

1. 사전 학습된 BERT 모델을 기반으로 특정 태스크 수행
- 텍스트 분류 (Sentiment Analysis)
- 질의응답 (SQuAD, Question Answering)
- 개체명 인식 (NER, Named Entity Recognition)
- 자연어 추론 (NLI, Natural Language Inference)

2. 미세 조정 방식
- 사전 학습된 가중치를 초기화한 후, 해당 태스크에 맞게 라벨이 있는 데이터로 추가 학습을 진행한다.
  - [CLS] 토큰을 활용한 분류 태스크
  - [MASK] 토큰을 활용한 MLM 기반 태스크



