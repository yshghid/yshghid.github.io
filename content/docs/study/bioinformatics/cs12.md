---
date : 2024-12-31
tags: ['2024-12']
categories: ['deep learning', 'python']
bookHidden: true
title: "혼자 공부하는 딥러닝 | ANN"
bookComments: true
---

# [딥러닝] 혼자 공부하는 딥러닝 | ANN

## 목록

*2024-12-31* ⋯ [17. 간단한 인공 신경망 모델 만들기](https://yshghid.github.io/docs/study/cs/cs12/#17-%ea%b0%84%eb%8b%a8%ed%95%9c-%ec%9d%b8%ea%b3%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%aa%a8%eb%8d%b8-%eb%a7%8c%eb%93%a4%ea%b8%b0)

*2024-12-31* ⋯ [18. 인공 신경망에 층을 추가하여 심층 신경망 만들어 보기](https://yshghid.github.io/docs/study/cs/cs12/#18-%ec%9d%b8%ea%b3%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%97%90-%ec%b8%b5%ec%9d%84-%ec%b6%94%ea%b0%80%ed%95%98%ec%97%ac-%ec%8b%ac%ec%b8%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%a7%8c%eb%93%a4%ea%b8%b0)

*2024-12-31* ⋯ [19. 인경 신경망 모델 훈련의 모범 사례 학습하기](https://yshghid.github.io/docs/study/cs/cs12/#19-%ec%9d%b8%ea%b2%bd-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%aa%a8%eb%8d%b8-%ed%9b%88%eb%a0%a8%ec%9d%98-%eb%aa%a8%eb%b2%94-%ec%82%ac%eb%a1%80-%ed%95%99%ec%8a%b5%ed%95%98%ea%b8%b0)

---

## 17. 간단한 인공 신경망 모델 만들기

1. 데이터 준비

fashion_mnist 데이터셋에서 학습과 테스트용 이미지 데이터를 가져온다. 학습 데이터는 60,000개의 28x28 픽셀 이미지, 테스트 데이터는 10,000개의 28x28 픽셀 이미지. train_target과 test_target은 각 이미지에 해당하는 레이블(0~9)을 갖고있다. 

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)
print(test_input.shape, test_target.shape)
```
```plain text
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
29515/29515 [==============================] - 0s 3us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
26421880/26421880 [==============================] - 2s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
5148/5148 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
4422102/4422102 [==============================] - 0s 0us/step

(60000, 28, 28) (60000,)
(10000, 28, 28) (10000,)
```

2. 데이터 시각화

첫 10개의 이미지 샘플을 출력하기.

```python
import numpy as np
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()

print([train_target[i] for i in range(10)])

print(np.unique(train_target, return_counts=True))
```
![image](https://github.com/user-attachments/assets/c7fe3237-3150-435f-adf8-ec0fba7fd0ff)

```plain text
[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]
(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
```

np.unique()로 레이블 분포를 확인해보니 각 클래스에 6,000개씩 균일하게 분포해있다.

3. 로지스틱 회귀

이미지를 0~255의 픽셀 값을 [0, 1] 범위로 정규화한다. 그리고 데이터를 2D 배열로 펼친다. (60000, 28, 28) → (60000, 784). 즉 각 이미지를 784차원 벡터로 변환한다.

```python
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_validate

train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
print(train_scaled.shape)
```

로지스틱 회귀모델을 학습한다. 손실함수는 로지스틱 손실함수를 사용한다. 

```python
sc = SGDClassifier(loss='log', max_iter=5, random_state=42)
scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
```
```plain text
(60000, 784)
0.8195666666666668
```
학습 결과 테스트 세트 정확도는 81.96%이다.

4. 케라스 신경망 모델 생성

```python
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
print(train_scaled.shape, train_target.shape)
print(val_scaled.shape, val_target.shape)
```
```plain text
(38400, 784) (38400,)
(9600, 784) (9600,)
```

학습 데이터를 학습 세트와 검증 세트로 나눴다. 학습 세트는 (38400, 784) 검증 세트는 (9600, 784).

```python
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
model = keras.Sequential(dense)
```
```plain text
2025-01-23 17:30:40.924465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-23 17:30:40.934329: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
```

Dense Layer는 각 입력 뉴런이 모든 출력 뉴런에 연결되는 신경망의 기본 층이다. Dense(10)으로 10개의 뉴런을 가지는 층을 만들어줬다. 입력 데이터는 784차원 벡터이고, 활성화 함수는 softmax 함수가 사용되었다. keras.Sequential(dense)는 하나의 Dense 층으로 이루어진 간단한 순차 모델을 정의한다.

다시 말해, Dense Layer는 784차원 입력을 10개 클래스의 출력으로 변환하며, 각 출력은 Softmax를 통해 확률로 계산된다

5. 모델 컴파일

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
print(train_target[:10])
```
```plain text
/data1/home/ysh980101/miniconda3/envs/workspace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:165: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
  warnings.warn(
/data1/home/ysh980101/miniconda3/envs/workspace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:704: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
[9 4 9 0 4 9 3 6 4 7]
```

- 모델을 학습하기 전에 손실 함수, optimizer, 평가 지표(metric)을 설정. 모델이 학습 과정에서 어떻게 성능을 평가하고 손실을 줄이고 가중치를 업데이트할지 정의한다.

- 손실 함수 (Loss Function)는 모델의 예측값과 정답 사이의 차이를 측정함. sparse_categorical_crossentropy는 레이블이 정수 형태로 제공되는 경우 즉 다중 클래스 분류 문제(Multi-class Classification)에 사용된다. (원 핫 인코딩 아니라)
- 모델의 출력값은 softmax 활성화 함수를 통해 각 클래스에 대한 확률 분포를 반환하는데 손실 함수는 정답 클래스와 예측된 확률 분포 간의 교차 엔트로피(Cross Entropy)를 계산한다.

> Loss = $- \sum_{i=1}^C y_i \cdot \log(\hat{y}_i)$

- $y_i$은정답 레이블의 원-핫 인코딩 값 (sparse일 경우 해당 위치만 1), $\hat{y}_i$: 모델의 예측 확률값, $C$: 클래스의 총 개수이다. 확률값이 정답 클래스에 가까울수록 손실이 작아진다.

> 모델의 전체 동작 흐름
> 1. 모델은 마지막 Dense 층에서 softmax를 사용해 10개의 클래스 확률을 출력
> 2. 손실 함수는 정답 레이블(예: 2)과 예측 확률(0.7)의 차이를 교차 엔트로피로 계산.
> 3. 예측 클래스(가장 높은 확률을 가진 클래스)가 정답 레이블과 일치하면 평가 지표 accuracy 즉 모델이 정확하게 예측한 비율이 높아진다.
> 4. 손실 값이 최소화되도록 가중치(모델 파라미터)가 옵티마이저에 의해 업데이트된다.


6. 모델 훈련

```python
model.fit(train_scaled, train_target, epochs=5)
```
```plain text
Epoch 1/5
1200/1200 [==============================] - 3s 2ms/step - loss: 0.6326 - accuracy: 0.7853
Epoch 2/5
1200/1200 [==============================] - 3s 3ms/step - loss: 0.4910 - accuracy: 0.8344
Epoch 3/5
1200/1200 [==============================] - 3s 3ms/step - loss: 0.4656 - accuracy: 0.8444
Epoch 4/5
1200/1200 [==============================] - 3s 3ms/step - loss: 0.4512 - accuracy: 0.8499
Epoch 5/5
1200/1200 [==============================] - 3s 3ms/step - loss: 0.4417 - accuracy: 0.8526
<keras.callbacks.History at 0x7fa1e4c12be0>
```

- 학습 반복(Epoch)은 5회로 설정되었다. 
- 각 Epoch 결과 손실은 0.6326 → 0.4417로 감소, 정확도(accuracy)는 78.5% → 85.3%로 증가했다.

7. 모델 평가

```python
model.evaluate(val_scaled, val_target)
```
```plain text
300/300 [==============================] - 1s 3ms/step - loss: 0.4335 - accuracy: 0.8590
[0.4334854781627655, 0.8589583039283752]
```

- 검증 데이터에서 모델 평가 결과 손실은 0.4335, 정확도는 85.9%.
- 손실 값(0.4335)은 모델의 예측이 검증 데이터에서 큰 오류를 범하지 않았음을 보여주고 정확도(85.9%)**는 모델이 Fashion MNIST 데이터셋에서 상당히 높은 성능을 보였으며, 의류 이미지를 잘 분류할 수 있음을 나타낸다.

- 손실과 정확도는 상관관계가 있지만 동일하지 않음. 손실은 모델의 예측이 얼마나 잘 정답 분포를 따르는지(확률 수준)를 나타내며, 확률이 높은 정답일수록 손실 값이 낮아진다.
- 정확도는 모델이 정답을 맞췄는지 여부(0 또는 1)를 측정한다. 손실이 감소해도 정확도는 일정 범위에서 정체될 수 있다. 이는 모델이 정답 분포를 더 잘 학습했지만, 예측 결과가 다른 클래스에 대한 잘못된 선택으로 여전히 분류 문제를 일으킬 수 있기 때문.

8. 사이킷런-케라스 비교

- 로지스틱 회귀(SGDClassifier): 정확도 약 81.96%. 단순한 선형 모델.
- 케라스 신경망 모델: 정확도 약 85.9%. 더 높은 성능을 보였으며, 신경망의 유연성 덕분에 복잡한 데이터를 잘 학습했다.

9. 요약

- 데이터 준비 → 정규화 → 펼침.
- 간단한 신경망 모델(1개 층, 10개 뉴런) 설계.
- 로지스틱 회귀와 비교해 신경망이 더 나은 성능을 보였다.

> **강의 링크**
> 
> https://www.youtube.com/watch?v=ZiP9erf5Fo0&list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX&index=17
> ![image](https://github.com/user-attachments/assets/ff879984-8cfd-4876-9840-6ded05e02517)

---

## 18. 인공 신경망에 층을 추가하여 심층 신경망 만들기

1. 데이터 가져오기

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)
print(test_input.shape, test_target.shape)

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_validate

train_scaled = train_input / 255.0
print(train_scaled.shape)
```
```plain text
(60000, 28, 28) (60000,)
(10000, 28, 28) (10000,)
(60000, 28, 28)
```

2. 심층 신경망

```python
dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')

model = keras.Sequential([dense1, dense2])
```

cf) 층을 추가하는 다른 방법

```python
model = keras.Sequential([
    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),
    keras.layers.Dense(10, activation='softmax', name='output')
], name = '패션 MNIST 모델')

model = keras.Sequential()
model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))
```

3. 렐루 함수와 Flatten 층

```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
Model: "sequential_2"
```
```plain text
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 784)               0         
                                                                 
 dense_5 (Dense)             (None, 100)               78500     
                                                                 
 dense_6 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
```

4. 옵티마이저

```python
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')
sgd = keras.optimizers.SGD()
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')

sgd = keras.optimizers.SGD(learning_rate=0.1)
sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(100, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)
model.evaluate(val_scaled, val_target)
```
```plain text
Epoch 1/5
1500/1500 [==============================] - 59s 39ms/step - loss: 0.8167 - accuracy: 0.7495
Epoch 2/5
1500/1500 [==============================] - 39s 26ms/step - loss: 0.4167 - accuracy: 0.8520
Epoch 3/5
1500/1500 [==============================] - 32s 21ms/step - loss: 0.3710 - accuracy: 0.8665
Epoch 4/5
1500/1500 [==============================] - 33s 22ms/step - loss: 0.3345 - accuracy: 0.8790
Epoch 5/5
1500/1500 [==============================] - 51s 34ms/step - loss: 0.3218 - accuracy: 0.8816

375/375 [==============================] - 4s 11ms/step - loss: 0.3423 - accuracy: 0.8785
[0.34229394793510437, 0.8784999847412109]
```

> **강의 링크**
>
> https://www.youtube.com/watch?v=JskWW5MlzOg&list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX&index=18
> ![image](https://github.com/user-attachments/assets/ae4eb3de-705d-48d2-beb3-33acc71d116b)


---

## 19. 인경 신경망 모델 훈련의 모범 사례 학습하기 

1. 손실 곡선

```python
model.compile(loss="sparse_categorical_crossentropy", metrics="accuracy")
history = model.fit(train_scaled, train_target, epochs=5, verbose=0)
print(history.history.keys())
dict_keys(['loss','accuracy'])

plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('epoch')
plt.show()

plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()    
```
![image](https://github.com/user-attachments/assets/1e80bc5e-c436-4737-9cfa-24a905848b02)

![image](https://github.com/user-attachments/assets/bef36be1-dd18-40cb-a88d-3b9d7f9c3d87)


cf) 더 많은 에포크?

```python
history = model.fit(train_scaled, train_target, epochs=20, verbose=0)

plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```
![image](https://github.com/user-attachments/assets/d146dcb5-2181-4d4f-bb74-74e2b9f3ce2f)


2. 검증 손실

```python
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))
print(history.history.keys())

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()  
```

3. 드롭아웃

```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```

4. 모델 저장과 복원

```python
model.save_weights('model-weights.h5')
model.load_weights('model-weights.h5')
model.save('model-whole.h5')

model = keras.models.load_model('model-whole.h5')

val_labels = np.argmax(model.predict(val_scaled), axis=-1)
print(np.mean(val_labels == val_target))
```

5. 콜백

```python
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb])

model = keras.models.load_model('best-model.h5')
```

6. 조기종료

```python
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
early_stopping_cb = keras.callbecks.EarlyStopping(patience=2, restore_best_weights=True)

history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])
print(early_stopping_cb.stopped_epoch)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

> **강의 링크**
>
> https://www.youtube.com/watch?v=2by0Fz3XC84&list=PLVsNizTWUw7HpqmdphX9hgyWl15nobgQX&index=19
> ![image](https://github.com/user-attachments/assets/1290784e-b9f5-43ec-aaad-c4dac12db883)

(여기 코드 왤케 오류 많이나지 ㅠㅠ...)
