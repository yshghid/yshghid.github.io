---
date : 2024-12-31
tags: ['2024-12']
categories: ['deep learning']
bookHidden: true
title: "구글 BERT의 정석 | 트랜스포머 입문"
bookComments: true
---

# [딥러닝] 구글 BERT의 정석 | 트랜스포머 입문

## 목록

*2024-12-31* ⋯ [1.2 트랜스포머의 인코더 이해하기](https://yshghid.github.io/docs/study/cs/cs15/#%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8%ec%9d%98-%ec%9d%b8%ec%bd%94%eb%8d%94-%ec%9d%b4%ed%95%b4%ed%95%98%ea%b8%b0)

*2024-12-31* ⋯ [1.3 트랜스포머의 디코더 이해하기](https://yshghid.github.io/docs/study/cs/cs15/#13-%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8%ec%9d%98-%eb%94%94%ec%bd%94%eb%8d%94-%ec%9d%b4%ed%95%b4%ed%95%98%ea%b8%b0)

---

## 1.2 트랜스포머의 인코더 이해하기

### 셀프 어텐션

- 셀프 어텐션은 문장 내 단어들이 서로 얼마나 중요한지를 계산하는 과정.
- 트랜스포머는 이를 위해 입력 단어를 쿼리(Query), 키(Key), 밸류(Value) 세 가지 벡터로 변환하여 연관성을 구한다.

### 어텐션 점수 계산 예제

> "The cat sat on the mat."

1. 각 단어 벡터(예: 512차원)를 가중치 행렬과 곱하여 쿼리(Q), 키(K), 밸류(V)벡터를 생성한다.
2. 어떤 단어가 다른 단어와 얼마나 연관되는지를 측정하기 위해, Q와 K벡터 간의 내적(dot product)을 계산한다.

   | 단어   | The  | cat  | sat  | on   | the  | mat  |
   |--------|------|------|------|------|------|------|
   | Query: "cat" | 0.2  | 1.0  | 0.8  | 0.1  | 0.3  | 0.5  |

- "cat"의 쿼리 벡터와 모든 단어의 키 벡터를 곱해서 점수를 계산하는 경우.
- 여기서 "cat"은 "sat"과 가장 연관이 높고(0.8), "on"과는 거의 연관이 없다(0.1).

3. 소프트맥스 적용

   | 단어        | The  | cat  | sat  | on   | the  | mat  |
   |------------|------|------|------|------|------|------|
   | Softmax 값 | 0.05 | 0.4  | 0.35 | 0.02 | 0.08 | 0.1  |
- 위에서 구한 점수에 대해 소프트맥스를 적용하여 확률로 변환
- 이제 "cat"은 "sat"(0.35)과 "cat" 자체(0.4)에 높은 가중치를 부여함.

4. 각 단어의 밸류(V) 벡터를 위의 확률로 가중합하여 최종 어텐션 출력을 얻는다.

### 멀티 헤드 어텐션

- 단어 간의 관계를 한 가지 방식으로만 학습하면, 문맥을 완전히 반영하지 못할 수 있음. 예를 들어, 단어 "cat"은 문장에서 다음과 같은 다양한 방식으로 다른 단어와 관계를 맺을 수 있다.
  1. 문법적 관계(Head 1): "cat" → "sat" (주어와 동사의 관계)
  2. 의미적 관계(Head 2): "cat" → "mat" (동물과 사물이 놓여 있는 관계)
  3. 위치적 관계(Head 3): "on" → "mat" ("on"이 "mat"과 어떤 방식으로 연결되는지)

- 만약 하나의 어텐션만 사용한다면, 위 관계 중 하나만 학습할 수 있다. 멀티 헤드 어텐션은 여러 개의 독립적인 어텐션 연산을 수행하여, 이러한 다양한 패턴을 동시에 학습하는 역할을 함.

### 멀티헤드 어텐션 수행 과정

1. 문장을 입력하면, 각 단어는 일정한 차원의 벡터(예: 512차원)로 변환된다. 각 단어의 벡터를 이용하여 쿼리(Q), 키(K), 밸류(V)를 생성한다.

2. 여러 개의 어텐션 헤드 생성

- 멀티 헤드 어텐션에서는 각 단어 벡터를 여러 개의 서로 다른 가중치 행렬을 사용하여 여러 개의 쿼리(Q), 키(K), 밸류(V)로 변환한다.

- 각 헤드는 서로 다른 관계를 학습할 수 있도록 다른 가중치를 가진다.
  > ![image](https://github.com/user-attachments/assets/7db3165d-589b-4d30-8d26-849b7edb9683)

3. 각 헤드는 독립적으로 셀프 어텐션(Self-Attention)을 수행한다. 소프트맥스를 적용하여 확률값으로 변환한 후, 밸류(V)에 가중합하여 최종 출력을 생성한다.

- 어텐션 점수 계산
  > ![image](https://github.com/user-attachments/assets/f93e8ee9-72dd-41a2-bb54-052a9de30828)

4. 각 헤드에서 나온 결과를 병합(Concatenation)한 후, 최종적으로 선형 변환을 적용한다. 즉, 여러 개의 어텐션을 병렬로 수행하고, 최종적으로 선형 변환을 적용하여 하나의 벡터로 변환하는 것.
- 선형 변환
  > ![image](https://github.com/user-attachments/assets/79e29b3a-2e36-4374-8165-c9d46957d194)

### 위치 인코딩

- 트랜스포머는 문장을 한 번에 입력받아 병렬로 처리하는 구조이다. 이러한 구조는 속도 면에서 유리하지만, 단어들의 순서(sequence)를 직접적으로 학습할 수 없다.
- 따라서 위치 정보를 인코딩하여 단어의 순서를 반영하는 기법이 필요함.
- 즉, 특정 단어의 위치 pos와 벡터의 차원 위치 i에 따라 사인과 코사인 값을 계산하여 각 차원별 위치 인코딩 값을 생성함으로써 위치 정보를 반영한다.

### 위치 인코딩 예제

> "The cat sat on the mat."

- 단어 벡터를 4차원으로 설정한다고 가정하고 각 단어에 대해 위치 인코딩 값 계산하기.

1. 첫 번째 차원 (i=0) 계산 (짝수이므로 sin 사용)
   > ![image](https://github.com/user-attachments/assets/c50b8576-25bf-4550-8da1-7316b885e65e)

2. 두 번째 차원 (i=1) 계산 (홀수이므로 cos 사용)
   > ![image](https://github.com/user-attachments/assets/7c8023bb-c0a7-4813-a4d6-72357df8005b)

3. 생성된 각 단어의 위치 인코딩 벡터
   | 단어  | 위치 인코딩 벡터 (4차원)           |
   |------|-----------------------------------|
   | The  | [0.000, 1.000, 0.841, 0.540]      |
   | cat  | [0.841, 0.540, 0.909, -0.416]     |
   | sat  | [0.909, -0.416, 0.141, -0.990]    |
   | on   | [0.141, -0.990, -0.757, -0.654]   |
   | the  | [-0.757, -0.654, -0.958, 0.283]   |
   | mat  | [-0.958, 0.283, -0.279, 0.750]    |

4. 단어의 임베딩 벡터와 더하면, 위치 정보가 반영된 최종 벡터가 생성된다.

### 피드포워드 네트워크(Feedforward Network, FFN)

- FFN은 트랜스포머의 각 단어 벡터에 대해 독립적으로 적용되는 두 개의 선형 변환(fully connected layer)과 활성화 함수(ReLU)로 구성된 신경망.
- 과정
  1. 첫 번째 선형 변환 (Fully Connected Layer 1): 입력 벡터를 확장된 차원(2048)의 벡터로 변환한다.
  2. ReLU 활성화 함수 적용: 비선형성을 추가하여 복잡한 관계를 학습
  3. 두 번째 선형 변환 (Fully Connected Layer 2): 다시 원래 차원(512)으로 축소하여 출력

### 트랜스포머 인코더 블록에서 FFN의 위치

- 피드포워드 네트워크는 어텐션 이후에 적용됨.

1. 멀티 헤드 어텐션(Self-Attention) 수행
   - 각 단어가 다른 단어들과의 관계를 학습
   - 어텐션 가중치를 통해 정보를 집계
   - Add & Norm (Residual Connection + Layer Normalization) 적용

2. 피드포워드 네트워크(FFN) 적용
   - 개별 단어의 의미 표현을 강화 (독립적인 변환 수행)
   - ReLU를 활용하여 비선형성을 추가
   - Add & Norm (Residual Connection + Layer Normalization) 적용

### Add & Norm

- 트랜스포머는 매우 깊은 신경망이다. 깊은 신경망을 학습할 때 흔히 발생하는 문제가 기울기 소실(Vanishing Gradient)과 기울기 폭발(Exploding Gradient). 또한, 모델이 과도하게 변화하면 학습이 불안정해진다.
- Residual Connection을 사용하면 원래 정보를 유지하면서 학습할 수 있다.
- Layer Normalization을 사용하면 값의 스케일을 맞추어 학습을 안정화할 수 있다.

### 트랜스포머 인코더 전체 과정

1. 입력 벡터(임베딩 + 위치 인코딩) 생성
2. 멀티 헤드 어텐션 수행하여 단어 간 관계를 학습
3. Residual Connection 적용 (입력 + 어텐션 출력 더하기)
4. Layer Normalization 적용하여 학습 안정화
5. 피드포워드 네트워크(FFN) 적용하여 단어별 정보를 강화
6. Residual Connection 적용 (입력 + FFN 출력 더하기)
7. Layer Normalization 적용
8. 다음 인코더 블록으로 전달하여 반복 수행

---

## 1.3 트랜스포머의 디코더 이해하기

### 디코더의 구조

- 트랜스포머 디코더는 인코더와 함께 동작할 수도 있고(Google의 원래 Transformer 모델, BART), 독립적으로 동작할 수도 있다(GPT 시리즈).
- N개의 디코더 블록(stack)이 쌓여 있는 형태로 구성.

### 디코더 핵심 연산

1. Masked Multi-Head Self-Attention

- 입력 시퀀스 내에서 이전 단어까지만 참고하여 다음 단어를 예측해야 하므로, 일반적인 Multi-Head Self-Attention과 다르게 미래 정보를 차단(masking) 한다.
- 이를 위해 Casual Masking(Look-Ahead Masking)을 사용하여, 현재 위치 t에서 t+1, t+2, ... 등 미래의 단어들을 보지 못하도록 만든다.
- 계산 과정
  1. Q, K, V를 입력에서 생성
  2. 어텐션 스코어 계산
  3. 마스킹 적용: 미래 단어의 스코어를 −∞로 설정하여 Softmax에서 0이 되도록 만듦.
  4. Softmax & 가중합하여 최종 출력을 생성.

2. Cross-Attention

- 인코더에서 생성된 컨텍스트 정보를 활용하는 모듈.
- 인코더의 출력을 Key & Value로 사용하고, 디코더의 출력을 Query로 사용해서 Attention을 수행.
- 작동 방식
   - 디코더에서 나온 Query(Q)와 인코더에서 생성된 Key(K) 및 Value(V)를 활용하여 Multi-Head Attention 수행.
   - 이를 통해 코더가 인코더의 정보를 반영하여 다음 토큰을 예측하는 데 도움을 준다.

3. Feed Forward Network (FFN)

- 각 디코더 블록에는 FFN이 포함되어 있으며, 두 개의 완전 연결층(fully connected layers)으로 구성된다.
- 구조
  - 입력 차원 dmodel
  - 중간 차원 dff(보통 4dmodel)
  - 활성화 함수 ReLU 또는 GELU
  - 출력 차원 dmodel

4. Residual Connection & Layer Normalization

- 잔차 연결(Residual Connection): 각 서브 레이어의 입력을 더해줌.
- Layer Normalization: 학습 안정성을 높이고, 학습 속도를 향상.

### 디코더의 출력 (Output Processing)

- 마지막 디코더 블록에서 나온 결과는 완전 연결층(Dense layer)를 거쳐 차원을 조정한다.
- 소프트맥스(Softmax) 를 적용하여 단어 확률 분포를 계산한다.
- 가장 확률이 높은 단어를 선택하여 출력한다.

