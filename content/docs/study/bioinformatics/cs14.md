---
date : 2024-12-31
tags: ['2024-12']
categories: ['deep learning', 'python']
bookHidden: true
title: "딥러닝을 이용한 자연어 처리 입문 | BERT"
bookComments: true
---

# [딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT

## 목록

*2024-12-31* ⋯ [17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)](https://yshghid.github.io/docs/study/cs/cs14/#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert)

*2024-12-31* ⋯ [17-03 구글 BERT의 마스크드 언어 모델](https://yshghid.github.io/docs/study/cs/cs14/#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8)

*2024-12-31* ⋯ [17-04 한국어 BERT의 마스크드 언어 모델](https://yshghid.github.io/docs/study/cs/cs14/#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8)

*2024-12-31* ⋯ [17-05 구글 BERT의 다음 문장 예측](https://yshghid.github.io/docs/study/cs/cs14/#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1)

*2024-12-31* ⋯ [17-06 한국어 BERT의 다음 문장 예측](https://yshghid.github.io/docs/study/cs/cs14/#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1)

---

## 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)

> **BERT?**
> 1. BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.
> 2. 입/출력 구조
>    - 입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환.
>    - 출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것. 
>    - 문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다.
> 3. 구조와 연산
>    - BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조.
>    - 각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다.

> **BERT의 서브워드 토크나이저: WordPiece**
> 1. 서브워드 토크나이저: 자주 등장하는 단어는 단어 단위로, 드물게 등장하는 단어는 서브워드(subword) 단위로 분리하는 방식의 토크나이저.
> 2. WordPiece의 작동 원리
>    - 훈련 데이터로부터 단어 집합을 생성하는데, 자주 등장하는 단어는 단어 단위로 추가하고 드물게 등장하는 단어는 더 작은 단위(서브워드)로 쪼개어 추가한다.
>    - 토큰화: 단어가 단어 집합에 존재하면 그대로 사용하고 단어가 단어 집합에 없으면 서브워드로 분리한다.
>      ex) 단어 "embeddings"가 단어 집합에 없으면 서브워드로 분리: em, ##bed, ##ding, #s. ##는 단어의 중간이나 끝에서 온 서브워드라는 표시이다.
> 3. BERT는 서브워드 단위로 토큰화를 수행한 입력 데이터를 받아 문맥 정보를 반영한 임베딩을 생성한다.

> **요약**
> - BERT는 모든 단어가 서로를 참고하도록 트랜스포머 인코더(셀프 어텐션)를 활용해 문맥 정보를 포함한 임베딩을 생성한다.
> - WordPiece 토크나이저는 단어를 자주 등장 여부에 따라 단어 또는 서브워드로 분리하여 토큰화를 수행하는데 서브워드 표기(##)를 통해 단어 복원이 가능하며, 단어 집합의 크기를 줄이면서 표현력을 높인다.

1. transformers 패키지를 사용하여 BERT 토크나이저 사용하기

```python
import pandas as pd
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") # Bert-base의 토크나이저

result = tokenizer.tokenize('Here is the sentence I want embeddings for.')
print(result)
print(tokenizer.vocab['here'])
#print(tokenizer.vocab['embeddings'])
```
```plain text
['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']
2182
```

- 'Here is the sentence I want embeddings for.'라는 문장을 BERT의 토크나이저가 어떻게 토큰화하는지 확인하기.
- embeddings라는 단어는 단어 집합에 존재하지 않으므로 em, ##bed, ##ding, #s로 분리되었다.
- BERT의 단어 집합에 "here"가 있는지 조회 -> 단어 here이 정수 인코딩을 위해서 단어 집합 내부적으로 2182라는 정수로 맵핑되어져 있다.
- "embeddings"가 있는지 조회 -> KeyError: 'embeddings' 발생.

```python
# BERT의 단어 집합을 vocabulary.txt에 저장
with open('vocabulary.txt', 'w') as f:
    for token in tokenizer.vocab.keys():
        f.write(token + '\n')

df = pd.read_fwf('vocabulary.txt', header=None)
print('단어 집합의 크기 :',len(df))
```
```plain text
단어 집합의 크기 : 30522
```

> **자료 출처**
>
> https://wikidocs.net/115055

---

## 17-03 구글 BERT의 마스크드 언어 모델

```python
from transformers import TFBertForMaskedLM, AutoTokenizer
model = TFBertForMaskedLM.from_pretrained('bert-large-uncased')
tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased")
```
- TFBertForMaskedLM: 마스크드 언어 모델(Masked Language Model, MLM)을 위한 BERT 구조
- AutoTokenizer: 해당 모델 학습 시 사용된 토크나이저.

```python
inputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf')
print(inputs['input_ids'])
```
```plain text
tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)
```

- 사전 학습된 BERT로 마스크드 언어 모델 생성함.
- 예제 문장: 'Soccer is a really fun [MASK].'에 대해 토크나이저로 정수 인코딩을 수헹.
- [MASK] 토큰 예측하기?


```python
from transformers import FillMaskPipeline
pip = FillMaskPipeline(model=model, tokenizer=tokenizer)
pip('Soccer is a really fun [MASK].')
```
```plain text
[{'score': 0.7621169686317444,
  'token': 4368,
  'token_str': 'sport',
  'sequence': 'soccer is a really fun sport.'},
 {'score': 0.2034207135438919,
  'token': 2208,
  'token_str': 'game',
  'sequence': 'soccer is a really fun game.'},
 {'score': 0.01220863126218319,
  'token': 2518,
  'token_str': 'thing',
  'sequence': 'soccer is a really fun thing.'},
 {'score': 0.001863038633018732,
  'token': 4023,
  'token_str': 'activity',
  'sequence': 'soccer is a really fun activity.'},
 {'score': 0.0013354964321479201,
  'token': 2492,
  'token_str': 'field',
  'sequence': 'soccer is a really fun field.'}]
```

- 모델과 토크나이저를 파이프라인에 지정.
  - FillMaskPipeline을 사용하여 문장에서 [MASK] 위치에 들어갈 단어를 예측
  - 결과는 [MASK]에 들어갈 가능성이 높은 단어 5개와 각 단어의 관련 정보
- 예제 결과
  1. sport가 가장 높은 확률 0.7621을 가짐. 문장이 자연스럽고 문맥상 가장 적합하기 때문에 MLM 모델이 이를 첫 번째 후보로 예측했다.
  2. game은 두 번째로 높은 확률 0.2034을 가짐.
  3. thing, activity, field는 1.2%, 0.19, 0.13% 확률을 가짐.

> **자료 출처**
>
> https://wikidocs.net/153992

---

## 17-04 한국어 BERT의 마스크드 언어 모델

> '축구는 정말 재미있는 [MASK]다'를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측한다.

```python
from transformers import TFBertForMaskedLM
from transformers import AutoTokenizer

model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')
print(inputs['input_ids'])
print(inputs['token_type_ids'])
print(inputs['attention_mask'])
```
```plain text
tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)
tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)
tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)
```

- klue/bert-base의 토크나이저를 사용해서 '축구는 정말 재미있는 [MASK]다'를 변환.
- 토크나이저로 변환된 결과: inputs
  - input_ids: 정수로 변환된 토큰 시퀀스.
  - token_type_ids: 문장 구분 (한 개 문장이므로 모두 0).
  - attention_mask: 패딩 토큰 구분 (패딩 없음 → 모두 1).

```python
from transformers import FillMaskPipeline
pip = FillMaskPipeline(model=model, tokenizer=tokenizer)
pip('축구는 정말 재미있는 [MASK]다.')
```
```plain text
[{'score': 0.8963565230369568,
  'token': 4559,
  'token_str': '스포츠',
  'sequence': '축구는 정말 재미있는 스포츠 다.'},
 {'score': 0.025957893580198288,
  'token': 568,
  'token_str': '거',
  'sequence': '축구는 정말 재미있는 거 다.'},
 {'score': 0.010034064762294292,
  'token': 3682,
  'token_str': '경기',
  'sequence': '축구는 정말 재미있는 경기 다.'},
 {'score': 0.007924459874629974,
  'token': 4713,
  'token_str': '축구',
  'sequence': '축구는 정말 재미있는 축구 다.'},
 {'score': 0.007844261825084686,
  'token': 5845,
  'token_str': '놀이',
  'sequence': '축구는 정말 재미있는 놀이 다.'}]

```

- FillMaskPipeline으로 [MASK] 위치에 들어갈 수 있는 상위 5개 후보 단어 예측.
- "스포츠"가 문맥상 가장 적합한 단어로 높은 점수를 받았다.


> **자료 출처**
>
> https://wikidocs.net/152922


---

## 17-05 구글 BERT의 다음 문장 예측

```python
import tensorflow as tf
from transformers import TFBertForNextSentencePrediction, AutoTokenizer

model = TFBertForNextSentencePrediction.from_pretrained('bert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand."

encoding = tokenizer(prompt, next_sentence, return_tensors='tf')
print(encoding['input_ids'])

print(tokenizer.cls_token, ':', tokenizer.cls_token_id)
print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)

print(tokenizer.decode(encoding['input_ids'][0]))

print(encoding['token_type_ids'])
```
```plain text
tf.Tensor(
[[  101  1999  3304  1010 10733  2366  1999  5337 10906  1010  2107  2004
   2012  1037  4825  1010  2003  3591  4895 14540  6610  2094  1012   102
  10733  2003  8828  2007  1996  2224  1997  1037  5442  1998  9292  1012
   1999 10017 10906  1010  2174  1010  2009  2003  3013  2046 17632  2015
   2000  2022  8828  2096  2218  1999  1996  2192  1012   102]], shape=(1, 58), dtype=int32)
[CLS] : 101
[SEP] : 102
[CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP]
tf.Tensor(
[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1
  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32)
```
- 모델과 토크나이저를 로드하고, 토크나이저로 두 문장을 정수 인코딩했다.
- input_ids는 정수로 변환된 토큰 시퀀스이다.
  - 여기서 101과 102는 특별 토큰인 [CLS] 토큰과 [SEP] 토큰이다.
  - 정수 인코딩 결과를 다시 디코딩해서 현재 입력의 구성을 확인해보면 BERT에서 두 개의 문장이 입력으로 들어갈 경우에 맨 앞에는 [CLS] 토큰, 문장이 끝나면 [SEP] 토큰, 두번째 문장이 종료되었을 때 다시 [SEP] 토큰이 추가된다
- token_type_ids는 두 문장을 구분하기 위한 세그먼트 인코딩이다.
  - 첫 번째 문장은 0, 두 번째 문장은 1.

```python
logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]
softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print(probs)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())
```
```plain text
tf.Tensor([[9.9999714e-01 2.8381860e-06]], shape=(1, 2), dtype=float32)
최종 예측 레이블 : [0]
```
- 다음 문장 예측하기
  1. BERT 모델에 입력 데이터를 넣어 logits(예측 점수)를 반환
  2. 소프트맥스를 적용해 각 레이블(0 또는 1)에 대한 확률 계산.
- 예측 결과
  - 이어지는 문장일 확률(레이블 0): 99.9997%
  - 이어지지 않는 문장일 확률(레이블 1) 0.00028%
  - 최종 예측은 레이블 0으로써 두 문장이 이어진다고 판단함.

```python
# 상관없는 두 개의 문장
prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
next_sentence = "The sky is blue due to the shorter wavelength of blue light."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())
```
```plain text
최종 예측 레이블 : [1]
```
- 이어지지 않는 두 개의 문장으로 테스트
- 예측 결과: 이어지지 않는다고 판단.

> **자료 출처**
>
> https://wikidocs.net/156767

---

## 17-06 한국어 BERT의 다음 문장 예측

```python
import tensorflow as tf
from transformers import TFBertForNextSentencePrediction
from transformers import AutoTokenizer

model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

# 이어지는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())
```
```plain text
최종 예측 레이블 : [0]
```
- 모델과 토크나이저 로드
  - TFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드.
  - AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드.

- 예측 결과: 두 문장이 이어진다고 판단.

```python
# 상관없는 두 개의 문장
prompt = "2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다."
next_sentence = "극장가서 로맨스 영화를 보고싶어요"
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())
```
```plain text
최종 예측 레이블 : [1]
```

- 이어지지 않는 두 개의 문장으로 테스트
- 예측 결과: 이어지지 않는다고 판단.

> **자료 출처**
>
> https://wikidocs.net/156774


