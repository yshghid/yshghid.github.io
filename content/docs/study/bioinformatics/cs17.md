---
date : 2024-12-31
tags: ['2024-12']
categories: ['deep learning']
bookHidden: true
title: "구글 BERT의 정석 | BERT의 파생 모델"
bookComments: true
---

# [딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT

## 목록

*2024-12-31* ⋯ [4.1 ALBERT](https://yshghid.github.io/docs/study/cs/cs17/#41-albert)

*2024-12-31* ⋯ [4.3 RoBERTa](https://yshghid.github.io/docs/study/cs/cs17/#43-roberta)

*2024-12-31* ⋯ [4.4 ELECTRA](https://yshghid.github.io/docs/study/cs/cs17/#44-electra)

---

## 4.1 ALBERT

> ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.

### 크로스 레이어 변수 공유

- BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다.
- ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.

### 펙토라이즈 임베딩 변수화

- BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.
- ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션).
  - 행렬1: vocab_size x embedding_size
  - 행렬2: embedding_size x hidden_size

### 문장 순서 예측

- BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용.
  - NSP은 두 문장이 연속적으로 존재하는지를 예측하는 태스크. 문장 간 관계를 학습하는 데 사용됨.
- ALBERT는 문장 순서 예측 (SOP, Sentence Order Prediction) 라는 새로운 학습 태스크를 도입.
  - 두 문장이 주어졌을 때, 두 문장의 순서가 올바른지를 예측함.

- SOP는 문장 간의 순서 관계를 이해하는 데 NSP보다 적합하다.

### ALBERT와 BERT 비교

1. 크로스 레이어 변수 공유: ALBERT는 여러 레이어에서 파라미터를 공유하여 파라미터 수를 크게 줄임. BERT는 각 레이어마다 독립적인 파라미터 집합을 사용.
2. 펙토라이즈 임베딩: ALBERT는 임베딩 행렬을 분해하여 파라미터 수를 줄임. BERT는 한 번에 큰 임베딩 행렬을 사용.
3. 문장 순서 예측 (SOP): ALBERT는 NSP 대신 SOP를 사용하여 문장 순서를 더 잘 예측할 수 있게 하여, 문장 간 관계 학습을 개선.

### ALBERT에서 임베딩 추출

1. 단어 임베딩
   - 입력 텍스트의 각 단어를 vocab_size x embedding_size 크기의 행렬을 사용하여 고차원 벡터로 변환함. 이 벡터는 각 단어의 의미를 반영하는 고차원적인 특징을 갖고있다.

2. 레벨 별 임베딩 추출 (Layer-wise Embedding Extraction)
   - 입력 텍스트가 Transformer 모델을 통과하면서 각 레이어에서 벡터 표현이 점진적으로 변환된다.
   - ALBERT에서는 주로 첫 번째 레이어 또는 최종 레이어에서 추출된 임베딩을 사용할 수 있음.
   - 첫 번째 레이어에는 주로 단어의 기본적인 의미와 구조적 특징 정보.
   - 최종 레이어에는 문장 전체의 복합적인 의미와 문맥이 결합되어, 더 구체화되고 세부적인 정보.

3. 중간 레이어에서의 임베딩 추출
  - ALBERT는 다중 레이어 구조를 갖기 때문에, 중간 레이어의 출력도 사용할 수 있음.
  - 문장 내 특정 단어의 문맥을 더 잘 반영하는 중간 레이어의 임베딩을 추출할 수 있다.
  - 사용자가 수행하는 작업에 따라, 특정 작업에 적합한 레이어의 출력을 선택.
    - 예를 들어, 문장 분류 작업에서는 모델의 최종 레이어에서 추출된 임베딩이 더 중요할 수 있으며, 개체명 인식(NER) 작업에서는 중간 레이어에서 나온 임베딩이 더 유용할 수 있다.

---

## 4.3 RoBERTa

### 정적 마스크 대신 동적 마스크 사용

- BERT는 정적 마스크 (Static Masking) 방식을 사용하여 훈련함. 훈련 데이터에서 마스킹할 단어를 고르고, 그 마스크를 모든 훈련 단계에서 동일하게 유지한다. 즉 같은 단어가 훈련 내내 계속 마스크된다.
- RoBERTa는 동적 마스크 (Dynamic Masking) 방식을 사용. 즉, 각 훈련 배치마다 문장에서 마스크되는 단어가 랜덤하게 변경된다.
- 정적 마스크에서는 동일한 문맥을 반복해서 학습하므로, 모델이 특정 단어의 패턴을 암기할 수 있는데, 동적 마스크에서는 훈련마다 마스크가 달라져 모델이 더 다양한 방식으로 문맥을 학습할 수 있도록 돕고, 일관된 마스크 패턴에 의한 편향을 줄여 모델이 더 일반화된 특징을 학습할 수 있게 해준다.

### NSP 테스크 제거

- BERT 모델은 훈련 과정에서 MLM, NSP 테스크를 사용한다.
- RoBERTa는 NSP 대신 MLM만을 사용하여 훈련을 진행. NSP 제거의 이유는 문장 간의 관계 학습에 NSP가 크게 기여하지 않으며 제거 시 훈련이 더 간단해지고, 모델이 더욱 집중해서 문맥을 학습할 수 있음.


### 더 많은 데이터로 학습

- RoBERTa는 BERT보다 훨씬 더 많은 데이터로 훈련. BERT는 16GB 크기의 BooksCorpus와 English Wikipedia로 훈련되었지만, RoBERTa는 여기에 추가로 Common Crawl 데이터, CC-News, OpenWebText, Stories 등의 더 많은 데이터를 포함하여 훈련됨.

### 큰 배치 크기로 학습 

- RoBERTa는 훈련에 더 큰 배치 크기를 사용합니다. BERT는 일반적으로 배치 크기를 32 또는 64로 설정하여 훈련하지만, RoBERTa는 배치 크기 8,000까지 사용하여 훈련했습니다.

> **큰 배치 크기?**
> - 배치가 크면 모델이 더 많은 데이터를 한 번에 처리할 수 있게 해주고, 훈련 속도를 높이는 데 기여함.
> - 학습 안정성을 높여, 학습 과정에서 발생할 수 있는 불안정한 그래디언트 문제를 완화하는 데 도움을 줌.

### BBPE 토크나이저 사용

- BERT는 WordPiece 토크나이저를 사용하여 텍스트를 서브워드 단위로 분할.
- RoBERTa는 BBPE (Byte Pair Encoding) 토크나이저를 사용. 단어를 자주 발생하는 문자쌍으로 분할하여 서브워드 토큰을 만든다.
  - 이는 드문 단어나 외래어가 포함된 텍스트에서 더욱 효과적임.
- BBPE는 단어를 더 작은 조각으로 나누고, 이를 더 자주 사용되는 문자쌍으로 합치는 방식으로 작동함.
- 효과: 어휘 집합 크기를 줄이면서도 다양한 단어를 처리할 수 있게 해주며, 모델의 효율성을 높이고, 모든 언어에서 유연한 처리가 가능.

---

## 4.4 ELECTRA

### 교체한 토큰 판별 테스크

- BERT와 같은 기존 모델들은 일부 단어를 마스킹하고 예측하는 방식(Masked Language Modeling, MLM)을 사용해서 모델을 학습.
- 이 방식은 마스크된 단어의 예측이 실제 문맥을 잘 반영하지 않게 될 수 있다는 단점이 있다.
- ELECTRA는 교체한 토큰 판별 테스크 (Replaced Token Detection) 를 사용.
- 이 방식은 문장을 구성하는 각 토큰이 원래의 문장에서 그대로 있었는지 아니면 다른 토큰으로 교체되었는지를 구분하는 문제이며 이렇게 하면 모델은 교체된 단어를 구별하는 법을 배운다.

### ELECTRA의 생성자와 판별자 

- ELECTRA는 두 가지 모델로 구성된다.

1. 생성자 (Generator)
   - 기존 BERT와 같은 Masked Language Model (MLM) 구조.
   - 입력 문장에서 일부 단어를 [MASK]로 변환한 후, 이를 생성자의 예측 값으로 대체함.

2. 판별자 (Discriminator)
   - 문장 내 각 토큰이 진짜인지(fake) 가짜인지(real)를 분류하는 이진 분류(Binary Classification) 문제를 해결.

### ELECTRA 모델 학습

1. 생성자 학습
   - 문장에서 일부 단어를 마스킹한 후, 생성자가 그 단어를 예측.
   - 예측된 단어는 원래 단어 대신 교체된 단어(replaced token)로 사용됨.

2. 판별자 학습
   - 생성자가 만든 교체된 단어를 포함한 문장을 입력받음.
   - 판별자는 문장 내 각 단어가 원래 단어인지, 교체된 단어인지 판별하는 작업을 수행.
   - 판별자가 더 정확한 예측을 할수록 모델의 언어 이해 능력이 향상됨.

3. 손실 함수 계산
   - 생성자는 Cross-Entropy Loss (MLM 방식)
   - 판별자는 Binary Classification Loss (Replaced Token Detection 방식)

4. 반복 학습
   - 생성자의 성능이 향상될수록 판별자의 분류 작업이 더 어려워짐.
   - 결국 판별자가 더 정교한 문맥 이해 능력을 갖도록 최적화됨.


### 효율적인 학습 방법 탐색

> ELECTRA 모델을 효율적으로 학습시키기 위해서 생성자와 판별자의 가중치를 공유한다.

- 기존 BERT는 마스킹된 토큰만 학습에 사용하지만, ELECTRA는 모든 토큰을 판별 작업에 사용하여 훨씬 더 높은 학습 데이터 활용률을 가짐.
- 기존의 MLM 방식보다 80% 적은 연산량으로 동일한 성능을 유지, 동일한 연산량을 사용했을 때 BERT보다 2~4배 더 빠르게 학습 가능.

- 생성자는 BERT와 같은 크기를 사용할 필요가 없어서, 생성자를 작은 크기의 모델로 설정하여 연산량을 절감.
- ELECTRA-Small (14M parameters) → BERT-Small보다 86% 더 높은 성능 / ELECTRA-Large는 BERT-Large보다 적은 연산량으로 더 높은 성능을 보임.


