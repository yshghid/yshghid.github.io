---
date : 2025-09-22
tags: ['2025-09']
categories: ['SW']
bookHidden: true
title: "Langchain #2 LECL 템플릿"
---

# Langchain #2 LECL 템플릿

#2025-09-22

---

### <mark>1. 네이버 뉴스 기반 검색 수행</mark>

#1 실습개요

- 실습 목적
  - 네이버 뉴스 기사를 수집·정제하여 벡터 DB(Chroma)에 임베딩으로 저장하고, 이를 기반으로 RAG(Retrieval-Augmented Generation) 구조를 구현하여 질문응답을 수행하고 Gradio UI를 통해 시각화된 인터페이스로 제공
- 실습 설계
  - 뉴스 데이터 수집: requests + Naver Open API -> 키워드별로 네이버 뉴스 기사 링크 수집
  - 문서 로딩 및 정제: WebBaseLoader + bs4(BeautifulSoup) -> 기사 본문만 추출하고 노이즈 텍스트 제거
  - 문서 분할: RecursiveCharacterTextSplitter -> 긴 본문을 2000자 단위로 청크 분할
  - 임베딩 생성 및 벡터 DB 구축: OpenAIEmbeddings(text-embedding-3-small) + Chroma -> 문서를 벡터화하여 저장 및 검색 가능 DB 생성
  - RAG 체인 구성: ChatPromptTemplate + ChatOpenAI(gpt-4o-mini) + RunnablePassthrough -> 검색된 문서를 기반으로 LLM이 답변 생성
  - 질문응답 실행: retriever.invoke() + 체인 실행 -> 사용자 질의에 대해 관련 기사 기반 답변 도출
  - Gradio UI 구현: gr.Blocks -> 검색 쿼리, 뉴스 기사 수, 질문 입력을 받아 RAG 결과를 실시간으로 출력하는 인터페이스 제공

###

#2 RAG?

- RAG는 Retrieval-Augmented Generation (RAG) 의 약자로, 질문이 주어지면 관련 있는 문서를 찾아 프롬프트에 추가하는 방식의 어플리케이션입니다.   
- RAG의 과정은 아래와 같이 진행됩니다.
  1. Indexing : 문서를 받아 검색이 잘 되도록 저장합니다.
  2. Processing : 입력 쿼리를 전처리하여 검색에 적절한 형태로 변환합니다<br>(여기서는 수행하지 않습니다)
  3. Search(Retrieval) : 질문이 주어진 상황에서 가장 필요한 참고자료를 검색합니다.
  4. Augmenting : Retrieval의 결과와 입력 프롬프트를 이용해 LLM에 전달할 프롬프트를 생성합니다.
  5. Generation : LLM이 출력을 생성합니다.

###

### 2. SerpAPI 기반 PDF, SQL, 웹 서칭 통합 검색 수행

#1

- 실습 목적
  - PDF 문서, SQLite DB, 웹 검색 결과를 통합하여 벡터 DB(FAISS) + LLM(OpenAI GPT) 기반 RAG 파이프라인을 구축하고, SQL/웹/문서 정보를 함께 활용한 질의응답 시스템을 구현
- 실습 설계
  - SQLite DB 초기화: sqlite3 -> 예제 문장을 포함한 articles 테이블 생성 및 질의어(SQL 검색) 테스트 환경 구축
  - 문서 임베딩 및 벡터 DB 생성: PyPDFLoader + RecursiveCharacterTextSplitter + OpenAIEmbeddings + FAISS -> PDF를 청크 단위로 분할 후 임베딩하여 로컬 벡터 DB에 저장
  - 문서 검색: FAISS.load_local + retriever.invoke() -> 쿼리에 대해 유사도가 높은 청크 검색
  - 검색된 문서 요약: PromptTemplate + ChatOpenAI -> 문서 청크를 LLM으로 간단 요약
  - SQL 검색: SQLDatabase -> RAG DB(rdb_rag.db)에서 관련된 레코드를 LIKE 조건으로 질의
  - 웹 스크래핑: serpapi.GoogleSearch -> Google Search API를 활용해 웹 결과 요약 추출
  - RAG 파이프라인 통합: ChatOpenAI + 사용자 쿼리 + Vector 요약 + SQL 검색 + 웹 검색 -> 종합 프롬프트 작성 후 최종 답변 생성
  - 출처 제시: VectorDB, SQL, 웹 검색 각각의 결과를 별도 출력하여 투명한 근거 기반 QA 제공

###

#2

- 응답이 이런식인데 그래서 무슨 의미인가?


- 구조화된 구글 검색 정보.
- serpapi가 크롤링이랑 비교했을때 장점?
  - 크롤링보다 안정적·표준화된 JSON으로 제공해해서 RAG나 LLM 응용에 바로 연결할 수 있다.
  - SQL/웹/문서 정보를 함께 활용할수있다.

###

### 3. LangGraph, SerpAPI 기반 Tool Calling Agent 생성

- 실습 목적
  - LangGraph 기반 워크플로우에서 OpenAI LLM(gpt-4o)과 SerpAPI 웹 검색 툴을 결합하여, 사용자 질의에 맞는 정보를 자동 검색·요약하는 Tool Calling Agent 시스템을 구현
- 실습 설계
  - 웹 검색 툴 정의: @tool + GoogleSearch(serpapi) -> 구글 검색 API를 호출하여 상위 organic_results 제목을 추출
  - 프롬프트 설계: ChatPromptTemplate + MessagesPlaceholder -> 시스템 역할(검색 도우미) 정의 및 대화 이력, scratchpad 반영
  - LLM 구성: ChatOpenAI(model="gpt-4o") -> Tool 호출을 지원하는 최신 OpenAI 모델 설정
  - Tool Calling Agent 생성: create_tool_calling_agent + AgentExecutor -> LLM과 웹 검색 툴을 결합해 질의 시 Tool 호출 자동화
  - LangGraph 워크플로우: StateGraph -> 상태(AgentState)를 정의하고 run_agent_node에서 Agent 실행, 그래프의 entry/finish point를 agent로 설정
  - 실행 및 결과 출력: app.invoke() -> HumanMessage 입력을 기반으로 Agent 실행, 웹 검색 툴 호출, 검색 결과 요약을 포함한 최종 응답 반환

###

### 4. Tool Calling LLM RAG 시스템 구현

- 실습 목적
  - PDF 문서와 네이버 뉴스 데이터를 벡터 DB(FAISS)에 임베딩하여 저장하고, LLM(gpt-4) 기반 에이전트가 Tool Calling을 통해 계산·임베딩·웹 검색·요약 및 빈도 분석을 자동으로 수행하는 통합 RAG 시스템을 구현
- 실습 설계
  - PDF 임베딩: PyMuPDFLoader + OpenAIEmbeddings + FAISS → 지정된 PDF를 로드하고 텍스트를 임베딩하여 벡터 DB에 저장
  - 웹 뉴스 트렌드 임베딩: requests + BeautifulSoup + Naver News Open API + FAISS → 키워드 기반으로 최신 뉴스 데이터를 수집, 정제 후 벡터 DB에 저장
  - 계산 기능: @tool + eval → 사용자가 입력한 수식을 직접 계산하는 계산기 기능 제공
  - 키워드 요약 및 빈도 분석: RecursiveCharacterTextSplitter + 정규표현식(re) + Counter → PDF 및 웹 데이터에서 특정 키워드 관련 내용을 요약하고 등장 횟수를 계산
  - 에이전트 실행: ChatOpenAI(model="gpt-4") + initialize_agent(AgentType.OPENAI_FUNCTIONS) → LLM이 자연어 질의를 해석하여 적절한 Tool을 호출하고 결과를 반환
  - 시나리오 테스트:
    - "다음 PDF 파일을 벡터 DB에 저장" → PDF 임베딩 실행
    - "GPU 라는 키워드로 뉴스 트렌드 검색" → 네이버 뉴스 크롤링 후 임베딩
    - "GPU 키워드에 대해 PDF와 웹 요약 및 단어 빈도" → 두 소스를 통합 분석 후 요약 및 등장 횟수 산출


```plain text
✅ 'GPU'에 대한 뉴스 트렌드가 벡터 DB에 저장되었습니다.'GPU'에 대한 뉴스 트렌드를 성공적으로 검색하고 벡터 DB에 저장했습니다. 다른 도움이 필요하시면 알려주세요.

> Finished chain.
{'input': 'GPU 라는 키워드로 뉴스 트렌드를 검색하라.', 'output': "'GPU'에 대한 뉴스 트렌드를 성공적으로 검색하고 벡터 DB에 저장했습니다. 다른 도움이 필요하시면 알려주세요."}


> Entering new AgentExecutor chain...

Invoking: `summarize_and_count` with `{'keyword': 'GPU'}`


📘 PDF 요약: 1
Introduction
Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numero...
🕸 웹 트렌드 요약: 광주 AI산업 '어디까지 왔나'…급속 확장에도 구조적 과제 산적
더불어 GPU는 H100·A100 등 2000대 규모로, 관계기관 설명에 따르면 국내 보유량의 절반 이상을 차지하고 있다. 전문가들은 해결 과제로 ▲수요 다변화 ▲인재 정착 ▲융복합 생태계 연계를 우선 꼽았다. 구체적으로...
AI 양대 산맥이 손잡았다…엔비디아·오픈AI, '10GW 초거대 두뇌' 세운다
이어 &quot;10기가와트는 400만∼500만 개 GPU(그래픽처리장치)에 해당하며, 이는 엔비디아가 올해 출하할 총량과 같고 작년 대비 두 배&quot;라고 말했다...
'📊 등장 횟수 - PDF: 7회, 웹: 5회GPU에 대한 요약은 다음과 같습니다:

PDF에서는 GPU가 7번 언급되었습니다. 주요 내용은 순환 신경망, 특히 장기 단기 메모리와 게이트 순환 신경망이 언어 모델링과 기계 번역과 같은 시퀀스 모델링 및 전사 문제에서 최첨단 접근법으로 확고하게 자리 잡았다는 것입니다.

웹에서는 GPU가 5번 언급되었습니다. 주요 내용은 광주 AI산업의 현황과 그 중 GPU의 역할에 대한 내용입니다. GPU는 H100·A100 등 2000대 규모로, 국내 보유량의 절반 이상을 차지하고 있다고 합니다. 또한, 엔비디아와 오픈AI가 '10GW 초거대 두뇌'를 구축하고 있으며, 이는 400만∼500만 개의 GPU에 해당한다고 합니다.

> Finished chain.
{'input': 'GPU 라는 키워드에 대해 PDF와 웹을 요약하고 자주 나온 단어를 제시하라.', 'output': "GPU에 대한 요약은 다음과 같습니다:\n\nPDF에서는 GPU가 7번 언급되었습니다. 주요 내용은 순환 신경망, 특히 장기 단기 메모리와 게이트 순환 신경망이 언어 모델링과 기계 번역과 같은 시퀀스 모델링 및 전사 문제에서 최첨단 접근법으로 확고하게 자리 잡았다는 것입니다.\n\n웹에서는 GPU가 5번 언급되었습니다. 주요 내용은 광주 AI산업의 현황과 그 중 GPU의 역할에 대한 내용입니다. GPU는 H100·A100 등 2000대 규모로, 국내 보유량의 절반 이상을 차지하고 있다고 합니다. 또한, 엔비디아와 오픈AI가 '10GW 초거대 두뇌'를 구축하고 있으며, 이는 400만∼500만 개의 GPU에 해당한다고 합니다."}
```

###

### 5 LangGraph 기반 질의 분류 Agent 구현

- 실습 목적
  - LangGraph를 활용해 질의 분류(router)를 통해 웹 검색(SerpAPI)과 산술 계산(ast 기반 수식 파서, LangChain Tool calc_add)을 자동으로 분기 실행하는 멀티 도구 기반 AI 에이전트를 구현
- 실습 설계
  - 질의 라우팅: StateGraph + 정규식(re) 기반 is_math_query() -> 사용자의 입력을 수학식 여부에 따라 "web" 또는 "math"로 분기
  - 웹 검색 처리: SerpAPI(GoogleSearch) + web_scraper Tool -> 구글 검색 API를 호출하여 상위 검색 결과 제목을 요약 반환
  - 산술 계산 처리: ast 모듈 기반 _eval_expr → 안전한 파싱 후 operator 연산자로 계산 수행, 또는 Tool calc_add를 통한 덧셈 실행
  - 그래프 실행 플로우: router -> (web_node | math_node) 구조 -> 입력 질의가 자동으로 적합한 노드로 전달되어 처리
  - LLM + Tool AgentExecutor: ChatOpenAI(model="gpt-4") + create_openai_functions_agent -> 프롬프트(ChatPromptTemplate)와 함께 각각 웹 검색 전용, 수학 연산 전용 에이전트 실행
  - 테스트 시나리오:
    - "3 + 5" → router에서 수학 질의로 분류 -> math_node에서 산술 계산
    - "LangGraph 사용법" → router에서 일반 질의로 분류 -> web_node에서 웹 검색 요약
    - "3과 5를 더하라" -> 수학 전용 에이전트(calc_add) Tool 호출로 결과 반환
    - "LangChain을 검색해서 요약하라" -> 웹 전용 에이전트(web_scraper) Tool 호출로 결과 반환

#
