---
date : 2025-06-26
tags: ['2025-06']
categories: ['AI']
bookHidden: true
title: "#3 Random Forest"
bookComments: true
---

# #3 Random Forest

#2025-06-26

---

### 1. Random Forest의 분류와 회귀

랜덤 포레스트(Random Forest)는 
- RandomForestClassifier: 분류용
- RandomForestRegressor: 회귀용 이다.

분류와 회귀의 핵심 차이는
- 분류는 각 leaf node에 속한 클래스의 비율을 기반으로 확률 예측
- 회귀는 leaf node에 있는 target 값들의 평균을 예측값으로 사용

랜덤 포레스트의 트리 구조(= 리프 분기 방식)는 분류나 회귀나 똑같고
- 단지 리프 노드에 어떤 데이터 형식이 들어가느냐에 따라
  - 분류이면 라벨 비율(확률 분포)
  - 회귀이면 값의 평균으로 예측을 내놓는다

### 2. 트리 기반 모델과 클러스터링의 차이

랜덤 포레스트(혹은 결정 트리)의 리프 분기 방식은 '거리 기반'이 아님
- 대신, 목표 변수(y)를 가장 잘 구분할 수 있도록
  - feature를 기준으로 데이터 공간을 분할한다.

분기 기준
- 분류 문제
  - Gini 불순도, Entropy(정보이득) 등을 기준으로
    - 분기를 통해 클래스가 더 순수하게 나뉘도록 자름
- 회귀 문제
  - MSE (Mean Squared Error) 또는 MAE (평균 절댓값 오차) 감소가 큰 방향으로 분기

- 분기의 본질은 분기를 할 때 두 점 사이의 거리를 따지지 않음. 대신 "어떤 feature에서 자르면, y가 더 잘 나눠지냐"만을 고려함.

궁극적으로 차이점

| 항목 | 결정 트리 / 랜덤 포레스트 | 계층적 클러스터링 |
| --- | --- | --- |
| 학습 방식 | 지도 학습 (y 필요) | 비지도 학습 (y 없음) |
| 분기 기준 | y를 잘 나누는 feature 기준 | 입력 간 거리 기준 |
| 분할 구조 | 트리 구조 (특정 feature 기준 분할) | 덴드로그램 구조 (거리 기반 병합/분할) |
| 목적 | 예측 성능 향상 | 그룹 내 유사성 확보 |
| 거리 개념 | 사용 안 함 | 핵심 기준 |

학습 방식이 
- RF는 지도 학습으로 y필요, 클러스터링은 비지도 학습으로 y 불필요
분기 기준이
- RF는 y를 잘 나누는 feature 기준, 클러스터링은 입력 간 거리 기준
목적이
- RF는 얘측 성능 향상, 클러스터링은 그룹 내 유사성 확보.

### 3. 트리 기반 모델과 클러스터링의 차이 (2)

랜덤 포레스트(RF)의 분기 조건이 리프 내 순도(클래스의 동질성)를 높이는 거라면, 클러스터링의 목적(그룹 내 유사성 확보)과 본질적으로 같은 거 아닌가?

핵심 차이 1: 무엇을 기준으로 유사하다고 보는지.
- 결정트리 (RF)는 "예측값 y가 비슷하면 유사하다"고 생각함. 즉, 입력 X가 다르더라도 y가 비슷하면 같은 노드로 분기
- 클러스터링은 "입력 값 X가 비슷하면 유사하다"고 생각함 즉 y는 고려하지 않음
  - 예를 들어 두 환자의 면역 프로파일이 완전히 달라도 둘 다 사망(y=1)이라면, RF는 둘을 같은 리프에 보낼 수 있다.
    - 반대로 클러스터링은 면역 프로파일이 다르면 y와 무관하게 다른 그룹으로 나눈다.

핵심 차이 2: 지도 vs 비지도
- RF는 정답(y)이 있는 지도학습이고
- 클러스터링은 y 없이 입력 X의 분포만으로 구조를 파악
  - 즉 클러스터링은 "데이터 간 관계"에 집중, RF는 "데이터와 정답 간 관계"에 집중

예시
- Feature X1, X2로 된 점 100개
- Class 0/1 이 섞여 있음
  - Random Forest: 어떤 feature (예: X1 < 5)로 나눴더니 클래스 0/1이 잘 나뉜다 -> 분기 수행. 이 과정에서 X 간의 거리나 모양은 고려 안 한다.
  - 클러스터링: X1, X2 기준으로 거리상 가까운 점들끼리 묶음. 클래스(y) 정보는 전혀 고려하지 않는다.
 
비슷해보이는 이유
- 결정트리는 리프 내 클래스가 비슷해지도록 데이터를 쪼개다 보니 결국 리프 안의 X 값들도 어느 정도 비슷해지는 경향이 발생.
- 이 때문에 시각적으로 보면 "트리가 일종의 분할 기반 클러스터링"처럼 보이기도 함 특히, y 자체가 X의 분포에 강하게 의존할 경우에는 트리 분기 ≈ 거리 기반 분할처럼 보인다.

하지만 유사성이 목표인지 수단인지가 다름:
- 클러스터링은	목표 자체
- 결정 트리 / RF는 예측을 위한 수단.
  - 둘 다 "비슷한 것들끼리 묶는다"는 점에서 결과적으로 유사한 구조를 만들 수 있지만 클러스터링은 유사성 자체가 목적 결정 트리는 예측을 위한 수단으로 유사한 샘플을 묶을 뿐.

