---
date : 2025-09-29
tags: ['2025-09']
categories: ['BE']
bookHidden: true
title: "Langchain #3 주간 보고서 자동 생성 AI"
---

# Langchain #3 주간 보고서 자동 생성 AI

#2025-09-29

---

### 1. 프로젝트 개요

- 목적
  - Slack/Notion/Outlook/OneDrive 데이터를 API로 수집하여 LangChain Document 형식으로 통합·저장하고, OpenAI GPT-4o 기반 정형화된 주간 보고서를 생성
  - 전 직원 보고서를 벡터화하여 Vector DB에 보관한 뒤, 관리자의 질의와 유사한 보고서를 검색·활용해 프로젝트 별 관리자 요약을 자동 생성

- 서비스 플로우
  - 데이터 수집: Slack API / Notion API / Microsoft API를 통해 플랫폼별 활동 데이터를 수집하고, 이를 LangChain Document 공통 스키마로 변환하여 DB에 저장
  - 임베딩 생성: SentenceTransformer("all-MiniLM-L6-v2") 모델을 사용해 보고서 및 활동 데이터를 벡터화하여 Vector DB(pgvector)에 저장
  - 보고서 저장: 프로젝트·직원·작업시간 단위로 데이터가 자동 필터링되어 DB에 기록되며, 보고서는 정형화된 템플릿 형태로 저장
  - 유사도 검색: 관리자의 요청(admin_request)을 벡터화하여 Vector DB(pgvector)에서 가장 유사한 보고서를 코사인 유사도로 검색
  - 자동 요약: 검색된 보고서를 LangChain 프롬프트 체인을 통해 GPT-4o와 연결하여, 단순 요약이 아닌 항목화된 정형 보고서(성과/문제/계획)로 생성
  - 프로젝트 분류: 추후에는 Vector DB 내 프로젝트 키워드와 보고서 유사도 검색을 통해 프로젝트 ID를 자동 할당(현재는 더미 데이터 기반 ID 할당)
  - 관리자 활용: 관리자는 특정 키워드 기반으로 요청 시 단순 전체 요약이 아니라 RAG 기반 요청 특이적 요약을 제공받음
  - API 제공: FastAPI를 통해 /reports/weekly(주간 보고서 생성 + 저장)과 /api/generate-summary(관리자 요청 기반 요약) 등 REST API 형태로 제공

###

### 2. 구현 흐름 정리

0. revised.sql을 기반으로 db 구축
1. db에서 sql문 수행 -> all_platform_data 생성 
2. all_platform_data -> grouped 생성 
3. grouped를 platform_data로 전달 
4. platform_data의 content만 뽑아서 Langchain Document 리스트 생성 (docs) 
5. docs 안의 page_content들을 합쳐서 content 문자열 생성 
6. LLM 체인을 실행해서 최종 보고서 generated_report 생성
7. generated_report를 report 테이블의 report 컬럼에 저장 
8. generated_report를 임베딩해서 report 테이블의 report_embedded 컬럼에 저장 
9. task_id, start_date, end_date, admin_request를 입력받아
10. admin_request를 임베딩해서 admin_request_embedded를 생성하고
11. report 테이블의 report_embedded 컬럼의 값 중 task_id, start_date, end_date 에 해당하는 값들을 rep_embedded_candidate 리스트로 받아와서 
12. admin_request_embedded와 유사도 측정, top k relevant를 반환
13. top k relevant에 해당하는 보고서 문자열을 report 테이블의 report 컬럼으로부터 가져와서 하나로 합쳐서 report_context 문자열을 생성
14. LLM 체인을 실행해서 관리자 요약 보고서 생성

###

### 3. 코드 정리 - 주간 보고서 생성

1. db에서 sql문 수행 -> all_platform_data 생성 
2.all_platform_data -> grouped 생성 
3. grouped를 platform_data로 전달 
4. platform_data의 content만 뽑아서 Langchain Document 리스트 생성 (docs) 
5. docs 안의 page_content들을 합쳐서 content 문자열 생성 
6. LLM 체인을 실행해서 최종 보고서 생성

###

#1 db에서 sql문 수행 -> all_platform_data 생성

```python
# --- 주간 보고서 생성 ---
@app.post("/reports/weekly")
async def make_weekly_report(p: ReportIn, session: AsyncSession = Depends(get_db_session)):
    reports = []

    # 1. 모든 플랫폼 데이터 수집
    all_platform_data = []
    for platform, ids in p.platform_ids.items():
        if not ids:
            continue

        query = None
        if platform == "slack":
            query = text("SELECT id, content, sender AS actor, receiver, task_id, \"timestamp\"::text as ts FROM public.slack WHERE id = ANY(:ids)")
        elif platform == "notion":
            query = text("SELECT id, content, NULL as actor, task_id, \"timestamp\"::text as ts FROM public.notion WHERE id = ANY(:ids)")
        elif platform == "outlook":
            query = text("SELECT id, content, sender AS actor, receiver, task_id, \"timestamp\"::text as ts FROM public.outlook WHERE id = ANY(:ids)")
        elif platform == "onedrive":
            query = text("SELECT id, content, writer AS actor, task_id, \"timestamp\"::text as ts FROM public.onedrive WHERE id = ANY(:ids)")

        # ✅ 여기 수정됨
        if query is not None:
            result = await session.execute(query, {"ids": ids})
            rows = [dict(r._mapping) for r in result.fetchall()]
            all_platform_data.extend(rows)
```

- function: `make_weekly_report`
- input: p (ReportIn), session - AsyncSession
- output: all_platform_data - List[Dict[str, Any]

###

#2 all_platform_data → grouped 생성

```python
    # 2. task_id별 그룹핑
    grouped = {}
    for d in all_platform_data:
        task_id = d.get("task_id")
        if not task_id:
            continue
        grouped.setdefault(task_id, []).append(d)
```

- function: `make_weekly_report`
- input: all_platform_data - `List[Dict[str, Any]]`
- output: grouped - `Dict[int, List[Dict[str, Any]]]`


###

#3 grouped를 platform_data로 전달

```python
# func make_weekly_report 
    # 3. 보고서 생성
    for task_id, items in grouped.items():
        task_id_int = int(task_id)
        report_md = await generate_report_for_task(task_id_int, items, p.start, p.end, session)
        
# func generate_report_for_task
async def generate_report_for_task(task_id: int, platform_data: List[Dict[str, Any]], start_ts: str, end_ts str, session: AsyncSession) -> str:
```

- function: `make_weekly_report`, `generate_report_for_task`
- input: grouped - `List[Dict[str, Any]]`
- output: platform_data - `List[Dict[str, Any]]`

###

#4 platform_data의 content만 뽑아서 LangChain Document 리스트 생성

```python
async def generate_report_for_task(task_id: int, platform_data: List[Dict[str, Any]], start_ts: str, end_ts: str, session: AsyncSession) -> str:
    # 4. LangChain Document 리스트 생성
    docs = [Document(page_content=d.get("content", "")) for d in platform_data]
```

- function: `generate_report_for_task`
- input: platform_data - `List[Dict[str, Any]]`
- output: docs - `List[Document]` (`langchain.schema.Document`)

###

#5 docs 안의 page_content들을 합쳐서 content 문자열 생성

```python
    # 5. docs 안의 page_content들을 합쳐서 content 문자열 생성
    actors = {d.get("actor") for d in platform_data if d.get("actor")}
    actor_list = "- " + "\n- ".join(actors) if actors else "- (none)"
    task_description = await get_task_description(task_id, session)
    context = "\n".join([doc.page_content for doc in docs])
```

###

#6 LLM 체인을 실행해서 최종 보고서 generated_report 생성

```python
llm = ChatOpenAI(model="gpt-4o", temperature=0.3, api_key=OPENAI_API_KEY)

output_parser = StrOutputParser()

REPORT_TEMPLATE = """
## 1) 주간 요약
Task {task_id} ({task_description}) 관련 진행 상황 요약:
{context}

## 2) 사람별 주요 산출물
{member_list}

## 3) 협업 내역
Slack/Notion/Outlook/OneDrive 기록 기반 협업 내역 정리.

## 4) 리스크/이슈
문제점, 리스크, 해결 필요 사항.

## 5) 차주 계획
후속 작업 및 개선점.

(기간: {start} ~ {end})
"""

report_prompt = PromptTemplate(
    template=REPORT_TEMPLATE,
    input_variables=["context", "task_id", "task_description", "member_list", "start", "end"],
)

async def generate_report_for_task(task_id: int, platform_data: List[Dict[str, Any]], start_ts: str, end_ts: str, session: AsyncSession) -> str:
    
    # 6. LLM 체인을 실행해서 최종 보고서 생성
    chain = report_prompt | llm | output_parser
    body = await chain.ainvoke({
        "context": context,
        "task_id": task_id,
        "task_description": task_description,
        "member_list": actor_list,
        "start": start_ts,
        "end": end_ts,
    })
    
    generated_report = f"# 업무 {task_id}: {task_description} 주간 보고서\n\n{body}" 
    return generated_report
```

- function: `generate_report_for_task`
- input
    - chain(`RunnableSequence`) - report_prompt (`PromptTemplate`), llm(`ChatOpenAI`), output_parser(`StrOutputParser`)
    - body (`Dict[str, str | int]`) - context, task_description, actor_list, start_ts, end_ts (`str`), task_id (`int`)
- output
    - generated_report - `str`

###

이어서 구현해야햇던 내용은?

- 기존: task_id를 받아서 해당 task_id에 해당하는 보고서를 llm에 넣어서 관리자 요약 생성
- 요청1: 전 직원 보고서를 벡터화하여 Vector DB에 보관한 뒤 RAG를 적용해보자
  - 적용 방안 1: task_id 할당 구현
    - 직원들이 참여중인 프로젝트 명이 정해져있다고 할때, 각 업무자료 및 보고서에 task_id를 적용하기 위해서 업무자료를 vector db에 보관한 뒤 프로젝트 명과의 유사도를 기반으로 task_id를 할당하기.
  - 적용 방안 2: 관리자 요청 보고서 필터링
    - 직원들이 작성한 보고서를 vector db에 보관한 뒤 관리자 요청과의 유사도를 기반으로 요청에 해당하는 보고서를 기반으로 관리자 요약을 생성.
- 요청2: 관리자 ui에서 task_id를 입력하는 부분을 좀더 현실적으로 수정해보자
  - 구현 방안: task_id를 입력하는 대신에 해당 관리자가 참여중인 프로젝트를 토글로 입력받도록 구현.

###

#7 관리자 요약 생성: generate_summary function 

```python
class ReportRequest(BaseModel): 
    start_date: str 
    end_date: str 
    task_name: str 
    admin_request: str 

# --- 요약 생성 - 소현 0927 --- 
@app.post("/api/generate-summary", response_model=ReportResponse) 
async def generate_summary(request: ReportRequest): 
    """기존 관리자 요약API (기존 기능 유지)""" 
    task_mapping = { 
        "프로젝트 1: 온라인 쇼핑몰 시스템 구축": 1, 
        "프로젝트 2: 병원 예약·진료 시스템 통합": 2, 
    } 
    task_id = task_mapping.get(request.task_name) 
    dummy_reports = f"Task {request.task_name} 보고서 (기간 {request.start_date}~{request.end_date})" 
    manager_summary = await manager_chain.ainvoke(
        {"team_reports": dummy_reports}) 
    return ReportResponse(summary=manager_summary)
```

- ReportRequest에서 input을 task_name, admin_request를 받도록 수정하고
- task_mapping 딕셔너리를 통해 task_name으로부터 task_id를 뽑아내는 말도안되는 하드코딩을 하고 아래 프롬프팅을 넣었다.

```plain text
이렇게 수정했거든? 그리고 어떻게 할거냐면 먼저 admin_request 값을 임베딩해서 벡터로 만들어서 request_embedded 로 받아. public.report 테이블에서 task_id 컬럼의 값이 task_id인 행의 report_embedded 컬럼의 값을 current_rep_emb 리스트에 저장해. 그리고 request_embedded와 cosine 유사도가 0.5 이상인 값만 reports_for_summary_embedded 리스트에 저장해. 그리고 reports_for_summary_embedded 리스트 내 값들에 대해서 public.report 테이블에서 report_embedded 컬럼에 해당 값이 있으면 그 행의 report 컬럼의 값을 rep_sum 으로 받아서 reports_for_summary 리스트에 저장해. 그리고 그 리스트 내 값들을 '\n' 으로 붙여서 context_summary라는 하나의 문자열로 만들어. 여기까지 수행하도록 코드 작성해줘
```

그렇게 만들어진 generate_summary 코드

```python
# --- 요약 생성 - 소현 0927 ---
@app.post("/api/generate-summary", response_model=ReportResponse)
async def generate_summary(request: ReportRequest, session: AsyncSession = Depends(get_db_session)):
    """관리자 요약 API (임베딩 + 코사인 유사도 기반 + 디버깅 정보 포함)"""

    task_mapping = {
        "프로젝트 1: 온라인 쇼핑몰 시스템 구축": 1,
        "프로젝트 2: 병원 예약·진료 시스템 통합": 2,
    }
    task_id = task_mapping.get(request.task_name)
    if not task_id:
        raise HTTPException(status_code=400, detail="task_name이 올바르지 않습니다.")

    # 1. admin_request 임베딩 생성
    request_embedded = embedding_service.create_embedding(request.admin_request)
    request_embedded = np.array(request_embedded, dtype=np.float32)

    # 2. 해당 task_id 보고서 조회
    query = text("""
        SELECT id, report, report_embedded
        FROM public.report
        WHERE task_id = :task_id
    """)
    result = await session.execute(query, {"task_id": task_id})
    rows = result.fetchall()

    if not rows:
        return ReportResponse(
            success=False,
            summary="❌ 해당 task_id에 저장된 보고서가 없습니다.",
            used_reports=[],
            similarities=[]
        )

    reports_for_summary = []
    similarities = []

    for row in rows:
        rep_id = row[0]
        rep_content = row[1]
        rep_emb_str = row[2]

        if not rep_emb_str:
            continue

        # PostgreSQL vector → numpy array
        rep_emb = np.array(
            list(map(float, rep_emb_str.strip("[]").split(","))),
            dtype=np.float32
        )

        # cosine similarity 계산
        cosine_sim = float(
            np.dot(request_embedded, rep_emb) /
            (np.linalg.norm(request_embedded) * np.linalg.norm(rep_emb))
        )

        similarities.append({
            "report_id": rep_id,
            "similarity": cosine_sim
        })

        if cosine_sim >= 0.3:
            reports_for_summary.append({
                "report_id": rep_id,
                "content": rep_content,
                "similarity": cosine_sim
            })

    # context_summary 생성
    if reports_for_summary:
        context_summary = "\n".join([r["content"] for r in reports_for_summary])
    else:
        context_summary = "⚠️ 유사도가 0.5 이상인 보고서가 없습니다."

    # LLM 요약 실행
    manager_summary = await manager_chain.ainvoke({"team_reports": context_summary})

    return ReportResponse(
        success=True,
        summary=manager_summary,
        used_reports=reports_for_summary,
        similarities=similarities
    )
```

