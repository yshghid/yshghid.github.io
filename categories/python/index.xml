<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on  </title>
    <link>http://localhost:1313/categories/python/</link>
    <description>Recent content in Python on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>HLA 결합력 변화 비교</title>
      <link>http://localhost:1313/docs/study/tech/tech37/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech37/</guid>
      <description>HLA 결합력 변화 비교 # #2025-06-27&#xA;1. Load package # import pandas as pd import numpy as np 2. Load affinity data # with open(&amp;#39;/data/home/ysh980101/2411/data-mhc/patient_id.txt&amp;#39;, &amp;#39;r&amp;#39;) as file: patients = [line.strip() for line in file] len(patients) 388 #387+reference 3. Merge affinity tables # hotspot = &amp;#34;c315&amp;#34; dfs = [] for pid in patients: file_path = f&amp;#34;/data/home/ysh980101/2411/data-mhc/{hotspot}/{pid}/binding_affinities_HLA-I.csv&amp;#34; df = pd.read_csv(file_path) df.rename(columns={&amp;#39;Affinity&amp;#39;: f&amp;#39;{pid}&amp;#39;}, inplace=True) df.rename(columns={&amp;#39;Peptide&amp;#39;: f&amp;#39;Peptide_{pid}&amp;#39;}, inplace=True) if pid == &amp;#39;reference&amp;#39;: dfs.</description>
    </item>
    <item>
      <title>﹂슈도코드</title>
      <link>http://localhost:1313/docs/study/tech/tech36/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech36/</guid>
      <description>﹂슈도코드 # #2025-06-26&#xA;#Clustering # def DBSCAN(sequence, eps, min_samples): cores = [] clusters = [] for nt in sequence: neighbors = find_neighbors(nt, eps) if len(neighbors) &amp;gt;= min_samples: label of nt = 1 #core append nt in cores for core in cores: label, clusters = expand_cluster(core, neighbors, eps, min_samples) label of nt = -1 for nt in sequence if not in clusters #noise not in cluster return clusters def MUTCLUST(sequence, eps_scaler, dim_factor, min_samples): ccms = [] hscore = [] deps = [] label = [] clusters = [] for nt in sequence: hscore[nt], deps[nt] = calculate_hscore(nt), calculate_deps(nt) append nt in ccms if select_ccm(hscore, deps, min_samples) for ccm in ccms: label of ccm = 1 #core clusters = expand_cluster(ccm, sequence, eps, min_samples, eps_scaler, dim_factor) label of nt = -1 for nt in sequence if not in clusters #noise return hscore, ccms, clusters #functions used in dbscan() def expand_cluster(cur_nt, cur_neighbors, min_samples, clusters): #expand cluster of cur_nt for ne in cur_neighbors: ne_neighbors = find_neighbors(ne, eps) if ne_neighbors &amp;gt;= min_samples: label of ne = 0 #border append ne in clusters[cur_nt] append ne in cur_neighbors else: label of nt = -1 #noise in cluster append ne in clusters[cur_nt] return label, clusters def find_neighbors(nt, eps): for potential_ne in sequence: append potential_ne in neighbors if euclidean distance &amp;lt;= eps return neighbors #functions used in mutclust() def expand_cluster(cur_nt, cur_neighbors, min_samples, clusters): #expand cluster of cur_nt eps = [] cur_deps = deps[cur_nt] ne = cur_nt while cur_deps &amp;lt; min_samples: ne = next_ne(ne) label of ne = 0 #border append ne in clusters[cur_nt] ne_deps = deps[ne] cur_deps = diminish_deps(cur_deps, ne_deps, dim_factor) #diminish cur_deps by ne_deps eps[cur_nt] = cur_deps return label, clusters def calculate_hscore(): freq, ent, ratio = info.</description>
    </item>
    <item>
      <title>#5 결과 검증: 계통 결정 돌연변이와 연관성</title>
      <link>http://localhost:1313/docs/study/tech/tech35/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech35/</guid>
      <description>#5 결과 검증: 계통 결정 돌연변이와 연관성 # #2025-06-24&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) from Bin.sc import * os.chdir(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) 2. Load data # lineage_info_dir = &amp;#39;/data/home/ysh980101/2411/data/mutation_info&amp;#39; covid_annotation = &amp;#34;/data/home/ysh980101/2404/Data/covid_annotation.tsv&amp;#34; sig_hotspots = &amp;#34;result/sig_hotspots.csv&amp;#34; lineage_info = make_lineage_info(lineage_info_dir) hotspot_lineage = make_hotspot_lineage(lineage_info, sig_hotspots_path, covid_annotation) hotspot_lineage plot_hotspot_lineage(hotspot_lineage) outdir = &amp;#34;result/&amp;#34; hotspot_lineage.</description>
    </item>
    <item>
      <title>#6 알고리즘 성능 평가 - k dist plot</title>
      <link>http://localhost:1313/docs/study/tech/tech34/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech34/</guid>
      <description>#6 알고리즘 성능 평가 - k dist plot # #2025-06-24&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) from Bin.sc import * os.chdir(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) 2. Load data # indir = &amp;#39;result/&amp;#39; resdir = &amp;#39;result/GISAID_test1/&amp;#39; with open(f&amp;#34;{indir}GISAID_total.pickle&amp;#34;, &amp;#34;rb&amp;#34;) as f: Input_df = pickle.load(f) hotspots = pd.read_csv(f&amp;#34;{resdir}clusters_test1.txt&amp;#34;, sep=&amp;#39;\t&amp;#39;) sig_hotspots = pd.</description>
    </item>
    <item>
      <title>#1 입력 데이터 생성</title>
      <link>http://localhost:1313/docs/study/tech/tech32/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech32/</guid>
      <description>#1 입력 데이터 생성 # #2025-06-23&#xA;1. Load package # %load_ext autoreload %autoreload 2 import sys import pandas as pd import numpy as np import os import pickle import ast sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH&amp;#39;) 2. Check data # datadir = &amp;#39;/data3/projects/2025_Antibiotics/PreprocessedData/TimecourseData&amp;#39; outdir = &amp;#39;res&amp;#39; pids =[d for d in os.listdir(datadir) if os.path.isdir(os.path.join(datadir, d))] len(pids) 4589 4589명 환자의 의료 데이터.&#xA;cur_pid = pids[0] sev = pd.read_csv(f&amp;#34;{datadir}/{cur_pid}/SeverityScore.csv&amp;#34;) lab = pd.</description>
    </item>
    <item>
      <title>#3 모델 구축</title>
      <link>http://localhost:1313/docs/study/tech/tech33/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech33/</guid>
      <description>#3 모델 구축 # #2025-06-23&#xA;1. Load package # import warnings warnings.filterwarnings(&amp;#34;ignore&amp;#34;) import copy from pathlib import Path import warnings import lightning.pytorch as pl from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor from lightning.pytorch.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) import pytorch_forecasting import torch import pytorch_lightning as pl print(&amp;#34;PyTorch Forecasting:&amp;#34;, pytorch_forecasting.</description>
    </item>
    <item>
      <title>#5 타겟 넘버</title>
      <link>http://localhost:1313/docs/study/tech/algo4/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/algo4/</guid>
      <description>#5 타겟 넘버 # #2025-06-22&#xA;1. 문제 # #문제 설명&#xA;n개의 음이 아닌 정수들이 있습니다. 이 정수들을 순서를 바꾸지 않고 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다.&#xA;-1+1+1+1+1 = 3&#xA;+1-1+1+1+1 = 3&#xA;+1+1-1+1+1 = 3&#xA;+1+1+1-1+1 = 3&#xA;+1+1+1+1-1 = 3&#xA;사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요.</description>
    </item>
    <item>
      <title>#2 중요도 지표 계산</title>
      <link>http://localhost:1313/docs/study/tech/tech30/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech30/</guid>
      <description>#2 중요도 지표 계산 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load GISAID data # indir = &amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/&amp;#34; Refseq = getNucleotideRefSeq() GISAID_Freq = pd.read_csv(f&amp;#39;{indir}gisaid_freq_all.csv&amp;#39;, index_col=0) GISAID_meta = get_GISAID_meta() print(GISAID_Freq) A C G T R Y S W K M B D H V N 1 10612 390 415 785 11 1 3 4 24 2 1 2 0 0 219995 2 287 502 218 12942 3 31 14 4 61 0 1 2 1 0 218179 3 166 461 348 18168 1 12 29 10 15 1 0 1 1 0 213032 4 19398 267 502 972 12 5 1 33 37 6 1 1 0 1 211009 5 24962 281 334 699 6 21 6 17 15 10 5 1 1 1 205886 .</description>
    </item>
    <item>
      <title>#3 밀도 기반 클러스터링</title>
      <link>http://localhost:1313/docs/study/tech/tech29/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech29/</guid>
      <description>#3 밀도 기반 클러스터링 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Find CCMs # i = 1 tag = f&amp;#34;test{i}&amp;#34; input_path = &amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.pickle&amp;#34; outdir = f&amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/&amp;#34; Path(outdir).mkdir(parents=True, exist_ok=True) info = set_env(input = input_path, output = outdir) Input_df = readPickle(input_path) init(Input_df, info) mutInfo, ccms = get_candidate_core_mutations(Input_df, info, tag, i) --- Configurations --- Input data: &amp;#39;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.</description>
    </item>
    <item>
      <title>#4 결과 검증: 임상 결과와의 연관성</title>
      <link>http://localhost:1313/docs/study/tech/tech31/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech31/</guid>
      <description>#4 결과 검증: 임상 결과와의 연관성 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load COVID19 data # i = 1 tag = f&amp;#34;test{i}&amp;#34; resdir = f&amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/&amp;#34; covid19_dir = &amp;#34;/data3/projects/2020_MUTCLUST/Data/Projects/COVID19/Sequence/Preprocessed/Nucleotide/Mutationinfo&amp;#34; meta_path = &amp;#34;/data/home/ysh980101/2506/data/meta.csv&amp;#34; hotspots = pd.read_csv(f&amp;#34;{resdir}clusters_{tag}.txt&amp;#34;,sep=&amp;#34;\t&amp;#34;) metaData = pd.read_csv(meta_path, index_col=0) mutInfo = make_mutInfo_covid19(covid19_dir) mutSignature = make_mutSignature(mutInfo, hotspots, metaData) print(mutSignature) COV-CCO-001 COV-CCO-002 COV-CCO-003 COV-CCO-004 COV-CCO-006 \ c0 0 0 0 0 0 c1 0 0 0 0 0 c2 0 0 0 0 0 c3 0 0 0 0 0 c4 0 0 0 0 0 .</description>
    </item>
    <item>
      <title>#5 Revision</title>
      <link>http://localhost:1313/docs/study/tech/tech28/</link>
      <pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech28/</guid>
      <description>#5 Revision # #2025-06-19&#xA;Reviewer 1 - Comment 1 # &amp;ldquo;In the introduction section, the authors note that most computational methods focus on the frequency of mutation occurrences rather than mutation diversity. This point should be more thoroughly discussed, with a clear explanation of the advantages and potential insights offered by analyzing mutation diversity.&amp;rdquo;&#xA;“서론에서 저자들은 대부분의 계산 방법들이 돌연변이 발생 빈도에 집중하고 있으며, 돌연변이 다양성(mutation diversity)을 간과한다고 언급하였습니다. 돌연변이 다양성을 분석하는 것의 장점과 잠재적인 통찰에 대해 보다 명확하게 논의해 주시기 바랍니다.</description>
    </item>
    <item>
      <title>Related Study #3 Density-based approach</title>
      <link>http://localhost:1313/docs/study/tech/tech24/</link>
      <pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech24/</guid>
      <description>Related Study #3 Density-based approach # #2025-06-18&#xA;1. Density based approach가 잘 적용되는 데이터의 특성 # (에 mutation 데이터 끼워맞추면)?&#xA;비정규적 분포 (non-uniform): 돌연변이는 일정 위치에 집중되는 hotspot 현상을 보인다. ex) spike 단백질 특정 영역에 몰림. 클러스터 수 미정: 몇 개의 변이 집단(hotspot)이 존재하는지 사전 지식이 없다. 군집의 불규칙한 모양과 크기: hotspot의 길이, 모양(밀도, 거리)이 다양하다. 노이즈 존재: 무작위적 돌연변이, 측정 오류 등으로 인해 의미 없는 변이들(outlier)가 섞여 있다. # 2.</description>
    </item>
    <item>
      <title>Related Study #4 Clustering 알고리즘의 parametric test</title>
      <link>http://localhost:1313/docs/study/tech/tech25/</link>
      <pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech25/</guid>
      <description> Related Study #4 Clustering 알고리즘의 parametric test # #2025-06-18&#xA;Parametric test&#xA;정답 label이 없는 unsupervised learning인 clustering은 supervised learning과 달리 정확도, AUC curve 등으로 성능 평가 불가.&#xA;정량적 평가 지표?&#xA;Intra-cluster genetic distance (클러스터 내 유전 거리): 작을수록 내부 군집 응집도가 좋음 Silhouette score, SSE, BIC 등의 지표 사용 그 외 방법?&#xA;방향성이 같은 또는 같지 않아야 하는 비교 feature를 선택하고 비교 ex) 계통학적 구조가 지리적 패턴과 일치함 t‑SNE 시각화 등 시각적 확인 t‑SNE로 축소된 2D scatter plot 위에 DBSCAN으로 얻은 cluster를 색상별로 표시해서 군집 간의 명확한 경계, 군집 내 응집성이 시각적으로 확인 # </description>
    </item>
    <item>
      <title>#1 입력 데이터 생성</title>
      <link>http://localhost:1313/docs/study/tech/tech20/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech20/</guid>
      <description>#1 입력 데이터 생성 # #2025-06-17&#xA;Load package # %load_ext autoreload %autoreload 2 import sys import os sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH&amp;#39;) Check data # raw_path = &amp;#39;/data3/projects/2025_Antibiotics/YSH/res/sev_dict_filtered.pkl&amp;#39; with open(raw_path, &amp;#39;rb&amp;#39;) as f: raw_data = pickle.load(f) keys = list(raw_data.keys()) print(len(keys)) print(keys[0], &amp;#39;\n&amp;#39;, raw_data[keys[0]]) 4515 74374 Date NEWS med_cnt med_list \ 0 2020-10-30 4 2 Trizele;Cefotaxime 1 2020-10-31 4 2 Trizele;Cefotaxime 2 2020-11-01 12 2 Pospenem;Pospenem_2 3 2020-11-02 9 3 Pospenem;Meropen;Vanco Kit 4 2020-11-03 12 2 Vanco Kit;Meropen 5 2020-11-04 8 2 Vanco Kit;Meropen 6 2020-11-05 9 0 strain 0 [] 1 [] 2 [] 3 [Enterobacter cloacae ssp cloacae] 4 [Enterobacter cloacae ssp cloacae] 5 [Enterobacter cloacae ssp cloacae] 6 [Enterobacter cloacae ssp cloacae] 4515명 환자 데이터이고</description>
    </item>
    <item>
      <title>#1 입력 데이터 생성</title>
      <link>http://localhost:1313/docs/study/tech/tech27/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech27/</guid>
      <description>#1 입력 데이터 생성 # #2025-06-18&#xA;1. Load package # import pandas as pd import numpy as np import os import pickle import ast os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH/&amp;#39;) 2. Load raw data # datadir = &amp;#39;/data3/projects/2025_Antibiotics/PreprocessedData/TimecourseData&amp;#39; outdir = &amp;#39;res&amp;#39; pids =[d for d in os.listdir(datadir) if os.path.isdir(os.path.join(datadir, d))] len(pids) 4589 datadir에 4589명 환자의 의료 데이터가 존재한다.&#xA;cur_pid = pids[0] sev = pd.read_csv(f&amp;#34;{datadir}/{cur_pid}/SeverityScore.csv&amp;#34;) lab = pd.read_csv(f&amp;#34;{datadir}/{cur_pid}/Laboratory_processed.csv&amp;#34;) med = pd.read_csv(f&amp;#34;{datadir}/{cur_pid}/Medication.csv&amp;#34;) print(cur_pid) print(len(sev.columns.tolist()), sev.columns.tolist()) print(len(lab.columns.tolist()), lab.</description>
    </item>
    <item>
      <title>#2 베스트앨범</title>
      <link>http://localhost:1313/docs/study/tech/algo2/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/algo2/</guid>
      <description>#2 베스트앨범 # #2025-06-17&#xA;문제 # #문제 설명&#xA;스트리밍 사이트에서 장르 별로 가장 많이 재생된 노래를 두 개씩 모아 베스트 앨범을 출시하려 합니다. 노래는 고유 번호로 구분하며, 노래를 수록하는 기준은 다음과 같습니다.&#xA;속한 노래가 많이 재생된 장르를 먼저 수록합니다. 장르 내에서 많이 재생된 노래를 먼저 수록합니다. 장르 내에서 재생 횟수가 같은 노래 중에서는 고유 번호가 낮은 노래를 먼저 수록합니다. 노래의 장르를 나타내는 문자열 배열 genres와 노래별 재생 횟수를 나타내는 정수 배열 plays가 주어질 때, 베스트 앨범에 들어갈 노래의 고유 번호를 순서대로 return 하도록 solution 함수를 완성하세요.</description>
    </item>
    <item>
      <title>#2 입력 feature 생성</title>
      <link>http://localhost:1313/docs/study/tech/tech21/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech21/</guid>
      <description>#2 입력 feature 생성 # #2025-06-17&#xA;1. Load package # %load_ext autoreload %autoreload 2 import sys import os import pickle import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages import pandas as pd sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH&amp;#39;) 2. Previous # seqdir = &amp;#39;data/res_dict&amp;#39; seq_list = os.listdir(seqdir) print(len(seq_list)) 169 항생제 169종에 대해서 size 10 sequence를 생성했었는데&#xA;모델 입력 feature로 다음을 제외하는대신 antibiotics 리스트 strain 리스트 저 2개 feature를 반영하는 새로운 feature를 2개 생성하려고 한다: 현재 antibiotics가 현재 strain 환자의 NEWS를 감소시킨 이력이 있는지?</description>
    </item>
    <item>
      <title>#3 네트워크</title>
      <link>http://localhost:1313/docs/study/tech/algo7/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/algo7/</guid>
      <description>#3 네트워크 # #2025-06-17&#xA;문제 # #문제 설명&#xA;네트워크란 컴퓨터 상호 간에 정보를 교환할 수 있도록 연결된 형태를 의미합니다. 예를 들어, 컴퓨터 A와 컴퓨터 B가 직접적으로 연결되어있고, 컴퓨터 B와 컴퓨터 C가 직접적으로 연결되어 있을 때 컴퓨터 A와 컴퓨터 C도 간접적으로 연결되어 정보를 교환할 수 있습니다. 따라서 컴퓨터 A, B, C는 모두 같은 네트워크 상에 있다고 할 수 있습니다.&#xA;컴퓨터의 개수 n, 연결에 대한 정보가 담긴 2차원 배열 computers가 매개변수로 주어질 때, 네트워크의 개수를 return 하도록 solution 함수를 작성하시오.</description>
    </item>
    <item>
      <title>#4 모델 학습</title>
      <link>http://localhost:1313/docs/study/tech/tech26/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech26/</guid>
      <description>#4 모델 학습 # #2025-06-18&#xA;1. Load package # import warnings warnings.filterwarnings(&amp;#34;ignore&amp;#34;) import copy from pathlib import Path import warnings import lightning.pytorch as pl from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor from lightning.pytorch.loggers import TensorBoardLogger import numpy as np import pandas as pd import torch from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet from pytorch_forecasting.data import GroupNormalizer from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss from pytorch_forecasting.models.temporal_fusion_transformer.tuning import ( optimize_hyperparameters, ) 2. Check version # import pytorch_forecasting import torch import pytorch_lightning as pl print(&amp;#34;PyTorch Forecasting:&amp;#34;, pytorch_forecasting.</description>
    </item>
    <item>
      <title>#4 완전범죄</title>
      <link>http://localhost:1313/docs/study/tech/algo3/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/algo3/</guid>
      <description>#4 완전범죄 # #2025-06-19&#xA;1. 문제 # #문제 설명&#xA;A도둑과 B도둑이 팀을 이루어 모든 물건을 훔치려고 합니다. 단, 각 도둑이 물건을 훔칠 때 남기는 흔적이 누적되면 경찰에 붙잡히기 때문에, 두 도둑 중 누구도 경찰에 붙잡히지 않도록 흔적을 최소화해야 합니다.&#xA;물건을 훔칠 때 조건은 아래와 같습니다.&#xA;물건 i를 훔칠 때, A도둑이 훔치면 info[i][0]개의 A에 대한 흔적을 남깁니다. B도둑이 훔치면 info[i][1]개의 B에 대한 흔적을 남깁니다. 각 물건에 대해 A도둑과 B도둑이 남기는 흔적의 개수는 1 이상 3 이하입니다.</description>
    </item>
    <item>
      <title>Related Study #1 샤넌 엔트로피</title>
      <link>http://localhost:1313/docs/study/tech/tech22/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech22/</guid>
      <description>Related Study #1 샤넌 엔트로피 # #2025-06-17&#xA;1 # 샤넌 엔트로피?&#xA;단백질 서열 상 특정 위치에 다양한 아미노산이 얼마나 골고루 존재하는지를 나타내는 지표 어떤 위치에 여러 아미노산이 비슷한 비율로 존재한다면 엔트로피가 높고, 하나의 아미노산이 압도적으로 우세하다면 엔트로피가 낮다. 돌연변이 데이터에서 샤넌 엔트로피&#xA;전통적인 샤논 엔트로피에 대한 해석은 논코딩 영역의 식별.&#xA;염기의 돌연변이에 따른 아미노산의 결실 및 변동은 개체에 대부분은 부정적인 영향을 줌으로써 돌연변이를 가진 개체가 태어날 수 없게 할 확률이 높기 때문.</description>
    </item>
    <item>
      <title>Related Study #2 Cluster detection algorithm</title>
      <link>http://localhost:1313/docs/study/tech/tech23/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech23/</guid>
      <description>Related Study #2 Cluster detection algorithm # #2025-06-17&#xA;돌연변이는 무작위로 발생하지만 실제 분포를 확인해보면 그렇지 않다.&#xA;엄연히 군집을 형성하고 있으며 이는 해당 돌연변이의 &amp;lsquo;생존&amp;rsquo;에 관여한 외부 요인의 존재를 보여준다. # 논문 &amp;ldquo;Computational methods for detecting cancer hotspots&amp;rdquo;&#xA;암에서 반복적으로 관찰되는 돌연변이 즉 핫스팟(hotspot)을 식별하기 위한 계산적 방법 40여개에 대한 리뷰 논문.&#xA;암에서 Hotspot mutation은 여러 환자에서 동일한 위치에 반복적으로 나타나는 돌연변이로써 우연히 발생할 가능성이 낮기 때문에 기능적 역할을 할 가능성이 높다고 간주됨에 따라 무의미한 hotspot을 거르고 중요한 hotspot 식별을 위한 여러 알고리즘이 고안되었다.</description>
    </item>
    <item>
      <title>#1 완주하지 못한 선수</title>
      <link>http://localhost:1313/docs/study/tech/algo1/</link>
      <pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/algo1/</guid>
      <description>#1 완주하지 못한 선수 # #2025-06-16&#xA;#문제 설명&#xA;수많은 마라톤 선수들이 마라톤에 참여하였습니다. 단 한 명의 선수를 제외하고는 모든 선수가 마라톤을 완주하였습니다.&#xA;마라톤에 참여한 선수들의 이름이 담긴 배열 participant와 완주한 선수들의 이름이 담긴 배열 completion이 주어질 때, 완주하지 못한 선수의 이름을 return 하도록 solution 함수를 작성해주세요.&#xA;#제한사항&#xA;마라톤 경기에 참여한 선수의 수는 1명 이상 100,000명 이하입니다. completion의 길이는 participant의 길이보다 1 작습니다. 참가자의 이름은 1개 이상 20개 이하의 알파벳 소문자로 이루어져 있습니다.</description>
    </item>
    <item>
      <title>프로그래머스 알고리즘 고득점 kit - 스택/큐</title>
      <link>http://localhost:1313/docs/study/tech/tech8/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/tech8/</guid>
      <description>프로그래머스 알고리즘 고득점 kit - 스택/큐 # 목록 # 2024-04-09 ⋯ [스택/큐] 기능개발&#xA;2024-04-10 ⋯ [스택/큐] 올바른 괄호&#xA;2024-04-10 ⋯ [스택/큐] 프로세스&#xA;2024-04-10 ⋯ [스택/큐] 다리를 지나는 트럭&#xA;기능개발 # 입출력 예 # progresses = [93, 30, 55] speeds = [1, 30, 5] return = [2, 1] 개념 # progresses = [99,99,97] speeds = [1,1,1]이면 cnt=0 progresses = [100,100,98] -&amp;gt; cnt=1 -&amp;gt; cnt=2 -&amp;gt; answer = [2] cnt=0 progresses = [99] -&amp;gt; cnt=0, answer = [2] cnt=0 progresses = [100] -&amp;gt; cnt=1 -&amp;gt; answer = [1] cnt=0 progresses = [] -&amp;gt; 종료 코드 # def solution(progresses, speeds): answer = [] while progresses: for i in range(len(progresses)): progresses[i] += speeds[i] cnt = 0 while progresses and progresses[0] &amp;gt;= 100: progresses.</description>
    </item>
    <item>
      <title>프로그래머스 알고리즘 고득점 kit - 해시, 정렬</title>
      <link>http://localhost:1313/docs/study/tech/study2/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/tech/study2/</guid>
      <description>프로그래머스 알고리즘 고득점 kit - 해시, 정렬 # 목록 # 2024-04-09 ⋯ [해시] 완주하지 못한 선수&#xA;2024-04-09 ⋯ [해시] 전화번호 목록&#xA;2024-04-09 ⋯ [해시] 의상&#xA;2024-04-09 ⋯ [정렬] 완주하지 못한 선수&#xA;2024-04-09 ⋯ [정렬] H-Index&#xA;2024-04-10 ⋯ [해시] 베스트앨범&#xA;완주하지 못한 선수 # 입출력 예 # participant = [&amp;#34;leo&amp;#34;, &amp;#34;kiki&amp;#34;, &amp;#34;eden&amp;#34;]&#x9;completion = [&amp;#34;eden&amp;#34;, &amp;#34;kiki&amp;#34;]&#x9;return = &amp;#34;leo&amp;#34; 개념 # Counter([&amp;#34;leo&amp;#34;, &amp;#34;kiki&amp;#34;, &amp;#34;eden&amp;#34;]) -&amp;gt; {&amp;#39;leo&amp;#39;:1, &amp;#39;kiki&amp;#39;:1, &amp;#39;eden&amp;#39;:1} Counter([&amp;#34;leo&amp;#34;, &amp;#34;kiki&amp;#34;, &amp;#34;eden&amp;#34;]) - Counter([&amp;#34;kiki&amp;#34;, &amp;#34;eden&amp;#34;]) -&amp;gt; {&amp;#39;leo&amp;#39;:1} (key별로 value를 빼서 0이나 음수되면 제거) 코드 # from collections import Counter def solution(participant, completion): answer = Counter(participant) - Counter(completion) return list(answer.</description>
    </item>
  </channel>
</rss>
