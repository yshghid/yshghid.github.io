<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on Lifelog 2025</title><link>https://yshghid.github.io/categories/ai/</link><description>Recent content in AI on Lifelog 2025</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 26 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://yshghid.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>#2 Explainable AI</title><link>https://yshghid.github.io/docs/study/etc/etc2/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/etc/etc2/</guid><description>&lt;h1 id="2-explainable-ai">
 #2 Explainable AI
 &lt;a class="anchor" href="#2-explainable-ai">#&lt;/a>
&lt;/h1>
&lt;p>#2025-06-26&lt;/p>
&lt;hr>
&lt;h3 id="1-explainable-ai란">
 1. Explainable AI란?
 &lt;a class="anchor" href="#1-explainable-ai%eb%9e%80">#&lt;/a>
&lt;/h3>
&lt;p>Explainable AI는 인공지능(AI) 또는 머신러닝(ML) 모델이 어떤 방식으로 특정 결과를 도출했는지 사람이 이해할 수 있도록 설명하는 기술과 방법론.&lt;/p>
&lt;h3 id="2-xai-기법-분류">
 2. XAI 기법 분류
 &lt;a class="anchor" href="#2-xai-%ea%b8%b0%eb%b2%95-%eb%b6%84%eb%a5%98">#&lt;/a>
&lt;/h3>
&lt;p>모델 구조&lt;/p>
&lt;ul>
&lt;li>Intrinsic:	모델 자체가 설명 가능한 구조 (예: 의사결정나무, 선형회귀 등)&lt;/li>
&lt;li>Post-hoc:	모델 학습 후 별도로 설명 생성 (예: SHAP, LIME)
대상&lt;/li>
&lt;li>Global:	전체 모델의 작동 원리를 설명&lt;/li>
&lt;li>Local:	특정 샘플의 예측 결과를 설명&lt;/li>
&lt;/ul>
&lt;h3 id="3-주요-post-hoc-설명-기법">
 3. 주요 Post-hoc 설명 기법
 &lt;a class="anchor" href="#3-%ec%a3%bc%ec%9a%94-post-hoc-%ec%84%a4%eb%aa%85-%ea%b8%b0%eb%b2%95">#&lt;/a>
&lt;/h3>
&lt;p>LIME (Local Interpretable Model-Agnostic Explanations): 주변 입력을 랜덤하게 생성하고, 단순 모델(선형 회귀 등)을 학습해 근사&lt;/p></description></item></channel></rss>