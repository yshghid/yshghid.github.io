<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on</title><link>https://yshghid.github.io/categories/deep-learning/</link><description>Recent content in Deep Learning on</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 31 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://yshghid.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>구글 BERT의 정석 | BERT 입문</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs16/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs16/</guid><description>&lt;h1 id="딥러닝-구글-bert의-정석--bert-입문">
 [딥러닝] 구글 BERT의 정석 | BERT 입문
 &lt;a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%ec%a0%95%ec%84%9d--bert-%ec%9e%85%eb%ac%b8">#&lt;/a>
&lt;/h1>
&lt;h2 id="목록">
 목록
 &lt;a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs16/#23-bert%ec%9d%98-%ea%b5%ac%ec%a1%b0">2.3 BERT의 구조&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs16/#24-bert-%ec%82%ac%ec%a0%84-%ed%95%99%ec%8a%b5">2.4 BERT 사전 학습&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="23-bert의-구조">
 2.3 BERT의 구조
 &lt;a class="anchor" href="#23-bert%ec%9d%98-%ea%b5%ac%ec%a1%b0">#&lt;/a>
&lt;/h2>
&lt;h3 id="bert의-전체-구조">
 BERT의 전체 구조
 &lt;a class="anchor" href="#bert%ec%9d%98-%ec%a0%84%ec%b2%b4-%ea%b5%ac%ec%a1%b0">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>트랜스포머의 인코더(Encoder) 블록을 여러 개 쌓은 형태.
&lt;ul>
&lt;li>입력: 문장 (토큰화된 형태)&lt;/li>
&lt;li>내부 구조: N개의 Transformer Encoder Blocks (기본 모델은 12개, 큰 모델은 24개)&lt;/li>
&lt;li>출력: 각 토큰의 벡터 표현 (Contextual Embedding)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>cf) BERT의 대표적인 모델 크기&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>모델&lt;/th>
 &lt;th># 인코더 층&lt;/th>
 &lt;th>숨겨진 차원 (dmodel)&lt;/th>
 &lt;th>어텐션 헤드 수&lt;/th>
 &lt;th>파라미터 수&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>BERT-Base&lt;/td>
 &lt;td>12&lt;/td>
 &lt;td>768&lt;/td>
 &lt;td>12&lt;/td>
 &lt;td>110M&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>BERT-Large&lt;/td>
 &lt;td>24&lt;/td>
 &lt;td>1024&lt;/td>
 &lt;td>16&lt;/td>
 &lt;td>340M&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>&lt;/blockquote>
&lt;h3 id="bert의-입력-처리">
 BERT의 입력 처리
 &lt;a class="anchor" href="#bert%ec%9d%98-%ec%9e%85%eb%a0%a5-%ec%b2%98%eb%a6%ac">#&lt;/a>
&lt;/h3>
&lt;ol>
&lt;li>입력 토큰 (Token Embedding)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>WordPiece Tokenization을 사용하며, 단어를 서브워드(subword) 단위로 분할하고 각 토큰은 고유한 임베딩 벡터로 변환된다.
ex) &amp;ldquo;playing&amp;rdquo; -&amp;gt; [&amp;ldquo;play&amp;rdquo;, &amp;ldquo;##ing&amp;rdquo;]&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>문장 구분 정보 (Segment Embedding)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>BERT는 두 개의 문장을 함께 입력할 수 있으며, 이때 각 문장이 어디에 속하는지를 구분하기 위해 Segment Embedding을 추가한다.
ex) 문장 A: 0 (Segment A) / 문장 B: 1 (Segment B)&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>위치 정보 (Position Embedding)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>트랜스포머는 순서를 고려하지 않는 구조이므로, 단어 순서를 반영하기 위해 위치 임베딩을 추가한다.&lt;/li>
&lt;li>BERT는 고정된 학습 가능한 위치 임베딩을 사용하며, 트랜스포머에서 사용되는 사인(sine) 및 코사인(cosine) 위치 임베딩을 사용하지 않음.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>최종 입력 형식&lt;/strong>&lt;/p></description></item><item><title>구글 BERT의 정석 | BERT의 파생 모델</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs17/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs17/</guid><description>&lt;h1 id="딥러닝-구글-bert의-정석--bert의-파생-모델-albert-roberta-electra-spanbert">
 [딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT
 &lt;a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%ec%a0%95%ec%84%9d--bert%ec%9d%98-%ed%8c%8c%ec%83%9d-%eb%aa%a8%eb%8d%b8-albert-roberta-electra-spanbert">#&lt;/a>
&lt;/h1>
&lt;h2 id="목록">
 목록
 &lt;a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs17/#41-albert">4.1 ALBERT&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs17/#43-roberta">4.3 RoBERTa&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs17/#44-electra">4.4 ELECTRA&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="41-albert">
 4.1 ALBERT
 &lt;a class="anchor" href="#41-albert">#&lt;/a>
&lt;/h2>
&lt;blockquote>
&lt;p>ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.&lt;/p>&lt;/blockquote>
&lt;h3 id="크로스-레이어-변수-공유">
 크로스 레이어 변수 공유
 &lt;a class="anchor" href="#%ed%81%ac%eb%a1%9c%ec%8a%a4-%eb%a0%88%ec%9d%b4%ec%96%b4-%eb%b3%80%ec%88%98-%ea%b3%b5%ec%9c%a0">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다.&lt;/li>
&lt;li>ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.&lt;/li>
&lt;/ul>
&lt;h3 id="펙토라이즈-임베딩-변수화">
 펙토라이즈 임베딩 변수화
 &lt;a class="anchor" href="#%ed%8e%99%ed%86%a0%eb%9d%bc%ec%9d%b4%ec%a6%88-%ec%9e%84%eb%b2%a0%eb%94%a9-%eb%b3%80%ec%88%98%ed%99%94">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.&lt;/li>
&lt;li>ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션).
&lt;ul>
&lt;li>행렬1: vocab_size x embedding_size&lt;/li>
&lt;li>행렬2: embedding_size x hidden_size&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="문장-순서-예측">
 문장 순서 예측
 &lt;a class="anchor" href="#%eb%ac%b8%ec%9e%a5-%ec%88%9c%ec%84%9c-%ec%98%88%ec%b8%a1">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용.&lt;/p></description></item><item><title>구글 BERT의 정석 | 트랜스포머 입문</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs15/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs15/</guid><description>&lt;h1 id="딥러닝-구글-bert의-정석--트랜스포머-입문">
 [딥러닝] 구글 BERT의 정석 | 트랜스포머 입문
 &lt;a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%ec%a0%95%ec%84%9d--%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8-%ec%9e%85%eb%ac%b8">#&lt;/a>
&lt;/h1>
&lt;h2 id="목록">
 목록
 &lt;a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs15/#%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8%ec%9d%98-%ec%9d%b8%ec%bd%94%eb%8d%94-%ec%9d%b4%ed%95%b4%ed%95%98%ea%b8%b0">1.2 트랜스포머의 인코더 이해하기&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs15/#13-%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8%ec%9d%98-%eb%94%94%ec%bd%94%eb%8d%94-%ec%9d%b4%ed%95%b4%ed%95%98%ea%b8%b0">1.3 트랜스포머의 디코더 이해하기&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="12-트랜스포머의-인코더-이해하기">
 1.2 트랜스포머의 인코더 이해하기
 &lt;a class="anchor" href="#12-%ed%8a%b8%eb%9e%9c%ec%8a%a4%ed%8f%ac%eb%a8%b8%ec%9d%98-%ec%9d%b8%ec%bd%94%eb%8d%94-%ec%9d%b4%ed%95%b4%ed%95%98%ea%b8%b0">#&lt;/a>
&lt;/h2>
&lt;h3 id="셀프-어텐션">
 셀프 어텐션
 &lt;a class="anchor" href="#%ec%85%80%ed%94%84-%ec%96%b4%ed%85%90%ec%85%98">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>셀프 어텐션은 문장 내 단어들이 서로 얼마나 중요한지를 계산하는 과정.&lt;/li>
&lt;li>트랜스포머는 이를 위해 입력 단어를 쿼리(Query), 키(Key), 밸류(Value) 세 가지 벡터로 변환하여 연관성을 구한다.&lt;/li>
&lt;/ul>
&lt;h3 id="어텐션-점수-계산-예제">
 어텐션 점수 계산 예제
 &lt;a class="anchor" href="#%ec%96%b4%ed%85%90%ec%85%98-%ec%a0%90%ec%88%98-%ea%b3%84%ec%82%b0-%ec%98%88%ec%a0%9c">#&lt;/a>
&lt;/h3>
&lt;blockquote>
&lt;p>&amp;ldquo;The cat sat on the mat.&amp;rdquo;&lt;/p>&lt;/blockquote>
&lt;ol>
&lt;li>각 단어 벡터(예: 512차원)를 가중치 행렬과 곱하여 쿼리(Q), 키(K), 밸류(V)벡터를 생성한다.&lt;/li>
&lt;li>어떤 단어가 다른 단어와 얼마나 연관되는지를 측정하기 위해, Q와 K벡터 간의 내적(dot product)을 계산한다.
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>단어&lt;/th>
 &lt;th>The&lt;/th>
 &lt;th>cat&lt;/th>
 &lt;th>sat&lt;/th>
 &lt;th>on&lt;/th>
 &lt;th>the&lt;/th>
 &lt;th>mat&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>Query: &amp;ldquo;cat&amp;rdquo;&lt;/td>
 &lt;td>0.2&lt;/td>
 &lt;td>1.0&lt;/td>
 &lt;td>0.8&lt;/td>
 &lt;td>0.1&lt;/td>
 &lt;td>0.3&lt;/td>
 &lt;td>0.5&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&amp;ldquo;cat&amp;quot;의 쿼리 벡터와 모든 단어의 키 벡터를 곱해서 점수를 계산하는 경우.&lt;/li>
&lt;li>여기서 &amp;ldquo;cat&amp;quot;은 &amp;ldquo;sat&amp;quot;과 가장 연관이 높고(0.8), &amp;ldquo;on&amp;quot;과는 거의 연관이 없다(0.1).&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>소프트맥스 적용
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>단어&lt;/th>
 &lt;th>The&lt;/th>
 &lt;th>cat&lt;/th>
 &lt;th>sat&lt;/th>
 &lt;th>on&lt;/th>
 &lt;th>the&lt;/th>
 &lt;th>mat&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>Softmax 값&lt;/td>
 &lt;td>0.05&lt;/td>
 &lt;td>0.4&lt;/td>
 &lt;td>0.35&lt;/td>
 &lt;td>0.02&lt;/td>
 &lt;td>0.08&lt;/td>
 &lt;td>0.1&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>위에서 구한 점수에 대해 소프트맥스를 적용하여 확률로 변환&lt;/li>
&lt;li>이제 &amp;ldquo;cat&amp;quot;은 &amp;ldquo;sat&amp;rdquo;(0.35)과 &amp;ldquo;cat&amp;rdquo; 자체(0.4)에 높은 가중치를 부여함.&lt;/li>
&lt;/ul>
&lt;ol start="4">
&lt;li>각 단어의 밸류(V) 벡터를 위의 확률로 가중합하여 최종 어텐션 출력을 얻는다.&lt;/li>
&lt;/ol>
&lt;h3 id="멀티-헤드-어텐션">
 멀티 헤드 어텐션
 &lt;a class="anchor" href="#%eb%a9%80%ed%8b%b0-%ed%97%a4%eb%93%9c-%ec%96%b4%ed%85%90%ec%85%98">#&lt;/a>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>단어 간의 관계를 한 가지 방식으로만 학습하면, 문맥을 완전히 반영하지 못할 수 있음. 예를 들어, 단어 &amp;ldquo;cat&amp;quot;은 문장에서 다음과 같은 다양한 방식으로 다른 단어와 관계를 맺을 수 있다.&lt;/p></description></item><item><title>딥러닝을 이용한 자연어 처리 입문 | BERT</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs14/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs14/</guid><description>&lt;h1 id="딥러닝-딥러닝을-이용한-자연어-처리-입문--bert">
 [딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT
 &lt;a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%eb%94%a5%eb%9f%ac%eb%8b%9d%ec%9d%84-%ec%9d%b4%ec%9a%a9%ed%95%9c-%ec%9e%90%ec%97%b0%ec%96%b4-%ec%b2%98%eb%a6%ac-%ec%9e%85%eb%ac%b8--bert">#&lt;/a>
&lt;/h1>
&lt;h2 id="목록">
 목록
 &lt;a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs14/#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert">17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs14/#17-03-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8">17-03 구글 BERT의 마스크드 언어 모델&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs14/#17-04-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%a7%88%ec%8a%a4%ed%81%ac%eb%93%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8">17-04 한국어 BERT의 마스크드 언어 모델&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs14/#17-05-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1">17-05 구글 BERT의 다음 문장 예측&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs14/#17-06-%ed%95%9c%ea%b5%ad%ec%96%b4-bert%ec%9d%98-%eb%8b%a4%ec%9d%8c-%eb%ac%b8%ec%9e%a5-%ec%98%88%ec%b8%a1">17-06 한국어 BERT의 다음 문장 예측&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="17-02-버트bidirectional-encoder-representations-from-transformers-bert">
 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)
 &lt;a class="anchor" href="#17-02-%eb%b2%84%ed%8a%b8bidirectional-encoder-representations-from-transformers-bert">#&lt;/a>
&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>BERT?&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.&lt;/li>
&lt;li>입/출력 구조
&lt;ul>
&lt;li>입력은 각 단어를 768차원의 임베딩 벡터로 변환한 것. ex) [CLS], I, love, you → 각각 768차원의 벡터로 변환.&lt;/li>
&lt;li>출력은 BERT의 내부 연산을 거쳐, 문맥을 반영한 768차원의 벡터로 변환된 것.&lt;/li>
&lt;li>문맥 반영? 입력된 단어의 벡터에 대한 출력 임베딩은 입력 문장의 모든 단어 정보를 반영한 벡터. [CLS] 벡터는 문장의 전체 정보를 요약한 벡터로 활용된다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>구조와 연산
&lt;ul>
&lt;li>BERT는 트랜스포머 인코더를 12층 쌓아 올린 구조.&lt;/li>
&lt;li>각 층에서 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**과 포지션 와이즈 피드포워드 네트워크(Position-wise Feed Forward Network) 연산을 수행해서 입력 단어가 다른 모든 단어와 상호작용하여 문맥 정보를 반영하도록 한다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>BERT의 서브워드 토크나이저: WordPiece&lt;/strong>&lt;/p></description></item><item><title>혼자 공부하는 딥러닝 | ANN</title><link>https://yshghid.github.io/docs/study/bioinformatics/cs12/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://yshghid.github.io/docs/study/bioinformatics/cs12/</guid><description>&lt;h1 id="딥러닝-혼자-공부하는-딥러닝--ann">
 [딥러닝] 혼자 공부하는 딥러닝 | ANN
 &lt;a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ed%98%bc%ec%9e%90-%ea%b3%b5%eb%b6%80%ed%95%98%eb%8a%94-%eb%94%a5%eb%9f%ac%eb%8b%9d--ann">#&lt;/a>
&lt;/h1>
&lt;h2 id="목록">
 목록
 &lt;a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs12/#17-%ea%b0%84%eb%8b%a8%ed%95%9c-%ec%9d%b8%ea%b3%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%aa%a8%eb%8d%b8-%eb%a7%8c%eb%93%a4%ea%b8%b0">17. 간단한 인공 신경망 모델 만들기&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs12/#18-%ec%9d%b8%ea%b3%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%97%90-%ec%b8%b5%ec%9d%84-%ec%b6%94%ea%b0%80%ed%95%98%ec%97%ac-%ec%8b%ac%ec%b8%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%a7%8c%eb%93%a4%ea%b8%b0">18. 인공 신경망에 층을 추가하여 심층 신경망 만들어 보기&lt;/a>&lt;/p>
&lt;p>&lt;em>2024-12-31&lt;/em> ⋯ &lt;a href="https://yshghid.github.io/docs/study/cs/cs12/#19-%ec%9d%b8%ea%b2%bd-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%aa%a8%eb%8d%b8-%ed%9b%88%eb%a0%a8%ec%9d%98-%eb%aa%a8%eb%b2%94-%ec%82%ac%eb%a1%80-%ed%95%99%ec%8a%b5%ed%95%98%ea%b8%b0">19. 인경 신경망 모델 훈련의 모범 사례 학습하기&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="17-간단한-인공-신경망-모델-만들기">
 17. 간단한 인공 신경망 모델 만들기
 &lt;a class="anchor" href="#17-%ea%b0%84%eb%8b%a8%ed%95%9c-%ec%9d%b8%ea%b3%b5-%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%aa%a8%eb%8d%b8-%eb%a7%8c%eb%93%a4%ea%b8%b0">#&lt;/a>
&lt;/h2>
&lt;ol>
&lt;li>데이터 준비&lt;/li>
&lt;/ol>
&lt;p>fashion_mnist 데이터셋에서 학습과 테스트용 이미지 데이터를 가져온다. 학습 데이터는 60,000개의 28x28 픽셀 이미지, 테스트 데이터는 10,000개의 28x28 픽셀 이미지. train_target과 test_target은 각 이미지에 해당하는 레이블(0~9)을 갖고있다.&lt;/p></description></item></channel></rss>