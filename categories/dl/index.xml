<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dl on Lifelog 2025</title>
    <link>http://localhost:1313/categories/dl/</link>
    <description>Recent content in Dl on Lifelog 2025</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>공부</title>
      <link>http://localhost:1313/docs/hobby/study/cs12/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/study/cs12/</guid>
      <description>[딥러닝] 혼자 공부하는 딥러닝 | ANN # 목록 # 간단한 인공 신경망 모델 만들기&#xA;인공 신경망에 층을 추가하여 심층 신경망 만들어 보기&#xA;인경 신경망 모델 훈련의 모범 사례 학습하기&#xA;17. 간단한 인공 신경망 모델 만들기 # 2024-12-31 # 데이터 준비 fashion_mnist 데이터셋에서 학습과 테스트용 이미지 데이터를 가져온다. 학습 데이터는 60,000개의 28x28 픽셀 이미지, 테스트 데이터는 10,000개의 28x28 픽셀 이미지. train_target과 test_target은 각 이미지에 해당하는 레이블(0~9)을 갖고있다.&#xA;from tensorflow import keras (train_input, train_target), (test_input, test_target) = keras.</description>
    </item>
    <item>
      <title>공부</title>
      <link>http://localhost:1313/docs/hobby/study/cs14/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/study/cs14/</guid>
      <description>[딥러닝] 딥러닝을 이용한 자연어 처리 입문 | BERT # 목록 # 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT)&#xA;17-03 구글 BERT의 마스크드 언어 모델&#xA;17-04 한국어 BERT의 마스크드 언어 모델&#xA;17-05 구글 BERT의 다음 문장 예측&#xA;17-06 한국어 BERT의 다음 문장 예측&#xA;17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) # 2024-12-31 # BERT?&#xA;BERT는 문맥 정보를 반영한 임베딩(Contextual Embedding)을 사용함. 이는 단어의 의미가 문맥에 따라 달라질 수 있음을 모델이 학습하도록 설계된 방식.</description>
    </item>
    <item>
      <title>공부</title>
      <link>http://localhost:1313/docs/hobby/study/cs15/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/study/cs15/</guid>
      <description>[딥러닝] 구글 BERT의 정석 | 트랜스포머 입문 # 목록 # 1.2 트랜스포머의 인코더 이해하기&#xA;1.3 트랜스포머의 디코더 이해하기&#xA;1.2 트랜스포머의 인코더 이해하기 # 2024-12-31 # 셀프 어텐션 # 셀프 어텐션은 문장 내 단어들이 서로 얼마나 중요한지를 계산하는 과정. 트랜스포머는 이를 위해 입력 단어를 쿼리(Query), 키(Key), 밸류(Value) 세 가지 벡터로 변환하여 연관성을 구한다. 어텐션 점수 계산 예제 # &amp;ldquo;The cat sat on the mat.&amp;rdquo;&#xA;각 단어 벡터(예: 512차원)를 가중치 행렬과 곱하여 쿼리(Q), 키(K), 밸류(V)벡터를 생성한다.</description>
    </item>
    <item>
      <title>공부</title>
      <link>http://localhost:1313/docs/hobby/study/cs16/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/study/cs16/</guid>
      <description>[딥러닝] 구글 BERT의 정석 | BERT 입문 # 목록 # 2.3 BERT의 구조&#xA;2.4 BERT 사전 학습&#xA;2.3 BERT의 구조 # 2024.12.31 # BERT의 전체 구조 # 트랜스포머의 인코더(Encoder) 블록을 여러 개 쌓은 형태. 입력: 문장 (토큰화된 형태) 내부 구조: N개의 Transformer Encoder Blocks (기본 모델은 12개, 큰 모델은 24개) 출력: 각 토큰의 벡터 표현 (Contextual Embedding) cf) BERT의 대표적인 모델 크기&#xA;모델 # 인코더 층 숨겨진 차원 (dmodel) 어텐션 헤드 수 파라미터 수 BERT-Base 12 768 12 110M BERT-Large 24 1024 16 340M BERT의 입력 처리 # 입력 토큰 (Token Embedding) WordPiece Tokenization을 사용하며, 단어를 서브워드(subword) 단위로 분할하고 각 토큰은 고유한 임베딩 벡터로 변환된다.</description>
    </item>
    <item>
      <title>공부</title>
      <link>http://localhost:1313/docs/hobby/study/cs17/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/study/cs17/</guid>
      <description>[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 4.1 ALBERT&#xA;4.3 RoBERTa&#xA;4.4 ELECTRA&#xA;4.1 ALBERT # 2024-12-31 # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.&#xA;크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다. 펙토라이즈 임베딩 변수화 # BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.</description>
    </item>
  </channel>
</rss>
