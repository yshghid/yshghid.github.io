<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="머신러닝의 다양한 알고리즘 # 목록 # 회귀-선형회귀 &raquo; 분류-로지스틱회귀 &raquo; 분류-SVM(서포트 벡터 머신) &raquo; 분류-결정 트리 &raquo; 분류-랜덤 포레스트 &raquo; 분류-k-NN (k 최근접 이웃법) &raquo; 클러스터링-k-means &raquo; 1. 회귀-선형회귀 # 선형회귀는 피쳐 X를 수치로 입력하면 예측 결과 y를 수치로 출력해준다. 예측하고싶은 상황 X와 예측되는 결과 y에 강한 상관관계가 있을때 쓸수있는 방법이다. 상관관계가 강한 데이터를 산포도로 나타내면 점의 나열이 선을 그은것처럼 보임. 선이 아닌 이유는 현실세계에 오차가 있기 때문이고 &lsquo;오차가 없으면 이런 선이 될것이다&rsquo;라고 예상되는 선을 생각할 수 있다.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/study/cs/cs18/">
  <meta property="og:site_name" content="Lifelog 2025">
  <meta property="og:title" content="CS">
  <meta property="og:description" content="머신러닝의 다양한 알고리즘 # 목록 # 회귀-선형회귀 » 분류-로지스틱회귀 » 분류-SVM(서포트 벡터 머신) » 분류-결정 트리 » 분류-랜덤 포레스트 » 분류-k-NN (k 최근접 이웃법) » 클러스터링-k-means » 1. 회귀-선형회귀 # 선형회귀는 피쳐 X를 수치로 입력하면 예측 결과 y를 수치로 출력해준다. 예측하고싶은 상황 X와 예측되는 결과 y에 강한 상관관계가 있을때 쓸수있는 방법이다. 상관관계가 강한 데이터를 산포도로 나타내면 점의 나열이 선을 그은것처럼 보임. 선이 아닌 이유는 현실세계에 오차가 있기 때문이고 ‘오차가 없으면 이런 선이 될것이다’라고 예상되는 선을 생각할 수 있다.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>CS | Lifelog 2025</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/study/cs/cs18/">
<link rel="stylesheet" href="/book.min.b79d7c33395061c8f79ecaf2ed506fabfbb4f7a048c6bf40218447335d11296c.css" integrity="sha256-t518MzlQYcj3nsry7VBvq/u096BIxr9AIYRHM10RKWw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.3cee19c080ab174413ad44df468ef1f12bf9ec37b69659caac14ca71cd7844e4.js" integrity="sha256-PO4ZwICrF0QTrUTfRo7x8Sv57De2llnKrBTKcc14ROQ=" crossorigin="anonymous"></script>

  

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/logo.png" alt="Logo" class="book-icon" /><span>Lifelog 2025</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>기록</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/book/" class="">책</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/movie/" class="">영화</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/daily/" class="">일상</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>공부</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/study/bi/" class="">BI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/cs/" class="">CS</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/disk/" class="">◡̈⋆*</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/about/" class=""> </a>
  

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>CS</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#목록">목록</a></li>
    <li><a href="#1-회귀-선형회귀">1. 회귀-선형회귀</a></li>
    <li><a href="#2-분류-로지스틱회귀">2. 분류-로지스틱회귀</a></li>
    <li><a href="#3-분류-svm서포트-벡터-머신">3. 분류-SVM(서포트 벡터 머신)</a></li>
    <li><a href="#4-분류-결정-트리">4. 분류-결정 트리</a></li>
    <li><a href="#5-분류-랜덤-포레스트">5. 분류-랜덤 포레스트</a></li>
    <li><a href="#6-분류-k-nn-k-최근접-이웃법">6. 분류-k-NN (k 최근접 이웃법)</a></li>
    <li><a href="#7-클러스터링-k-means">7. 클러스터링-k-means</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="머신러닝의-다양한-알고리즘">
  머신러닝의 다양한 알고리즘
  <a class="anchor" href="#%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9d%ec%9d%98-%eb%8b%a4%ec%96%91%ed%95%9c-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98">#</a>
</h1>
<h2 id="목록">
  목록
  <a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#</a>
</h2>
<ol>
<li>회귀-선형회귀 <a href="https://yshghid.github.io/docs/study/cs/cs18/#1-%ed%9a%8c%ea%b7%80-%ec%84%a0%ed%98%95%ed%9a%8c%ea%b7%80">&raquo;</a></li>
<li>분류-로지스틱회귀 <a href="https://yshghid.github.io/docs/study/cs/cs18/#2-%eb%b6%84%eb%a5%98-%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80">&raquo;</a></li>
<li>분류-SVM(서포트 벡터 머신) <a href="https://yshghid.github.io/docs/study/cs/cs18/#3-%eb%b6%84%eb%a5%98-svm%ec%84%9c%ed%8f%ac%ed%8a%b8-%eb%b2%a1%ed%84%b0-%eb%a8%b8%ec%8b%a0">&raquo;</a></li>
<li>분류-결정 트리 <a href="https://yshghid.github.io/docs/study/cs/cs18/#4-%eb%b6%84%eb%a5%98-%ea%b2%b0%ec%a0%95-%ed%8a%b8%eb%a6%ac">&raquo;</a></li>
<li>분류-랜덤 포레스트 <a href="https://yshghid.github.io/docs/study/cs/cs18/#2-%eb%b6%84%eb%a5%98-%eb%9e%9c%eb%8d%a4-%ed%8f%ac%eb%a0%88%ec%8a%a4%ed%8a%b8">&raquo;</a></li>
<li>분류-k-NN (k 최근접 이웃법) <a href="https://yshghid.github.io/docs/study/cs/cs18/#6-%eb%b6%84%eb%a5%98-k-nn-k-%ec%b5%9c%ea%b7%bc%ec%a0%91-%ec%9d%b4%ec%9b%83%eb%b2%95">&raquo;</a></li>
<li>클러스터링-k-means <a href="https://yshghid.github.io/docs/study/cs/cs18/#7-%ed%81%b4%eb%9f%ac%ec%8a%a4%ed%84%b0%eb%a7%81-k-means">&raquo;</a></li>
</ol>
<hr>
<h2 id="1-회귀-선형회귀">
  1. 회귀-선형회귀
  <a class="anchor" href="#1-%ed%9a%8c%ea%b7%80-%ec%84%a0%ed%98%95%ed%9a%8c%ea%b7%80">#</a>
</h2>
<ul>
<li>선형회귀는 피쳐 X를 수치로 입력하면 예측 결과 y를 수치로 출력해준다. 예측하고싶은 상황 X와 예측되는 결과 y에 강한 상관관계가 있을때 쓸수있는 방법이다.</li>
<li>상관관계가 강한 데이터를 산포도로 나타내면 점의 나열이 선을 그은것처럼 보임. 선이 아닌 이유는 현실세계에 오차가 있기 때문이고 &lsquo;오차가 없으면 이런 선이 될것이다&rsquo;라고 예상되는 선을 생각할 수 있다.</li>
<li>이 선을 직선으로 연결하면 선형 회귀, 직선이 아닌 선으로 연결하면 비선형 회귀.</li>
<li>선형회귀는 직선을 어느 각도로 어느 위치로 그릴것인가를 알고리즘으로 구하며 최소 제곱법을 사용한다.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_regression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#데이터 세트 만들기</span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_regression(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, noise<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(df[<span style="color:#ae81ff">0</span>], y, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/ab403ef4-ea55-4055-8a24-d662a73c69f9" alt="image" /></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> r2_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#훈련,테스트 데이터로 나누기</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#선형회귀 학습모델 만들기</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#모델의 정답률 확인</span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> r2_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#데이터포인트를 많이 예측해서 산포도에 그리고 선으로 만들기.</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(X, y, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>) 
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(X, model<span style="color:#f92672">.</span>predict(X), color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 84.98344774428922 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/0c816443-c86d-4e79-bbe0-844a7acc452d" alt="image" /></p>
<p>편차가 적은 데이터를 사용해서 정답률이 꽤 높지만 편차가 많은 데이터를 사용하면? 노이즈를 80으로 늘린 회귀용 데이터 세트로 수행하기.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#데이터 생성</span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_regression(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, noise<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#훈련,테스트 데이터로 나누기</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#선형회귀 학습모델 만들기</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#모델의 정답률 확인</span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> r2_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#데이터포인트를 많이 예측해서 산포도에 그리고 선으로 만들기.</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(X, y, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>) 
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(X, model<span style="color:#f92672">.</span>predict(X), color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 33.025689869605145 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/2071630f-9b2f-4fec-aab7-f9fe50c9f89c" alt="image" /></p>
<p>선을 그렸지만 데이터를 설명하기엔 무리가있는 선임. 정답률도 낮다.</p>
<hr>
<h2 id="2-분류-로지스틱회귀">
  2. 분류-로지스틱회귀
  <a class="anchor" href="#2-%eb%b6%84%eb%a5%98-%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80">#</a>
</h2>
<p><a href="https://ebbnflow.tistory.com/129"><img src="https://github.com/user-attachments/assets/34bb7cbf-2980-43c8-844e-00fa52369d64" alt="image" /></a></p>
<ul>
<li>선형 회귀는 피쳐 X가 어떤 값일때 결과가 되는 목적변수 y를 예측하는 알고리즘. 선형회귀는 결과가 &lsquo;Y or N&rsquo;와 같은 답이 2개인 데이터에 적절하지 않다.</li>
<li>그래서 이 선에 모든 값을 0~1 사이로 변환하는 &lsquo;로지스틱 시그모이드 함수&rsquo;를 적용하면 결과가 0,1 로 수렴되어서 두개의 답에 적합한 선이 된다.</li>
</ul>
<p>(&lsquo;데이터의 규칙은 오차없는경우 원래의 선의 형태로 돌아갈것이다&rsquo;라는 의미에서 회귀지만 얻어지는 예측은 &lsquo;0,1 중 어느쪽이 되는가&rsquo;이므로 로지스틱 회귀는 분류에 사용된다)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#산포도에 분류 상태를 그리는 함수</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_boundary</span>(model,X,Y,target,xlabel,ylabel):
</span></span><span style="display:flex;"><span>    cmap_dots <span style="color:#f92672">=</span> ListedColormap([<span style="color:#e6db74">&#34;#1f77b4&#34;</span>, <span style="color:#e6db74">&#34;#ff7f0e&#34;</span>, <span style="color:#e6db74">&#34;#2ca02c&#34;</span>])
</span></span><span style="display:flex;"><span>    cmap_fills <span style="color:#f92672">=</span> ListedColormap([<span style="color:#e6db74">&#34;#c6dcec&#34;</span>, <span style="color:#e6db74">&#34;#ffdec2&#34;</span>, <span style="color:#e6db74">&#34;#cae7ca&#34;</span>])
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> model:
</span></span><span style="display:flex;"><span>        XX,YY <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(
</span></span><span style="display:flex;"><span>            np<span style="color:#f92672">.</span>linspace(X<span style="color:#f92672">.</span>min()<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,X<span style="color:#f92672">.</span>max()<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">200</span>),
</span></span><span style="display:flex;"><span>            np<span style="color:#f92672">.</span>linspace(Y<span style="color:#f92672">.</span>min()<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,Y<span style="color:#f92672">.</span>max()<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">200</span>))
</span></span><span style="display:flex;"><span>        pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(np<span style="color:#f92672">.</span>c_[XX<span style="color:#f92672">.</span>ravel(),YY<span style="color:#f92672">.</span>ravel()])<span style="color:#f92672">.</span>reshape(XX<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>pcolormesh(XX,YY,pred,cmap<span style="color:#f92672">=</span>cmap_fills,shading<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>)
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>contour(XX,YY,pred,colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gray&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#target값(0~2) 점을 그림</span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(X,Y,c<span style="color:#f92672">=</span>target,cmap<span style="color:#f92672">=</span>cmap_dots)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(xlabel)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(ylabel)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_blobs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#데이터 세트 생성</span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_blobs(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>)
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X)
</span></span><span style="display:flex;"><span>plot_boundary(<span style="color:#66d9ef">None</span>, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/d87a245f-0d28-4f7a-a67f-b55143225caf" alt="image" /></p>
<p>두종류로 분류 가능해보이는 데이터 생성.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#데이터 세트 생성</span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_blobs(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>)
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X)
</span></span><span style="display:flex;"><span>plot_boundary(<span style="color:#66d9ef">None</span>, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/a9aec965-75cb-45a6-a4b1-842bab3f0cd1" alt="image" /></p>
<p>세종류로 분류 가능한 데이터 생성.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#훈련,테스트 데이터로 나누기</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#로지스틱회귀 학습모델 만들기</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LogisticRegression()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#모델의 정답률 확인</span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#데이터포인트를 많이 예측해서 산포도에 그리고 선으로 만들기.</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 82.66666666666667 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/2ed3320a-a1f9-4980-9995-c87cfc7881f4" alt="image" /></p>
<p>cf) 선형회귀 학습모델 만들면?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 60.868106468551275 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/255d456e-95cd-4d57-9cef-133f8afa85d5" alt="image" /></p>
<hr>
<h2 id="3-분류-svm서포트-벡터-머신">
  3. 분류-SVM(서포트 벡터 머신)
  <a class="anchor" href="#3-%eb%b6%84%eb%a5%98-svm%ec%84%9c%ed%8f%ac%ed%8a%b8-%eb%b2%a1%ed%84%b0-%eb%a8%b8%ec%8b%a0">#</a>
</h2>
<ul>
<li>SVM에서는 분류의 경계선을 구할 때 &lsquo;서포트 벡터로부터의 마진(여백)&rsquo; 즉 경계선까지의 거리가 가장 멀어지도록 선을 그린다.</li>
<li>학습 데이터에 충실하게 구한 경계선이 하드 마진(hard margin), 약간의 오차를 허용해서 자연스러운 경계선을 구하면 소프트 마진(soft margin). 머신러닝에서는 soft margin을 이용한다.</li>
</ul>
<p><a href="https://velog.io/@vector13/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EA%B5%90%EA%B3%BC%EC%84%9C-Ch03-2"><img src="https://github.com/user-attachments/assets/9d6256f7-2341-4820-803c-2e425069013b" alt="image" /></a></p>
<ul>
<li>직선으로 분류할 수 없는 데이터에서 분류 경계를 구하려면? 2차원에서 보면 원형인 데이터가 3차원으로 보면 산 모양의 데이터일수도 있다. 이 경우 수평으로 둥글게 자르면(decision surface) 분류할 수 있고 이를 2차원으로 되돌리면(decision line) 분류 경계를 구할 수 있다.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> svm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_blobs(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;linear&#34;</span>) <span style="color:#75715e">#선형 SVM</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 89.60000000000001 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/fe23d6b3-f5f4-463a-b996-d530f3a665fc" alt="image" /></p>
<p>직선을 이용해 셋으로 분류할수있다.</p>
<p>비선형 분류 해보면?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rbf&#34;</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e">#비선형, 감마는 경계선의 복잡도.</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 85.6 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/f1c06ae6-ebe8-4a7e-a343-3ef162a8cdc5" alt="image" /></p>
<p>감마=10으로 하면?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rbf&#34;</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 72.8 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/8a21ec73-21e4-45eb-a852-89568e61196b" alt="image" /></p>
<p>경계선이 개별 데이터의 영향을 지나치게 받고 있어서 학습 데이터에 조금의 오차가 있는 경우 정답률이 떨어질수있다.</p>
<p>감마=0.1로 하면?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rbf&#34;</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 89.60000000000001 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/04f6ded7-7d08-46f8-81ba-3ef02103bf55" alt="image" /></p>
<p>경계선이 단순해졌다. 복잡도를 데이터 개수나 편차로부터 자동으로 생성할수도 있다. 기본인 scale을 적용하면?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rbf&#34;</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 90.4 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/8aa5b32a-23f1-49b6-8e16-904d8becd2b6" alt="image" /></p>
<p>깔끔하게 분류됨!</p>
<hr>
<h2 id="4-분류-결정-트리">
  4. 분류-결정 트리
  <a class="anchor" href="#4-%eb%b6%84%eb%a5%98-%ea%b2%b0%ec%a0%95-%ed%8a%b8%eb%a6%ac">#</a>
</h2>
<ul>
<li>효과적인 조건으로 분기를 반복해서 분류를 예측하는 알고리즘</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X,y <span style="color:#f92672">=</span> make_blobs(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> DecisionTreeClassifier(max_depth<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 96.0 %
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/4f729da9-2909-4bf2-9ca1-43c2d3e4000c" alt="image" /></p>
<p>트리 구조를 확인해보기.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> plot_tree
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>,<span style="color:#ae81ff">12</span>))
</span></span><span style="display:flex;"><span>plot_tree(model, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, filled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, feature_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;df[0]&#34;</span>,<span style="color:#e6db74">&#34;df[1]&#34;</span>], class_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;0&#34;</span>,<span style="color:#e6db74">&#34;1&#34;</span>,<span style="color:#e6db74">&#34;2&#34;</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/3833f5cf-ff8d-4d21-b463-97703723553c" alt="image" /></p>
<p>1번 분기에서는 df[1]이 1.874이하인가?로 분할하고 YES라면 class=1로 분류한다. 2번 분기에서는 df[0]이 -0.721이하인가?로 분할하고 YES라면 class=2로 분류한다. 이와 같이 분할을 반복했기 때문에 퍼즐 형태로 분할된것임. 여기서는 4개의 질문으로 분기했지만 &lsquo;어느 깊이까지 분기를 반복할것인지&rsquo;는 정할 수 있다. 분기 횟수가 적으면 정확도가 떨어지고 너무 깊게 분기하면 오차에 과도하게 적응해서 정확도가 떨어질 수 있다.</p>
<hr>
<h2 id="5-분류-랜덤-포레스트">
  5. 분류-랜덤 포레스트
  <a class="anchor" href="#5-%eb%b6%84%eb%a5%98-%eb%9e%9c%eb%8d%a4-%ed%8f%ac%eb%a0%88%ec%8a%a4%ed%8a%b8">#</a>
</h2>
<p>결정 트리는 &lsquo;어떻게 분기할 것인지&rsquo;에 대한 한 가지 패턴으로 분류했다. 여러 가지 분기 패턴의 결정 트리를 사용해서 예측하고 그 예측 결과로부터 다수결로 결정하는 알고리즘이 랜덤 포레스트.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> RandomForestClassifier()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot_boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 100.0%
</span></span></code></pre></div><p><img src="https://github.com/user-attachments/assets/cdf1a7d6-69ae-402c-b171-bddd98ffbb65" alt="image" /></p>
<hr>
<h2 id="6-분류-k-nn-k-최근접-이웃법">
  6. 분류-k-NN (k 최근접 이웃법)
  <a class="anchor" href="#6-%eb%b6%84%eb%a5%98-k-nn-k-%ec%b5%9c%ea%b7%bc%ec%a0%91-%ec%9d%b4%ec%9b%83%eb%b2%95">#</a>
</h2>
<p>산포도상에서 가까운 데이터는 특징이 비슷하고 먼 것은 특징이 비슷하지 않다. 거리가 가까우면 특징도 가까우므로 근처에 있는 k개 데이터의 분류를 조사해서 다수결로 어느 분류에 가까울지를 예측한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> KNeighborsClassifier()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> accuracy_score(y_test, pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;정답률:&#34;</span>, score<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_test)
</span></span><span style="display:flex;"><span>plot<span style="color:#f92672">.</span>boundary(model, df[<span style="color:#ae81ff">0</span>], df[<span style="color:#ae81ff">1</span>], y_test, <span style="color:#e6db74">&#34;df[0]&#34;</span>, <span style="color:#e6db74">&#34;df[1]&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>정답률: 100.0%
</span></span></code></pre></div><hr>
<h2 id="7-클러스터링-k-means">
  7. 클러스터링-k-means
  <a class="anchor" href="#7-%ed%81%b4%eb%9f%ac%ec%8a%a4%ed%84%b0%eb%a7%81-k-means">#</a>
</h2>
<p>k NN은 지도 학습의 분류 알고리즘이고 k means는 비지도 학습의 클러스터링 알고리즘이다.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments"><script src="https://giscus.app/client.js"
        data-repo="yshghid/yshghid.github.io"
        data-repo-id="R_kgDONkMkNg"
        data-category-id="DIC_kwDONkMkNs4CloJh"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>
</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#목록">목록</a></li>
    <li><a href="#1-회귀-선형회귀">1. 회귀-선형회귀</a></li>
    <li><a href="#2-분류-로지스틱회귀">2. 분류-로지스틱회귀</a></li>
    <li><a href="#3-분류-svm서포트-벡터-머신">3. 분류-SVM(서포트 벡터 머신)</a></li>
    <li><a href="#4-분류-결정-트리">4. 분류-결정 트리</a></li>
    <li><a href="#5-분류-랜덤-포레스트">5. 분류-랜덤 포레스트</a></li>
    <li><a href="#6-분류-k-nn-k-최근접-이웃법">6. 분류-k-NN (k 최근접 이웃법)</a></li>
    <li><a href="#7-클러스터링-k-means">7. 클러스터링-k-means</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












