<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 2024-12-31 ⋯ 4.1 ALBERT
2024-12-31 ⋯ 4.3 RoBERTa
2024-12-31 ⋯ 4.4 ELECTRA
4.1 ALBERT # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.
크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/study/bioinformatics/cs17/">
  <meta property="og:site_name" content=" ">
  <meta property="og:title" content="구글 BERT의 정석 | BERT의 파생 모델">
  <meta property="og:description" content="[딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT # 목록 # 2024-12-31 ⋯ 4.1 ALBERT
2024-12-31 ⋯ 4.3 RoBERTa
2024-12-31 ⋯ 4.4 ELECTRA
4.1 ALBERT # ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.
크로스 레이어 변수 공유 # BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다. ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2024-12-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-31T00:00:00+00:00">
    <meta property="article:tag" content="2024-12">
<title>구글 BERT의 정석 | BERT의 파생 모델 |  </title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/study/bioinformatics/cs17/">
<link rel="stylesheet" href="/book.min.30a7836b6a89342da3b88e7afd1036166aeced16c8de12df060ded2031837886.css" integrity="sha256-MKeDa2qJNC2juI56/RA2Fmrs7RbI3hLfBg3tIDGDeIY=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.79deebc72f3440b676cd1f5545b8e4ca3b5f305d6a7275d76af83081d3de87fe.js" integrity="sha256-ed7rxy80QLZ2zR9VRbjkyjtfMF1qcnXXavgwgdPeh/4=" crossorigin="anonymous"></script>

  

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/logo.png" alt="Logo" class="book-icon" /><span> </span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>기록</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/daily/" class="">일상</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/book/" class="">글</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>공부</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/study/bioinformatics/" class="">Bioinformatics</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/ai/" class="">AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/sw/" class="">SW</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/algorithm/" class="">알고리즘</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/career/" class="">취업</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>구글 BERT의 정석 | BERT의 파생 모델</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#목록">목록</a></li>
    <li><a href="#41-albert">4.1 ALBERT</a>
      <ul>
        <li><a href="#크로스-레이어-변수-공유">크로스 레이어 변수 공유</a></li>
        <li><a href="#펙토라이즈-임베딩-변수화">펙토라이즈 임베딩 변수화</a></li>
        <li><a href="#문장-순서-예측">문장 순서 예측</a></li>
        <li><a href="#albert와-bert-비교">ALBERT와 BERT 비교</a></li>
        <li><a href="#albert에서-임베딩-추출">ALBERT에서 임베딩 추출</a></li>
      </ul>
    </li>
    <li><a href="#43-roberta">4.3 RoBERTa</a>
      <ul>
        <li><a href="#정적-마스크-대신-동적-마스크-사용">정적 마스크 대신 동적 마스크 사용</a></li>
        <li><a href="#nsp-테스크-제거">NSP 테스크 제거</a></li>
        <li><a href="#더-많은-데이터로-학습">더 많은 데이터로 학습</a></li>
        <li><a href="#큰-배치-크기로-학습">큰 배치 크기로 학습</a></li>
        <li><a href="#bbpe-토크나이저-사용">BBPE 토크나이저 사용</a></li>
      </ul>
    </li>
    <li><a href="#44-electra">4.4 ELECTRA</a>
      <ul>
        <li><a href="#교체한-토큰-판별-테스크">교체한 토큰 판별 테스크</a></li>
        <li><a href="#electra의-생성자와-판별자">ELECTRA의 생성자와 판별자</a></li>
        <li><a href="#electra-모델-학습">ELECTRA 모델 학습</a></li>
        <li><a href="#효율적인-학습-방법-탐색">효율적인 학습 방법 탐색</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="딥러닝-구글-bert의-정석--bert의-파생-모델-albert-roberta-electra-spanbert">
  [딥러닝] 구글 BERT의 정석 | BERT의 파생 모델: ALBERT, RoBERTa, ELECTRA, SpanBERT
  <a class="anchor" href="#%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b5%ac%ea%b8%80-bert%ec%9d%98-%ec%a0%95%ec%84%9d--bert%ec%9d%98-%ed%8c%8c%ec%83%9d-%eb%aa%a8%eb%8d%b8-albert-roberta-electra-spanbert">#</a>
</h1>
<h2 id="목록">
  목록
  <a class="anchor" href="#%eb%aa%a9%eb%a1%9d">#</a>
</h2>
<p><em>2024-12-31</em> ⋯ <a href="https://yshghid.github.io/docs/study/cs/cs17/#41-albert">4.1 ALBERT</a></p>
<p><em>2024-12-31</em> ⋯ <a href="https://yshghid.github.io/docs/study/cs/cs17/#43-roberta">4.3 RoBERTa</a></p>
<p><em>2024-12-31</em> ⋯ <a href="https://yshghid.github.io/docs/study/cs/cs17/#44-electra">4.4 ELECTRA</a></p>
<hr>
<h2 id="41-albert">
  4.1 ALBERT
  <a class="anchor" href="#41-albert">#</a>
</h2>
<blockquote>
<p>ALBERT (A Lite BERT)는 BERT 모델의 성능을 유지하면서도 파라미터 수를 줄이고, 더 효율적인 학습을 목표로 한 모델.</p>
</blockquote>
<h3 id="크로스-레이어-변수-공유">
  크로스 레이어 변수 공유
  <a class="anchor" href="#%ed%81%ac%eb%a1%9c%ec%8a%a4-%eb%a0%88%ec%9d%b4%ec%96%b4-%eb%b3%80%ec%88%98-%ea%b3%b5%ec%9c%a0">#</a>
</h3>
<ul>
<li>BERT는 각 Transformer 레이어마다 별도의 가중치와 바이어스를 갖는다.</li>
<li>ALBERT는 동일한 파라미터 집합을 여러 레이어에 걸쳐 사용하여 모델의 파라미터 수를 크게 줄인다.</li>
</ul>
<h3 id="펙토라이즈-임베딩-변수화">
  펙토라이즈 임베딩 변수화
  <a class="anchor" href="#%ed%8e%99%ed%86%a0%eb%9d%bc%ec%9d%b4%ec%a6%88-%ec%9e%84%eb%b2%a0%eb%94%a9-%eb%b3%80%ec%88%98%ed%99%94">#</a>
</h3>
<ul>
<li>BERT는 vocab_size x hidden_size 크기의 임베딩 행렬을 사용하여, vocab_size와 hidden_size에 비례하는 수의 파라미터를 필요로 한다.</li>
<li>ALBERT는 임베딩 행렬을 두 개의 더 작은 행렬로 분해하여, 파라미터 수를 줄인다(임베딩 팩토라이제이션).
<ul>
<li>행렬1: vocab_size x embedding_size</li>
<li>행렬2: embedding_size x hidden_size</li>
</ul>
</li>
</ul>
<h3 id="문장-순서-예측">
  문장 순서 예측
  <a class="anchor" href="#%eb%ac%b8%ec%9e%a5-%ec%88%9c%ec%84%9c-%ec%98%88%ec%b8%a1">#</a>
</h3>
<ul>
<li>
<p>BERT에서는 Masked Language Modeling (MLM)과 Next Sentence Prediction (NSP)을 사용.</p>
<ul>
<li>NSP은 두 문장이 연속적으로 존재하는지를 예측하는 태스크. 문장 간 관계를 학습하는 데 사용됨.</li>
</ul>
</li>
<li>
<p>ALBERT는 문장 순서 예측 (SOP, Sentence Order Prediction) 라는 새로운 학습 태스크를 도입.</p>
<ul>
<li>두 문장이 주어졌을 때, 두 문장의 순서가 올바른지를 예측함.</li>
</ul>
</li>
<li>
<p>SOP는 문장 간의 순서 관계를 이해하는 데 NSP보다 적합하다.</p>
</li>
</ul>
<h3 id="albert와-bert-비교">
  ALBERT와 BERT 비교
  <a class="anchor" href="#albert%ec%99%80-bert-%eb%b9%84%ea%b5%90">#</a>
</h3>
<ol>
<li>크로스 레이어 변수 공유: ALBERT는 여러 레이어에서 파라미터를 공유하여 파라미터 수를 크게 줄임. BERT는 각 레이어마다 독립적인 파라미터 집합을 사용.</li>
<li>펙토라이즈 임베딩: ALBERT는 임베딩 행렬을 분해하여 파라미터 수를 줄임. BERT는 한 번에 큰 임베딩 행렬을 사용.</li>
<li>문장 순서 예측 (SOP): ALBERT는 NSP 대신 SOP를 사용하여 문장 순서를 더 잘 예측할 수 있게 하여, 문장 간 관계 학습을 개선.</li>
</ol>
<h3 id="albert에서-임베딩-추출">
  ALBERT에서 임베딩 추출
  <a class="anchor" href="#albert%ec%97%90%ec%84%9c-%ec%9e%84%eb%b2%a0%eb%94%a9-%ec%b6%94%ec%b6%9c">#</a>
</h3>
<ol>
<li>
<p>단어 임베딩</p>
<ul>
<li>입력 텍스트의 각 단어를 vocab_size x embedding_size 크기의 행렬을 사용하여 고차원 벡터로 변환함. 이 벡터는 각 단어의 의미를 반영하는 고차원적인 특징을 갖고있다.</li>
</ul>
</li>
<li>
<p>레벨 별 임베딩 추출 (Layer-wise Embedding Extraction)</p>
<ul>
<li>입력 텍스트가 Transformer 모델을 통과하면서 각 레이어에서 벡터 표현이 점진적으로 변환된다.</li>
<li>ALBERT에서는 주로 첫 번째 레이어 또는 최종 레이어에서 추출된 임베딩을 사용할 수 있음.</li>
<li>첫 번째 레이어에는 주로 단어의 기본적인 의미와 구조적 특징 정보.</li>
<li>최종 레이어에는 문장 전체의 복합적인 의미와 문맥이 결합되어, 더 구체화되고 세부적인 정보.</li>
</ul>
</li>
<li>
<p>중간 레이어에서의 임베딩 추출</p>
</li>
</ol>
<ul>
<li>ALBERT는 다중 레이어 구조를 갖기 때문에, 중간 레이어의 출력도 사용할 수 있음.</li>
<li>문장 내 특정 단어의 문맥을 더 잘 반영하는 중간 레이어의 임베딩을 추출할 수 있다.</li>
<li>사용자가 수행하는 작업에 따라, 특정 작업에 적합한 레이어의 출력을 선택.
<ul>
<li>예를 들어, 문장 분류 작업에서는 모델의 최종 레이어에서 추출된 임베딩이 더 중요할 수 있으며, 개체명 인식(NER) 작업에서는 중간 레이어에서 나온 임베딩이 더 유용할 수 있다.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="43-roberta">
  4.3 RoBERTa
  <a class="anchor" href="#43-roberta">#</a>
</h2>
<h3 id="정적-마스크-대신-동적-마스크-사용">
  정적 마스크 대신 동적 마스크 사용
  <a class="anchor" href="#%ec%a0%95%ec%a0%81-%eb%a7%88%ec%8a%a4%ed%81%ac-%eb%8c%80%ec%8b%a0-%eb%8f%99%ec%a0%81-%eb%a7%88%ec%8a%a4%ed%81%ac-%ec%82%ac%ec%9a%a9">#</a>
</h3>
<ul>
<li>BERT는 정적 마스크 (Static Masking) 방식을 사용하여 훈련함. 훈련 데이터에서 마스킹할 단어를 고르고, 그 마스크를 모든 훈련 단계에서 동일하게 유지한다. 즉 같은 단어가 훈련 내내 계속 마스크된다.</li>
<li>RoBERTa는 동적 마스크 (Dynamic Masking) 방식을 사용. 즉, 각 훈련 배치마다 문장에서 마스크되는 단어가 랜덤하게 변경된다.</li>
<li>정적 마스크에서는 동일한 문맥을 반복해서 학습하므로, 모델이 특정 단어의 패턴을 암기할 수 있는데, 동적 마스크에서는 훈련마다 마스크가 달라져 모델이 더 다양한 방식으로 문맥을 학습할 수 있도록 돕고, 일관된 마스크 패턴에 의한 편향을 줄여 모델이 더 일반화된 특징을 학습할 수 있게 해준다.</li>
</ul>
<h3 id="nsp-테스크-제거">
  NSP 테스크 제거
  <a class="anchor" href="#nsp-%ed%85%8c%ec%8a%a4%ed%81%ac-%ec%a0%9c%ea%b1%b0">#</a>
</h3>
<ul>
<li>BERT 모델은 훈련 과정에서 MLM, NSP 테스크를 사용한다.</li>
<li>RoBERTa는 NSP 대신 MLM만을 사용하여 훈련을 진행. NSP 제거의 이유는 문장 간의 관계 학습에 NSP가 크게 기여하지 않으며 제거 시 훈련이 더 간단해지고, 모델이 더욱 집중해서 문맥을 학습할 수 있음.</li>
</ul>
<h3 id="더-많은-데이터로-학습">
  더 많은 데이터로 학습
  <a class="anchor" href="#%eb%8d%94-%eb%a7%8e%ec%9d%80-%eb%8d%b0%ec%9d%b4%ed%84%b0%eb%a1%9c-%ed%95%99%ec%8a%b5">#</a>
</h3>
<ul>
<li>RoBERTa는 BERT보다 훨씬 더 많은 데이터로 훈련. BERT는 16GB 크기의 BooksCorpus와 English Wikipedia로 훈련되었지만, RoBERTa는 여기에 추가로 Common Crawl 데이터, CC-News, OpenWebText, Stories 등의 더 많은 데이터를 포함하여 훈련됨.</li>
</ul>
<h3 id="큰-배치-크기로-학습">
  큰 배치 크기로 학습
  <a class="anchor" href="#%ed%81%b0-%eb%b0%b0%ec%b9%98-%ed%81%ac%ea%b8%b0%eb%a1%9c-%ed%95%99%ec%8a%b5">#</a>
</h3>
<ul>
<li>RoBERTa는 훈련에 더 큰 배치 크기를 사용합니다. BERT는 일반적으로 배치 크기를 32 또는 64로 설정하여 훈련하지만, RoBERTa는 배치 크기 8,000까지 사용하여 훈련했습니다.</li>
</ul>
<blockquote>
<p><strong>큰 배치 크기?</strong></p>
<ul>
<li>배치가 크면 모델이 더 많은 데이터를 한 번에 처리할 수 있게 해주고, 훈련 속도를 높이는 데 기여함.</li>
<li>학습 안정성을 높여, 학습 과정에서 발생할 수 있는 불안정한 그래디언트 문제를 완화하는 데 도움을 줌.</li>
</ul>
</blockquote>
<h3 id="bbpe-토크나이저-사용">
  BBPE 토크나이저 사용
  <a class="anchor" href="#bbpe-%ed%86%a0%ed%81%ac%eb%82%98%ec%9d%b4%ec%a0%80-%ec%82%ac%ec%9a%a9">#</a>
</h3>
<ul>
<li>BERT는 WordPiece 토크나이저를 사용하여 텍스트를 서브워드 단위로 분할.</li>
<li>RoBERTa는 BBPE (Byte Pair Encoding) 토크나이저를 사용. 단어를 자주 발생하는 문자쌍으로 분할하여 서브워드 토큰을 만든다.
<ul>
<li>이는 드문 단어나 외래어가 포함된 텍스트에서 더욱 효과적임.</li>
</ul>
</li>
<li>BBPE는 단어를 더 작은 조각으로 나누고, 이를 더 자주 사용되는 문자쌍으로 합치는 방식으로 작동함.</li>
<li>효과: 어휘 집합 크기를 줄이면서도 다양한 단어를 처리할 수 있게 해주며, 모델의 효율성을 높이고, 모든 언어에서 유연한 처리가 가능.</li>
</ul>
<hr>
<h2 id="44-electra">
  4.4 ELECTRA
  <a class="anchor" href="#44-electra">#</a>
</h2>
<h3 id="교체한-토큰-판별-테스크">
  교체한 토큰 판별 테스크
  <a class="anchor" href="#%ea%b5%90%ec%b2%b4%ed%95%9c-%ed%86%a0%ed%81%b0-%ed%8c%90%eb%b3%84-%ed%85%8c%ec%8a%a4%ed%81%ac">#</a>
</h3>
<ul>
<li>BERT와 같은 기존 모델들은 일부 단어를 마스킹하고 예측하는 방식(Masked Language Modeling, MLM)을 사용해서 모델을 학습.</li>
<li>이 방식은 마스크된 단어의 예측이 실제 문맥을 잘 반영하지 않게 될 수 있다는 단점이 있다.</li>
<li>ELECTRA는 교체한 토큰 판별 테스크 (Replaced Token Detection) 를 사용.</li>
<li>이 방식은 문장을 구성하는 각 토큰이 원래의 문장에서 그대로 있었는지 아니면 다른 토큰으로 교체되었는지를 구분하는 문제이며 이렇게 하면 모델은 교체된 단어를 구별하는 법을 배운다.</li>
</ul>
<h3 id="electra의-생성자와-판별자">
  ELECTRA의 생성자와 판별자
  <a class="anchor" href="#electra%ec%9d%98-%ec%83%9d%ec%84%b1%ec%9e%90%ec%99%80-%ed%8c%90%eb%b3%84%ec%9e%90">#</a>
</h3>
<ul>
<li>ELECTRA는 두 가지 모델로 구성된다.</li>
</ul>
<ol>
<li>
<p>생성자 (Generator)</p>
<ul>
<li>기존 BERT와 같은 Masked Language Model (MLM) 구조.</li>
<li>입력 문장에서 일부 단어를 [MASK]로 변환한 후, 이를 생성자의 예측 값으로 대체함.</li>
</ul>
</li>
<li>
<p>판별자 (Discriminator)</p>
<ul>
<li>문장 내 각 토큰이 진짜인지(fake) 가짜인지(real)를 분류하는 이진 분류(Binary Classification) 문제를 해결.</li>
</ul>
</li>
</ol>
<h3 id="electra-모델-학습">
  ELECTRA 모델 학습
  <a class="anchor" href="#electra-%eb%aa%a8%eb%8d%b8-%ed%95%99%ec%8a%b5">#</a>
</h3>
<ol>
<li>
<p>생성자 학습</p>
<ul>
<li>문장에서 일부 단어를 마스킹한 후, 생성자가 그 단어를 예측.</li>
<li>예측된 단어는 원래 단어 대신 교체된 단어(replaced token)로 사용됨.</li>
</ul>
</li>
<li>
<p>판별자 학습</p>
<ul>
<li>생성자가 만든 교체된 단어를 포함한 문장을 입력받음.</li>
<li>판별자는 문장 내 각 단어가 원래 단어인지, 교체된 단어인지 판별하는 작업을 수행.</li>
<li>판별자가 더 정확한 예측을 할수록 모델의 언어 이해 능력이 향상됨.</li>
</ul>
</li>
<li>
<p>손실 함수 계산</p>
<ul>
<li>생성자는 Cross-Entropy Loss (MLM 방식)</li>
<li>판별자는 Binary Classification Loss (Replaced Token Detection 방식)</li>
</ul>
</li>
<li>
<p>반복 학습</p>
<ul>
<li>생성자의 성능이 향상될수록 판별자의 분류 작업이 더 어려워짐.</li>
<li>결국 판별자가 더 정교한 문맥 이해 능력을 갖도록 최적화됨.</li>
</ul>
</li>
</ol>
<h3 id="효율적인-학습-방법-탐색">
  효율적인 학습 방법 탐색
  <a class="anchor" href="#%ed%9a%a8%ec%9c%a8%ec%a0%81%ec%9d%b8-%ed%95%99%ec%8a%b5-%eb%b0%a9%eb%b2%95-%ed%83%90%ec%83%89">#</a>
</h3>
<blockquote>
<p>ELECTRA 모델을 효율적으로 학습시키기 위해서 생성자와 판별자의 가중치를 공유한다.</p>
</blockquote>
<ul>
<li>
<p>기존 BERT는 마스킹된 토큰만 학습에 사용하지만, ELECTRA는 모든 토큰을 판별 작업에 사용하여 훨씬 더 높은 학습 데이터 활용률을 가짐.</p>
</li>
<li>
<p>기존의 MLM 방식보다 80% 적은 연산량으로 동일한 성능을 유지, 동일한 연산량을 사용했을 때 BERT보다 2~4배 더 빠르게 학습 가능.</p>
</li>
<li>
<p>생성자는 BERT와 같은 크기를 사용할 필요가 없어서, 생성자를 작은 크기의 모델로 설정하여 연산량을 절감.</p>
</li>
<li>
<p>ELECTRA-Small (14M parameters) → BERT-Small보다 86% 더 높은 성능 / ELECTRA-Large는 BERT-Large보다 적은 연산량으로 더 높은 성능을 보임.</p>
</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments"><script src="https://giscus.app/client.js"
        data-repo="yshghid/yshghid.github.io"
        data-repo-id="R_kgDONkMkNg"
        data-category-id="DIC_kwDONkMkNs4CloJh"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>
</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#목록">목록</a></li>
    <li><a href="#41-albert">4.1 ALBERT</a>
      <ul>
        <li><a href="#크로스-레이어-변수-공유">크로스 레이어 변수 공유</a></li>
        <li><a href="#펙토라이즈-임베딩-변수화">펙토라이즈 임베딩 변수화</a></li>
        <li><a href="#문장-순서-예측">문장 순서 예측</a></li>
        <li><a href="#albert와-bert-비교">ALBERT와 BERT 비교</a></li>
        <li><a href="#albert에서-임베딩-추출">ALBERT에서 임베딩 추출</a></li>
      </ul>
    </li>
    <li><a href="#43-roberta">4.3 RoBERTa</a>
      <ul>
        <li><a href="#정적-마스크-대신-동적-마스크-사용">정적 마스크 대신 동적 마스크 사용</a></li>
        <li><a href="#nsp-테스크-제거">NSP 테스크 제거</a></li>
        <li><a href="#더-많은-데이터로-학습">더 많은 데이터로 학습</a></li>
        <li><a href="#큰-배치-크기로-학습">큰 배치 크기로 학습</a></li>
        <li><a href="#bbpe-토크나이저-사용">BBPE 토크나이저 사용</a></li>
      </ul>
    </li>
    <li><a href="#44-electra">4.4 ELECTRA</a>
      <ul>
        <li><a href="#교체한-토큰-판별-테스크">교체한 토큰 판별 테스크</a></li>
        <li><a href="#electra의-생성자와-판별자">ELECTRA의 생성자와 판별자</a></li>
        <li><a href="#electra-모델-학습">ELECTRA 모델 학습</a></li>
        <li><a href="#효율적인-학습-방법-탐색">효율적인 학습 방법 탐색</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












