<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions # Abstract # Accurate prediction of binding interaction between T cell receptors (TCRs) and host cells is fundamental to understanding the regulation of the adaptive immune system as well as to developing data-driven approaches for personalized immunotherapy. While several machine learning models have been developed for this prediction task, the question of how to specifically embed TCR sequences into numeric representations remains largely unexplored compared to protein sequences in general.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/study/bi/bi14/">
  <meta property="og:site_name" content="Lifelog 2025">
  <meta property="og:title" content="BI">
  <meta property="og:description" content="Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions # Abstract # Accurate prediction of binding interaction between T cell receptors (TCRs) and host cells is fundamental to understanding the regulation of the adaptive immune system as well as to developing data-driven approaches for personalized immunotherapy. While several machine learning models have been developed for this prediction task, the question of how to specifically embed TCR sequences into numeric representations remains largely unexplored compared to protein sequences in general.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2025-02-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-02T00:00:00+00:00">
    <meta property="article:tag" content="2025-02">
<title>BI | Lifelog 2025</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/study/bi/bi14/">
<link rel="stylesheet" href="/book.min.b79d7c33395061c8f79ecaf2ed506fabfbb4f7a048c6bf40218447335d11296c.css" integrity="sha256-t518MzlQYcj3nsry7VBvq/u096BIxr9AIYRHM10RKWw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.3cee19c080ab174413ad44df468ef1f12bf9ec37b69659caac14ca71cd7844e4.js" integrity="sha256-PO4ZwICrF0QTrUTfRo7x8Sv57De2llnKrBTKcc14ROQ=" crossorigin="anonymous"></script>

  

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/logo.png" alt="Logo" class="book-icon" /><span>Lifelog 2025</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>기록</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/book/" class="">책</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/movie/" class="">영화</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/hobby/daily/" class="">일상</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>공부</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/study/bi/" class="">BI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/study/cs/" class="">CS</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/disk/" class="">◡̈⋆*</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/about/" class=""> </a>
  

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>BI</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#1-introduction">1 Introduction</a></li>
    <li><a href="#2-results">2 Results</a>
      <ul>
        <li><a href="#21catelmooutperforms-the-existing-embedding-methods-at-discriminating-binding-and-non-binding-tcr-epitope-pairs">2.1 catELMo outperforms the existing embedding methods at discriminating binding and non-binding TCR-epitope pairs</a></li>
        <li><a href="#22catelmoreduces-a-significant-amount-of-annotation-cost-for-achieving-comparable-prediction-power">2.2 catELMo reduces a significant amount of annotation cost for achieving comparable prediction power</a></li>
        <li><a href="#23catelmoallows-clustering-of-tcr-sequences-with-high-performance">2.3 catELMo allows clustering of TCR sequences with high performance</a></li>
        <li><a href="#24-elmo-based-architecture-is-preferable-to-bert-based-architecture-in-tcr-embedding-models">2.4 ELMo-based architecture is preferable to BERT-based architecture in TCR embedding models</a></li>
        <li><a href="#25-within-domain-transfer-learning-is-preferable-to-cross-domain-transfer-learning-in-tcr-analysis">2.5 Within-domain transfer learning is preferable to cross-domain transfer learning in TCR analysis</a></li>
      </ul>
    </li>
    <li><a href="#3-discussion">3 Discussion</a></li>
    <li><a href="#4-methods">4 Methods</a>
      <ul>
        <li><a href="#41-data">4.1 Data</a></li>
        <li><a href="#tcrs-for-trainingcatelmo"><em>TCRs for training catELMo</em></a></li>
        <li><a href="#tcr-epitope-pairs-for-binding-affinity-prediction"><em>TCR-epitope pairs for binding affinity prediction</em></a></li>
        <li><a href="#tcrs-for-antigen-specific-tcr-clustering"><em>TCRs for antigen-specific TCR clustering</em></a></li>
        <li><a href="#42-amino-acid-embedding-methods">4.2 Amino acid embedding methods</a></li>
        <li><a href="#421-static-embeddings"><em>4.2.1 Static embeddings</em></a></li>
        <li><a href="#blosum">BLOSUM</a></li>
        <li><a href="#word2vec-and-doc2vec">Word2vec and Doc2vec</a></li>
        <li><a href="#422-context-aware-embeddings"><em>4.2.2 Context-aware embeddings</em></a></li>
        <li><a href="#elmo">ELMo</a></li>
        <li><a href="#bert">BERT</a></li>
        <li><a href="#43-our-approachcatelmo">4.3 Our approach: catELMo</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="context-aware-amino-acid-embedding-advances-analysis-of-tcr-epitope-interactions">
  Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions
  <a class="anchor" href="#context-aware-amino-acid-embedding-advances-analysis-of-tcr-epitope-interactions">#</a>
</h1>
<h2 id="abstract">
  Abstract
  <a class="anchor" href="#abstract">#</a>
</h2>
<p>Accurate prediction of binding interaction between T cell receptors (TCRs) and host cells is fundamental to understanding the regulation of the adaptive immune system as well as to developing data-driven approaches for personalized immunotherapy. While several machine learning models have been developed for this prediction task, the question of how to specifically embed TCR sequences into numeric representations remains largely unexplored compared to protein sequences in general. Here, we investigate whether the embedding models designed for protein sequences, and the most widely used BLOSUM-based embedding techniques are suitable for TCR analysis. Additionally, we present our context-aware amino acid embedding models (catELMo) designed explicitly for TCR analysis and trained on 4M unlabeled TCR sequences with no supervision. We validate the effectiveness of catELMo in both supervised and unsupervised scenarios by stacking the simplest models on top of our learned embeddings. For the supervised task, we choose the binding affinity prediction problem of TCR and epitope sequences and demonstrate notably significant performance gains (up by at least 14% AUC) compared to existing embedding models as well as the state-of-the-art methods. Additionally, we also show that our learned embeddings reduce more than 93% annotation cost while achieving comparable results to the state-of-the-art methods. In TCR clustering task (unsupervised), catELMo identifies TCR clusters that are more homogeneous and complete about their binding epitopes. Altogether, our catELMo trained without any explicit supervision interprets TCR sequences better and negates the need for complex deep neural network architectures in downstream tasks.</p>
<h2 id="1-introduction">
  1 Introduction
  <a class="anchor" href="#1-introduction">#</a>
</h2>
<p>T cell receptors (TCRs) play critical roles in adaptive immune systems as they enable T cells to distinguish abnormal cells from healthy cells. TCRs carry this important function by binding to antigens presented by major histocompatibility complex (MHC) and recognizing whether the antigens are self or foreign [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-1"><strong>1</strong></a>]. It is widely accepted that the third complementarity-determining region (CDR3) of the TCRβ chain is the most important in determining its binding specificity to epitope—a part of an antigen [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-2"><strong>2</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-3"><strong>3</strong></a>]. The advent of publicly available databases of TCR-epitope cognate pairs opened the door to computational methods to predict the binding affinity of a given pair of TCR and epitope sequences. Computational prediction of binding affinity is important as it can drastically reduce the cost and the time needed to narrow down a set of candidate TCR targets, thereby accelerating the development of personalized immunotherapy leading to vaccine development and cancer treatment [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-4"><strong>4</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-5"><strong>5</strong></a>]. Computational prediction is challenging primarily due to 1) many-to-many binding characteristics [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-6"><strong>6</strong></a>] and 2) the limited amount of currently available data.</p>
<p>Despite the challenges, many deep neural networks have been leveraged to predict binding affinity between TCRs and epitopes [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-7"><strong>7</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-8"><strong>8</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-9"><strong>9</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-10"><strong>10</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-11"><strong>11</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-12"><strong>12</strong></a>]. While each model has its own strengths and weaknesses, they all suffer from poor generalizability when applied to unseen epitopes, not present in the training data [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-7"><strong>7</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-13"><strong>13</strong></a>]. In order to alleviate this, we focus mainly on embedding, as embedding an amino acid sequence into a numeric representation is the very first step needed to train and run a deep neural network. Furthermore, a ‘good’ embedding has been shown to boost downstream performance even with a few numbers of downstream samples [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-14"><strong>14</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-15"><strong>15</strong></a>].</p>
<ul>
<li>
<p>TCR과 항원 간의 결합 친화도 예측하는 DL 모델</p>
<ul>
<li>훈련 데이터에 존재하지 않는 미지의 항원에 적용될 때 일반화 성능이 떨어진다는 공통의 문제점</li>
</ul>
</li>
<li>
<p>문제점을 완화하는 방법으로 아미노산 임베딩을 고려하기.</p>
<ul>
<li>아미노산 서열을 수치적 표현으로 임베딩하는 것이 딥 뉴럴 네트워크를 학습하고 실행하는 데 필요한 첫 번째 단계.</li>
<li>‘우수한’ 임베딩은 소수의 다운스트림 샘플만 있어도 후속 성능을 향상시킨다.</li>
</ul>
</li>
</ul>
<p>BLOSUM matrices [16] are widely used for representing amino acids into biological-related numeric vectors in TCR analysis [7, 9, 11, 17]. However, BLOSUM matrices are static embedding methods as they always map an amino acid to the same vector regardless of its context. For example, in static word embedding, the word “mouse” in phrases “a mouse in desperate search of cheese” and “to click, press and release the left mouse button” will be embedded as the same numeric representation even though it is used in different contexts. Similarly, the amino acid residue G appearing five times in a TCRβ CDR3 sequence CASGGTGGANTGQLYF may play different roles in binding to antigens as each occurrence has a different position and neighboring residues. The loss of such contextual information from static embedding may inevitably compromise model performances [18, 19].</p>
<ul>
<li>BLOSUM 행렬[16]은 TCR 분석에서 아미노산을 생물학적으로 관련된 수치 벡터로 표현한다.</li>
<li>BLOSUM 행렬은 정적 임베딩 방법으로, 아미노산을 그 문맥과 무관하게 항상 동일한 벡터에 매핑.
<ul>
<li>ex) 정적 단어 임베딩에서는 “mouse”라는 단어가 “치즈를 찾아 절실히 헤매는 쥐”와 “왼쪽 마우스 버튼을 클릭, 누름, 해제”라는 문구에서 동일한 수치 표현으로 임베딩되는데, 이는 단어가 서로 다른 문맥에서 사용되더라도 동일하게 표현된다는 것을 의미.</li>
<li>ex2) TCRβ CDR3 서열 CASGGTGGANTGQLYF에 다섯 번 등장하는 아미노산 잔기 G는 각 위치와 이웃 잔기가 달라 결합 시 항원에 대해 다른 역할을 할 수 있는데 정적 단어 임베딩에서는 이런 문맥 정보가 소실된다.</li>
</ul>
</li>
</ul>
<p>Recent successes of large language models [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-14"><strong>14</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-15"><strong>15</strong></a>] have been prompting new research applying text embedding techniques to amino acid embedding. Large language models are generally trained on a large text corpus in a self-supervised manner where no labels are required [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-18"><strong>18</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-20"><strong>20</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-21"><strong>21</strong></a>]. A large number of (unlabeled) protein sequences has been available via high quality and manually curated databases such as UniProt [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-22"><strong>22</strong></a>]. With the latest development of targeted sequencing assays of TCR repertoire, a large number (unlabeled) of TCR sequences has also been accessible to the public via online databases such as ImmunoSEQ [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-23"><strong>23</strong></a>]. These databases have allowed researchers to develop large-scale amino acid embedding models that can be used for various downstream tasks. Asgari et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-24"><strong>24</strong></a>] first utilized Word2vec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-20"><strong>20</strong></a>] model with 3-mers of amino acids to learn embeddings of protein sequences. By considering a 3-mer amino acids as a word and a protein sequence as a sentence, they learn amino acid representations by predicting the context of a given target 3-mer in a large corpus of surrounding ones. Yang et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>] applied Doc2vec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-21"><strong>21</strong></a>] models to protein sequences with different sizes of k-mers in a similar manner to Asgari et al. and showed better performance over sparse one-hot encoding. One-hot encoding produces static embeddings, like BLOSUM, which leads to the loss of positional and contextual information.</p>
<p>Later, SeqVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>] and ProtTrans [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>] experimented with dynamic protein sequence embeddings via multiple context-aware language models [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-18"><strong>18</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-19"><strong>19</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-28"><strong>28</strong></a>], showing advantages across multiple tasks. Note that the aforementioned amino acid embedding models were designed for protein sequence analysis. Although these models may have learned general representations of protein sequences, it does not necessarily signify their generalization performance on TCR-related downstream tasks.</p>
<p>Here, we explore strategies to develop amino acid embedding models and emphasize the importance of using “good” amino acid embeddings for a significant performance gain in TCR-related downstream tasks. It includes neural network depth, architecture, types and numbers of training samples, and parameter initialization. Based on our experimental observation, we propose catELMo, whose architecture is adapted from ELMo (Embeddings from Language Models [18]), a bi-directional context-aware language model. catELMo is trained on more than four million TCR sequences collected from ImmunoSEQ [23] in an unsupervised manner, by contextualizing amino acid inputs and predicting the next amino acid token. We compare its performance with state-of-the-art amino acid embedding methods on two TCR-related downstream tasks. In TCR-epitope binding affinity prediction application, catELMo significantly outperforms the state-of-the-art method by at least 14% (absolute improvement) of AUCs. We also show catELMo achieves an equivalent performance to the state-of-the-art method while dramatically reducing downstream training sample annotation cost (more than 93% absolute reduction). In the epitope-specific TCR clustering application, catELMo also achieves comparable to or better cluster results than state-of-the-art methods.</p>
<h2 id="2-results">
  2 Results
  <a class="anchor" href="#2-results">#</a>
</h2>
<p>catELMo is a bi-directional amino acid embedding model that learns contextualized amino acid representations (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F1"><strong>Fig. 1a</strong></a>), treating an amino acid as a word and a sequence as a sentence. It learns patterns of amino acid sequences with its self-supervision signal, by predicting each the next amino acid token given its previous tokens. It has been trained on 4,173,895 TCR<em>β</em> CDR3 sequences (52 million of amino acid tokens) from ImmunoSEQ [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-23"><strong>23</strong></a>] (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T1"><strong>Table 1</strong></a>). catELMo yields a real-valued representation vector for a sequence of amino acids, which can be used as input features of various downstream tasks. We evaluated catELMo on two different TCR-related downstream tasks, and compared its performance with existing amino acid embedding methods, namely BLOSUM62 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-16"><strong>16</strong></a>], Yang et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>], ProtBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>], SeqVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>], and TCRBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-29"><strong>29</strong></a>]. We also investigated various components of catELMo in order to account for its high performance, including the neural network architecture, layer depth and size, types of training data, and the size of downstream training data.</p>
<p>We briefly summarize the two downstream tasks here and refer further details to <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#sec-22"><strong>Section 4.4</strong></a>. The first downstream task is TCR-epitope binding affinity prediction (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F1"><strong>Fig. 1b</strong></a>). All embedding models compared were to embed input sequences into the identical prediction model. Each prediction model was trained on 300,016 TCR-epitope binding and non-binding pairs (1:1 ratio), embedded by each embedding model. We used a neural network with three linear layers for the prediction model, which takes a pair of TCR and epitope as input and returns a binding affinity (0–1) of the pair. The prediction performance was evaluated on testing sets each defined by two types of splitting methods [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-10"><strong>10</strong></a>], called TCR and epitope splits. The testing set of TCR split has no TCRs overlapped with training and validation sets, allowing us to measure out-of-sample TCR performance. Similarly, the testing set of epitope split has no epitopes overlapped with training and validation sets, allowing us to measure out-of-sample epitope performance. For a fair comparison, a consistent embedding method was applied to both TCR and epitope sequences within a single prediction model. The second task is epitope-specific TCR clustering that aims at grouping TCRs that bind to the same epitope (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F1"><strong>Fig. 1c</strong></a>). We tested with TCR sequences of human and mouse species sampled from McPAS [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-30"><strong>30</strong></a>] database.</p>
<p>We applied the hierarchical clustering [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-31"><strong>31</strong></a>] and reported normalized mutual information (NMI) [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-32"><strong>32</strong></a>] and cluster purity to qualify the goodness of the clustering partition of TCR sequences.</p>
<h3 id="21catelmooutperforms-the-existing-embedding-methods-at-discriminating-binding-and-non-binding-tcr-epitope-pairs">
  2.1 catELMo outperforms the existing embedding methods at discriminating binding and non-binding TCR-epitope pairs
  <a class="anchor" href="#21catelmooutperforms-the-existing-embedding-methods-at-discriminating-binding-and-non-binding-tcr-epitope-pairs">#</a>
</h3>
<p>We investigated the downstream performance of TCR-epitope binding affinity prediction models trained using catELMo embeddings. In order to compare performance across different embedding methods, we used the identical downstream model architecture for each method. The competing embedding methods compared are BLOSUM62 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-16"><strong>16</strong></a>], Yang et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>], ProBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>], SeqVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>] and TCRBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-29"><strong>29</strong></a>]. We observed that the prediction model using catELMo embeddings significantly outperformed those using existing amino acid embedding methods in both TCR (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F2"><strong>Figure 2a</strong></a>, b) and epitope (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F2"><strong>Figure 2d</strong></a>, e) split. In TCR split, where no TCRs in the testing set exist in the training and validation set, catELMo’s prediction performance was significantly greater than the second best method (p-value &lt; 6.28 × 10−23, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T2"><strong>Table 2</strong></a>). It achieved AUC 96.04% which was 14% points higher than that of the second-highest performing method, while the rest of the methods performed worse than or similar to BLOSUM62. In epitope split, where no epitopes in the testing set exist in the training and validation set, the prediction model using catELMo also outperformed others with even larger performance gaps. catELMo significantly boosted 17% points of AUCs than the second-highest performing method (p-value &lt; 1.18 × 10−7, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T3"><strong>Table 3</strong></a>). Similar performance gains from catELMo were also observed in other metrics such as Precision, Recall, and F1 scores (Supplementary Fig. 1).</p>
<p>We also visually observed that catELMo aided the model to better discriminate binding and non-binding TCRs for the five most frequent epitopes that appeared in our collected TCR-epitope pairs (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F3"><strong>Fig. 3</strong></a>). These five epitopes account for a substantial portion of our dataset, comprising 14.73% (44,292 pairs) of the total TCR-epitope pairs collected. For visualization, we performed t-SNE [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-35"><strong>35</strong></a>] on the top fifty principal components of the last latent vectors of each prediction model. Each point represents a pair of TCR-epitope, colored by epitope (lighter shade for positive binding and darker shade for negative binding). Different degrees of overlapping between positive pairs and negative ones in regard to the same epitope can be seen in the t-SNE plots. For example, most of the binding and non-binding data points from SeqVec embeddings are barely separated within each epitope group. On the other hand, the t-SNE plot of catELMo exhibits noticeable contrast between binding and non-binding pairs, indicating that catELMo aids the prediction model to distinguish labels. We also observed catELMo outperformed the other embedding methods in discriminating binding and non-binding TCRs for almost all individual epitopes. As shown in Supplementary Fig. 2, the prediction model using catELMo embeddings achieved the highest AUCs in 39 out of 40 epitopes, and the second highest AUC on an epitope (GTSGSPIVNR) with only 1.09% lower than the highest score. Addtionally, we observed that catELMo consistently outperformed other embedding methods in predicting the binding affinity between TCRs and epitopes from a diverse range of pathogens (Supplementary Table 1).</p>
<h3 id="22catelmoreduces-a-significant-amount-of-annotation-cost-for-achieving-comparable-prediction-power">
  2.2 catELMo reduces a significant amount of annotation cost for achieving comparable prediction power
  <a class="anchor" href="#22catelmoreduces-a-significant-amount-of-annotation-cost-for-achieving-comparable-prediction-power">#</a>
</h3>
<p>Language models trained on large corpus are known to improve downstream task performance with a smaller number of downstream training data [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-14"><strong>14</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-19"><strong>19</strong></a>]. Similarly in TCR-epitope binding, we show that catELMo trained entirely on unlabeled TCR sequences facilitates its downstream prediction model to achieve the same performance with a significantly smaller amount of TCR-epitope training pairs (i.e., epitope-labeled TCR sequence). We trained a binding affinity prediction model for each k% of downstream data (i.e., catELMo embeddings of TCR-epitope pairs) where k = 1, 2, ···, 10, 20, 30, ···, 100. The widely used BLOSUM62 embedding matrix was used as a comparison baseline under the same ks as it performs better than or is comparable to the other embedding methods.</p>
<p>A positive log-linear relationship between the number of (downstream) training data and AUCs was observed for both TCR and epitope split (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F2"><strong>Fig. 2c, f</strong></a>). The steeper slope in catELMo suggests that prediction models utilizing catELMo embeddings exhibit higher performance gain per number of training pairs compared to the BLOSUM62-based models. In TCR split, we observed that catELMo’s binding affinity prediction models with just 7% of the training data significantly outperform ones that use a full size of BLOSUM62 embeddings (p-value = 0.0032, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F2"><strong>Fig. 2c</strong></a>). catELMo with just 3%, 4%, and 6% of the downstream training data achieved similar performances to when using a full size of Yang et al., ProtBert, and SeqVec embeddings, respectively. Similarly, in epitope split, we showed catELMo’s prediction models with just 3% of training data achieved equivalent performance as ones built on a full size of BLOSUM62 embeddings (p-value = 0.8531, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F2"><strong>Fig. 2f</strong></a>). Compare to the other embedding methods, catELMo with just 1%, 2%, and 5% of the downstream training data achieved similar or better performance than when using a full size of Yang et al., ProtBert, and SeqVec embeddings, separately. Similar performance gains from catELMo were also observed in other metrics such as Precision, Recall, and F1 scores (Supplementary Fig. 3). Achieving accurate prediction with a small amount of training data is important for TCR analysis as obtaining the binding affinity of TCR-epitope pairs is costly.</p>
<h3 id="23catelmoallows-clustering-of-tcr-sequences-with-high-performance">
  2.3 catELMo allows clustering of TCR sequences with high performance
  <a class="anchor" href="#23catelmoallows-clustering-of-tcr-sequences-with-high-performance">#</a>
</h3>
<p>Clustering TCRs of similar binding profiles is important in TCR repertoire analysis as it facilitates discoveries of TCR clonotypes that are condition-specific [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-36"><strong>36</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-37"><strong>37</strong></a>]. In order to demonstrate that catELMo embeddings can be used for other TCR-related downstream tasks, we performed hierarchical clustering [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-31"><strong>31</strong></a>] using each method’s embedding (catELMo, BLOSUM62 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-16"><strong>16</strong></a>], Yang et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>], ProBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>], SeqVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>] and TCRBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-29"><strong>29</strong></a>]) and evaluated the identified clusters against the ground-truth TCR groups labeled by their binding epitopes. We additionally compared our results with state-of-the-art TCR clustering methods, TCRdist [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-37"><strong>37</strong></a>] and GIANA [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-38"><strong>38</strong></a>], both of which were developed from BLOSUM62 matrix (see <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#sec-24"><strong>Section 4.4.2</strong></a>). Normalized mutual information (<em>NMI</em>) [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-32"><strong>32</strong></a>] and cluster purity are used to measure the clustering quality.</p>
<p>Significant disparities in TCR binding frequencies exist across different epitopes. To construct more balanced clusters, we targeted TCR sequences bound to the top eight frequent epitopes identified in the McPAS database. We find that the cluster model for both human and mouse species built on catELMo embeddings maintains either the best or second best NMI and purity scores compared with ones that are computed on other embeddings (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F4"><strong>Fig. 4a, d</strong></a>). To investigate whether this observation remains true on individual species, we conducted the same clustering analysis on human and mouse species, separately. We showcase comparison for the top eight epitopes in human (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F4"><strong>Fig. 4b, e</strong></a>) and mouse (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F4"><strong>Fig. 4c, f</strong></a>) species and observe a similar pattern that clustering results with catELMo achieve the highest or second-highest NMI and purity scores. Since the real-world scenarios likely involve more epitopes, we conducted additional clustering experiments using the top most abundant epitopes whose combined cognate TCRs make up at least 70% of TCRs across three databases (34 epitopes). Similar performance gains were observed (Supplementary Fig. 4). Altogether, catELMo embedding can assist TCR clustering with no supervision while achieving similar or better performance than other state-of-the-art methods in both human and mouse species.</p>
<h3 id="24-elmo-based-architecture-is-preferable-to-bert-based-architecture-in-tcr-embedding-models">
  2.4 ELMo-based architecture is preferable to BERT-based architecture in TCR embedding models
  <a class="anchor" href="#24-elmo-based-architecture-is-preferable-to-bert-based-architecture-in-tcr-embedding-models">#</a>
</h3>
<p>We observed catELMo using ELMo-based architecture outperformed the model using embeddings of TCRBert which uses BERT (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T4"><strong>Table 4</strong></a>). The performance differences were approximately 15% AUCs in TCR split (p-value &lt; 3.86 × 10−30) and 19% AUCs in epitope split (p-value &lt; 3.29 × 10−8). Because TCRBert was trained on a smaller amount of TCR sequences (around 0.5 million sequences) than catELMo, we further compared catELMo with various sizes of BERT-like models trained on the same dataset as catELMo: BERT-Tiny-TCR, BERT-Base-TCR, and BERT-Large-TCR having a stack of 2, 12, and 30 Transformer layers respectively (see <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#sec-32"><strong>Section 4.6.2</strong></a>). Note that BERT-Base-TCR uses the same number of Transformer layers as TCRBert. Additionally, we compared different versions of catELMo by varying the number of BiLSTM layers (2, 4–default, and 8, see <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#sec-31"><strong>Section 4.6.1</strong></a>). As summarized in <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T4"><strong>Table 4</strong></a>, TCR-epitope binding affinity prediction models trained on catELMo embeddings (AUC 96.04% and 94.70% on TCR and epitope split) consistently outperformed models trained on these Transformer-based embeddings (AUC 81.23–81.91% and 74.20–74.94% on TCR and epitope split). The performance gaps between catELMo and Transformer-based models (14% AUCs in TCR split and 19% AUCs in epitope split) were statistically significant (p-values &lt; 6.72 × 10−26 and &lt; 1.55 × 10−7 for TCR and epitope split respectively). We observed that TCR-epitope binding affinity prediction models trained on catELMo-based embeddings consistently outperformed the ones using Transformer-based embeddings (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T4"><strong>Table 4</strong></a>, 5). Even the worst- performed BiLSTM-based embedding model achieved higher AUCs than the best-performed Transformer-based embeddings at discriminating binding and non-binding TCR-epitope pairs in both TCR (p-value &lt; 2.84 × 10−28) and epitope split (p-value &lt; 5.86 × 10−6).</p>
<h3 id="25-within-domain-transfer-learning-is-preferable-to-cross-domain-transfer-learning-in-tcr-analysis">
  2.5 Within-domain transfer learning is preferable to cross-domain transfer learning in TCR analysis
  <a class="anchor" href="#25-within-domain-transfer-learning-is-preferable-to-cross-domain-transfer-learning-in-tcr-analysis">#</a>
</h3>
<p>We showed that catELMo, trained on TCR sequences, significantly outperformed amino acid embedding methods trained on generic protein sequences. catELMo-Shallow and SeqVec shared the same architecture consisting of character-level convolutional layers and a stack of two bi-directional LSTM layers but were trained on different types of training data. catELMo-Shallow was trained on TCR sequences (about 4 million) while SeqVec was trained on generic protein sequences (about 33 million). Although catELMo-Shallow was trained on a relatively smaller amount of sequences compared to SeqVec, the binding affinity prediction model built on catELMo-Shallow embeddings (AUC 95.67% in TCR split and 86.32% in epitope split) significantly outperformed the one built on SeqVec embeddings (AUC 81.61% in TCR split and 76.71% in epitope split) by 14.06% and 9.61% on TCR and epitope split respectively. This suggests that knowledge transfer within the same domain is preferred whenever possible in TCR analysis.</p>
<h2 id="3-discussion">
  3 Discussion
  <a class="anchor" href="#3-discussion">#</a>
</h2>
<p>catELMo is an effective embedding model that brings substantial performance improvement in TCR-related downstream tasks. Our study emphasizes the importance of choosing the right embedding models. The embedding of amino acids into numeric vectors is the very first and crucial step that enables the training of a deep neural network. It has been previously demonstrated that a well-designed embedding can lead to significantly improved results on downstream analysis [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>]. The reported performance of catELMo embedding on TCR-epitope binding affinity prediction and TCR clustering tasks indicates that catELMo is able to learn patterns of amino acid sequences more effectively than state-of-the-art embedding methods. While all other methods compared (except BLOSUM62) leverage a large number of unlabeled amino acid sequences, only our prediction model using catELMo significantly outperforms widely used BLOSUM62 and other models such as netTCR [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-9"><strong>9</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-17"><strong>17</strong></a>] and ATM-TCR [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-10"><strong>10</strong></a>] trained on paired (TCR-epitope) samples only (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T6"><strong>Table 6</strong></a>). Our work suggests the need for developing sophisticated strategies to train amino acid embedding models that can enhance the performance of TCR-related downstream tasks even while requiring less amount of data and simpler prediction model structures.</p>
<p>Two important observations made from our experiments are 1) the type of data used for training amino acid embedding models is far more important than the amount of data and 2) ELMo-based embedding models consistently perform much better than BERT-based embedding models. While previously developed amino acid embedding models such as SeqVec and ProtBert were trained on 184- and 1,690-times more amino acid tokens compared to the training data used for catELMo, the prediction models using SeqVec and ProtBert performed poorly compared to the model using catELMo (see Sections 2.1 and 2.3). SeqVec and ProtBert were trained based on generic protein sequences, whereas catELMo was trained on a collection of TCR sequences from pooled TCR repertoires across many samples, indicating that the use of TCR data to train embedding models is more critical than much larger amount of generic protein sequences.</p>
<p>In the field of natural language processing, Transformer-based models have been bolstered as the superior embedding model [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-14"><strong>14</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-15"><strong>15</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-19"><strong>19</strong></a>]. However, for TCR-related downstream tasks, catELMo using biLSTM layer-based design outperforms BERT using Transformer layers (see <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#sec-6"><strong>Section 2.4</strong></a>). While it is difficult to pinpoint the reasons, the bi-directional architecture to predict the next token based on its previous tokens in ELMo may mimic the interaction process of TCR and epitope sequences either from left to right or from right to left. In contrast, BERT uses Transformer encoder layers that attend tokens both on the left and right to predict a masked token, refer to as masked language modeling. As the Transformer layer can be along with the next token prediction objectives, it remains as a future work to investigate Transformer with causal language models, such as GPT-3 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-14"><strong>14</strong></a>], for amino acid embedding. Additionally, the clear differences of TCR sequences compared to natural languages are 1) the compact vocabulary size (20 standard amino acids vs. over 170k English words) and 2) the length of peptides in TCRs being smaller than the number of words in sentences or paragraphs in natural languages. These differences may allow catELMo to learn sequential dependence without losing long-term memory from the left end.</p>
<p>Often in classification problems in life sciences, the difference in the number of available positive and negative data can be very large and TCR-epitope binding affinity prediction problem is no exception. In fact, the number of experimentally generated non-binding pairs are practically non-existent and obtaining experimental negative data is costly [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-17"><strong>17</strong></a>]. This requires researchers to come up with a strategy to generate negative samples and it can be non-trivial. A common practice is to sample new TCRs from repertoires and pair them with existing epitopes [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-9"><strong>9</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-39"><strong>39</strong></a>], a strategy we also used. Another approach is to randomly shuffle TCR-epitope pairs within positive binding dataset, resulting in TCRs and epitopes that are not known to bind paired together [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-10"><strong>10</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-11"><strong>11</strong></a>]. Given the vast diversity of human TCR clonotypes, which can exceed 1015 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-40"><strong>40</strong></a>], the chance of randomly selecting a TCR that specifically recognizes a target epitope is relatively small. The prediction model consistently outperformed the other embedding methods by large margins in both TCR and epitope splits as shown in Supplementary Fig. 5. The model using catELMo achieves 24% and 36% higher AUCs over the second best embedding method for TCR (p-value &lt; 1.04 × 10−18, Supplementary Table 2) and epitope (p-value &lt; 6.26 × 10−14, Supplementary Table 3) split, respectively. Moreover, we observe that using catELMo embeddings, prediction models that are trained with only 2% downstream samples still statistically outperform ones that are built on a full size of BLOSUM62 embeddings in TCR split (p-value = 0.0005). Similarly, with only 1% training samples, catELMo reaches comparable results as BLOSUM62 with a full size of downstream samples in epitope split (p-value = 0.1438). In other words, catELMo dramatically reduces about 98% annotation cost (Supplementary Table 4). To mitigate potential batch effects, we generated new negative pairs using different seeds and observed consistent prediction performance across these variations. Our experiment results confirm that the embeddings from catELMo maintain high performance regardless of the methodology used to generate negative samples.</p>
<p>Parameter fine-tuning in neural networks is a training scheme where initial weights of the network are set to the weights of a pre-trained network. Fine-tuning has been shown to bring performance gain to the model over using random initial weights [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-41"><strong>41</strong></a>]. We investigated the possibility of performance boost of our prediction model using fine-tuned catELMo. Since SeqVec shares the same architecture with catELMo-Shallow and is trained on generic protein sequences, we used the weights of SeqVec as initial weights when fine-turning catELMo-Shallow. We compared the performance of binding affinity prediction models using the fine-tuned catELMo-Shallow and vanilla catELMo-Shallow (trained from scratch with random initial weights from a standard normal distribution). We observed that the performance when using fine-tuned catELMo-Shallow embeddings was significantly improved by approximately 2% AUCs in TCR split (p-value &lt; 4.17 × 10−9) and 9% AUCs in epitope split (p-value &lt; 5.46 × 10−7).</p>
<p>While epitope embeddings are a part of our prediction models, their impact on overall performance appears to be less significant compared to that of TCR embeddings. To understand the contribution of epitope embeddings, we performed additional experiments. First, we kept epitope embeddings unchanged using the widely-used BLOSUM62-based while varying different embedding methods exclusively for TCRs. The results (Supplementary Table 5) closely aligns with our previous findings (<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T2"><strong>Tables 2</strong></a> and <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#T3"><strong>3</strong></a>), suggesting that the choice of epitope embedding method may not strongly affect the final predictive performance.</p>
<p>Furthermore, we investigated alternative embedding approaches for epitope sequences. Specifically, we replaced epitope embeddings with randomly initialized matrices containing trainable parameters, while employing catELMo for TCR embeddings. This setting yielded predictive performance comparable to the scenario where both TCR and epitope embeddings were catELMo-based (Supplementary Table 6).</p>
<p>Similarly, using BLOSUM62 for TCR embeddings and catELMo for epitope embeddings resulted in performance similar to when both embeddings were based on BLOSUM62. These consistent findings support the proposition that the influence of epitope embeddings may not be as significant as that of TCR embeddings (Supplementary Table 7).</p>
<p>We believe these observations may be attributed to the substantial data scale discrepancy between TCRs (more than 290k) and epitopes (less than 1k). Moreover, TCRs tend to exhibit high similarity, whereas epitopes display greater distinctiveness from one another. These features of TCRs require robust embeddings to facilitate effective separation and improve downstream performance, while epitope embeddings primarily serve as categorical encodings.</p>
<p>While TCR<em>β</em> CDR3 is known to be the primary determinant for TCR-epitope binding specificity, other regions such as CDR1 and CDR2 on TCR<em>β</em> V gene along with TCRα chain are also known to contribute to specificity in antigen recognition [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-42"><strong>42</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-43"><strong>43</strong></a>].There are models that can take advantage of these additional features when making predictions [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-17"><strong>17</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-44"><strong>44</strong></a>]. However, our study focuses on modeling CDR3 of TCR<em>β</em> chains because of the limited availability of sample data from other regions. Future work may explore strategies to incorporate these regions while mitigating the challenges of working with limited samples.</p>
<h2 id="4-methods">
  4 Methods
  <a class="anchor" href="#4-methods">#</a>
</h2>
<p>We first present data used for training the amino acid embedding models and the downstream tasks. We then review existing amino acid embedding methods and their usage on TCR-related tasks. We introduce our approach, catELMo, a bi-directional amino acid embedding method that computes contextual representation vectors of amino acids of a TCR (or epitope) sequence. We describe in detail how to apply catELMo to two different TCR-related downstream tasks. Lastly, we provide details on the experimental design, including the methods and parameters used in comparison and ablation studies.</p>
<h3 id="41-data">
  4.1 Data
  <a class="anchor" href="#41-data">#</a>
</h3>
<h3 id="tcrs-for-trainingcatelmo">
  <em>TCRs for training catELMo</em>
  <a class="anchor" href="#tcrs-for-trainingcatelmo">#</a>
</h3>
<p>We collected 5,893,249 TCR sequences from repertoires of seven projects in the ImmunoSEQ database: HIV [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-45"><strong>45</strong></a>], SARS-CoV2 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-23"><strong>23</strong></a>], Epstein Barr Virus [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-46"><strong>46</strong></a>], Human Cytomegalovirus [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-47"><strong>47</strong></a>], Influenza A [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-48"><strong>48</strong></a>], Mycobacterium Tuberculosis [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-49"><strong>49</strong></a>], and Cancer Neoantigens [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-50"><strong>50</strong></a>]. CDR3 sequences of TCR<em>β</em> chains were used to train the amino acid embedding models as those are the major segment interacting with epitopes [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-2"><strong>2</strong></a>] and exist in large numbers. We excluded duplicated copies and sequences containing wildcards such as ‘*’ or ‘X’. Altogether, we obtained 4,173,895 TCR sequences (52,546,029 amino acid tokens) of which 85% were used for training and 15% were used for testing.</p>
<h3 id="tcr-epitope-pairs-for-binding-affinity-prediction">
  <em>TCR-epitope pairs for binding affinity prediction</em>
  <a class="anchor" href="#tcr-epitope-pairs-for-binding-affinity-prediction">#</a>
</h3>
<p>We collected TCR-epitope pairs known to bind each other from three publicly available databases: IEDB [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-34"><strong>34</strong></a>], VDJdb [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-33"><strong>33</strong></a>], and McPAS [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-30"><strong>30</strong></a>]. Unlike the (unlabeled) TCR dataset for catELMo training, each TCR is annotated with an epitope known to bind each other, which we referred to as a TCR-epitope pair. We only used pairs with human MHC class I epitopes and CDR3 sequences of the TCR<em>β</em> chain. We further filtered out sequences containing wildcards such as ‘*’ or ‘X’. For VDJdb, we excluded pairs with a confidence score of 0 as it means a critical aspect of sequencing or specificity validation is missing. We removed duplicated copies and merged datasets collected from the three databases. For instance, 29.85% of pairs from VDJdb overlapped with IEDB, and 55.41% of pairs from McPAS overlapped with IEDB. Altogether, we obtained 150,008 unique TCR-epitope pairs known to bind to each other having 140,675 unique TCRs and 982 unique epitopes. We then generated the same number of non-binding TCR-epitope pairs as negative samples by randomly pairing each epitope of the positive pairs with a TCR sampled from the healthy TCR repertoires of ImmunoSEQ. We note that it includes no identical TCR sequences with the TCRs used for training the embedding models. Altogether, we obtained 300,016 TCR-epitope pairs where 150,008 pairs are positive and 150,008 pairs are negative. The average length of TCRs and epitope sequences are 14.78 and 11.05, respectively. Our data collection and preprocessing procedures closely followed those outlined in our previous work [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-10"><strong>10</strong></a>].</p>
<h3 id="tcrs-for-antigen-specific-tcr-clustering">
  <em>TCRs for antigen-specific TCR clustering</em>
  <a class="anchor" href="#tcrs-for-antigen-specific-tcr-clustering">#</a>
</h3>
<p>We collected 9,822 unique TCR sequences of humans and mice hosts from McPAS [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-30"><strong>30</strong></a>]. Each TCR is annotated with an epitope known to bind, which is used as a ground-truth label for TCR clustering. We excluded TCR sequences that bind to neoantigen pathogens or multiple epitopes and only used CDR3 sequences of TCR<em>β</em> chain. We composed three subsets for different experimental purposes. The first dataset contains both human and mice TCRs. We used TCRs associated with the top eight frequent epitopes, resulting in 5,607 unique TCRs. The second dataset consists of only human TCRs, and the third dataset consists of only mouse TCRs. In a similar manner, we selected TCRs that bind to the top eight frequent epitopes. As a result, we obtained 5,528 unique TCR sequences for the second dataset and 1,322 unique TCR sequences for the third dataset.</p>
<h3 id="42-amino-acid-embedding-methods">
  4.2 Amino acid embedding methods
  <a class="anchor" href="#42-amino-acid-embedding-methods">#</a>
</h3>
<p>In this section, we review amino acid embedding methods previously proposed. There are two categories of the existing approaches: static and context-aware embedding methods. Static embedding method represents an amino acid as a static representation vector remaining the same regardless of its context. Context-aware embedding method, however, represents an amino acid differently in accordance with its context. Context-aware embedding is also called dynamic embedding in contrast to static embedding. We explain the key ideas of various embedding methods, and introduce their usage in previous works.</p>
<ul>
<li>기존 아미노산 임베딩 방법
<ul>
<li>정적 임베딩: 아미노산을 문맥에 관계없이 동일한 표현 벡터로 나타냄.</li>
<li>문맥 인식 임베딩: 아미노산을 그 문맥에 따라 다르게 표현</li>
</ul>
</li>
</ul>
<h3 id="421-static-embeddings">
  <em>4.2.1 Static embeddings</em>
  <a class="anchor" href="#421-static-embeddings">#</a>
</h3>
<h3 id="blosum">
  BLOSUM
  <a class="anchor" href="#blosum">#</a>
</h3>
<p>BLOSUM [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-16"><strong>16</strong></a>] is a scoring matrix where each element represents how likely an amino acid residue is to be substituted by another over evolutionary time. It has been commonly used to measure alignment scores between two protein sequences. There are various BLOSUM matrices such as BLOSUM45, BLOSUM62, and BLOSUM80 where a matrix with a higher number is used for the alignment of less divergent sequences. BLOSUM have also served as the de facto standard embedding method for various TCR analyses. For example, BLOSUM62 was used to embed TCR and epitope sequences for training deep neural network models predicting their binding affinity [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-7"><strong>7</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-9"><strong>9</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-17"><strong>17</strong></a>]. BLOSUM62 was also used to embed TCR sequences for antigen-specific TCR clustering and TCR repertoire clustering. GIANA [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-38"><strong>38</strong></a>] clustered TCRs based on the Euclidean distance between TCR embeddings. TCRdist [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-37"><strong>37</strong></a>] used BLOSUM62 matrix to compute the dissimilarity matrix between TCR sequences for clustering.</p>
<h3 id="word2vec-and-doc2vec">
  Word2vec and Doc2vec
  <a class="anchor" href="#word2vec-and-doc2vec">#</a>
</h3>
<p>Word2vec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-20"><strong>20</strong></a>] and Doc2vec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-21"><strong>21</strong></a>] are a family of embedding models to learn a single linear mapping of words, which takes a one-hot word indicator vector as input and returns a real-valued word representation vector as output. There are two types of Word2vec architectures: continuous bag-of-words (CBOW) and skip-gram. CBOW predicts a word from its surrounding words in a sentence. It embeds each input word via a linear map, sums all input words’ representations, and applies a softmax layer to predict an output word. Once training is completed, the linear mapping is used to obtain a representation vector of a word. On the contrary, skip-gram predicts the surrounding words given a word while it also uses a linear mapping to obtain a representation vector. Doc2vec is a model further generalized from Word2vec, which introduces a paragraph vector representing paragraph identity as an additional input. Doc2vec also has two types of architectures: distributed memory (DM) and distributed bag-of-words (DBOW). DM predicts a word from its surrounding words and the paragraph vector, while DBOW uses the paragraph vector to predict randomly sampled context words. In a similar way, linear mapping is used to obtain a continuous representation vector of a word.</p>
<p>Several studies adapted Word2vec and Doc2vec to embed amino acid sequences [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-24"><strong>24</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-51"><strong>51</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>]. ProtVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-24"><strong>24</strong></a>] is the first Word2vec representation model trained on a large number of amino acid sequences. Its embeddings were used for several downstream tasks such as protein family classification, disordered protein visualization, and classification. Kimothi et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-51"><strong>51</strong></a>] adapted Doc2vec to embed amino acid sequences for protein sequence classification and retrieval. Yang et al. [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-25"><strong>25</strong></a>] trained Doc2vec models on 524,529 protein sequences of UniProt [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-22"><strong>22</strong></a>] database. They considered a k-mer amino acids as a word, and a protein sequence as a paragraph. They trained DM models to predict a word from w surrounding words and a paragraph with various sizes of k and w.</p>
<ul>
<li>정적 임베딩은 아미노산을 항상 같은 벡터 값으로 변환하는 방식.</li>
<li>BLOSUM (Blocks Substitution Matrix)
<ul>
<li>단백질 서열 비교에 사용되는 점수 행렬.</li>
<li>아미노산 치환 가능성을 기반으로 벡터 생성(예: BLOSUM62).</li>
<li>하지만 TCR 서열 내에서 아미노산의 문맥을 반영하지 못함.</li>
</ul>
</li>
<li>Word2Vec &amp; Doc2Vec 기반 임베딩
<ul>
<li>단백질 서열을 자연어 문장처럼 취급하여 학습.</li>
<li>Word2Vec: 단어(아미노산 서열)를 고정된 벡터로 변환.</li>
<li>Doc2Vec: 문장(서열) 전체를 고정된 벡터로 변환.</li>
<li>문맥을 일부 반영하지만, 단어(아미노산) 간 관계 학습이 제한적.</li>
</ul>
</li>
</ul>
<h3 id="422-context-aware-embeddings">
  <em>4.2.2 Context-aware embeddings</em>
  <a class="anchor" href="#422-context-aware-embeddings">#</a>
</h3>
<h3 id="elmo">
  ELMo
  <a class="anchor" href="#elmo">#</a>
</h3>
<p>ELMo [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-18"><strong>18</strong></a>] is a deep context-aware word embedding model trained on a large corpus. It learns each token’s (e.g., a word) contextual representation in forward and backward directions using a stack of two bi-directional LSTM layers. Each word of a text string is first mapped into a numerical representation vector via the character-level convolutional layers. The forward (left-to-right) pass learns a token’s contextual representation depending on itself and the previous context in which it is used. The backward (left-to-right) pass learns a token’s representation depending on itself and its subsequent context.</p>
<p>ELMo is less commonly implemented for amino acid embedding than Transformer-based deep neural networks. One example is SeqVec [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-26"><strong>26</strong></a>]. It is an amino acid embedding model using ELMo’s architecture. It feeds each amino acid as a training token of size 1, and learns its contextual representation both forward and backward within a protein sequence. The data was collected from UniRef50 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-52"><strong>52</strong></a>], which consists of 9 billion amino acid tokens and 33 million protein sequences. SeqVec was applied to several protein-related downstream tasks such as secondary structure and long intrinsic disorder prediction, and subcellular localization.</p>
<h3 id="bert">
  BERT
  <a class="anchor" href="#bert">#</a>
</h3>
<p>BERT [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-19"><strong>19</strong></a>] is a large language model leveraging Transformer [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-53"><strong>53</strong></a>] layers to learn context-aware word embeddings jointly conditioned on both directions. BERT is learned for two objectives. One is the masked language model to learn contextual relationships between words in a sentence. It aims to predict the original value of masked words. The other is the next sentence prediction which aims to learn the dependency between consecutive sentences. It feeds a pair of sentences as input and predicts whether the first sentence in the pair is contextually followed by the second sentence.</p>
<p>BERT’s architecture has been used in several amino acid embedding methods [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-54"><strong>54</strong></a>, <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-29"><strong>29</strong></a>]. They treated an amino acid residue as a word and a protein sequence as a sentence. ProtBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-27"><strong>27</strong></a>] was trained on 216 million protein sequences (88 billion amino acid tokens) of UniRef100 [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-52"><strong>52</strong></a>]. It was applied for several protein sequence applications such as secondary structure prediction and sub-cellular localization. ProteinBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-54"><strong>54</strong></a>] combined language modeling and gene ontology annotation prediction together during training. It was applied to protein secondary structure, remote homology, fluorescence and stability prediction. TCRBert [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-29"><strong>29</strong></a>] was trained on 47,040 TCR<em>β</em> and 4,607 TCRα sequences of PIRD [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-55"><strong>55</strong></a>] dataset and evaluated on TCR-antigen binding prediction and TCR engineering tasks.</p>
<h3 id="43-our-approachcatelmo">
  4.3 Our approach: catELMo
  <a class="anchor" href="#43-our-approachcatelmo">#</a>
</h3>
<p>We propose catELMo, a bi-directional amino acid embedding model designed for TCR analysis. catELMo adapts ELMo’s architecture to learn context-aware representations of amino acids. It is trained on TCR sequences, which is different from the existing amino acid embedding models such as SeqVec trained on generic protein sequences. As illustrated in <a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#F1"><strong>Fig. 1a</strong></a>, catELMo is composed of a character CNN (CharCNN) [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-56"><strong>56</strong></a>] layer converting each one-hot encoded amino acid token to a continuous representation vector, a stack of four bi-directional LSTM [<a href="https://www.biorxiv.org/content/10.1101/2023.04.12.536635v2.full#ref-57"><strong>57</strong></a>] layers learning contextual relationship between amino acid residues, and a softmax layer predicting the next (or previous) amino acid residue.</p>
<p>Given a sequence of N amino acid tokens, (<em>t</em>1, <em>t</em>2, …, <em>tN</em>), CharCNN maps each one-hot encoded amino acid token <em>tk</em> to a latent vector c<em>k</em> through seven convolutional layers with kernel sizes ranging from 1 to 7, and the numbers of filters of 32, 32, 64, 128, 256, 512, and 512, each of which is followed by a max-pooling layer, resulting in a 1,024-dimensional vector. The output of the CharCNN, (<em>c</em>1, <em>c</em>2, …, <em>cN</em>), is then fed into a stack of four bidirectional LSTM layers consisting of forward and backward passes. For the forward pass, the sequence of the CharCNN output is fed into the first forward LSTM layer followed by the second forward LSTM layer, and so on. Each LSTM cell in every forward layer has 4,096 hidden states and returns a 512-dimensional representation vector. Each output vector of the last LSTM layer is then fed into a softmax layer to predict the right next amino acid token. Residual connection is applied between the first and second layers and between the third and fourth layers to prevent gradient vanishing. Similarly, the sequence of the CharCNN output is fed into the backward pass, in which each cell returns a 512-dimensional representation vector. Unlike the forward layer, each output vector of the backward layer followed by a softmax layer aims to predict the left next amino acid token.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments"><script src="https://giscus.app/client.js"
        data-repo="yshghid/yshghid.github.io"
        data-repo-id="R_kgDONkMkNg"
        data-category-id="DIC_kwDONkMkNs4CloJh"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>
</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#1-introduction">1 Introduction</a></li>
    <li><a href="#2-results">2 Results</a>
      <ul>
        <li><a href="#21catelmooutperforms-the-existing-embedding-methods-at-discriminating-binding-and-non-binding-tcr-epitope-pairs">2.1 catELMo outperforms the existing embedding methods at discriminating binding and non-binding TCR-epitope pairs</a></li>
        <li><a href="#22catelmoreduces-a-significant-amount-of-annotation-cost-for-achieving-comparable-prediction-power">2.2 catELMo reduces a significant amount of annotation cost for achieving comparable prediction power</a></li>
        <li><a href="#23catelmoallows-clustering-of-tcr-sequences-with-high-performance">2.3 catELMo allows clustering of TCR sequences with high performance</a></li>
        <li><a href="#24-elmo-based-architecture-is-preferable-to-bert-based-architecture-in-tcr-embedding-models">2.4 ELMo-based architecture is preferable to BERT-based architecture in TCR embedding models</a></li>
        <li><a href="#25-within-domain-transfer-learning-is-preferable-to-cross-domain-transfer-learning-in-tcr-analysis">2.5 Within-domain transfer learning is preferable to cross-domain transfer learning in TCR analysis</a></li>
      </ul>
    </li>
    <li><a href="#3-discussion">3 Discussion</a></li>
    <li><a href="#4-methods">4 Methods</a>
      <ul>
        <li><a href="#41-data">4.1 Data</a></li>
        <li><a href="#tcrs-for-trainingcatelmo"><em>TCRs for training catELMo</em></a></li>
        <li><a href="#tcr-epitope-pairs-for-binding-affinity-prediction"><em>TCR-epitope pairs for binding affinity prediction</em></a></li>
        <li><a href="#tcrs-for-antigen-specific-tcr-clustering"><em>TCRs for antigen-specific TCR clustering</em></a></li>
        <li><a href="#42-amino-acid-embedding-methods">4.2 Amino acid embedding methods</a></li>
        <li><a href="#421-static-embeddings"><em>4.2.1 Static embeddings</em></a></li>
        <li><a href="#blosum">BLOSUM</a></li>
        <li><a href="#word2vec-and-doc2vec">Word2vec and Doc2vec</a></li>
        <li><a href="#422-context-aware-embeddings"><em>4.2.2 Context-aware embeddings</em></a></li>
        <li><a href="#elmo">ELMo</a></li>
        <li><a href="#bert">BERT</a></li>
        <li><a href="#43-our-approachcatelmo">4.3 Our approach: catELMo</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












