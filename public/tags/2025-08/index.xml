<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2025-08 on  </title>
    <link>http://localhost:1313/tags/2025-08/</link>
    <description>Recent content in 2025-08 on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/2025-08/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MLflow #3</title>
      <link>http://localhost:1313/docs/study/ai/ai27/</link>
      <pubDate>Sun, 24 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai27/</guid>
      <description>MLflow #3 # #2025-08-24&#xA;1. 코드 # #1 run 시작 &amp;amp; 종료</description>
    </item>
    <item>
      <title>MLflow #2 mlflow 파이프라인</title>
      <link>http://localhost:1313/docs/study/ai/ai25/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai25/</guid>
      <description>MLflow #2 mlflow 파이프라인 # #2025-08-22&#xA;1. 코드 # #1 트래킹 서버 설정&#xA;import os import mlflow # 1. 로그를 저장할 서버/위치 지정 mlflow.set_tracking_uri(uri=os.getenv(&amp;#34;MLFLOW_TRACKING_URI&amp;#34;, &amp;#34;&amp;#34;)) # MLFLOW_TRACKING_URI로 MLflow 서버를 연결 current_uri = mlflow.get_tracking_uri() print(f&amp;#34;Current Tracking URI: {current_uri}&amp;#34;) # #2 Experiment 생성&#xA;# 2. Experiment 생성 experiment = mlflow.set_experiment(&amp;#34;new_experiment&amp;#34;) print(f&amp;#34;Experiment ID: {experiment.experiment_id}&amp;#34;) print(f&amp;#34;Experiment Name: {experiment.name}&amp;#34;) print(f&amp;#34;Artifact Location: {experiment.artifact_location}&amp;#34;) print(f&amp;#34;Lifecycle Stage: {experiment.lifecycle_stage}&amp;#34;) Experiment ID: 2 Experiment Name: new_experiment Artifact Location: /mlflow/mlruns/2 Lifecycle Stage: active # #3 information 확인, 로그 기록</description>
    </item>
    <item>
      <title>MLflow #3</title>
      <link>http://localhost:1313/docs/study/ai/ai26/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai26/</guid>
      <description>MLflow #3 # #2025-08-22&#xA;1. 개념 # MLflow&#xA;머신러닝 실험을 관리하기 위한 플랫폼. 모델을 학습하는 과정에서 파라미터, 메트릭, 아티팩트, 실행(run) 기록을 남긴다. Run(실행 단위)&#xA;하나의 학습 또는 실험 과정 start_run / end_run&#xA;새로운 run을 열고 닫는 과정 active_run&#xA;현재 열려 있는 run last_active_run&#xA;최근에 끝났거나 여전히 열려 있는 run log_param / log_metric&#xA;하이퍼파라미터나 성능 지표를 기록하는 함수 # 2. 코드 # #1 active_run.py&#xA;import mlflow # Start and end a run with mlflow.</description>
    </item>
    <item>
      <title>MLflow #1 설치 &amp; 실습</title>
      <link>http://localhost:1313/docs/study/ai/ai24/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai24/</guid>
      <description>MLflow #1 설치 &amp;amp; 실습 # #2025-08-21&#xA;1. mlflow 설치 및 docker 띄우기 # $ export CR_PAT=* # *: github token 블라인드 처리 $ echo $CR_PAT | docker login ghcr.io -u yshghid --password-stdin Login Succeeded 로그인햇으면 도커를 켠다음에 다음을 수행.&#xA;$ docker pull ghcr.io/mlflow/mlflow:v2.0.1 v2.0.1: Pulling from mlflow/mlflow 7a6db449b51b: Pull complete e238bceb2957: Pull complete ce77f44508b5: Pull complete 455a39ac3ab8: Pull complete f8c2fbfe5046: Pull complete 60e3c6e8536b: Pull complete Digest: sha256:1e1f28a6134e7e6c4b0d0a4f5f8647ff31c953ad53eb3bb5af4c51ae4e8dd14d Status: Downloaded newer image for ghcr.</description>
    </item>
    <item>
      <title>python #3 pgvector 유사 리뷰 검색</title>
      <link>http://localhost:1313/docs/study/sw/sw22/</link>
      <pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw22/</guid>
      <description>python #3 pgvector 유사 리뷰 검색 # #2025-08-20&#xA;1. 목적 # 고객 리뷰 문장을 벡터로 임베딩하고 PostgreSQL의 pgvector 기능을 활용하여 비슷한 리뷰를 검색하는 기능을 구현&#xA;# 2. 코드 # import torch import transformers import sentence_transformers import sklearn import numpy import scipy print(f&amp;#34;torch: {torch.__version__}&amp;#34;) print(f&amp;#34;transformers: {transformers.__version__}&amp;#34;) print(f&amp;#34;sentence-transformers: {sentence_transformers.__version__}&amp;#34;) print(f&amp;#34;scikit-learn: {sklearn.__version__}&amp;#34;) print(f&amp;#34;numpy: {numpy.__version__}&amp;#34;) print(f&amp;#34;scipy: {scipy.__version__}&amp;#34;) from dotenv import load_dotenv import os load_dotenv() # 같은 폴더에 있는 .env 로드 torch: 2.2.2 transformers: 4.</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/docs/hobby/book/book53/</link>
      <pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/book/book53/</guid>
      <description># #2025-08-19&#xA;#1&#xA;2008년 여름 샌타모니카 대로에 테슬라의 첫 번째 쇼룸을 여는 작업을 하던 동료 라이더가 폰 홀츠하우젠의 이름을 머스크에게 알려주었다. 피스커와의 계약을 해지한 머스크는 테슬라에 사내 디자인 스튜디오를 만들 사람을 물색하던 중이었다. 폰 홀츠하우젠은 머스크의 전화를 받고 바로 그날 오후에 찾아가기로 했다. 머스크는 그에게 스페이스X를 둘러보게 했고, 그는 깜짝 놀라지 않을 수 없었다. “헐, 우주로 로켓을 쏘아 올린다고요? 자동차는 이것과 비교하면 식은 죽 먹기지요.” 폰 홀츠하우젠은 경탄을 토해냈다.</description>
    </item>
    <item>
      <title>LLM #2 LLM과 AI 기술요소를 활용하여 비즈니스 서비스 기획안 작성</title>
      <link>http://localhost:1313/docs/study/ai/ai23/</link>
      <pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai23/</guid>
      <description>LLM #2 LLM과 AI 기술요소를 활용하여 비즈니스 서비스 기획안 작성 # #2025-08-19&#xA;1. 목적 # 등기부등본/건축물대장 업로드 시 AI가 자동으로 문서를 분석하여 전세사기 위험 요소를 탐지하고 수치화한다. # 2. 모델 구성도 # #1 데이터 수집및 정규화&#xA;기술요소: PaddleOCR 선택 이유: 한국어 인식 정확도와 속도가 좋고, 오픈소스+온프레미스 운영 가능(비용·보안 유리), 표 레이아웃/좌표 추출 지원. 입력 파일: PDF/스캔 이미지(JPG/PNG) 매개변수: lang=&amp;ldquo;korean&amp;rdquo;, det+rec 사용, dpi(≥300) 출력 텍스트 블록: [{page, bbox, text}] 정규화 결과: 주소/금액/날짜/권리유형 표준화(JSON) #2 위험 특약/권리 분석</description>
    </item>
    <item>
      <title>데이터분석 #4 리뷰 데이터 분석</title>
      <link>http://localhost:1313/docs/study/ai/ai22/</link>
      <pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai22/</guid>
      <description>데이터분석 #4 리뷰 데이터 분석 # #2025-08-19&#xA;1. 목적 # 리뷰 데이터를 보고&#xA;감성 점수와 평점의 관계 리뷰 길이와 감성 점수의 관계 카테고리별 감성 차이 Review_length가 AI 임베딩 유사도에 영향을 줄 수 있는지 인사이트 생성하기.&#xA;# 2. 코드 # import os import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.pyplot as plt import matplotlib as mpl from sentence_transformers import SentenceTransformer, util # Mac 환경 한글 폰트 설정 plt.</description>
    </item>
    <item>
      <title>LLM #1 LLM 이해와 Transformer</title>
      <link>http://localhost:1313/docs/study/ai/ai21/</link>
      <pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai21/</guid>
      <description>LLM #1 LLM 이해와 Transformer # #2025-08-11&#xA;1. LLM 기본이해 # #1 Word Embedding (p.27-28)&#xA;Word Embedding&#xA;핵심 아이디어는 단어가 어떤 맥락에서 자주 함께 등장하는지를 학습. “you say goodbye and I say hello”에서 ‘goodbye’주변에는 ‘you’, ‘say’, ‘and’, ‘I’ 같은 단어가 함께 등장하고 그 관계를 학습하도록 신경망을 훈련시킨다. 학습이 반복되면 각 단어는 벡터로 표현되고 의미가 비슷한 단어일수록 벡터 공간에서 가깝게 위치한다. Input이 ‘goodbye’이고 Target이 ‘you’, ‘say’, ‘and’, ‘I’여도 된다. Word Embedding - 신경망 구조 그림</description>
    </item>
    <item>
      <title>MutClust 논문 어셉.. ㅠㅠ</title>
      <link>http://localhost:1313/docs/study/career/career7/</link>
      <pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/career/career7/</guid>
      <description> MutClust 논문 어셉.. ㅠㅠ # #2025-08-16&#xA;학위논문이랑 skala 병행하면서 신체/정신적 체력이슬슬 고갈되던중이었는데&#xA;여느날처럼 새벽에 깼는데 어셉메일이 와있었다 ㅎㅎㅎ&#xA;# 리비전때 사실 잘못적은내용이있어서 계속걸렸었고 2차리비전 각오도 하고있었는데 돼버리니깐 안와닿는데 너무 좋다. ㅎㅎ 진짜 한시름 덜었따&#xA;어제오늘 좀쳐져서 잠도너무많이자고그랬는데 진짜이번주안에 학위논문이랑 피피티 마무리할수있을거같다 ㅎㅎㅎ&#xA;# # </description>
    </item>
    <item>
      <title>학위논문작업 #5 클러스터링 로그 뽑기 (4)</title>
      <link>http://localhost:1313/docs/study/algorithm/algo17/</link>
      <pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo17/</guid>
      <description>학위논문작업 #5 클러스터링 로그 뽑기 (4) # #2025-08-14&#xA;1. CCM selection # 비교하기 좋은 샘플 CCM 두개를 뽑았다!&#xA;# CCM1 [ccm_idx 28615] Start expand_cluster: left_cur_dist=0, right_cur_dist=0, es_l=65, left_max_dist=325, right_max_dist=325 #CCM2 [ccm_idx 28624] Start expand_cluster: left_cur_dist=0, right_cur_dist=0, es_l=1, left_max_dist=5, right_max_dist=5 CCM1 position: 28881 position index: 28615 H-score: 0.05290 Eps scaler: 1 Deps: 5 CCM2 position: 28890 position index: 28624 H-score: 6.4062 Eps scaler: 65 Deps: 325 # 핵심로직&#xA;최대허용거리: Deps CCM2의 H-중요도는 6.</description>
    </item>
    <item>
      <title>python #2 객체지향 프로그래밍, 병렬처리</title>
      <link>http://localhost:1313/docs/study/sw/sw21/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw21/</guid>
      <description>python #2 객체지향 프로그래밍, 병렬처리 # #2025-08-13&#xA;1. 객체지향 프로그래밍 # #1 property &amp;amp; dataclass (p.139-140)&#xA;@property&#xA;diameter 메서드는 사실 _radius * 2라는 계산을 수행하지만 외부에선 c.diameter라고 쓰면 바로 10이라는 결과를 얻을 수 있다. @diameter.setter를 사용하면 c.diameter = 20 형태로 diameter을 수정할수있고 내부에서는 diameter을 받아 _radius=10으로 변환 저장한다. fastapi에서 젤많이쓰는 기능이 속성화이다. @dataclass&#xA;보통 클래스를 만들면 __init__으로 생성자, __repr__으로 객체 출력 형식, __eq__로 동등성 비교 등을 직접 정의해야 하는데 @dataclass를 붙이면 이런 메서드들이 자동 생성된다.</description>
    </item>
    <item>
      <title>python #1 기본문법, 가상환경, 로깅</title>
      <link>http://localhost:1313/docs/study/sw/sw19/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw19/</guid>
      <description>python #1 기본문법, 가상환경, 로깅 # #2025-08-12&#xA;1. 기본문법 # #1 break와 continue의 차이 (p.29)&#xA;# break for i in range(10): if i==5: break print(i) # continue for i in range(5): if i==2: continue print(i) break 0부터 9까지 세는 반복문에서 i가 5가 되는 순간 break를 만나면 그 뒤의 숫자는 전혀 세지 않고 반복이 끝난다. continue 0부터 4까지 세는 반복문에서 i가 2인 경우 continue를 만나면 2를 출력하지 않고 바로 다음 숫자인 3으로 넘어가고 반복문 자체는 끝나지 않는다.</description>
    </item>
    <item>
      <title>python #2 리스트 vs 제너레이터 비교 실습</title>
      <link>http://localhost:1313/docs/study/sw/sw20/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw20/</guid>
      <description>python #2 리스트 vs 제너레이터 비교 실습 # #2025-08-12&#xA;1. 100만 개의 숫자 합 구하기 # 1) 리스트 방식&#xA;import sys # 1) 리스트 방식 numbers = list(range(1000000)) # 0부터 999,999 리스트 생성 list_sum = sum(numbers) # 합계 구하기 list_mem = sys.getsizeof(numbers) # 메모리 사용량 확인 (리스트 객체 크기) print(f&amp;#34;리스트 합: {list_sum:,}&amp;#34;) print(f&amp;#34;리스트 메모리 사용량: {list_mem} bytes&amp;#34;) 리스트 합: 499,999,500,000 리스트 메모리 사용량: 8000056 bytes numbers=list(range(1000000)) -&amp;gt; sum(numbers) 0~999,999를 리스트(numbers)로 만들어 합계를 구함 sys.</description>
    </item>
    <item>
      <title>학위논문작업 #6 Intro 구성</title>
      <link>http://localhost:1313/docs/study/algorithm/algo18/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo18/</guid>
      <description>학위논문작업 #6 Intro 구성 # #2025-08-16&#xA;1. 고민인점 # 저널 제출용은 Background로 다음 내용을 사용함.&#xA;바이러스의 전파력, 중증 질환 유발 능력, 항체 회피 능력과 같은 특성을 변화시키는 많은 돌연변이가 발생&#xA;돌연변이와 바이러스 특성, 특히 환자의 질병 중증도 간의 연관성을 설명하는 연구들이 있고 주로 아미노산 또는 뉴클레오타이드 수준에서의 돌연변이 빈도를 활용하며 빈도 높은 돌연변이가 기능적으로 더 중요할 가능성이 높다는 가정에 기반하는데 이 접근법은 계통에 따른 편향에 취약하고 빈도가 다소 낮지만 다양한 돌연변이들이 신호하는 바이러스 적응이나 면역 회피를 간과하는 경우가 많다.</description>
    </item>
    <item>
      <title>MLOps #1</title>
      <link>http://localhost:1313/docs/study/ai/ai20/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai20/</guid>
      <description>MLOps #1 # #2025-08-11&#xA;실습 # 메이크파일, 린팅, 테스트와 같이 파이썬 프로젝트 스캐폴딩에 필수적인 요소가 포함된 깃허브 저장소를 생성해보자. 그리고 간단하게 코드 포매팅을 수행하도록 메이크파일 스크립트를 작성해보자.&#xA;깃허브 액션을 사용하여 두개 이상의 파이썬 버전에 대해 깃허브 프로젝트 테스트를 수행해보자.&#xA;클라우드 네이티브 빌드 서버(AWS 코드빌드, GCP 클라우드 빌드, 애저 DevOps 파이프라인)를 사용하여 지속적 통합을 수행해보자.&#xA;깃허브 프로젝트를 도커 파일로 컨테이너화하고, 자동으로 컨테이너 레지스트리에 새로운 컨테이너가 등록되도록 만들어보자.&#xA;locust 또는 loader io와 같은 부하 테스트 프레임워크를 사용하여 애플리케이션에 대한 간단한 부하 테스트 코드를 작성한다.</description>
    </item>
    <item>
      <title>학위논문작업 #3 클러스터링 로그 뽑기 (2)</title>
      <link>http://localhost:1313/docs/study/algorithm/algo16/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo16/</guid>
      <description>학위논문작업 #3 클러스터링 로그 뽑기 (2) # #2025-08-11&#xA;1. Init # [ccm_idx 28624] Start expand_cluster: left_cur_dist=0, right_cur_dist=0, es_l=1, left_max_dist=5, right_max_dist=5 es_l=1, left_max_dist=5, es_r=1, right_max_dist=5 초기 반경 mut_deps: 5*1 = 5 bp # 2. Left expansion # [ccm_idx 28624] Left expansion: left_index=28623, ld=1, updated es_l=1.0, mut_deps=5.0, left_max_dist=5 ld = 1 확장 가능?&#xA;ld = POS(center) - POS(28623) = 1 현재 한도 left_max_dist(0)=5 이므로 ld(=1) ≤ 5 여서 확장 가능 scaler update?</description>
    </item>
    <item>
      <title>학위논문작업 #4 클러스터링 로그 뽑기 (3)</title>
      <link>http://localhost:1313/docs/study/algorithm/algo15/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo15/</guid>
      <description>학위논문작업 #4 클러스터링 로그 뽑기 (3) # #2025-08-11&#xA;1. Previous # [ccm_idx 28624] Start expand_cluster: left_cur_dist=0, right_cur_dist=0, es_l=1, left_max_dist=5, right_max_dist=5 [ccm_idx 28624] Left expansion: left_index=28623, ld=1, updated es_l=1.0, mut_deps=5.0, left_max_dist=5 [ccm_idx 28624] Left expansion: left_index=28622, ld=2, updated es_l=1.0, mut_deps=5.0, left_max_dist=5.0 [ccm_idx 28624] Left expansion: left_index=28621, ld=3, updated es_l=1.6666666666666665, mut_deps=8.333333333333332, left_max_dist=5.0 [ccm_idx 28624] Left expansion: left_index=28620, ld=4, updated es_l=1.4444444444444444, mut_deps=7.222222222222222, left_max_dist=8.333333333333332 [ccm_idx 28624] Left expansion: left_index=28619, ld=5, updated es_l=1.2962962962962963, mut_deps=6.481481481481481, left_max_dist=7.</description>
    </item>
    <item>
      <title>학위논문작업 #2 클러스터링 로그 뽑기</title>
      <link>http://localhost:1313/docs/study/algorithm/algo14/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo14/</guid>
      <description>학위논문작업 #2 클러스터링 로그 뽑기 # #2025-08-10&#xA;Objective # MutClust의 기존 코드에서는 클러스터링 수행후 클러스터 정보만 출력할뿐 neighbor eps scaler에 따른 ccm eps scaler의 업데이트와 그에 따른 eps 업데이트 내역을 따로 빼진 않았었다. 근데 클러스터링 과정을 설명하기에 좋은 예시를 만들기가 어려워서 (기존 예시는 맘에 안들고..) 그냥 로그를 다 뽑고 괜찮아 보이는걸 건져보기로 했다. # 1. 로깅 코드 추가하기 # 일단 로그는 총 4번뽑을건데&#xA;시작 (left_cur_dist &amp;amp; right_cur_dist=0일때) Left expansion 과정 Right expansion 과정 최종 결과 이렇게뽑을려고한다.</description>
    </item>
    <item>
      <title>생성형 AI #1 생성형 AI 기초 및 Prompt Engineering</title>
      <link>http://localhost:1313/docs/study/ai/ai18/</link>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai18/</guid>
      <description>생성형 AI #1 생성형 AI 기초 및 Prompt Engineering # #2025-08-09&#xA;#1 RAG (p.27)&#xA;RAG의 역할?&#xA;질문을 LLM에 던지기 전에 knowledge corpus에 질문을 미리 검색한다(회사 데이터에 대한 지식 벡터 db). 질문과 연관된 문서를 찾고 적절하게 만들어서 retrieval 던지면 의도대로 답변이 잘 나온다. # #2 LLM 출력 구성 (p.42-45)&#xA;Output Length (Max Tockens)&#xA;500자로 제한을 걸면 500자로 맞춰주는게 아니라 500자 넘으면 출력을 멈춘다. Sampling Controls&#xA;LLM은 다음에 올 단어를 고를 때 미리 계산된 사전 확률분포를 가지고 거기서 하나를 뽑는다</description>
    </item>
    <item>
      <title>생성형 AI #2 Prompt Engineering 실습 미리돌려보기</title>
      <link>http://localhost:1313/docs/study/ai/ai19/</link>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai19/</guid>
      <description>생성형 AI #2 Prompt Engineering 실습 미리돌려보기 # #2025-08-09&#xA;1. VOC 분석 # setting&#xA;https://openrouter.ai/ Model: GPT-5 Temperature: 0.2 (낮게: 일관성 있는 분류 결과) Top-k / Top-p: default Max tokens: 1024 system prompt&#xA;너는 IT 시스템의 평가전문가야. 이번에 개발한 AI를 적용한 회계세무 시스템을 테스트한 고객의 평가내용인 VOC를 분석하는 것이 너의 역할이야. 판단근거를 2가지로 함께 제시해줘. user prompt&#xA;아래에 제공하는 모든 VOC 문장을 긍정, 중립, 부정 중 하나로 분류하고, 특히 부정일 경우 그렇게 판단한 이유를 2가지로 요약해줘.</description>
    </item>
    <item>
      <title>데이터 분석 #3 회귀분석</title>
      <link>http://localhost:1313/docs/study/ai/ai17/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai17/</guid>
      <description>데이터 분석 #3 회귀분석 # #2025-08-07&#xA;#1 Oversampling Techinique (p.69-71)&#xA;SMOTE&#xA;소수 클래스 포인트 중 하나를 랜덤하게 고르고 이웃 포인트 k개를 찾고 이 이웃들과의 연결선을 따라 중간 어딘가에 새로운 샘플을 만든다. 즉 원본과 이웃 사이에 위치한 점들을 생성한다. 소수 클래스 포인트들 사이의 직선 위에서만 새로운 데이터를 만들기 때문에 실제로는 decision boundary 근처에서 중요한 데이터를 놓칠 수 있다 Borderline-SMOTE&#xA;소수 클래스의 포인트에 대해 kNN을 수행해서 이웃들을 찾는데 이때 이웃 중에서 과반수 이상이 다수 클래스인 경우 위험한 샘플(danger set)으로 간주된다 즉 이 샘플은 결정 경계에 가깝기 때문에 모델 입장에서 헷갈릴 가능성이 높다.</description>
    </item>
    <item>
      <title>데이터 분석 #2 Preprocessing</title>
      <link>http://localhost:1313/docs/study/ai/ai16/</link>
      <pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai16/</guid>
      <description>데이터 분석 #2 Preprocessing # #2025-08-06&#xA;#1 머신러닝 프로세스 (p.25)&#xA;test data가 필요한 이유? hyperparameter tuning을 하면서 validation data는 모델이 이미 참고했다 즉 간접적으로 학습에 영향을 줬기 때문에 모델 학습 과정에서 한번도 보지않은 데이터가 필요함. # #2 Box plot (p.38)&#xA;그림이 7개 차종에서 연비 플롯이라고 가정&#xA;투입됏을때 예측에 긍정적영향을 줄수잇는건?&#xA;납작한애들. 두꺼우면 대표성이 떨어진다. 2번에서 이상치들이 많으니까 잘 처리해야하겠다.&#xA;만약 그림같지 않고 y축 높이가 다 비슷비슷했다면?&#xA;이 변수들이 연비를 결정하는데 큰 영향을 못줌.</description>
    </item>
    <item>
      <title>Python #1 가상환경 구성 및 패키지 관리</title>
      <link>http://localhost:1313/docs/study/algorithm/algo11/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo11/</guid>
      <description>Python #1 가상환경 구성 및 패키지 관리 # #2025-08-05&#xA;1. 개념 # #1 가상환경의 필요성?&#xA;우리가 파이썬을 사용할 때, 가장 먼저 겪게 되는 문제 중 하나는 바로 패키지 버전 충돌이다. 예를 들어 어떤 프로젝트에서는 numpy==1.18.5 버전을 사용하고 있고, 또 다른 프로젝트에서는 numpy==1.24.0 버전을 사용하고 있다고 하면 이 둘을 동시에 하나의 환경에 설치하게 되면 충돌이 일어나거나 예상치 못한 에러가 발생할 가능성이 커진다. 특히 머신러닝, 데이터분석, 웹개발 프로젝트를 하다 보면 프로젝트마다 사용하는 패키지와 버전이 다르기 때문에 이러한 문제는 일상적으로 발생하며 따라서 각 프로젝트가 독립적으로 실행될 수 있는 ‘가상환경(Virtual Environment)’을 만들어서 관리해야 한다.</description>
    </item>
    <item>
      <title>Python #2 logging 활용한 로깅 구조 설계 관리</title>
      <link>http://localhost:1313/docs/study/algorithm/algo12/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo12/</guid>
      <description> Python #2 logging 활용한 로깅 구조 설계 # #2025-08-05&#xA;1. 개념 # logging은 실행 중 일어나는 다양한 이벤트, 경고, 에러, 정보 등을 기록해두고, 나중에 문제가 생겼을 때 정확히 어떤 일이 있었는지 기록을 통해 재구성할 수 있도록 도와준다.&#xA;2. 실습 # </description>
    </item>
    <item>
      <title>데이터 분석 #1 기초통계</title>
      <link>http://localhost:1313/docs/study/ai/ai14/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai14/</guid>
      <description>데이터 분석 #1 기초통계 # #2025-08-05&#xA;1. 기술 통계 # #1 IQR (p.34)&#xA;IQR은? 가운데 50%의 거리.&#xA;그림 설명&#xA;그림의 2,3: 각각 IQR의 1.5배 선, median 값 선. 그림의 B: ⚬ 가 많으면 특이값이 많은 것. 그림의 1,2,3: 1,2는 각각 IQR의 1.5배 선이라고 했는데 3과의 거리가 서로 다른 이유는? 1.5배 안쪽에 데이터들이 다 분포해서. 즉max가 1.5배보다 작아서. # #2 변이 계수(Coefficient of Variables)&#xA;평균치가 다른 집단 비교. 변이 계수 = 표준편차 / 평균.</description>
    </item>
    <item>
      <title>학위논문작업 #1 핵심함수 로직 정리</title>
      <link>http://localhost:1313/docs/study/algorithm/algo13/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo13/</guid>
      <description>학위논문작업 #1 핵심함수 로직 정리 # #2025-08-05&#xA;1. input # def expand_cluster(ccmIdx, mutData, info): ccm의 인덱스 ccmIdx 돌연변이 중요도 정보 mutData info: 기본 세팅 파라미터 # 2 # scaler_l = mutData[ccmIdx][&amp;#39;eps_scaler&amp;#39;] idx_l = ccmIdx - 1 eps_l = mutData[ccmIdx][&amp;#39;left_distance&amp;#39;] pos_l = mutData[ccmIdx][POS] scaler_l: ccm의 eps scaler idx_l: 최초 이웃의 인덱스 eps_l: ccm의 최초 eps pos_l: ccm의 postion # 3 # while idx_l &amp;gt;= 0 and (pos_l - mutData[idx_l][POS]) &amp;lt;= eps_l: delta = scaler_l - mutData[idx_l][&amp;#39;eps_scaler&amp;#39;] scaler_l -= delta / info.</description>
    </item>
    <item>
      <title>Docker #3</title>
      <link>http://localhost:1313/docs/study/sw/sw16/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw16/</guid>
      <description>Docker #3 # #2025-08-04&#xA;1. 레지스트리에 접속하고 이미지를 pull/push하기 # # Docker 로그인 $ docker login https://{실습링크}.com # ID: * # Password: * $ Login Succeeded # 이미지 Pull (이미지 내려받기): 예를 들어 container-linux:1.1 이미지를 다운로드 $ docker pull {실습링크}.com/{실습id}/container-linux:1.1 # 이미지 Push (Image Push 정보 사용): Push 권한은 일반 계정이 아니라 로봇 계정(CI/CD 용)을 사용합니다. # 로봇 계정 로그인 $ docker login https://{실습링크}.com # ID: robot$skala25a # Password: 1qB9cyusbNComZPHAdjNIFWinf52xaBJ # 태깅 (Tag local image) $ docker tag container-linux:1.</description>
    </item>
    <item>
      <title>Docker #4 자신의 Frontend (HTML, JS, CSS) 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자</title>
      <link>http://localhost:1313/docs/study/sw/sw17/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw17/</guid>
      <description>Docker #4 자신의 Frontend (HTML, JS, CSS) 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자 # #2025-08-04&#xA;#조건&#xA;nginx:alpine 이미지를 사용 노출 Port는80 nginx를실행하는방식은 -nginx -g daemon off; nginx의 routing 설정은 default.conf에설정한다. #path&#xA;$ pwd /Users/yshmbid/rde/config/workspace/exec-template $ ls Dockerfile default.conf deploy deploy.yaml docker-build.sh docker-push.sh service.yaml src # 1. docker-build.sh와 docker-push.sh 복사 # $ pwd /Users/yshmbid/rde/config/workspace/container/05.webserver $ ls Dockerfile default.conf deploy docker-build.sh docker-push.sh src # docker-build.sh #!/bin/bash NAME=sk019 IMAGE_NAME=&amp;#34;healthcheck-server&amp;#34; #IMAGE_NAME=&amp;#34;webserver&amp;#34; VERSION=&amp;#34;1.0.0&amp;#34; CPU_PLATFORM=arm64 #amd64 # Docker 이미지 빌드 docker build \ --tag ${NAME}-${IMAGE_NAME}:${VERSION} \ --file Dockerfile \ --platform linux/${CPU_PLATFORM} \ ${IS_CACHE} .</description>
    </item>
    <item>
      <title>Docker #5 kubernetes 환경에 나의 앱을 배포해보자</title>
      <link>http://localhost:1313/docs/study/sw/sw18/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw18/</guid>
      <description>Docker #5 kubernetes 환경에 나의 앱을 배포해보자 # #2025-08-04&#xA;#path&#xA;$ pwd /Users/yshmbid/rde/config/workspace/exec-template #파일 구조&#xA;/workspace └── exec-template ├── Dockerfile ├── default.conf ├── docker-build.sh ├── docker-push.sh ├── cicd.sh ├── deploy/ │ ├── deploy.t │ ├── deploy.sh │ ├── service.t │ ├── service.sh │ └── env.properties └── src/ ├── index.html └── media/ #이전 실습과의 차이?&#xA;cicd.sh를 쓴다. deploy 디렉토리를 쓴다. docker-build.sh와 docker-push.sh에서 amd였던걸 arm으로 바꿔줬는데 이걸다시 amd로 바꿔준다. # 1. cicd.</description>
    </item>
    <item>
      <title>MutClust 연구: method contribution</title>
      <link>http://localhost:1313/docs/study/algorithm/algo10/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo10/</guid>
      <description> MutClust 연구: method contribution # #2025-08-04&#xA;#Paper&#xA;Identification of Severity Related Mutation Hotspots in SARS-CoV-2 Using a Density-Based Clustering Approach&#xA;0. 참여 파트 # #Algorithm └── Computing the H-score └── Density-based mutation hotspot clustering #Omics-analysis └── Selection of severity related hotspots └── Differentially expressed gene analysis └── Evaluation of HLA-peptide affinity #Validation └── Validation on Influenza genome └── K-dist plot </description>
    </item>
    <item>
      <title>RF-SHAP 연구 #1 모델 학습</title>
      <link>http://localhost:1313/docs/study/ai/ai12/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai12/</guid>
      <description>RF-SHAP 연구 #1 모델 학습 # #2025-08-04&#xA;1. Load data # import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split, cross_val_score from sklearn.metrics import accuracy_score import pickle with open(&amp;#39;/preprocessing/processed_data.pickle&amp;#39;,&amp;#39;rb&amp;#39;) as f: preproc_data = pickle.load(f) cytokine_df = preproc_data[&amp;#39;cytokine_data&amp;#39;] patient_meta = preproc_data[&amp;#39;metadata&amp;#39;] patient_info = preproc_data[&amp;#39;clinical&amp;#39;] 2. Train data split # normal_df = cytokine_df[cytokine_df.index.str.contains(&amp;#39;Healthy&amp;#39;)] severe_samples = patient_meta[patient_meta.Severity &amp;gt;= 6] severe_df = cytokine_df[cytokine_df.index.isin(severe_samples.Sample)] normal_df[&amp;#39;source&amp;#39;] = 0 severe_df[&amp;#39;source&amp;#39;] = 1 normal_df,severe_df ( CXCL9 LIF CXCL11 IL25 IL12B IL10 \ Healthy1 6.</description>
    </item>
    <item>
      <title>RF-SHAP 연구 #2 SHAP 분석</title>
      <link>http://localhost:1313/docs/study/ai/ai13/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai13/</guid>
      <description>RF-SHAP 연구 #2 SHAP 분석 # #2025-08-04&#xA;1. Load data # import pandas as pd import numpy as np import pickle import joblib import shap import matplotlib.pyplot as plt import seaborn as sns #Load rf model with open(&amp;#39;/model/rf_model.pkl&amp;#39;,&amp;#39;rb&amp;#39;) as f: rf_model = joblib.load(f) #Load dataset with open(&amp;#39;/preprocessing/processed_data.pickle&amp;#39;,&amp;#39;rb&amp;#39;) as f: preproc_data = pickle.load(f) cytokine_df = preproc_data[&amp;#39;cytokine_data&amp;#39;] patient_meta = preproc_data[&amp;#39;metadata&amp;#39;] patient_info = preproc_data[&amp;#39;clinical&amp;#39;] 2. Model evaluation - feature importance # # Get feature importances importances = rf_model.</description>
    </item>
    <item>
      <title>결단</title>
      <link>http://localhost:1313/docs/hobby/book/book52/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/book/book52/</guid>
      <description>결단 # #2025-08-04&#xA;#1&#xA;머스크는 로켓이 산소가 희박한 높이로 충분히 솟아올라 불꽃이 꺼지길 바랐다. 그러나 로켓은 추락하기 시작했다. 비디오 피드에서 오멜렉이 가까이 다가오더니 더 이상 화면에 아무것도 비치지 않았다. 그리고 불타는 파편들이 바다로 떨어졌다. “위장이 뒤틀렸지요.” 머스크의 말이다. 1시간 후, 머스크는 뮬러, 쾨니스만, 부자, 톰슨 등 수석 팀원들과 함께 잔해를 둘러보기 위해 육군 헬리콥터에 올랐다.&#xA;그날 밤 모두가 콰즈의 야외 바에 모여 조용히 맥주를 마셨다. 몇몇 엔지니어는 눈물을 흘렸다. 머스크는 돌처럼 굳은 얼굴과 먼 곳을 응시하는 눈빛으로 조용히 생각에 잠겼다.</description>
    </item>
    <item>
      <title>논문 contribution 정리: EBV DHT 연구</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi32/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi32/</guid>
      <description>논문 contribution 정리: EBV DHT 연구 # #2025-08-03&#xA;1. Link # Paper1 - Dihydrotestosterone Enhances MICA-Mediated Immune Responses to Epstein–Barr Virus-Associated Gastric Carcinoma&#xA;Paper2 - Dihydrotestosterone-androgen receptor signaling suppresses EBV-positive gastric cancer through DNA demethylation-mediated viral reactivation&#xA;#Paper1 └── 3. ChIP-Seq Assay #Paper2 └── 2. RNA-seq analysis └── 14. Bioinformatics analysis of methylome # 2. Contributions # 1. ChIP-Seq Assay&#xA;Among the above p65 ChIP samples, the sample treated with 100 nM DHT for 30 min showed the strong p65 enrichment on the SNU719 genome.</description>
    </item>
    <item>
      <title>Docker #1 Python 실행 컨테이너 만들기</title>
      <link>http://localhost:1313/docs/study/sw/sw14/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw14/</guid>
      <description>Docker #1 Python 실행 컨테이너 만들기 # #2025-08-01&#xA;Background # RDE #1 Local PC에서 RDE 환경 구성에서 Harbor registry로부터 RdE Container download를 수행했음 아이콘을 클릭해서 RDE 런처를 실행한다. # 1. 웹 서비스 실행 컨테이너 만들기 # /config/workspace/cloud/container/00.container-linux 경로로 이동 cd /config/workspace/cloud/container/00.container-linux 디렉토리 구조는? 00.container-linux/ ├── Dockerfile // 컨테이너 환경 설정 ├── Dockerfile.pytho-slim ├── Dockerfile.ubuntu ├── docker-build.sh ├── docker-push.sh ├── mycode.py ├── fastserver.py ├── webserver.py └── mydata/ Dockerfile 내용 확인하기 FROM python:3.</description>
    </item>
    <item>
      <title>Docker #2 작년 작업 복기: netmhcpan image 불러와서 패키지 돌리기</title>
      <link>http://localhost:1313/docs/study/sw/sw15/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw15/</guid>
      <description>Docker #2 작년 작업 복기: netmhcpan image 불러와서 패키지 돌리기 # #2025-08-01&#xA;1 # 2024.11.24 MutClust 작업중에 netmhcpan을 돌려야되는 상황이 왓었는데&#xA;netmhcpan이 유료였나 그래서 패키지 다운은 안되고 담당 박사님은 그만두셧고.. 서버 뒤지다가 위 README 파일 발견해서 결과물 저장까진 했던 기억이있다.&#xA;# 이때먼가 의문이 들었던게 새로운 conda 환경에 접속한거같은 느낌이 아니라 완전 다른 제2의서버에 접속한 느낌이었는데 이상하게 연구실 디렉토리들은 그대로 접근이 가능해서 혼란스럽지만 그냥 절대경로 다 박고 수행했는데 결과들이 문제없이 저장됐었다.</description>
    </item>
    <item>
      <title>MutClust 코드 리펙토링 #2 arg_parser</title>
      <link>http://localhost:1313/docs/study/algorithm/algo2/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo2/</guid>
      <description>MutClust 코드 리펙토링 #2 arg_parser # #2025-08-01&#xA;MutClust 알고리즘의 코드 구성은 아래와 같은데&#xA;MutClust ├── sc/ │ └── lib.py │ └── arg_parser.py // 실행 설정 │ └── utils.py └── Test arg_parser.py는 실험 환경 파라미터 세팅 및 CLI 인자 파싱을 포함한다.&#xA;# === arg_parser.py === import argparse from os.path import exists from src.mlib import ( DIMINISHING_FACTOR, EPSILON, EPSILON_SCALING_FACTOR, MAX_EPS, MIN_CLUSTER_LENGTH, CCM_MIN_PERCENTAGE_SUM ) class ArgsInfo: def __init__(self): self.args = {} self.fin = &amp;#39;&amp;#39; self.</description>
    </item>
    <item>
      <title>MutClust 코드 리펙토링 #3 utils</title>
      <link>http://localhost:1313/docs/study/algorithm/algo9/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo9/</guid>
      <description>MutClust 코드 리펙토링 #3 utils # #2025-08-01&#xA;MutClust 알고리즘의 코드 구성은 아래와 같은데&#xA;MutClust ├── sc/ │ └── lib.py │ └── arg_parser.py │ └── utils.py // 전처리 및 분석 └── Test utils.py는 데이터 전처리 및 분석 함수를 포함한다.&#xA;# === Fasta 전처리 === def fasta2csv(home_dir, nation_dir, filechunk, ref, outdir): for file in filechunk: path = os.path.join(home_dir, nation_dir, file) filename = os.path.splitext(os.path.basename(file))[0] outpath = os.path.join(outdir, f&amp;#34;{filename}.csv&amp;#34;) if not os.path.exists(outpath): df = DataFrame({&amp;#39;ref&amp;#39;: ref.</description>
    </item>
  </channel>
</rss>
