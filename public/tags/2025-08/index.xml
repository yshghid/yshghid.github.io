<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2025-08 on  </title>
    <link>http://localhost:1313/tags/2025-08/</link>
    <description>Recent content in 2025-08 on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/2025-08/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python #1 가상환경 구성 및 패키지 관리</title>
      <link>http://localhost:1313/docs/study/algorithm/algo11/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo11/</guid>
      <description>Python #1 가상환경 구성 및 패키지 관리 # #2025-08-05&#xA;1. 개념 # #1 가상환경의 필요성?&#xA;우리가 파이썬을 사용할 때, 가장 먼저 겪게 되는 문제 중 하나는 바로 패키지 버전 충돌이다. 예를 들어 어떤 프로젝트에서는 numpy==1.18.5 버전을 사용하고 있고, 또 다른 프로젝트에서는 numpy==1.24.0 버전을 사용하고 있다고 하면 이 둘을 동시에 하나의 환경에 설치하게 되면 충돌이 일어나거나 예상치 못한 에러가 발생할 가능성이 커진다. 특히 머신러닝, 데이터분석, 웹개발 프로젝트를 하다 보면 프로젝트마다 사용하는 패키지와 버전이 다르기 때문에 이러한 문제는 일상적으로 발생하며 따라서 각 프로젝트가 독립적으로 실행될 수 있는 ‘가상환경(Virtual Environment)’을 만들어서 관리해야 한다.</description>
    </item>
    <item>
      <title>Python #2 logging 활용한 로깅 구조 설계 관리</title>
      <link>http://localhost:1313/docs/study/algorithm/algo12/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo12/</guid>
      <description> Python #2 logging 활용한 로깅 구조 설계 # #2025-08-05&#xA;1. 개념 # logging은 실행 중 일어나는 다양한 이벤트, 경고, 에러, 정보 등을 기록해두고, 나중에 문제가 생겼을 때 정확히 어떤 일이 있었는지 기록을 통해 재구성할 수 있도록 도와준다.&#xA;2. 실습 # </description>
    </item>
    <item>
      <title>데이터 분석 #1 기초통계</title>
      <link>http://localhost:1313/docs/study/ai/ai14/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai14/</guid>
      <description>데이터 분석 #1 기초통계 # #2025-08-05&#xA;1. 기술 통계 # #1 IQR?&#xA;가운데 50%의 거리.&#xA;#2 IQR 그림 설명 (p.34)&#xA;그림의 2,3: 각각 IQR의 1.5배 선, median 값 선. 그림의 B: ⚬ 가 많으면 특이값이 많은 것. 그림의 1,2,3: 1,2는 각각 IQR의 1.5배 선이라고 했는데 3과의 거리가 서로 다른 이유는? 1.5배 안쪽에 데이터들이 다 분포해서. 즉max가 1.5배보다 작아서. #3 변이 계수(Coefficient of Variables)&#xA;평균치가 다른 집단 비교. 변이 계수 = 표준편차 / 평균.</description>
    </item>
    <item>
      <title>Docker #3 3일차 실습 1,2,3</title>
      <link>http://localhost:1313/docs/study/sw/sw16/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw16/</guid>
      <description>Docker #3 3일차 실습 1,2,3 # #2025-08-04&#xA;1. 레지스트리에 접속하고 이미지를 pull/push하기 # # Docker 로그인 $ docker login https://{실습링크}.com # ID: * # Password: * $ Login Succeeded # 이미지 Pull (이미지 내려받기): 예를 들어 container-linux:1.1 이미지를 다운로드 $ docker pull {실습링크}.com/{실습id}/container-linux:1.1 # 이미지 Push (Image Push 정보 사용): Push 권한은 일반 계정이 아니라 로봇 계정(CI/CD 용)을 사용합니다. # 로봇 계정 로그인 $ docker login https://{실습링크}.com # ID: robot$skala25a # Password: 1qB9cyusbNComZPHAdjNIFWinf52xaBJ # 태깅 (Tag local image) $ docker tag container-linux:1.</description>
    </item>
    <item>
      <title>Docker #4 3일차 실습: 자신의 Frontend (HTML, JS, CSS) 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자</title>
      <link>http://localhost:1313/docs/study/sw/sw17/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw17/</guid>
      <description>Docker #4 3일차 실습: 자신의 Frontend (HTML, JS, CSS) 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자 # #2025-08-04&#xA;조건&#xA;nginx:alpine 이미지를 사용 노출 Port는80 nginx를실행하는방식은 -nginx -g daemon off; nginx의 routing 설정은 default.conf에설정한다. 0. 작업 위치 # $ pwd /Users/yshmbid/rde/config/workspace/exec-template $ ls Dockerfile default.conf deploy deploy.yaml docker-build.sh docker-push.sh service.yaml src 1. docker-build.sh와 docker-push.sh 복사 # $ pwd /Users/yshmbid/rde/config/workspace/container/05.webserver $ ls Dockerfile default.conf deploy docker-build.sh docker-push.sh src 05.webserver의 docker-build.sh와 docker-push.sh를 작업 디렉토리인 exec-template로 복사해준다.</description>
    </item>
    <item>
      <title>Docker #5 3일차 실습: kubernetes 환경에 나의 앱을 배포해보자</title>
      <link>http://localhost:1313/docs/study/sw/sw18/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw18/</guid>
      <description>Docker #5 3일차 실습: kubernetes 환경에 나의 앱을 배포해보자 # #2025-08-04&#xA;0. 작업 정보 # #1 작업 위치&#xA;$ pwd /Users/yshmbid/rde/config/workspace/exec-template #2 파일 구조&#xA;/workspace └── exec-template ├── Dockerfile ├── default.conf ├── docker-build.sh ├── docker-push.sh ├── cicd.sh ├── deploy/ │ ├── deploy.t │ ├── deploy.sh │ ├── service.t │ ├── service.sh │ └── env.properties └── src/ ├── index.html └── media/ #3 이전 실습과의 차이&#xA;cicd.sh를 쓴다. deploy 디렉토리를 쓴다. docker-build.</description>
    </item>
    <item>
      <title>MutClust 연구: method contribution</title>
      <link>http://localhost:1313/docs/study/algorithm/algo10/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo10/</guid>
      <description> MutClust 연구: method contribution # #2025-08-04&#xA;#Paper&#xA;Identification of Severity Related Mutation Hotspots in SARS-CoV-2 Using a Density-Based Clustering Approach&#xA;0. 참여 파트 # #Algorithm └── Computing the H-score └── Density-based mutation hotspot clustering #Omics-analysis └── Selection of severity related hotspots └── Differentially expressed gene analysis └── Evaluation of HLA-peptide affinity #Validation └── Validation on Influenza genome └── K-dist plot </description>
    </item>
    <item>
      <title>RF-SHAP 연구 #1 모델 학습</title>
      <link>http://localhost:1313/docs/study/ai/ai12/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai12/</guid>
      <description>RF-SHAP 연구 #1 모델 학습 # #2025-08-04&#xA;1. Load data # import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split, cross_val_score from sklearn.metrics import accuracy_score import pickle with open(&amp;#39;/preprocessing/processed_data.pickle&amp;#39;,&amp;#39;rb&amp;#39;) as f: preproc_data = pickle.load(f) cytokine_df = preproc_data[&amp;#39;cytokine_data&amp;#39;] patient_meta = preproc_data[&amp;#39;metadata&amp;#39;] patient_info = preproc_data[&amp;#39;clinical&amp;#39;] 2. Train data split # normal_df = cytokine_df[cytokine_df.index.str.contains(&amp;#39;Healthy&amp;#39;)] severe_samples = patient_meta[patient_meta.Severity &amp;gt;= 6] severe_df = cytokine_df[cytokine_df.index.isin(severe_samples.Sample)] normal_df[&amp;#39;source&amp;#39;] = 0 severe_df[&amp;#39;source&amp;#39;] = 1 normal_df,severe_df ( CXCL9 LIF CXCL11 IL25 IL12B IL10 \ Healthy1 6.</description>
    </item>
    <item>
      <title>RF-SHAP 연구 #2 SHAP 분석</title>
      <link>http://localhost:1313/docs/study/ai/ai13/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai13/</guid>
      <description>RF-SHAP 연구 #2 SHAP 분석 # #2025-08-04&#xA;1. Load data # import pandas as pd import numpy as np import pickle import joblib import shap import matplotlib.pyplot as plt import seaborn as sns #Load rf model with open(&amp;#39;/model/rf_model.pkl&amp;#39;,&amp;#39;rb&amp;#39;) as f: rf_model = joblib.load(f) #Load dataset with open(&amp;#39;/preprocessing/processed_data.pickle&amp;#39;,&amp;#39;rb&amp;#39;) as f: preproc_data = pickle.load(f) cytokine_df = preproc_data[&amp;#39;cytokine_data&amp;#39;] patient_meta = preproc_data[&amp;#39;metadata&amp;#39;] patient_info = preproc_data[&amp;#39;clinical&amp;#39;] 2. Model evaluation - feature importance # # Get feature importances importances = rf_model.</description>
    </item>
    <item>
      <title>결단</title>
      <link>http://localhost:1313/docs/hobby/book/book52/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/hobby/book/book52/</guid>
      <description>결단 # #2025-08-04&#xA;#1&#xA;머스크는 로켓이 산소가 희박한 높이로 충분히 솟아올라 불꽃이 꺼지길 바랐다. 그러나 로켓은 추락하기 시작했다. 비디오 피드에서 오멜렉이 가까이 다가오더니 더 이상 화면에 아무것도 비치지 않았다. 그리고 불타는 파편들이 바다로 떨어졌다. “위장이 뒤틀렸지요.” 머스크의 말이다. 1시간 후, 머스크는 뮬러, 쾨니스만, 부자, 톰슨 등 수석 팀원들과 함께 잔해를 둘러보기 위해 육군 헬리콥터에 올랐다.&#xA;그날 밤 모두가 콰즈의 야외 바에 모여 조용히 맥주를 마셨다. 몇몇 엔지니어는 눈물을 흘렸다. 머스크는 돌처럼 굳은 얼굴과 먼 곳을 응시하는 눈빛으로 조용히 생각에 잠겼다.</description>
    </item>
    <item>
      <title>EBV DHT 연구: method contribution</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi32/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi32/</guid>
      <description>EBV DHT 연구: method contribution # #2025-08-03&#xA;#Paper1&#xA;Dihydrotestosterone Enhances MICA-Mediated Immune Responses to Epstein–Barr Virus-Associated Gastric Carcinoma&#xA;#Paper2&#xA;Dihydrotestosterone-androgen receptor signaling suppresses EBV-positive gastric cancer through DNA demethylation-mediated viral reactivation&#xA;0. 참여 파트 # #Paper1 └── 3. ChIP-Seq Assay #Paper2 └── 2. RNA-seq analysis └── 14. Bioinformatics analysis of methylome 1. ChIP-Seq Assay # Among the above p65 ChIP samples, the sample treated with 100 nM DHT for 30 min showed the strong p65 enrichment on the SNU719 genome.</description>
    </item>
    <item>
      <title>skala 강의자료공부 #1 DL-CNN, RNN</title>
      <link>http://localhost:1313/docs/study/ai/ai11/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai11/</guid>
      <description>skala 강의자료공부 #1 DL-CNN, RNN # #2025-08-03&#xA;#1 p.90-92&#xA;Convolution, 즉 합성곱은 CNN의 가장 핵심적인 연산이다. 말 그대로 &amp;lsquo;겹쳐서 곱하고 더하는&amp;rsquo; 방식이다. 이는 우리가 이미지를 처리할 때, 그 이미지의 일부분만을 보며 특징을 추출하는 원리와 매우 유사하다. CNN에서는 이 연산을 통해 이미지 속에서 선, 모서리, 윤곽선 같은 패턴을 뽑아낸다.&#xA;p.90에서는 합성곱을 아주 직관적으로 보여준다. 왼쪽에 있는 초록색 격자는 이미지이고, 그 위에 씌워진 주황색 네모는 필터(또는 커널)다. 이 필터는 보통 3x3 크기를 가지며, 그 내부에 있는 값들은 학습을 통해 결정된다.</description>
    </item>
    <item>
      <title>Docker #1 Python 실행 컨테이너 만들기</title>
      <link>http://localhost:1313/docs/study/sw/sw14/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw14/</guid>
      <description>Docker #1 Python 실행 컨테이너 만들기 # #2025-08-01&#xA;0. RDE 런처 실행 # RDE #1 Local PC에서 RDE 환경 구성에서 Harbor registry로부터 RdE Container download를 수행했고 아이콘을 클릭해서 RDE 런처를 실행한다.&#xA;1. 웹 서비스 실행 컨테이너 만들기 # #1 /config/workspace/cloud/container/00.container-linux 경로로 이동&#xA;cd /config/workspace/cloud/container/00.container-linux #2 디렉토리 구조는?&#xA;00.container-linux/ ├── Dockerfile // 컨테이너 환경 설정 ├── Dockerfile.pytho-slim ├── Dockerfile.ubuntu ├── docker-build.sh ├── docker-push.sh ├── mycode.py ├── fastserver.py ├── webserver.py └── mydata/ #3 Dockerfile 내용 확인하기</description>
    </item>
    <item>
      <title>Docker #2 netmhcpan 작년 작업 복기</title>
      <link>http://localhost:1313/docs/study/sw/sw15/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/sw/sw15/</guid>
      <description>Docker #2 netmhcpan 작년 작업 복기 # #2025-08-01&#xA;1 # 2024.11.24 MutClust 작업중에 netmhcpan을 돌려야되는 상황이 왓었는데&#xA;netmhcpan이 유료였나 그래서 패키지 다운은 안되고.. 서버 뒤지다가 아래 README.txt 파일 발견해서 결과물 저장까진 했던 기억이있다.&#xA;이때먼가 의문이 들었던게 새로운 conda 환경에 접속한거같은 느낌이 아니라 완전 다른 제2의서버에 접속한 느낌이었는데 이상하게 연구실 디렉토리들은 그대로 접근이 가능해서 혼란스럽지만 그냥 절대경로 다 박고 수행했는데 결과들이 문제없이 저장됐다.&#xA;그래서 그뒤로 잊어버리고있었는데 docker 배우고나니까 먼가 이해돼서 이해된김에 정리해보기!</description>
    </item>
    <item>
      <title>MutClust 코드 리펙토링 #2 arg_parser</title>
      <link>http://localhost:1313/docs/study/algorithm/algo2/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo2/</guid>
      <description>MutClust 코드 리펙토링 #2 arg_parser # #2025-08-01&#xA;MutClust 알고리즘의 코드 구성은 아래와 같은데&#xA;MutClust ├── sc/ │ └── lib.py │ └── arg_parser.py // 실행 설정 │ └── utils.py └── Test arg_parser.py는 실험 환경 파라미터 세팅 및 CLI 인자 파싱을 포함한다.&#xA;# === arg_parser.py === import argparse from os.path import exists from src.mlib import ( DIMINISHING_FACTOR, EPSILON, EPSILON_SCALING_FACTOR, MAX_EPS, MIN_CLUSTER_LENGTH, CCM_MIN_PERCENTAGE_SUM ) class ArgsInfo: def __init__(self): self.args = {} self.fin = &amp;#39;&amp;#39; self.</description>
    </item>
    <item>
      <title>MutClust 코드 리펙토링 #3 utils</title>
      <link>http://localhost:1313/docs/study/algorithm/algo9/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo9/</guid>
      <description>MutClust 코드 리펙토링 #3 utils # #2025-08-01&#xA;MutClust 알고리즘의 코드 구성은 아래와 같은데&#xA;MutClust ├── sc/ │ └── lib.py │ └── arg_parser.py │ └── utils.py // 전처리 및 분석 └── Test utils.py는 데이터 전처리 및 분석 함수를 포함한다.&#xA;# === Fasta 전처리 === def fasta2csv(home_dir, nation_dir, filechunk, ref, outdir): for file in filechunk: path = os.path.join(home_dir, nation_dir, file) filename = os.path.splitext(os.path.basename(file))[0] outpath = os.path.join(outdir, f&amp;#34;{filename}.csv&amp;#34;) if not os.path.exists(outpath): df = DataFrame({&amp;#39;ref&amp;#39;: ref.</description>
    </item>
  </channel>
</rss>
