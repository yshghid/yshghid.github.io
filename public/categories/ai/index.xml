<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on  </title>
    <link>http://localhost:1313/categories/ai/</link>
    <description>Recent content in AI on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 03 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>skala 강의자료공부 #1 DL-CNN, RNN</title>
      <link>http://localhost:1313/docs/study/ai/ai11/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai11/</guid>
      <description>skala 강의자료공부 #1 DL-CNN, RNN # #2025-08-03&#xA;#1 p.90-92&#xA;Convolution, 즉 합성곱은 CNN의 가장 핵심적인 연산이다. 말 그대로 &amp;lsquo;겹쳐서 곱하고 더하는&amp;rsquo; 방식이다. 이는 우리가 이미지를 처리할 때, 그 이미지의 일부분만을 보며 특징을 추출하는 원리와 매우 유사하다. CNN에서는 이 연산을 통해 이미지 속에서 선, 모서리, 윤곽선 같은 패턴을 뽑아낸다.&#xA;p.90에서는 합성곱을 아주 직관적으로 보여준다. 왼쪽에 있는 초록색 격자는 이미지이고, 그 위에 씌워진 주황색 네모는 필터(또는 커널)다. 이 필터는 보통 3x3 크기를 가지며, 그 내부에 있는 값들은 학습을 통해 결정된다.</description>
    </item>
    <item>
      <title>MutClust 코드 리펙토링 #1 lib.py</title>
      <link>http://localhost:1313/docs/study/algorithm/algo1/</link>
      <pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/algorithm/algo1/</guid>
      <description>MutClust 코드 리펙토링 #1 lib.py # #2025-07-31&#xA;MutClust 알고리즘의 코드 구성은 아래와 같은데&#xA;MutClust ├── sc/ │ └── lib.py // 핵심 알고리즘 로직 │ └── arg_parser.py │ └── utils.py └── Test lib.py는 후보 Core 선택 로직과 클러스터 탐지 알고리즘을 포함한다.&#xA;# 1. Config &amp;amp; Constant 선언 # # === mlib.py === from math import ceil import numpy as np from src.utils import mutation_filtering # --- Constants --- POS = &amp;#39;Position&amp;#39; FREQ = &amp;#39;Frequency&amp;#39; PER = &amp;#39;Percentage&amp;#39; ENT = &amp;#39;Entropy&amp;#39; HSCORE = &amp;#39;H-score&amp;#39; HSCORE_SUM = &amp;#39;H-score_sum&amp;#39; HSCORE_AVR = &amp;#39;H-score_avr&amp;#39; PER_SUM = &amp;#39;per_sum&amp;#39; ENT_SUM = &amp;#39;ent_sum&amp;#39; PER_AVR = &amp;#39;per_avr&amp;#39; ENT_AVR = &amp;#39;ent_avr&amp;#39; EPSILON = 5 EPSILON_SCALING_FACTOR = 10 DIMINISHING_FACTOR = 3 MIN_CLUSTER_LENGTH = 10 CCM_MIN_HSCORE_SUM = 0.</description>
    </item>
    <item>
      <title>DBSCAN #2 슈도코드</title>
      <link>http://localhost:1313/docs/study/ai/ai9/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai9/</guid>
      <description>DBSCAN #2 슈도코드 # #2025-07-28&#xA;1 # Input: - D: 데이터 포인트 집합 - eps: 이웃 거리 임계값 - minPts: 최소 이웃 수 (밀도 기준) Output: - cluster_labels: 각 데이터 포인트에 대한 클러스터 라벨 (노이즈는 -1) Initialize: - cluster_id ← 0 - label[x] ← UNVISITED for all x in D 데이터 집합 D, 파라미터 eps와 minPts가 들어간다.&#xA;2 # For each point x in D: If label[x] ≠ UNVISITED: continue N ← regionQuery(x, eps) // x 주변의 eps 이내 이웃 포인트 탐색 If |N| &amp;lt; minPts: label[x] ← NOISE // Else: // cluster_id ← cluster_id + 1 // expandCluster(x, N, cluster_id, eps, minPts, label) Function regionQuery(x, eps): return { all points y in D such that distance(x, y) ≤ eps } 주석 처리 안된 부분만 보기.</description>
    </item>
    <item>
      <title>DBSCAN #3 MutClust 슈도코드</title>
      <link>http://localhost:1313/docs/study/ai/ai10/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai10/</guid>
      <description>DBSCAN #3 MutClust 슈도코드 # #2025-07-28&#xA;1 # Input: - D: 데이터 포인트 집합 - Efactor: 이웃 거리 조정값 - DiminFactor: 클러스터 경계 조정값 - minPts: 최소 이웃 수 (밀도 기준) Output: - cluster_labels: 각 데이터 포인트에 대한 클러스터 라벨 (노이즈는 -1) Initialize: - cluster_id ← 0 - Label[x] ← UNVISITED for all x in D 데이터 집합 D, 파라미터 eps와 minPts가 들어간다.&#xA;2. H-중요도 계산 # For each point x in D: x.</description>
    </item>
    <item>
      <title>DBSCAN: #1 1D 클러스터링의 성능 평가</title>
      <link>http://localhost:1313/docs/study/ai/ai8/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai8/</guid>
      <description>DBSCAN: #1 1D 클러스터링의 성능 평가 # #2025-07-28&#xA;1. Problem # 클러스터 응집도는 보통 클러스터 내 데이터 간의 평균 거리나 분산, 혹은 실루엣 계수처럼 군집 내 응집도와 군집 간 분리도를 동시에 평가한다.&#xA;하지만 1차원 데이터에서는 클러스터 응집도(Cluster Cohesion) 또는 실루엣 계수(Silhouette coefficient) 같은 지표가 잘 작동하지 않는다.&#xA;2. 클러스터 응집도 # 클러스터링 성능을 평가하는 지표 중 하나인 응집도(Cohesion)는 클러스터 내부의 데이터들이 얼마나 서로 가까운지를 측정하는 지표다. 대표적으로는 클러스터 내 모든 점 간의 평균 거리, 클러스터 중심과 각 점 사이의 평균 거리, 혹은 분산을 사용하는 방식 등이 있다.</description>
    </item>
    <item>
      <title>TFT #0 연구 방향</title>
      <link>http://localhost:1313/docs/study/ai/ai4/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai4/</guid>
      <description>TFT #0 연구 방향 # #2025-07-23&#xA;(#2025-05-31 작성)&#xA;#1&#xA;사용하고자 하는 데이터는?&#xA;feature Clinical feature (17, float): Creatinine, Hemoglobin, LDH, Lymphocytes, Neutrophils, Platelet count, WBC count, hs-CRP, D-Dimer, BDTEMP, BREATH, DBP, SBP, PULSE, SPO2, O2_APPLY Antibiotics feature (2, str) Treatment (list, str): 투여한 항생제, 결측값일수도있고 2개 이상일수도 있음 Strain (str): 환자가 감염된 균주, 1개 NEWS (int): 중증도 Code (int/str): 환자 등록번호 time-series 10개 시점 (항생제 투여 기준 D-3 ~ D+6) TFT input 형식은?</description>
    </item>
    <item>
      <title>TFT #1 입력 시퀀스 생성</title>
      <link>http://localhost:1313/docs/study/ai/ai5/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai5/</guid>
      <description>TFT #1 입력 시퀀스 생성 # #2025-07-23&#xA;1. Load package # %load_ext autoreload %autoreload 2 import sys import pandas as pd import numpy as np import os import pickle import ast sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH/workspace&amp;#39;) 2. Load raw data # #data&#xA;/data ├── PreprocessedData/ │ └── TimecourseData/ │ └── * (*: patient id) │ ├── SeverityScore.csv │ ├── Laboratory_processed.csv │ └── Medication.csv ├── PreprocessedData_knuh/ │ └── (PreprocessedData와 동일) └── 병원체자원은행 균주현황(2014-2024.</description>
    </item>
    <item>
      <title>TFT #2 입력 feature 생성</title>
      <link>http://localhost:1313/docs/study/ai/ai6/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai6/</guid>
      <description>TFT #2 입력 feature 생성 # #2025-07-23&#xA;1. Load package # %load_ext autoreload %autoreload 2 import sys import pandas as pd import numpy as np import os import pickle import ast sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH/workspace&amp;#39;) 2. Make feature1 # #data&#xA;/data └── all_meds.txt /data_knuch └── sequence └── *.pkl (*: antibiotics) /data_knuh └── sequence └── *.pkl (*: antibiotics) medinfo = &amp;#39;/data/all_meds.txt&amp;#39; with open(medinfo, &amp;#39;r&amp;#39;) as f: meds = [line.</description>
    </item>
    <item>
      <title>TFT #3 모델 학습</title>
      <link>http://localhost:1313/docs/study/ai/ai7/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai7/</guid>
      <description>TFT #3 모델 학습 # #2025-07-23&#xA;1. Load package # import pytorch_lightning as pl from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor from pytorch_lightning.loggers import TensorBoardLogger from pytorch_forecasting import TimeSeriesDataSet from pytorch_forecasting.models import TemporalFusionTransformer from pytorch_forecasting.models.baseline import Baseline from pytorch_forecasting.metrics import QuantileLoss from pytorch_forecasting.metrics import MAE from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder import numpy as np import pandas as pd import torch import pickle import matplotlib.pyplot as plt #data&#xA;/data └── Sequence.pkl 2. Load data # sequence = pd.</description>
    </item>
    <item>
      <title>RAG #2 출력 파서의 개념, Pydantic/Json 출력 파서</title>
      <link>http://localhost:1313/docs/study/ai/ai2/</link>
      <pubDate>Sat, 19 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai2/</guid>
      <description>RAG #2 출력 파서의 개념, Pydantic/Json 출력 파서 # #2025-07-19&#xA;1. 출력 파서의 개념과 종류 그리고 세가지 주요 메서드 # 출력 파서(output parser)는 LLM에서 생성된 응답을 받아서 우리가 원하는 형식으로 변환해주는 역할을 한다. 쉽게 말해, LLM은 텍스트만 생성하지만 우리는 그 텍스트를 리스트, 딕셔너리, JSON, 숫자 등 구조화된 데이터로 바꾸어서 프로그램에 넘기거나, 다음 단계 체인으로 활용하길 원할 때가 많다. 출력 파서는 이 연결고리 역할을 한다. 출력 파서는 LLM이라는 기계가 말한 인간 언어를 다시 기계가 이해할 수 있는 언어로 &amp;lsquo;번역&amp;rsquo;하는 통역사 같은 존재이다.</description>
    </item>
    <item>
      <title>RAG #3 자동 대화 이력 관리</title>
      <link>http://localhost:1313/docs/study/ai/ai3/</link>
      <pubDate>Sat, 19 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai3/</guid>
      <description>RAG #3 자동 대화 이력 관리 # #2025-07-19&#xA;1. 자동 대화 이력 관리 # ChatPromptTemplate을 통해 시스템 메시지를 포함하는 프롬프트를 만든다. 시스템 메시지는 모델에게 “너는 금융 상담사야”라고 역할을 부여하는 것이다. 이어지는 (&amp;quot;placeholder&amp;quot;, &amp;quot;{messages}&amp;quot;)는 실제 사용자의 질문과 AI의 답변이 이 자리에 채워질 것이라는 의미다. 이 프롬프트는 chat = ChatOpenAI(model=&amp;quot;gpt-4o-mini&amp;quot;)와 연결되는데, 이는 OpenAI의 gpt-4o-mini 모델을 사용하는 챗 인터페이스이다. 이 프롬프트와 모델을 prompt | chat이라는 LCEL 표현으로 묶으면, 하나의 체인이 만들어진다. 이 체인은 주어진 메시지 목록을 받아, GPT 모델에 전달하고 응답을 생성하는 구조다.</description>
    </item>
    <item>
      <title>RAG #1 랭체인, LCEL, 프롬프트</title>
      <link>http://localhost:1313/docs/study/ai/ai1/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/ai/ai1/</guid>
      <description>RAG #1 랭체인, LCEL, 프롬프트 # #2025-07-17&#xA;1. 랭체인 생태계의 주요 패키지 # 랭체인(LangChain)은 LLM(Large Language Model)을 활용한 애플리케이션을 쉽게 만들 수 있도록 돕는 프레임워크이다. 이 생태계는 단일 라이브러리로 구성된 것이 아니라 여러 개의 하위 패키지로 나뉘어 있고, 각각의 역할이 명확하게 분리되어 있다. 랭체인의 주요 목적은 LLM을 단순한 텍스트 생성 도구가 아니라, 여러 시스템과 결합하여 유의미한 작업을 수행하는 &amp;ldquo;생각하고 행동하는&amp;rdquo; 에이전트로 만드는 것이다. 이 생태계의 핵심 구성 요소들을 쉽게 설명하자면, 마치 LLM이라는 뇌에 주변 감각기관과 기억장치, 도구들, 그리고 의사결정 능력을 붙여주는 것이라고 보면 된다.</description>
    </item>
    <item>
      <title>#2 Explainable AI</title>
      <link>http://localhost:1313/docs/study/etc/etc2/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/etc/etc2/</guid>
      <description>#2 Explainable AI # #2025-06-26&#xA;1. Explainable AI란? # Explainable AI는 인공지능(AI) 또는 머신러닝(ML) 모델이 어떤 방식으로 특정 결과를 도출했는지 사람이 이해할 수 있도록 설명하는 기술과 방법론.&#xA;2. XAI 기법 분류 # 모델 구조&#xA;Intrinsic:&#x9;모델 자체가 설명 가능한 구조 (예: 의사결정나무, 선형회귀 등) Post-hoc:&#x9;모델 학습 후 별도로 설명 생성 (예: SHAP, LIME) 대상 Global:&#x9;전체 모델의 작동 원리를 설명 Local:&#x9;특정 샘플의 예측 결과를 설명 3. 주요 Post-hoc 설명 기법 # LIME (Local Interpretable Model-Agnostic Explanations): 주변 입력을 랜덤하게 생성하고, 단순 모델(선형 회귀 등)을 학습해 근사</description>
    </item>
    <item>
      <title>#3 Random Forest</title>
      <link>http://localhost:1313/docs/study/etc/etc3/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/etc/etc3/</guid>
      <description>#3 Random Forest # #2025-06-26&#xA;1. Random Forest의 분류와 회귀 # 랜덤 포레스트(Random Forest)는&#xA;RandomForestClassifier: 분류용 RandomForestRegressor: 회귀용 이다. 분류와 회귀의 핵심 차이는&#xA;분류는 각 leaf node에 속한 클래스의 비율을 기반으로 확률 예측 회귀는 leaf node에 있는 target 값들의 평균을 예측값으로 사용 랜덤 포레스트의 트리 구조(= 리프 분기 방식)는 분류나 회귀나 똑같고&#xA;단지 리프 노드에 어떤 데이터 형식이 들어가느냐에 따라 분류이면 라벨 비율(확률 분포) 회귀이면 값의 평균으로 예측을 내놓는다 2.</description>
    </item>
  </channel>
</rss>
