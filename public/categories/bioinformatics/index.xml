<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bioinformatics on  </title>
    <link>http://localhost:1313/categories/bioinformatics/</link>
    <description>Recent content in Bioinformatics on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/bioinformatics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RNA-seq 전처리 #1 Quality control (FastQC)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi41/</link>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi41/</guid>
      <description>RNA-seq 전처리 #1 Quality control (FastQC) # #2026-01-25&#xA;DHT 약물 처리된 18개 샘플의 paired-end RNA-seq 데이터를 quality control 수행. FastQC 도구를 사용했고 run_fastqc.sh 스크립트 만들어서 작업.&#xA;#!/bin/bash # input fastq directory FASTQ_DIR=&amp;#34;../2306_tophat/data/Bowtie2Index&amp;#34; # output directory OUT_DIR=&amp;#34;FastQC&amp;#34; # run fastqc mkdir -p &amp;#34;${OUT_DIR}&amp;#34; for fq in \ 5-AZA_150-*_edited.fastq \ 5-AZA_33-*.fastq \ 5-AZA_con-*.fastq do echo &amp;#34;Running FastQC on ${fq}&amp;#34; fastqc &amp;#34;${FASTQ_DIR}/${fq}&amp;#34; -o &amp;#34;${OUT_DIR}&amp;#34; done echo &amp;#34;FastQC completed.&amp;#34; chmod +x run_fastqc.sh ./run_fastqc.sh 품질 레포트 확인 결과 모든 샘플에 공통적으로 read 5’ 말단 1~10bp 구간의 염기 조성이 균일하지 않음.</description>
    </item>
    <item>
      <title>DMR 분석 (methylKit)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi31/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi31/</guid>
      <description>DMR 분석 (methylKit) # #2025-07-28&#xA;# #1 Load packages&#xA;library(&amp;#34;methylKit&amp;#34;) library(&amp;#34;genomation&amp;#34;) library(&amp;#34;GenomicRanges&amp;#34;) # #2 Set path&#xA;setwd(&amp;#34;/data/home/ysh980101/2309_5-aza/Bismark/sorted_n&amp;#34;) getwd() &amp;#39;/data1/home/ysh980101/2309_5-aza/Bismark/sorted_n&amp;#39; # #3 Load data&#xA;# Define the list containing the bismark coverage files. covlist &amp;lt;- list( &amp;#34;KEB1/KEB01_1_bismark_bt2_pe.sorted_n.deduplicated.bismark.cov.gz&amp;#34;, &amp;#34;KEB2/KEB02_1_bismark_bt2_pe.sorted_n.deduplicated.bismark.cov.gz&amp;#34;, &amp;#34;KEB4/KEB04_1_bismark_bt2_pe.sorted_n.deduplicated.bismark.cov.gz&amp;#34;) myobj_lowCov &amp;lt;- methRead(covlist, sample.id=list(&amp;#34;KEB01&amp;#34;,&amp;#34;KEB02&amp;#34;,&amp;#34;KEB04&amp;#34;), pipeline = &amp;#34;bismarkCoverage&amp;#34;, assembly=&amp;#34;hg38&amp;#34;, treatment=c(0,1,2), mincov = 3 ) tiles &amp;lt;- tileMethylCounts(myobj_lowCov,win.size=1000,step.size=1000,cov.bases = 3 tiles.norm &amp;lt;- normalizeCoverage(tiles, method = &amp;#34;median&amp;#34;) meth.tiles &amp;lt;- unite(tiles.norm, destrand=FALSE) meth.tiles meth.tilesDf = getData(meth.</description>
    </item>
    <item>
      <title>Influenza 시퀀스 크롤링 (Selenium)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi28/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi28/</guid>
      <description>Influenza 시퀀스 크롤링 (Selenium) # #2025-07-28&#xA;1. Load package # import pandas as pd import numpy as np import os # 2. Set path # os.chdir(&amp;#39;/Users/yshmbid/Desktop/workspace/gisaid&amp;#39;) os.getcwd() &amp;#39;/Users/yshmbid/Desktop/workspace/gisaid&amp;#39; # 3. Run crawling # # ChromeDriver 경로를 설치하고 Service 객체로 전달 chrome_service = Service(ChromeDriverManager().install()) try: # ChromeDriver 실행 crawler = webdriver.Chrome(service=chrome_service) except: # 크롬드라이버가 없을 때 autoinstaller로 설치 chromedriver_autoinstaller.install(True) crawler = webdriver.Chrome(service=chrome_service) crawler.implicitly_wait(6) # 크롤러 대기 시간 설정 crawler.get(&amp;#39;https://gisaid.org/&amp;#39;) # 웹사이트 열기 # login 선택 engine = WebDriverWait(crawler, 10).</description>
    </item>
    <item>
      <title>MAFFT 작업 #1 Fasta 파일 전처리</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi29/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi29/</guid>
      <description>MAFFT 작업 #1 Fasta 파일 전처리 # #2025-07-28&#xA;1. Load package # import pandas as pd import numpy as np import os import matplotlib.pyplot as plt import random os.sys.path.append(&amp;#34;/data/home/ysh980101/2410/Mutclust2&amp;#34;) from Bin.sc import * # 2. Objective # Influenza type A의 H1N1 strain의 fasta 파일을 확인해보면?&#xA;&amp;gt;로 시작하는 행에 해당 시퀀스의 메타데이터가 있고&#xA;다음 &amp;gt;로 시작하는 행 이전까지 해당 시퀀스 정보가 있다.&#xA;&amp;gt;로 시작하는 행을 |로 분리했을때 제일 마지막값에 유전자 정보가 있다.</description>
    </item>
    <item>
      <title>MAFFT 작업 #2 MAFFT 실행</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi30/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi30/</guid>
      <description>MAFFT 작업 #2 MAFFT 실행 # #2025-07-28&#xA;1. Objective # Influenza의 Reference squence는 길이가 fix되어있지만,&#xA;각 sequence는 삽입/탈락 mutation이 일어남에 따라 모두 길이가 같지 않다. 이 길이를 맞춰주는 padding을 하기 위해 MAFFT를 이용해 정렬(Multiple Sequence Alignment)한다. # 2. MAFFT 실행 bash script # #data&#xA;/Influenza └── Preprocessed/ ├── HA/ │ ├── A-H1N1.fasta │ ├── A-H1N1.fasta │ ├── ... │ └── B.fasta └── ... └── (HA와 동일 구조) └── MAFFT/ └── (empty) #!</description>
    </item>
    <item>
      <title>netMHCpan 작업 #1 환자 시퀀스 생성</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi24/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi24/</guid>
      <description>netMHCpan 작업 #1 환자 시퀀스 생성 # #2025-07-23&#xA;path&#xA;data/ ├── clusters.tsv ├── meta.csv └── codon ├── reference_codon.csv └── *.codon.csv (*: patient id) # #1 Load package&#xA;import pandas as pd import numpy as np import os import sys import re sys.path.append(&amp;#39;/data/home/ysh980101/2409/bin&amp;#39;) from mhc_epitope import * # #2 Load data&#xA;import pandas as pd import os def make_sequence_df(): # 참조 시퀀스 파일 불러오기 및 컬럼 이름 변경 ref_sequence = pd.</description>
    </item>
    <item>
      <title>netMHCpan 작업 #2 HLA-I 펩타이드 추출</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi25/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi25/</guid>
      <description>netMHCpan 작업 #2 HLA-I 펩타이드 추출 # #2025-07-23&#xA;# #1 Patient id 추출&#xA;#data&#xA;data/ ├── c315 │ └── allprot.fasta └── c442 └── allprot.fasta #patients.bash&#xA;#!/bin/bash # FASTA에서 patient ID 추출하여 patient_id.txt로 저장 ALLPROT_PATH=&amp;#34;data/c315/allprot.fasta&amp;#34; OUT_FILE=&amp;#34;data/patient_id.txt&amp;#34; # 스크립트가 있는 디렉터리로 이동 cd &amp;#34;$(dirname &amp;#34;$0&amp;#34;)&amp;#34; # patient_id.txt 파일 초기화 &amp;gt; &amp;#34;$OUT_FILE&amp;#34; # FASTA 파일에서 ID 추출 grep &amp;#34;^&amp;gt;&amp;#34; &amp;#34;$ALLPROT_PATH&amp;#34; | cut -d&amp;#39;|&amp;#39; -f1 | sed &amp;#39;s/^&amp;gt;//&amp;#39; &amp;gt;&amp;gt; &amp;#34;$OUT_FILE&amp;#34; #result&#xA;data/ ├── c315 │ └── allprot.</description>
    </item>
    <item>
      <title>netMHCpan 작업 #3 HLA-peptide affinity 분석</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi26/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi26/</guid>
      <description>netMHCpan 작업 #3 HLA-peptide affinity 분석 # #2025-07-23&#xA;#data&#xA;data/ ├── c315 │ ├── allprot.fasta │ └── * (*: patient id) │ ├── proteome.fasta │ └── peptides_HLA-I.csv ├── c442 │ └── (c315와 동일한 구조로 생성됨) ├── patient_id.txt └── common_mhc.txt # #predict_affinity.bash&#xA;#!/bin/bash # 입력: # 1) 클러스터명 (예: c315) # 2) 병렬 프로세스 수 (NUM_PROC) # 출력: # 환자별 binding_affinities_HLA-I.csv CLUSTER=$1 NUM_PROC=$2 netMHCpan=&amp;#34;../netMHCpan-4.1/netMHCpan&amp;#34; OUT_DIR=&amp;#34;data/${CLUSTER}&amp;#34; PATIENT_TXT=&amp;#34;data/patient_id.txt&amp;#34; HLA_I_ALLELES_FILE=&amp;#34;data/common_mhc.txt&amp;#34; # 스크립트가 있는 디렉터리로 이동 cd &amp;#34;$(dirname &amp;#34;$0&amp;#34;)&amp;#34; # 환자별로 netMHCpan 예측 수행 while read -r PATIENT_ID; do PATIENT_DIR=&amp;#34;$OUT_DIR/$PATIENT_ID&amp;#34; RAW_DIR=&amp;#34;$PATIENT_DIR/raw_predictions&amp;#34; mkdir -p &amp;#34;$RAW_DIR&amp;#34; PEPTIDES_TABLE=&amp;#34;$PATIENT_DIR/peptides_HLA-I.</description>
    </item>
    <item>
      <title>netMHCpan 작업 #4 결과 확인 및 heatmap 시각화</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi27/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi27/</guid>
      <description>netMHCpan 작업 #4 결과 확인 및 heatmap 시각화 # #2025-07-23&#xA;# #1 netMHCpan 결과 확인&#xA;#data&#xA;data/ ├── c315 │ └── * (*: patient id) │ ├── peptides_HLA-I.csv │ └── binding_affinities_HLA-I.csv ├── c442 │ └── (c315와 동일한 구조로 생성됨) └── patient_id.txt result/ └── (empty) # Load package import pandas as pd import numpy as np import os # Load patient id f = open(&amp;#34;/data/patient_id.txt&amp;#34;, &amp;#34;r&amp;#34;) patients = f.read().split(&amp;#34;\n&amp;#34;) # Merge epitope table hotspots = [&amp;#34;c315&amp;#34;, &amp;#34;c442&amp;#34;] peptide_df_list = [] for hotspot in hotspots: for patient in patients: peptide_df = pd.</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #6 HLA 결합력 변화 비교</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi39/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi39/</guid>
      <description>변이 클러스터링 연구 #6 HLA 결합력 변화 비교 # #2025-06-27&#xA;1. Load package # import pandas as pd import numpy as np 2. Load affinity data # with open(&amp;#39;/data/home/ysh980101/2411/data-mhc/patient_id.txt&amp;#39;, &amp;#39;r&amp;#39;) as file: patients = [line.strip() for line in file] len(patients) 388 #387+reference 3. Merge affinity tables # hotspot = &amp;#34;c315&amp;#34; dfs = [] for pid in patients: file_path = f&amp;#34;/data/home/ysh980101/2411/data-mhc/{hotspot}/{pid}/binding_affinities_HLA-I.csv&amp;#34; df = pd.read_csv(file_path) df.rename(columns={&amp;#39;Affinity&amp;#39;: f&amp;#39;{pid}&amp;#39;}, inplace=True) df.rename(columns={&amp;#39;Peptide&amp;#39;: f&amp;#39;Peptide_{pid}&amp;#39;}, inplace=True) if pid == &amp;#39;reference&amp;#39;: dfs.</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #4 알고리즘 성능 평가 - k dist plot</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi37/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi37/</guid>
      <description>변이 클러스터링 연구 #4 알고리즘 성능 평가 - k dist plot # #2025-06-24&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) from Bin.sc import * os.chdir(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) 2. Load data # indir = &amp;#39;result/&amp;#39; resdir = &amp;#39;result/GISAID_test1/&amp;#39; with open(f&amp;#34;{indir}GISAID_total.pickle&amp;#34;, &amp;#34;rb&amp;#34;) as f: Input_df = pickle.load(f) hotspots = pd.</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #5 결과 검증: 계통 결정 돌연변이와 연관성</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi38/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi38/</guid>
      <description>변이 클러스터링 연구 #5 결과 검증: 계통 결정 돌연변이와 연관성 # #2025-06-24&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * os.sys.path.append(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) from Bin.sc import * os.chdir(&amp;#34;/data/home/ysh980101/2506/mutclust&amp;#34;) 2. Load data # lineage_info_dir = &amp;#39;/data/home/ysh980101/2411/data/mutation_info&amp;#39; covid_annotation = &amp;#34;/data/home/ysh980101/2404/Data/covid_annotation.tsv&amp;#34; sig_hotspots = &amp;#34;result/sig_hotspots.csv&amp;#34; lineage_info = make_lineage_info(lineage_info_dir) hotspot_lineage = make_hotspot_lineage(lineage_info, sig_hotspots_path, covid_annotation) hotspot_lineage plot_hotspot_lineage(hotspot_lineage) outdir = &amp;#34;result/&amp;#34; hotspot_lineage.</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #1 알고리즘 실행</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi34/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi34/</guid>
      <description>변이 클러스터링 연구 #1 알고리즘 실행 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Find CCMs # i = 1 tag = f&amp;#34;test{i}&amp;#34; input_path = &amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.pickle&amp;#34; outdir = f&amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/&amp;#34; Path(outdir).mkdir(parents=True, exist_ok=True) info = set_env(input = input_path, output = outdir) Input_df = readPickle(input_path) init(Input_df, info) mutInfo, ccms = get_candidate_core_mutations(Input_df, info, tag, i) --- Configurations --- Input data: &amp;#39;/data/home/ysh980101/2407/Mutclust/Testdata/Input/GISAID_total.</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #2 변이 중요도 계산</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi35/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi35/</guid>
      <description>변이 클러스터링 연구 #2 변이 중요도 계산 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load GISAID data # indir = &amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Input/&amp;#34; Refseq = getNucleotideRefSeq() GISAID_Freq = pd.read_csv(f&amp;#39;{indir}gisaid_freq_all.csv&amp;#39;, index_col=0) GISAID_meta = get_GISAID_meta() print(GISAID_Freq) A C G T R Y S W K M B D H V N 1 10612 390 415 785 11 1 3 4 24 2 1 2 0 0 219995 2 287 502 218 12942 3 31 14 4 61 0 1 2 1 0 218179 3 166 461 348 18168 1 12 29 10 15 1 0 1 1 0 213032 4 19398 267 502 972 12 5 1 33 37 6 1 1 0 1 211009 5 24962 281 334 699 6 21 6 17 15 10 5 1 1 1 205886 .</description>
    </item>
    <item>
      <title>변이 클러스터링 연구 #3 결과 검증: 임상 결과와의 연관성</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi36/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi36/</guid>
      <description>변이 클러스터링 연구 #3 결과 검증: 임상 결과와의 연관성 # #2025-06-20&#xA;1. Load package # import pandas as pd import numpy as np import os os.sys.path.append(&amp;#34;/data/home/ysh980101/2407/Mutclust&amp;#34;) from pathlib import Path from Bin.Utils.utils import * from Bin.arg_parser import * from Bin.mlib import * 2. Load COVID19 data # i = 1 tag = f&amp;#34;test{i}&amp;#34; resdir = f&amp;#34;/data/home/ysh980101/2407/Mutclust/Testdata/Output/GISAID_{tag}/&amp;#34; covid19_dir = &amp;#34;/data3/projects/2020_MUTCLUST/Data/Projects/COVID19/Sequence/Preprocessed/Nucleotide/Mutationinfo&amp;#34; meta_path = &amp;#34;/data/home/ysh980101/2506/data/meta.csv&amp;#34; hotspots = pd.read_csv(f&amp;#34;{resdir}clusters_{tag}.txt&amp;#34;,sep=&amp;#34;\t&amp;#34;) metaData = pd.read_csv(meta_path, index_col=0) mutInfo = make_mutInfo_covid19(covid19_dir) mutSignature = make_mutSignature(mutInfo, hotspots, metaData) print(mutSignature) COV-CCO-001 COV-CCO-002 COV-CCO-003 COV-CCO-004 COV-CCO-006 \ c0 0 0 0 0 0 c1 0 0 0 0 0 c2 0 0 0 0 0 c3 0 0 0 0 0 c4 0 0 0 0 0 .</description>
    </item>
    <item>
      <title>항생제 TFT 연구 #1 입력 데이터 생성</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi32/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi32/</guid>
      <description>항생제 TFT 연구 #1 입력 데이터 생성 # #2025-06-17&#xA;Load package # %load_ext autoreload %autoreload 2 import sys import os sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH&amp;#39;) Check data # raw_path = &amp;#39;/data3/projects/2025_Antibiotics/YSH/res/sev_dict_filtered.pkl&amp;#39; with open(raw_path, &amp;#39;rb&amp;#39;) as f: raw_data = pickle.load(f) keys = list(raw_data.keys()) print(len(keys)) print(keys[0], &amp;#39;\n&amp;#39;, raw_data[keys[0]]) 4515 74374 Date NEWS med_cnt med_list \ 0 2020-10-30 4 2 Trizele;Cefotaxime 1 2020-10-31 4 2 Trizele;Cefotaxime 2 2020-11-01 12 2 Pospenem;Pospenem_2 3 2020-11-02 9 3 Pospenem;Meropen;Vanco Kit 4 2020-11-03 12 2 Vanco Kit;Meropen 5 2020-11-04 8 2 Vanco Kit;Meropen 6 2020-11-05 9 0 strain 0 [] 1 [] 2 [] 3 [Enterobacter cloacae ssp cloacae] 4 [Enterobacter cloacae ssp cloacae] 5 [Enterobacter cloacae ssp cloacae] 6 [Enterobacter cloacae ssp cloacae] 4515명 환자 데이터이고</description>
    </item>
    <item>
      <title>항생제 TFT 연구 #2 입력 feature 생성</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi33/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi33/</guid>
      <description>항생제 TFT 연구 #2 입력 feature 생성 # #2025-06-17&#xA;1. Load package # %load_ext autoreload %autoreload 2 import sys import os import pickle import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages import pandas as pd sys.path.append(&amp;#39;/data3/projects/2025_Antibiotics/YSH/bin&amp;#39;) from sc import * os.chdir(&amp;#39;/data3/projects/2025_Antibiotics/YSH&amp;#39;) 2. Previous # seqdir = &amp;#39;data/res_dict&amp;#39; seq_list = os.listdir(seqdir) print(len(seq_list)) 169 항생제 169종에 대해서 size 10 sequence를 생성했었는데&#xA;모델 입력 feature로 다음을 제외하는대신 antibiotics 리스트 strain 리스트 저 2개 feature를 반영하는 새로운 feature를 2개 생성하려고 한다: 현재 antibiotics가 현재 strain 환자의 NEWS를 감소시킨 이력이 있는지?</description>
    </item>
    <item>
      <title>연구실 bashrc 스크립트</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi40/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi40/</guid>
      <description>연구실 bashrc 스크립트 # #2025-05-28&#xA;#1 local&#xA;1 #alias cobi2=&amp;#39;ssh -p 5290 ysh980101@155.230.28.211&amp;#39; 2 alias cobi2=&amp;#34;ssh -p 3160 ysh980101@155.230.110.91&amp;#34; 3 alias cobi3=&amp;#34;ssh -p 7777 ysh980101@155.230.110.92&amp;#34; 4 alias cobi4=&amp;#34;ssh -p 4712 ysh980101@155.230.110.93&amp;#34; 5 # &amp;gt;&amp;gt;&amp;gt; conda initialize &amp;gt;&amp;gt;&amp;gt; 6 # !! Contents within this block are managed by &amp;#39;conda init&amp;#39; !! 7 __conda_setup=&amp;#34;$(&amp;#39;/opt/anaconda3/bin/conda&amp;#39; &amp;#39;shell.bash&amp;#39; &amp;#39;hook&amp;#39; 2&amp;gt; /dev/null )&amp;#34; 8 if [ $? -eq 0 ]; then 9 eval &amp;#34;$__conda_setup&amp;#34; 10 else 11 if [ -f &amp;#34;/opt/anaconda3/etc/profile.</description>
    </item>
    <item>
      <title>Enrichment 분석 및 시각화 (gProfiler/ggplot2)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi3/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi3/</guid>
      <description>Enrichment 분석 및 시각화 (gProfiler/ggplot2) # #2025-04-21&#xA;# #1 Load Package&#xA;library(ggplot2) # #2 Set Path&#xA;setwd(&amp;#34;/data-blog/bi3&amp;#34;) getwd() &amp;#39;/data-blog/bi3&amp;#39; # #3 Functional Enrichment Bubble Plot&#xA;condition &amp;lt;- &amp;#39;150_con&amp;#39; gpsource &amp;lt;- &amp;#39;GO:BP&amp;#39; #gpsource &amp;lt;- &amp;#39;REAC&amp;#39; df_c1 &amp;lt;- read.csv(paste0(&amp;#34;./sleuth_ward/gprofiler/gProfiler_&amp;#34;,condition,&amp;#34;_termsize.csv&amp;#34;)) df_c2 &amp;lt;- read.csv(paste0(&amp;#34;gProfiler_&amp;#34;,condition,&amp;#34;_c2_padj0.1.csv&amp;#34;)) df_c1 &amp;lt;- df_c1[df_c1$source == gpsource, ] df_c2 &amp;lt;- df_c2[df_c2$source == gpsource, ] df_c1$reg_type &amp;lt;- &amp;#39;down&amp;#39; df_c2$reg_type &amp;lt;- &amp;#39;up&amp;#39; df_c1$nlog &amp;lt;- -abs(df_c1$negative_log10_of_adjusted_p_value) df_c2$nlog &amp;lt;- abs(df_c2$negative_log10_of_adjusted_p_value) df_c1 &amp;lt;- df_c1[order(df_c1$negative_log10_of_adjusted_p_value), ] df_c2 &amp;lt;- df_c2[order(-df_c2$negative_log10_of_adjusted_p_value), ] df &amp;lt;- rbind(df_c1, df_c2) ggplot(df, aes(x = reorder(term_name, nlog), y = negative_log10_of_adjusted_p_value, size = intersection_size, color = nlog)) + geom_point(alpha = 0.</description>
    </item>
    <item>
      <title>Kallisto Pseudoalignment 작업</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi4/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi4/</guid>
      <description>Kallisto Pseudoalignment 작업 # #2025-04-21&#xA;1. Build Index # $ kallisto index -i transcripts_cDNA.idx Homo_sapiens.GRCh38.cdna.all.fa.gz # 2. Pseudoalign # $ kallisto quant -i transcripts_cDNA.idx -o output_150-1 -t 40 ../2306_tophat/data/Bowtie2Index/5-AZA_150-1_1_edited.fastq ../2306_tophat/data/Bowtie2Index/5-AZA_150-1_2_edited.fastq 3개 파일 생성 abundance.h5 - HDF5 binary file containing run info, abundance esimates, bootstrap estimates, and transcript length information length. This file can be read in by sleuth abundance.tsv - plaintext file of the abundance estimates. It does not contains bootstrap estimates.</description>
    </item>
    <item>
      <title>RNA-seq 전처리 (TopHat, SAMtools, HTSeq)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi7/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi7/</guid>
      <description>RNA-seq 전처리 (TopHat, SAMtools, HTSeq) # #2025-04-21&#xA;# #1 TopHat 실행&#xA;$ tophatpy -o tophat_out_33-1 --no-mixed -p 40 \ $ /data3/PUBLIC_DATA/ref_genomes/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.dna.toplevel \ $ /data/home/ysh980101/2306_tophat/data/Bowtie2Index/5-AZA_33-1_1.fastq \ $ /data/home/ysh980101/2306_tophat/data/Bowtie2Index/5-AZA_33-1_2.fastq tophatpy: tophat2 안먹어서 커스텀한 명령어 (정식 명령어는 tophat2) -o tophat_out_33-1: 출력 디렉토리 설정 --no-mixed: 페어 중 하나만 매핑되면 제외 -p 40: 멀티스레딩, 40개 스레드 사용 /data3/PUBLIC_DATA/...dna.toplevel: reference genome FASTA (Bowtie2 인덱스가 이와 동일한 경로로 있어야 함) 2개의 paired-end read 입력 cf) tophat alias 확인</description>
    </item>
    <item>
      <title>Sleuth 작업</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi2/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi2/</guid>
      <description>Sleuth 작업 # #2025-04-21&#xA;1. Load Package, Run Sleuth # require(&amp;#34;sleuth&amp;#34;) packageVersion(&amp;#34;sleuth&amp;#34;) library(&amp;#34;gridExtra&amp;#34;) library(&amp;#34;cowplot&amp;#34;) library(&amp;#34;biomaRt&amp;#34;) library(readr) setwd(&amp;#34;/data/home/ysh980101/2307_kallisto&amp;#34;) getwd() sample_id &amp;lt;- dir(file.path(&amp;#34;./&amp;#34;)) sample_id &amp;lt;- grep(&amp;#34;^output_(150|con)&amp;#34;, sample_id, value = TRUE) sample_id &amp;lt;- substring(sample_id, 8) sample_id kal_dirs &amp;lt;- file.path(paste0(&amp;#34;./output_&amp;#34;, sample_id)) s2c &amp;lt;- read.table(file.path(&amp;#34;./kallisto_demo_150_con.tsv&amp;#34;), header = TRUE, stringsAsFactors = FALSE, sep = &amp;#34;\t&amp;#34;) s2c &amp;lt;- dplyr::mutate(s2c, path = kal_dirs) s2c marts &amp;lt;- listMarts() ensembl &amp;lt;- useMart(&amp;#34;ensembl&amp;#34;) datasets &amp;lt;- listDatasets(ensembl) filtered_datasets &amp;lt;- datasets[grepl(&amp;#34;hsapiens&amp;#34;, datasets$dataset), ] hsapiens_mart &amp;lt;- useMart(&amp;#34;ensembl&amp;#34;,dataset=&amp;#34;hsapiens_gene_ensembl&amp;#34;) datasets &amp;lt;- listDatasets(hsapiens_mart) filtered_datasets &amp;lt;- datasets[grepl(&amp;#34;hsapiens&amp;#34;, datasets$dataset), ] hsapiens_mart &amp;lt;- useMart(&amp;#34;ensembl&amp;#34;,dataset=&amp;#34;hsapiens_gene_ensembl&amp;#34;,host=&amp;#34;ensembl.</description>
    </item>
    <item>
      <title>DE 분석 (DESeq2)</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi1/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi1/</guid>
      <description>DE 분석 (DESeq2) # #2024-12-31&#xA;Tool&#xA;Bioconductor - DESeq2 https://bioconductor.org/packages/release/bioc/html/DESeq2.html&#xA;# 1. Load package # suppressMessages({ library(&amp;#34;DESeq2&amp;#34;) library(pheatmap) library(withr) #library(tidyverse) library(RColorBrewer) library(gplots) library(dplyr) }) # 2. Set path # setwd(&amp;#34;/data-blog/bi1&amp;#34;) getwd() &amp;#39;/data-blog/bi1&amp;#39; # 3. Run DESeq2 # S1 &amp;lt;- &amp;#39;33&amp;#39; S2 &amp;lt;- &amp;#39;150&amp;#39; countdata &amp;lt;- read.csv(&amp;#34;results.csv&amp;#34;, header=TRUE, sep=&amp;#39;,&amp;#39;) colnames(countdata) &amp;lt;- c(&amp;#39;GeneID&amp;#39;,&amp;#39;150-1&amp;#39;,&amp;#39;150-2&amp;#39;,&amp;#39;150-3&amp;#39;,&amp;#39;33-1&amp;#39;,&amp;#39;33-2&amp;#39;,&amp;#39;33-3&amp;#39;,&amp;#39;con-1&amp;#39;,&amp;#39;con-2&amp;#39;,&amp;#39;con-3&amp;#39;) countdata &amp;lt;- countdata[, c(&amp;#39;GeneID&amp;#39;,&amp;#39;150-1&amp;#39;,&amp;#39;150-2&amp;#39;,&amp;#39;150-3&amp;#39;,&amp;#39;33-1&amp;#39;,&amp;#39;33-2&amp;#39;,&amp;#39;33-3&amp;#39;,&amp;#39;con-1&amp;#39;,&amp;#39;con-2&amp;#39;,&amp;#39;con-3&amp;#39;)] selected_columns &amp;lt;- paste(c(&amp;#39;GeneID&amp;#39;,paste0(S2,&amp;#34;-1&amp;#34;), paste0(S2,&amp;#34;-2&amp;#34;), paste0(S2,&amp;#34;-3&amp;#34;),paste0(S1,&amp;#34;-1&amp;#34;), paste0(S1,&amp;#34;-2&amp;#34;), paste0(S1,&amp;#34;-3&amp;#34;)), sep=&amp;#34;&amp;#34;) countdata &amp;lt;- countdata[, selected_columns] countdata &amp;lt;- countdata[rowSums(countdata[, -1]) !</description>
    </item>
    <item>
      <title>RNA-seq 전처리: EBV genome</title>
      <link>http://localhost:1313/docs/study/bioinformatics/bi11/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/bioinformatics/bi11/</guid>
      <description>RNA-seq 전처리: EBV genome # #2024-12-31&#xA;0 # 분석 목적&#xA;제공받은 fastq를 human genome에 매핑해서 전처리, 분석 후 DE 결과 보냄 DE 분석시에 EBV 유전자도 포함해달라는 요청 해야하는것&#xA;fastq를 EBV genome에 매핑해서 전처리, EBV count 생성 human count에 EBV count를 붙이기 통합 count로 DE 분석 재수행 # 1. Alignment # Load package, Set Path&#xA;library(edgeR) library(Rsubread) library(org.Hs.eg.db) setwd(&amp;#34;/data/home/ysh980101/2311/RNA-seq_ebv/Rsubread&amp;#34;) getwd() &amp;#39;/data1/home/ysh980101/2311/RNA-seq_ebv/Rsubread&amp;#39; Build Index&#xA;# build index ref &amp;lt;- &amp;#34;/data3/PUBLIC_DATA/ref_genomes/Human_gammaherpesvirus_4_EBV/NC_007605.1.fa&amp;#34; output_basename &amp;lt;- &amp;#34;NC_007605.</description>
    </item>
  </channel>
</rss>
