<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BE on  </title>
    <link>http://localhost:1313/categories/be/</link>
    <description>Recent content in BE on  </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/be/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Langchain #2 LangGraph 기반 Multi-Agent &#43; Agentic RAG 시스템</title>
      <link>http://localhost:1313/docs/study/be/be9/</link>
      <pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be9/</guid>
      <description>Langchain #2 LangGraph 기반 Multi-Agent + Agentic RAG 시스템 # #2025-10-13&#xA;1. 실습 개요 # 목적&#xA;AI 헬스케어 스타트업의 투자 가치를 평가하기 위해 입력된 스타트업 정보에서 &amp;lsquo;경쟁사 유무를 자동 판별&amp;rsquo;하고, 판별 결과에 따라 워크플로우를 동적으로 분기하여 &amp;lsquo;Multi-Agent 시스템(10개 전문 에이전트)&amp;lsquo;이 각자의 역할(정보 수집, 기술력 분석, 시장성 평가, 경쟁사 비교)을 순차적으로 수행하며, 외부 문서(시장 보고서, 기술 리뷰, 규제 정보)를 &amp;lsquo;RAG 시스템(FAISS + OpenAI Embeddings)&amp;lsquo;으로 검색하여 LLM 분석에 참조 컨텍스트를 제공하고, &amp;lsquo;Scorecard Method 가중치 평가 방식&amp;rsquo;으로 6개 항목(창업자/팀, 시장성, 제품/기술력, 경쟁 우위, 실적, 투자조건)을 정량화하여 10점 만점 투자 점수를 산출한 뒤, 전체 프로세스를 &amp;lsquo;LangGraph 기반 상태 관리 워크플로우&amp;rsquo;로 자동화하고, 최종적으로 분석 결과를 Executive Summary, 기술력/시장성 평가, 경쟁 분석, 투자 판단을 포함한 전문적인 &amp;lsquo;Word/PDF 형식의 투자 평가 보고서&amp;rsquo;로 생성 실습 설계</description>
    </item>
    <item>
      <title>Langchain #2 RAG 기반 LLM API 서버 구축</title>
      <link>http://localhost:1313/docs/study/be/be29/</link>
      <pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be29/</guid>
      <description>Langchain #2 RAG 기반 LLM API 서버 구축 # #2025-09-23&#xA;1. 실습1 - LLM 질문-응답 Agent 구현 # #1 작업 위치 설정&#xA;# 1. 작업 위치 $ pwd /Users/yshmbid/Documents/home/github/MLops/template/#10.code # 2. 파일 확인 $ ls __pycache__ practice_LLM_App_main.py practice_LLM_App_front.vue # #2 백엔드 띄우기&#xA;# 3. 백엔드 띄우기 $ uvicorn practice_LLM_App_main:app --port 8005 --reload INFO: Will watch for changes in these directories: [&amp;#39;/Users/yshmbid/Documents/home/github/MLops/template/#10.code&amp;#39;] INFO: Uvicorn running on http://127.0.0.1:8005 (Press CTRL+C to quit) INFO: Started reloader process [7018] using StatReload 🖥 CPU 환경에서 로드합니다.</description>
    </item>
    <item>
      <title>FastAPI #1 MariaDB, DB Migration, Swagger UI</title>
      <link>http://localhost:1313/docs/study/be/be24/</link>
      <pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be24/</guid>
      <description>FastAPI #1 MariaDB, DB Migration, Swagger UI # #2025-09-17&#xA;1. 실습 내용 # #1 maria db container 띄우기&#xA;# 1. conda 가상환경 생성 $ conda create -n demo-app python=3.11 $ conda activate demo-app # 2. 작업 위치 # mariadb_tmplt 디렉토리를 다운받고 압축 해제함 $ pwd /Users/yshmbid/Documents/home/github/MLops/mariadb_tmplt $ ls conf.d data env maria_db.yaml # 3. Docker Compose로 MariaDB 실행 $ docker compose -p maria_db -f maria_db.yaml up -d # 4. 컨테이너가 잘떴는지확인 $ docker ps | grep mariadb ae333f330cc4 mariadb:10.</description>
    </item>
    <item>
      <title>FastAPI #2 논문 업로드 및 벡터화 API</title>
      <link>http://localhost:1313/docs/study/be/be26/</link>
      <pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be26/</guid>
      <description>FastAPI #2 논문 업로드 및 벡터화 API # #2025-09-17&#xA;1. 실행 # $ pwd /Users/yshmbid/Documents/home/github/MLops $ ls mariadb_tmplt pjt-main.py skala-fastapi-rpt.zip mariadb_tmplt.zip skala-fastapi-rpt template.zip $ uvicorn pjt-main:app --host 127.0.0.1 --port 8002 --reload INFO: Will watch for changes in these directories: [&amp;#39;/Users/yshmbid/Documents/home/github/MLops&amp;#39;] INFO: Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit) INFO: Started reloader process [75232] using WatchFiles INFO: Started server process [75234] INFO: Waiting for application startup. INFO: Application startup complete.</description>
    </item>
    <item>
      <title>FastAPI #3 비동기 데이터베이스</title>
      <link>http://localhost:1313/docs/study/be/be25/</link>
      <pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be25/</guid>
      <description>FastAPI #3 비동기 데이터베이스 # #2025-09-17&#xA;#1 main.py&#xA;#main.py # FastAPI 엔드포인트 정의 이해 # FastAPI는 아래 두 가지 방식 중 하나로 엔드포인트를 정의 # ① 직접 app에 정의 # ② 모듈화한 라우터 파일을 include from fastapi import FastAPI from api.routers import task_a from api.routers import done_a from fastapi.staticfiles import StaticFiles from fastapi.responses import FileResponse from fastapi.openapi.docs import get_swagger_ui_html import os main.py fastapi app 서버를 구성. fastapi 프레임워크 웹 요청이 들어오면 특정 함수로 연결해준다.</description>
    </item>
    <item>
      <title>Ray #1 Batch Prediction with Ray Core</title>
      <link>http://localhost:1313/docs/study/be/be6/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be6/</guid>
      <description>Ray #1 Batch Prediction with Ray Core # #2025-09-15&#xA;스터디때 준비해갔던 Ray Core를 사용해서 batch prediction 수행하는 예제!!&#xA;batch prediction이 batch를 예측하는건줄알았는데(..) batch로 prediction하는것이었다. 순서는 1. Task 기반 batch prediction 2. Actor 기반 batch prediction 3. GPU 기반 수행 코드 출처는 Ray Document의 Batch Prediction with Ray Core이다. # 0. 개요 # 목적 Parquet 형식의 대규모 데이터셋을 Ray를 이용해 분산 처리하며, 더미 모델을 로딩하여 배치 예측(batch prediction) 을 수행한다. Task와 Actor 두 가지 실행 방식을 비교하고, CPU/GPU 자원 활용 차이를 이해한다.</description>
    </item>
    <item>
      <title>Langchain #1 노션 데이터로 나만의 RAG 시스템 구축하기 (스터디)</title>
      <link>http://localhost:1313/docs/study/be/be5/</link>
      <pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be5/</guid>
      <description>Langchain #1 노션 데이터로 나만의 RAG 시스템 구축하기 (스터디) # #2025-09-10&#xA;스터디하는친구가 만들어준코드인데 내 노션으로 돌려봤다!&#xA;실습 목적&#xA;노션 데이터를 임베딩 생성하여 FAISS 벡터 스토어에 저장하고 이를 기반으로 유사 문서 검색을 수행하며, 청킹 기법을 통해 데이터 구조를 이해하고 LLM 프롬프트 제약을 적용한 뒤, RAG 구조를 접목해 자동 답변 구현 실습 설계&#xA;임베딩 생성: SentenceTransformer(&amp;ldquo;BAAI/bge-m3&amp;rdquo;) 유사 문서 검색: 코사인 유사도 + FAISS 벡터 스토어 기반 최근접 탐색 청킹 기법: Markdown 단위 분리 + 길이 기반 추가 분할 LLM 프롬프트 제약: 근거 기반 답변(추측 금지 규칙 포함) 자동 답변 구현: RAG 구조 + &amp;ldquo;meta-llama/llama-3.</description>
    </item>
    <item>
      <title>Kubernetes #2 ConfigMap, PVC, Liveness/Readiness, Blue/Green</title>
      <link>http://localhost:1313/docs/study/be/be33/</link>
      <pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be33/</guid>
      <description>Kubernetes #2 ConfigMap, PVC, Liveness/Readiness, Blue/Green # #2025-09-09&#xA;1. kubectl 명령어 실습 # #1 배포된 컨테이너를 쿠버네티스에서 확인하기&#xA;# 배포 상태 확인 $ kubectl get pod -n skala-practice | grep sk019 sk019-myfirst-api-server-57fddcd6c8-l4jms 1/1 Running 0 108m # 서비스 확인 $ kubectl get svc -n skala-practice | grep sk019 sk019-myfirst-api-server ClusterIP 10.100.83.86 &amp;lt;none&amp;gt; 8080/TCP,8081/TCP 18h # #2 로컬 &amp;lt;-&amp;gt; Pod 간 파일/디렉토리 복사&#xA;# 수행 위치 $ pwd /Users/yshmbid/Documents/home/github/Cloud/workspace/kubernetes/02.deploy # Pod 이름 확인 $ kubectl get pod -n skala-practice | grep sk019 sk019-myfirst-api-server-57fddcd6c8-l4jms 1/1 Running 0 120m # 로컬의 data 디렉토리를 Pod 내부 /app/data 로 복사 $ kubectl cp $(pwd)/data skala-practice/sk019-myfirst-api-server-57fddcd6c8-l4jms:/app/data # Pod /app/data → 로컬 .</description>
    </item>
    <item>
      <title>Kubernetes #1 Pod, Port-forward</title>
      <link>http://localhost:1313/docs/study/be/be32/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be32/</guid>
      <description>Kubernetes #1 Pod, Port-forward # #2025-09-08&#xA;1. 실습환경설정 # 필요 패키지 kubectl, jq, curl, maven, Java brew install kubectl jq curl maven kubectl&#xA;Kubernetes 클러스터와 통신하는 CLI 도구 쿠버네티스는 여러 개의 프로그램이 동시에 돌아가는 큰 시스템이고 여기에 지시를 내리는 도구. Java 17&#xA;여러 프로그램을 실행하는 공통 실행 환경(JVM)을 제공 공통 실행 환경? 여러 프로그램을 공통 언어로 사용하게해준다. 프로그램들이 Java가 어디 있는지 알아야 하니까 JAVA_HOME이라는 환경 변수를 설정해준다. export JAVA_HOME=/opt/homebrew/opt/openjdk@17 # 클라우드 인증 정보, 커맨드 스크립트 다운로드 # 클라우드 인증 정보 wsl-install.</description>
    </item>
    <item>
      <title>DBMS 및 SQL 활용 #5 Vector DB 스키마 설계</title>
      <link>http://localhost:1313/docs/study/be/be34/</link>
      <pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be34/</guid>
      <description>DBMS 및 SQL 활용 #5 Vector DB 스키마 설계 # #2025-09-02&#xA;1. 개념 # #1 KNN vs ANN&#xA;KNN과 ANN의 공통 목적&#xA;질문을 하고 그 질문과 비슷한 질문이나 답변을 데이터베이스에서 찾기 구현 차이&#xA;모든 데이터를 하나하나 다 비교해서 가장 가까운 것을 찾는다(KNN) 데이터 전체를 다 비교하지 않고 인덱스를 이용해서 후보군을좁혀서 그 안에서만 비교(ANN) 친구가 수십만 명 있으면 모든 친구에게 질문을 던져서 과거 답변을 확인하는 대신 비슷한 취향을 가진 대표 그룹 몇 개를 빠르게 찾고 그 안에서만 가장 가까운 답을 고르는 방식.</description>
    </item>
    <item>
      <title>DBMS 및 SQL 활용 #3 집계함수, 고급 객체기능, 고급 인덱스</title>
      <link>http://localhost:1313/docs/study/be/be37/</link>
      <pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be37/</guid>
      <description>DBMS 및 SQL 활용 #3 집계함수, 고급 객체기능, 고급 인덱스 # #2025-08-28&#xA;1. GROUPBY # GROUP BY&#xA;테이블 안에 있는 데이터를 특정 기준으로 묶어서 요약.&#xA;테이블 embedding_store에서&#xA;id, user_id, cluster_id, similarity, tag 5개 컬럼이 있는데 있는 그대로보면 큰 그림을 보기 힘들다 즉 해석이 어렵다. GROUP BY를 쓰면 요약 정보를 만들수있는데 user_id로 묶으면 “사용자 A는 총 10건, 사용자 B는 총 5건” 같은 식으로 정리 / cluster_id로 묶으면 “클러스터 1은 평균 유사도가 0.</description>
    </item>
    <item>
      <title>DBMS 및 SQL 활용 #4 pgvector 기반 유사도 검색 &#43; FastAPI 연동</title>
      <link>http://localhost:1313/docs/study/be/be38/</link>
      <pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be38/</guid>
      <description>DBMS 및 SQL 활용 #4 pgvector 기반 유사도 검색 + FastAPI 연동 # #2025-08-28&#xA;1. 실습 시나리오 # -- 1. 확장 설치 및 테이블 생성 -- 2. 예시 데이터 삽입 (10건만 임시) -- 3. 인덱스 생성 및 분석 -- 4. 성능 비교: LIMIT 5 vs LIMIT 50 -- 5. 인덱스 종류별 비교 (코사인 vs L2) -- 6. 사용자 입력 벡터를 Python에서 API로 전달하여 동적 쿼리 구성 예시 (FastAPI 측에서 처리) # 2.</description>
    </item>
    <item>
      <title>DBMS 및 SQL 활용 #1 설계안 데이터 적재 (postgresql, pgvector)</title>
      <link>http://localhost:1313/docs/study/be/be35/</link>
      <pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be35/</guid>
      <description>DBMS 및 SQL 활용 #1 설계안 데이터 적재 (postgresql, pgvector) # #2025-08-27&#xA;1. 실습1 # 실습 시나리오&#xA;사용자가 설계안 텍스트(예: description)를 입력 해당 텍스트에 대해 Python에서 AI 임베딩을 수행 임베딩 결과가 유효할 경우 design 테이블에 등록 (COMMIT) 실패하면 아무 데이터도 등록하지 않음 (ROLLBACK) PostgreSQL + pgvector 확장 사용 Python에서 psycopg2 + 임베딩 처리 # 코드&#xA;#1 SQL&#xA;CREATE EXTENSION IF NOT EXISTS vector; CREATE TABLE IF NOT EXISTS design ( id SERIAL PRIMARY KEY, description TEXT, embedding VECTOR(1536) -- OpenAI 임베딩 차원 ); # #2 python</description>
    </item>
    <item>
      <title>DBMS 및 SQL 활용 #2 트랜젝션 격리수준, pgaudit, AI 시스템 운영</title>
      <link>http://localhost:1313/docs/study/be/be36/</link>
      <pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be36/</guid>
      <description>DBMS 및 SQL 활용 #2 트랜젝션 격리수준, pgaudit, AI 시스템 운영 # #2025-08-27&#xA;1. 트랜젝션 격리수준 # 트랜젝션&#xA;데이터베이스에서 하나의 작업 단위. 여러 개의 쿼리나 연산이 묶여 하나로 실행되는데 그 결과는 전부 성공하거나 아니면 전부 실패해서 원래 상태로 되돌아가야 한다. 그렇지 않으면 데이터가 꼬인다. 문제는?&#xA;여러 사람이 동시에 같은 데이터베이스를 건드린다. 그래서 데이터가 뒤섞이지 않도록 격리 수준이라는 규칙을 둬야한다. 데이터가 뒤섞인다?&#xA;은행 계좌에서 A 트랜잭션이 “잔액 100만 원에서 10만 원 빼기” 작업을 하고 있고 동시에 B 트랜잭션이 “잔액 100만 원에서 20만 원 빼기” 작업을 한다고 하면 각각 따로 실행하면 당연히 최종 잔액은 70만 원이 되어야 한다.</description>
    </item>
    <item>
      <title>python #3 pgvector 유사 리뷰 검색</title>
      <link>http://localhost:1313/docs/study/be/be48/</link>
      <pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be48/</guid>
      <description>python #3 pgvector 유사 리뷰 검색 # #2025-08-20&#xA;1. 목적 # 고객 리뷰 문장을 벡터로 임베딩하고 PostgreSQL의 pgvector 기능을 활용하여 비슷한 리뷰를 검색하는 기능을 구현&#xA;# 2. 코드 # import torch import transformers import sentence_transformers import sklearn import numpy import scipy print(f&amp;#34;torch: {torch.__version__}&amp;#34;) print(f&amp;#34;transformers: {transformers.__version__}&amp;#34;) print(f&amp;#34;sentence-transformers: {sentence_transformers.__version__}&amp;#34;) print(f&amp;#34;scikit-learn: {sklearn.__version__}&amp;#34;) print(f&amp;#34;numpy: {numpy.__version__}&amp;#34;) print(f&amp;#34;scipy: {scipy.__version__}&amp;#34;) from dotenv import load_dotenv import os load_dotenv() # 같은 폴더에 있는 .env 로드 torch: 2.2.2 transformers: 4.</description>
    </item>
    <item>
      <title>LLM #2 LLM과 AI 기술요소를 활용하여 비즈니스 서비스 기획안 작성</title>
      <link>http://localhost:1313/docs/study/be/be8/</link>
      <pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be8/</guid>
      <description>LLM #2 LLM과 AI 기술요소를 활용하여 비즈니스 서비스 기획안 작성 # #2025-08-19&#xA;1. 목적 # 등기부등본/건축물대장 업로드 시 AI가 자동으로 문서를 분석하여 전세사기 위험 요소를 탐지하고 수치화한다. # 2. 모델 구성도 # #1 데이터 수집및 정규화&#xA;기술요소: PaddleOCR 선택 이유: 한국어 인식 정확도와 속도가 좋고, 오픈소스+온프레미스 운영 가능(비용·보안 유리), 표 레이아웃/좌표 추출 지원. 입력 파일: PDF/스캔 이미지(JPG/PNG) 매개변수: lang=&amp;ldquo;korean&amp;rdquo;, det+rec 사용, dpi(≥300) 출력 텍스트 블록: [{page, bbox, text}] 정규화 결과: 주소/금액/날짜/권리유형 표준화(JSON) #2 위험 특약/권리 분석</description>
    </item>
    <item>
      <title>LLM #1 LLM 이해와 Transformer</title>
      <link>http://localhost:1313/docs/study/be/be7/</link>
      <pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be7/</guid>
      <description>LLM #1 LLM 이해와 Transformer # #2025-08-11&#xA;1. LLM 기본이해 # #1 Word Embedding (p.27-28)&#xA;Word Embedding&#xA;핵심 아이디어는 단어가 어떤 맥락에서 자주 함께 등장하는지를 학습. “you say goodbye and I say hello”에서 ‘goodbye’주변에는 ‘you’, ‘say’, ‘and’, ‘I’ 같은 단어가 함께 등장하고 그 관계를 학습하도록 신경망을 훈련시킨다. 학습이 반복되면 각 단어는 벡터로 표현되고 의미가 비슷한 단어일수록 벡터 공간에서 가깝게 위치한다. Input이 ‘goodbye’이고 Target이 ‘you’, ‘say’, ‘and’, ‘I’여도 된다. Word Embedding - 신경망 구조 그림</description>
    </item>
    <item>
      <title>python #2 객체지향 프로그래밍, 병렬처리</title>
      <link>http://localhost:1313/docs/study/be/be47/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be47/</guid>
      <description>python #2 객체지향 프로그래밍, 병렬처리 # #2025-08-13&#xA;1. 객체지향 프로그래밍 # #1 property &amp;amp; dataclass (p.139-140)&#xA;@property&#xA;diameter 메서드는 사실 _radius * 2라는 계산을 수행하지만 외부에선 c.diameter라고 쓰면 바로 10이라는 결과를 얻을 수 있다. @diameter.setter를 사용하면 c.diameter = 20 형태로 diameter을 수정할수있고 내부에서는 diameter을 받아 _radius=10으로 변환 저장한다. fastapi에서 젤많이쓰는 기능이 속성화이다. @dataclass&#xA;보통 클래스를 만들면 __init__으로 생성자, __repr__으로 객체 출력 형식, __eq__로 동등성 비교 등을 직접 정의해야 하는데 @dataclass를 붙이면 이런 메서드들이 자동 생성된다.</description>
    </item>
    <item>
      <title>python #1 기본문법, 가상환경, 로깅</title>
      <link>http://localhost:1313/docs/study/be/be45/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be45/</guid>
      <description>python #1 기본문법, 가상환경, 로깅 # #2025-08-12&#xA;1. 기본문법 # #1 break와 continue의 차이 (p.29)&#xA;# break for i in range(10): if i==5: break print(i) # continue for i in range(5): if i==2: continue print(i) break 0부터 9까지 세는 반복문에서 i가 5가 되는 순간 break를 만나면 그 뒤의 숫자는 전혀 세지 않고 반복이 끝난다. continue 0부터 4까지 세는 반복문에서 i가 2인 경우 continue를 만나면 2를 출력하지 않고 바로 다음 숫자인 3으로 넘어가고 반복문 자체는 끝나지 않는다.</description>
    </item>
    <item>
      <title>python #2 리스트 vs 제너레이터 비교 실습</title>
      <link>http://localhost:1313/docs/study/be/be46/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be46/</guid>
      <description>python #2 리스트 vs 제너레이터 비교 실습 # #2025-08-12&#xA;1. 100만 개의 숫자 합 구하기 # 1) 리스트 방식&#xA;import sys # 1) 리스트 방식 numbers = list(range(1000000)) # 0부터 999,999 리스트 생성 list_sum = sum(numbers) # 합계 구하기 list_mem = sys.getsizeof(numbers) # 메모리 사용량 확인 (리스트 객체 크기) print(f&amp;#34;리스트 합: {list_sum:,}&amp;#34;) print(f&amp;#34;리스트 메모리 사용량: {list_mem} bytes&amp;#34;) 리스트 합: 499,999,500,000 리스트 메모리 사용량: 8000056 bytes numbers=list(range(1000000)) -&amp;gt; sum(numbers) 0~999,999를 리스트(numbers)로 만들어 합계를 구함 sys.</description>
    </item>
    <item>
      <title>Devops #1 Python 프로젝트 CI/CD &amp; 클라우드 빌드</title>
      <link>http://localhost:1313/docs/study/be/be10/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be10/</guid>
      <description>Devops #1 Python 프로젝트 CI/CD &amp;amp; 클라우드 빌드 # #2025-08-11&#xA;실습 # 메이크파일, 린팅, 테스트와 같이 파이썬 프로젝트 스캐폴딩에 필수적인 요소가 포함된 깃허브 저장소를 생성해보자. 그리고 간단하게 코드 포매팅을 수행하도록 메이크파일 스크립트를 작성해보자.&#xA;깃허브 액션을 사용하여 두개 이상의 파이썬 버전에 대해 깃허브 프로젝트 테스트를 수행해보자.&#xA;클라우드 네이티브 빌드 서버(AWS 코드빌드, GCP 클라우드 빌드, 애저 DevOps 파이프라인)를 사용하여 지속적 통합을 수행해보자.&#xA;깃허브 프로젝트를 도커 파일로 컨테이너화하고, 자동으로 컨테이너 레지스트리에 새로운 컨테이너가 등록되도록 만들어보자.</description>
    </item>
    <item>
      <title>Docker #3 레지스트리 접속, 이미지 관리</title>
      <link>http://localhost:1313/docs/study/be/be42/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be42/</guid>
      <description>Docker #3 # #2025-08-04&#xA;1. 레지스트리에 접속하고 이미지를 pull/push하기 # # Docker 로그인 $ docker login https://{실습링크}.com # ID: * # Password: * $ Login Succeeded # 이미지 Pull (이미지 내려받기): 예를 들어 container-linux:1.1 이미지를 다운로드 $ docker pull {실습링크}.com/{실습id}/container-linux:1.1 # 이미지 Push (Image Push 정보 사용): Push 권한은 일반 계정이 아니라 로봇 계정(CI/CD 용)을 사용합니다. # 로봇 계정 로그인 $ docker login https://{실습링크}.com # ID: robot$skala25a # Password: 1qB9cyusbNComZPHAdjNIFWinf52xaBJ # 태깅 (Tag local image) $ docker tag container-linux:1.</description>
    </item>
    <item>
      <title>Docker #4 자신의 Frontend 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자</title>
      <link>http://localhost:1313/docs/study/be/be43/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be43/</guid>
      <description>Docker #4 자신의 Frontend 개발 코드를 컨테이너로 만들고 이것을 실행시켜 보자 # #2025-08-04&#xA;#조건&#xA;nginx:alpine 이미지를 사용 노출 Port는80 nginx를실행하는방식은 -nginx -g daemon off; nginx의 routing 설정은 default.conf에설정한다. #path&#xA;$ pwd /Users/yshmbid/rde/config/workspace/exec-template $ ls Dockerfile default.conf deploy deploy.yaml docker-build.sh docker-push.sh service.yaml src # 1. docker-build.sh와 docker-push.sh 복사 # $ pwd /Users/yshmbid/rde/config/workspace/container/05.webserver $ ls Dockerfile default.conf deploy docker-build.sh docker-push.sh src # docker-build.sh #!/bin/bash NAME=sk019 IMAGE_NAME=&amp;#34;healthcheck-server&amp;#34; #IMAGE_NAME=&amp;#34;webserver&amp;#34; VERSION=&amp;#34;1.0.0&amp;#34; CPU_PLATFORM=arm64 #amd64 # Docker 이미지 빌드 docker build \ --tag ${NAME}-${IMAGE_NAME}:${VERSION} \ --file Dockerfile \ --platform linux/${CPU_PLATFORM} \ ${IS_CACHE} .</description>
    </item>
    <item>
      <title>Docker #5 kubernetes 환경에 나의 앱을 배포해보자</title>
      <link>http://localhost:1313/docs/study/be/be44/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be44/</guid>
      <description>Docker #5 kubernetes 환경에 나의 앱을 배포해보자 # #2025-08-04&#xA;#path&#xA;$ pwd /Users/yshmbid/rde/config/workspace/exec-template #파일 구조&#xA;/workspace └── exec-template ├── Dockerfile ├── default.conf ├── docker-build.sh ├── docker-push.sh ├── cicd.sh ├── deploy/ │ ├── deploy.t │ ├── deploy.sh │ ├── service.t │ ├── service.sh │ └── env.properties └── src/ ├── index.html └── media/ #이전 실습과의 차이?&#xA;cicd.sh를 쓴다. deploy 디렉토리를 쓴다. docker-build.sh와 docker-push.sh에서 amd였던걸 arm으로 바꿔줬는데 이걸다시 amd로 바꿔준다. # 1. cicd.</description>
    </item>
    <item>
      <title>Docker #1 Python 실행 컨테이너 만들기</title>
      <link>http://localhost:1313/docs/study/be/be40/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be40/</guid>
      <description>Docker #1 Python 실행 컨테이너 만들기 # #2025-08-01&#xA;Background # RDE #1 Local PC에서 RDE 환경 구성에서 Harbor registry로부터 RdE Container download를 수행했음 아이콘을 클릭해서 RDE 런처를 실행한다. # 1. 웹 서비스 실행 컨테이너 만들기 # /config/workspace/cloud/container/00.container-linux 경로로 이동 cd /config/workspace/cloud/container/00.container-linux 디렉토리 구조는? 00.container-linux/ ├── Dockerfile // 컨테이너 환경 설정 ├── Dockerfile.pytho-slim ├── Dockerfile.ubuntu ├── docker-build.sh ├── docker-push.sh ├── mycode.py ├── fastserver.py ├── webserver.py └── mydata/ Dockerfile 내용 확인하기 FROM python:3.</description>
    </item>
    <item>
      <title>Docker #2 작년 작업 복기: netmhcpan image 불러와서 패키지 돌리기</title>
      <link>http://localhost:1313/docs/study/be/be41/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be41/</guid>
      <description>Docker #2 작년 작업 복기: netmhcpan image 불러와서 패키지 돌리기 # #2025-08-01&#xA;1 # 2024.11.24 MutClust 작업중에 netmhcpan을 돌려야되는 상황이 왓었는데&#xA;netmhcpan이 유료였나 그래서 패키지 다운은 안되고 담당 박사님은 그만두셧고.. 서버 뒤지다가 위 README 파일 발견해서 결과물 저장까진 했던 기억이있다.&#xA;# 이때먼가 의문이 들었던게 새로운 conda 환경에 접속한거같은 느낌이 아니라 완전 다른 제2의서버에 접속한 느낌이었는데 이상하게 연구실 디렉토리들은 그대로 접근이 가능해서 혼란스럽지만 그냥 절대경로 다 박고 수행했는데 결과들이 문제없이 저장됐었다.</description>
    </item>
    <item>
      <title>SQL #6 AI 서비스 리뷰 시스템</title>
      <link>http://localhost:1313/docs/study/be/be39/</link>
      <pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be39/</guid>
      <description>SQL #6 AI 서비스 리뷰 시스템 # #2025-07-31&#xA;1. 문제 # AI 서비스 리뷰 시스템: 키워드 기반 텍스트 필터링과 AI 기반 방식의 비교를 통해 유사도 기반 검색에 대한 개념 이해&#xA;테이블 개요&#xA;Day 3 – ai_service_creator_ranking.sql 주제: AI 서비스 리뷰 (WITH (CTE) + 집계로 인기 기획자 추출) 목적: CTE(Common Table Expression)로 집계 테이블을 구성, AVG(평점)과 COUNT(리뷰)를 기준으로 인기 있는 기획자 선정, ROW_NUMBER()로 랭킹 부여, 향후 AI 추천(예: 유사도 기반 + 평점 기반 추천) 전단 필터링에 활용 실습 문제</description>
    </item>
    <item>
      <title>SQL #4 AI 피드백 분석 시스템의 테이블 정규화</title>
      <link>http://localhost:1313/docs/study/be/be22/</link>
      <pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be22/</guid>
      <description>SQL #4 AI 피드백 분석 시스템의 테이블 정규화 # #2025-07-30&#xA;1. 문제 # AI 피드백 분석 시스템의 테이블 정규화&#xA;시나리오&#xA;여러분은 AI 피드백 분석 시스템을 위한 데이터 모델링을 맡았습니다. 현재는 여러 실험 데이터를 한 테이블에 모아두었지만, 벡터 임베딩 처리, 학습데이터 전처리, RAG 문서 기반 검색 등을 고려해 정규화 설계가 필요합니다. [비정규 테이블 예시: Day 2 – 정규화와 제약조건_실습1_예제_ai_feedback_raw.csv] 실습 목표&#xA;LLM Feedback 데이터 정규화 (3NF까지 고려) model, user, prompt-response, tags 분리 tags 필드는:TEXT[ ] 배열로 유지한 구조 (빠른 전처리, FAISS 등 용이) feedback_tag라는 별도 테이블로 정규화 (통계, RAG 전처리 유리) AI 분석 목적의 전처리 성능 관점에서 두 방식 비교 설명 # 2.</description>
    </item>
    <item>
      <title>SQL #5 소셜미디어 포스트 리뷰 시스템</title>
      <link>http://localhost:1313/docs/study/be/be23/</link>
      <pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be23/</guid>
      <description>SQL #5 소셜미디어 포스트 리뷰 시스템 # #2025-07-30&#xA;1. 문제 # JSONB 기반의 메타정보 필드 설계 + 검색 + AI 분석 연계&#xA;테이블 개요&#xA;Day 2 – jsonb_metadata_sql_practice.sql 주제: 소셜미디어 포스트 리뷰 목적: 포스트에 대한 사용자 평가 + 해시태그/속성을 JSONB로 저장하여 AI 추천/필터 기반 만들기 실습 준비&#xA;특정 메타 속성 포함 검색(JSONB 검색 쿼리 실습) GIN 인덱스 생성 AI 필터링 활용 시나리오 (Hybrid Filtering 기반) 문제&#xA;sentiment가 negative인 리뷰만 출력 메타데이터에 &amp;ldquo;language&amp;rdquo; 키가 포함된 행 찾기 (?</description>
    </item>
    <item>
      <title>SQL #1 학사 관리 시스템 설계 - 엔터티 도출 및 ERD 작성</title>
      <link>http://localhost:1313/docs/study/be/be19/</link>
      <pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be19/</guid>
      <description>SQL #1 학사 관리 시스템 설계 - 엔터티 도출 및 ERD 작성 # #2025-07-29&#xA;1. 문제 # AI 기반 학사 관리 시스템 (Learning Management System) 설계를 위한 엔터티 도출 및 ERD 작성 실습입니다.&#xA;요구사항 . 교육과정, 수강생, 과정운영자, 강사, 과정 설명 텍스트, Review 등으로 구성 . 과정 설명 텍스트는 향후 AI 임베딩 대상이므로 충분한 길이와 자유 텍스트로 정의&#xA;순서 . 학사관리시스템 엔티티 도출 및 검증 . ERD 변환 작업 .</description>
    </item>
    <item>
      <title>SQL #2 학사 관리 시스템 설계 - 스키마 분리 및 멀티 프로젝트 설계</title>
      <link>http://localhost:1313/docs/study/be/be20/</link>
      <pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be20/</guid>
      <description>SQL #2 학사 관리 시스템 설계 - 스키마 분리 및 멀티 프로젝트 설계 # #2025-07-29&#xA;1. 문제 # 이전에 만든 ERD를 기반으로 PostgreSQL 로 스키마 분리 및 멀티 프로젝트 설계합니다.&#xA;주제 . 서울캠퍼스/제주캠퍼스별 학사 관리 시스템 (Learning Management System) 동일한 학사관리 시스템 구조를 기반으로, 캠퍼스에 따라 데이터를 스키마 단위로 분리 설계하고 향후 AI 분석 결과의 멀티 벡터 저장 구조로 확장 가능하도록 구조 설계 요구사항 . 교육과정, 수강생 과정운영자, 강사, 과정 설명 텍스트, Review 등으로 구성하되, 캠퍼스별 특성을 고려하여 스키마 분리 .</description>
    </item>
    <item>
      <title>SQL #3 스키마 분리와 AI 분석</title>
      <link>http://localhost:1313/docs/study/be/be21/</link>
      <pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be21/</guid>
      <description>SQL #3 스키마 분리와 AI 분석 # #2025-07-29&#xA;생각 정리&#xA;AI 분석이 들어갈 때 왜 별도 스키마로 나누는 것이 유리할까요? 스키마 vs. 테이블 분리, 어떤 방식이 어떤 상황에 적합할까요? 향후 pgvector 또는 AI 모델 결과를 넣기 위해 어떻게 테이블을 확장할 수 있을까요? # AI 분석이 들어갈 때 왜 별도 스키마로 나누는 것이 유리할까요? AI 분석이 포함된 시스템에서 데이터를 다룰 때, 별도 스키마로 나누는 것이 유리한 이유는 (1) 데이터의 사용 목적이 다르기 때문이고, (2) 데이터의 구조와 속성이 근본적으로 다르기 때문입니다.</description>
    </item>
    <item>
      <title>Linux #1 NPM 과 PIP 명령어 목록</title>
      <link>http://localhost:1313/docs/study/be/be1/</link>
      <pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be1/</guid>
      <description>Linux #1 NPM 과 PIP 명령어 목록 # #2025-07-22&#xA;1. NPM (Node Package Manager) # 패키지 설치&#xA;npm install &amp;lt;패키지명&amp;gt; - 패키지 설치 npm install -g &amp;lt;패키지명&amp;gt; - 전역 설치 npm install --save-dev &amp;lt;패키지명&amp;gt; - 개발 의존성으로 설치 npm install - package.json의 모든 의존성 설치 패키지 관리&#xA;npm uninstall &amp;lt;패키지명&amp;gt; - 패키지 제거 npm update &amp;lt;패키지명&amp;gt; - 패키지 업데이트 npm list - 설치된 패키지 목록 보기 npm list -g - 전역 설치된 패키지 목록 프로젝트 관리</description>
    </item>
    <item>
      <title>RDE #1 Local PC에서 RDE 환경 구성</title>
      <link>http://localhost:1313/docs/study/be/be2/</link>
      <pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be2/</guid>
      <description>RDE #1 Local PC에서 RDE 환경 구성 # #2025-07-22&#xA;1 # Docker Desktop 설치 링크 - https://www.docker.com/products/docker-desktop/&#xA;RdE Container download Harbor registry로부터 이미지 다운로드 (*에 이미지 경로)&#xA;docker pull * 다운로드 확인하면?&#xA;잘들어가있다!&#xA;# 2 # Local RDE 설치하기 https://mattermost..com 접속해서 다운로드. (: 링크 블라인드처리)&#xA;실행 아이콘 클릭해서 실행&#xA;============================================= RDE Launcher 시작 중... ============================================= 시작 시간: 2025-07-22 16:55:56 작업 디렉토리: /Users/yshmbid/rde 실행 파일: rde-launcher-macos-arm64 로그 파일: /Users/yshmbid/rde/rde-launcher.log 작업 디렉토리로 이동했습니다.</description>
    </item>
    <item>
      <title>개발환경 설정 (GIT, Docker, VScode, RDE 컨테이너)</title>
      <link>http://localhost:1313/docs/study/be/be4/</link>
      <pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/study/be/be4/</guid>
      <description>개발환경 설정 (GIT, Docker, VScode, RDE 컨테이너) # #2025-07-21&#xA;1. GIT 사용자 정보 설정 # [Git 설치 확인] git --version [사용자 이름 설정] git config --global user.name &amp;#34;윤소현&amp;#34; [이메일 주소 설정] GitHub에 등록된 이메일 주소와 일치하는지 확인 필요 git config --global user.email &amp;#34;yshggid@gmail.com&amp;#34; [설정 확인] git config --global --list 2. 로컬 GIT Repository 생성 # vscode에서 좌측 SOURCE CONTRIL 아이콘 &amp;gt; Initialize Repository &amp;gt; 로컬 폴더를 git repository로 생성</description>
    </item>
  </channel>
</rss>
